[
  {
    "id": 1,
    "topic": "1",
    "question_en": "A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple continents. The average volume of data that the company collects from each site daily is 500 GB. Each site has a high-speed Internet connection. The company wants to aggregate the data from all these global sites as quickly as possible in a single Amazon S3 bucket. The solution must minimize operational complexity. Which solution meets these requirements?",
    "options_en": {
      "A": "Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly upload site data to the destination S3 bucket.",
      "B": "Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 bucket.",
      "C": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to the closest Region. Use S3 Cross- Region Replication to copy objects to the destination S3 bucket.",
      "D": "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot and copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that Region."
    },
    "correct_answer": "A",
    "vote_percentage": "98%",
    "question_cn": "一家公司收集了全球多个大陆城市的气温、湿度和大气压力数据。该公司每天从每个站点收集的平均数据量为 500 GB。每个站点都有高速互联网连接。该公司希望尽快将来自所有这些全球站点的数据聚合到单个 Amazon S3 存储桶中。解决方案必须最大限度地减少运营复杂性。哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "在目标 S3 存储桶上打开 S3 Transfer Acceleration。使用 multipart uploads 将站点数据直接上传到目标 S3 存储桶。",
      "B": "将每个站点的数据上传到最近区域的 S3 存储桶。使用 S3 跨区域复制将对象复制到目标 S3 存储桶。然后从源 S3 存储桶中删除数据。",
      "C": "每天安排 AWS Snowball Edge 存储优化设备作业，将每个站点的数据传输到最近的区域。使用 S3 跨区域复制将对象复制到目标 S3 存储桶。",
      "D": "将每个站点的数据上传到最近区域的 Amazon EC2 实例。将数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。定期间隔拍摄 EBS 快照并将其复制到包含目标 S3 存储桶的区域。在该区域恢复 EBS 卷。"
    },
    "tags": [
      "Amazon S3",
      "S3 Transfer Acceleration",
      "multipart uploads",
      "S3 Cross-Region Replication",
      "AWS Snowball Edge",
      "Amazon EC2",
      "Amazon EBS",
      "EBS Snapshots"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 98%），解析仅供参考。】\n\n考查利用 S3 Transfer Acceleration 加速全球数据上传到 S3 的场景，并考察了与其他数据传输方案如 S3 跨区域复制、AWS Snowball Edge、EC2 和 EBS 的对比。",
      "why_correct": "S3 Transfer Acceleration 旨在通过利用 CloudFront 的全球边缘站点来加速数据传输，尤其适用于跨洲际的数据传输。它通过优化 TCP 连接和使用 Amazon 的全球网络来提高上传速度。结合 multipart uploads，能够提高大文件的上传效率和可靠性。对于全球数据聚合到单个 S3 存储桶的需求，该方案简单直接，运营复杂性最低。",
      "why_wrong": "选项 B 引入了额外的复杂性。虽然 S3 跨区域复制可以用于数据迁移，但它需要先将数据上传到中间的 S3 桶，然后通过复制到目标桶，增加了操作步骤。选项 C 使用了 AWS Snowball Edge 设备，Snowball Edge 适用于大规模离线数据传输，但对于每天 500GB 的数据量，并有高速互联网连接的情况下，其优势并不明显，反而增加了额外的设备管理成本和调度复杂性。选项 D 则涉及了 EC2 和 EBS，将数据存储在 EBS 卷中，再定期创建快照并复制到另一个区域，这是一种更为复杂的数据备份和恢复方案，而非直接的数据上传。同时，EBS 快照复制和 EBS 卷恢复，其运营成本和复杂性远高于直接使用 S3 Transfer Acceleration 和 multipart uploads。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Transfer Acceleration",
      "AWS Snowball Edge",
      "Amazon EC2",
      "Amazon EBS",
      "CloudFront",
      "TCP",
      "multipart uploads",
      "S3 Cross-Region Replication",
      "EBS Snapshots"
    ]
  },
  {
    "id": 2,
    "topic": "1",
    "question_en": "A company needs the ability to analyze the log files of its proprietary application. The logs are stored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A solutions architect needs to perform the analysis with minimal changes to the existing architecture. What should the solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.",
      "B": "Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon CloudWatch console.",
      "C": "Use Amazon Athena directly with Amazon S3 to run the queries as needed.",
      "D": "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run the SQL queries as needed."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要分析其专有应用程序的日志文件。 日志以 JSON 格式存储在 Amazon S3 存储桶中。 查询将很简单，并将按需运行。 解决方案架构师需要以最少的现有架构更改来执行分析。 解决方案架构师应该怎么做才能以最少的运营开销来满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Redshift 将所有内容加载到一个位置，并根据需要运行 SQL 查询。",
      "B": "使用 Amazon CloudWatch Logs 存储日志。 从 Amazon CloudWatch 控制台根据需要运行 SQL 查询。",
      "C": "直接将 Amazon Athena 与 Amazon S3 结合使用，以根据需要运行查询。",
      "D": "使用 AWS Glue 编目日志。 使用 Amazon EMR 上的瞬时 Apache Spark 集群，以根据需要运行 SQL 查询。"
    },
    "tags": [
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "Amazon EMR",
      "CloudWatch Logs",
      "Amazon Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在 S3 上存储的 JSON 格式日志文件的查询分析，以及如何以最小的架构变更和运营开销满足需求。涉及 Athena、Redshift、CloudWatch Logs、EMR 等服务的选型与对比。",
      "why_correct": "Amazon Athena 是一种交互式查询服务，允许使用 SQL 查询存储在 Amazon S3 中的数据。它无需加载数据或管理基础设施，按需查询，非常适合一次性或临时查询。Athena 能够直接读取 S3 中的 JSON 格式日志，并提供低运营开销的解决方案，完全满足题目需求。",
      "why_wrong": "选项 A 错误，Amazon Redshift 是一种数据仓库服务，需要将数据加载到 Redshift 集群中才能进行查询。这增加了架构的复杂性，并引入了 ETL（Extract, Transform, Load）流程和额外的数据管理开销，与“最少现有架构更改”的条件不符。 选项 B 错误，CloudWatch Logs 主要用于监控和日志分析，虽然可以查询日志，但其设计目的并非针对大规模日志分析和简单的按需查询。将日志复制到 CloudWatch Logs 也增加了额外的数据复制和存储成本。 选项 D 错误，虽然 AWS Glue 可以编目 S3 中的数据，且 Amazon EMR 上的瞬时 Apache Spark 集群可以用于查询，但相比 Athena，这种方案增加了复杂性，需要管理 EMR 集群、Spark 作业的配置与维护，不符合“最少的运营开销”的条件。此外，EMR 解决方案也带来了额外的成本，并且启动集群的时间也会增加延迟。另外，该方案也增加了一定架构变更成本。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon Athena",
      "Amazon Redshift",
      "CloudWatch Logs",
      "AWS Glue",
      "Amazon EMR",
      "Apache Spark",
      "SQL",
      "JSON",
      "ETL"
    ]
  },
  {
    "id": 3,
    "topic": "1",
    "question_en": "A company uses AWS Organizations to manage multiple AWS accounts for different departments. The management account has an Amazon S3 bucket that contains project reports. The company wants to limit access to this S3 bucket to only users of accounts within the organization in AWS Organizations. Which solution meets these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 bucket policy.",
      "B": "Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global condition key to the S3 bucket policy.",
      "C": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, and RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.",
      "D": "Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to the S3 bucket policy."
    },
    "correct_answer": "A",
    "vote_percentage": "97%",
    "question_cn": "一家公司使用 AWS Organizations 为不同部门管理多个 AWS 账户。管理账户有一个 Amazon S3 存储桶，其中包含项目报告。该公司希望将对该 S3 存储桶的访问权限限制为 AWS Organizations 中组织内的账户用户。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将 `aws:PrincipalOrgID` 全局条件键与对组织 ID 的引用添加到 S3 存储桶策略中。",
      "B": "为每个部门创建一个组织单元 (OU)。将 `aws:PrincipalOrgPaths` 全局条件键添加到 S3 存储桶策略中。",
      "C": "使用 AWS CloudTrail 监控 `CreateAccount`、`InviteAccountToOrganization`、`LeaveOrganization` 和 `RemoveAccountFromOrganization` 事件。相应地更新 S3 存储桶策略。",
      "D": "标记每个需要访问 S3 存储桶的用户。将 `aws:PrincipalTag` 全局条件键添加到 S3 存储桶策略中。"
    },
    "tags": [
      "Amazon S3",
      "IAM",
      "S3 Bucket Policy",
      "AWS Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 97%），解析仅供参考。】\n\n考查如何使用 S3 Bucket Policy 和 IAM condition keys 来控制对 S3 存储桶的访问，特别是针对 AWS Organizations 的账户用户。涉及 `aws:PrincipalOrgID` 和 `aws:PrincipalOrgPaths` 等 condition keys 的使用。与访问控制、最小权限原则相关。",
      "why_correct": "选项 A 正确。通过在 S3 Bucket Policy 中使用 `aws:PrincipalOrgID` 条件键，可以根据用户的组织 ID 来限制对存储桶的访问。这种方法允许组织内的所有账户访问存储桶，满足了题目需求。它提供了一种集中管理、最小化运营开销的方式。",
      "why_wrong": "选项 B 错误。`aws:PrincipalOrgPaths` 允许根据组织路径（OU）来限制访问，虽然可以实现访问控制，但创建 OU 并管理其路径会增加运营开销。选项 C 错误。使用 CloudTrail 监控账户变化并手动更新 S3 策略既复杂又容易出错，不符合最小运营开销的要求，并且反应不够及时。选项 D 错误。使用标签需要手动为每个用户进行配置，增加了管理复杂度，且不便于组织级别的访问控制，同样不符合题目中“最少运营开销”的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Organizations",
      "IAM",
      "CloudTrail",
      "OU",
      "S3 Bucket Policy",
      "aws:PrincipalOrgID",
      "aws:PrincipalOrgPaths",
      "CreateAccount",
      "InviteAccountToOrganization",
      "LeaveOrganization",
      "RemoveAccountFromOrganization",
      "aws:PrincipalTag"
    ]
  },
  {
    "id": 4,
    "topic": "1",
    "question_en": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are stored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without connectivity to the internet. Which solution will provide private network connectivity to Amazon S3?",
    "options_en": {
      "A": "Create a gateway VPC endpoint to the S3 bucket.",
      "B": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.",
      "C": "Create an instance profile on Amazon EC2 to allow S3 access.",
      "D": "Create an Amazon API Gateway API with a private link to access the S3 endpoint."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一个应用程序在 VPC 中的 Amazon EC2 实例上运行。该应用程序处理存储在 Amazon S3 存储桶中的日志。EC2 实例需要在没有互联网连接的情况下访问 S3 存储桶。哪种解决方案将提供到 Amazon S3 的私有网络连接？",
    "options_cn": {
      "A": "创建到 S3 存储桶的网关 VPC endpoint。",
      "B": "将日志流式传输到 Amazon CloudWatch Logs。将日志导出到 S3 存储桶。",
      "C": "在 Amazon EC2 上创建实例配置文件以允许访问 S3。",
      "D": "创建一个 Amazon API Gateway API，该 API 具有一个私有链接来访问 S3 endpoint。"
    },
    "tags": [
      "Amazon S3",
      "VPC Endpoint",
      "PrivateLink",
      "EC2",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在 VPC 内，EC2 实例访问 S3 的私有连接方案。与 VPC endpoint、Amazon PrivateLink 等相关。需要在没有 Internet 连接的情况下，实现私有访问。",
      "why_correct": "网关 VPC endpoint 允许 EC2 实例通过 VPC 内的私有连接访问 Amazon S3，而无需经过 Internet。网关 endpoint 是一种 VPC endpoint，通过 VPC 中的路由表将流量导向 S3。由于 EC2 实例位于 VPC 内且没有 Internet 连接，因此此方案满足题目的所有要求。",
      "why_wrong": "选项 B 方案首先将日志发送到 CloudWatch Logs，然后再导出到 S3。这增加了复杂性，并且与直接访问 S3 相比，会引入额外的延迟。选项 C 方案关注的是 EC2 实例的 IAM 权限，仅授权实例访问 S3，并没有提供私有网络连接，仍然需要 Internet 连接才能访问 S3。选项 D 方案使用了 API Gateway 以及 PrivateLink，虽然也可以创建私有连接，但针对访问 S3 存储桶来说，方案过于复杂，且引入了额外的成本，而 VPC endpoint 是更直接、更经济的选择。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "VPC",
      "EC2",
      "Internet",
      "IAM",
      "API Gateway",
      "PrivateLink",
      "Amazon CloudWatch Logs",
      "VPC Endpoint"
    ]
  },
  {
    "id": 5,
    "topic": "1",
    "question_en": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores user-uploaded documents in an Amazon EBS volume. For better scalability and availability, the company duplicated the architecture and created a second EC2 instance and EBS volume in another Availability Zone, placing both behind an Application Load Balancer. After completing this change, users reported that, each time they refreshed the website, they could see one subset of their documents or the other, but never all of the documents at the same time. What should a solutions architect propose to ensure users see all of their documents at once?",
    "options_en": {
      "A": "Copy the data so both EBS volumes contain all the documents",
      "B": "Configure the Application Load Balancer to direct a user to the server with the documents",
      "C": "Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new documents to Amazon EFS",
      "D": "Configure the Application Load Balancer to send the request to both servers. Return each document from the correct server"
    },
    "correct_answer": "C",
    "vote_percentage": "98%",
    "question_cn": "一家公司使用单个 Amazon EC2 实例在 AWS 上托管一个 Web 应用程序，该实例将用户上传的文档存储在 Amazon EBS 卷中。为了提高可扩展性和可用性，该公司复制了架构，并在另一个可用区中创建了第二个 EC2 实例和 EBS 卷，并将两者置于一个 Application Load Balancer 之后。完成此更改后，用户报告说，每次刷新网站时，他们只能看到文档的一个子集，但永远无法同时看到所有文档。解决方案架构师应该提出什么建议来确保用户一次看到他们的所有文档？",
    "options_cn": {
      "A": "复制数据，使两个 EBS 卷都包含所有文档",
      "B": "配置 Application Load Balancer，将用户定向到包含文档的服务器",
      "C": "将数据从两个 EBS 卷复制到 Amazon EFS。修改应用程序以将新文档保存到 Amazon EFS",
      "D": "配置 Application Load Balancer 将请求发送到两台服务器。从正确的服务器返回每个文档"
    },
    "tags": [
      "Amazon EC2",
      "EBS",
      "Application Load Balancer",
      "Amazon EFS",
      "High Availability",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 98%），解析仅供参考。】\n\n本题考查了构建高可用性和可扩展性 Web 应用程序时，共享存储方案的选择。与 EBS、EFS 以及 ALB 的配置相关。",
      "why_correct": "将数据从两个 EBS 卷复制到 Amazon EFS 是最佳方案。Amazon EFS 提供了可扩展的、共享的文件存储，可以被多个 EC2 实例同时访问。修改应用程序以将新文档保存到 Amazon EFS，确保了所有 EC2 实例都能访问到相同的文档，从而解决用户只能看到文档子集的问题。EFS 方案适用于这种需要共享文件存储的场景，并且易于扩展和维护。",
      "why_wrong": "选项 A 错误，复制数据到 EBS 卷并不能解决根本问题。EBS 卷是单可用区级别的存储，无法在多可用区之间自动复制，这违背了高可用性的设计初衷，还需要手动同步，维护复杂，且会造成数据不一致的问题。选项 B 错误，ALB 无法知道哪些 EC2 实例上存储了哪些文件，因此无法根据文件位置来定向请求，而且无法解决已经上传的数据不一致的问题。选项 D 错误，ALB 无法判断哪台 EC2 实例拥有所有文档，因此无法确保用户能够看到所有文档。这种方案同样无法解决数据同步的问题，而且会加剧数据不一致的可能性。"
    },
    "related_terms": [
      "Amazon EC2",
      "EBS",
      "Application Load Balancer",
      "Amazon EFS"
    ]
  },
  {
    "id": 6,
    "topic": "1",
    "question_en": "A company uses NFS to store large video files in on-premises network attached storage. Each video file ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The company decides to migrate the video files to Amazon S3. The company must migrate the video files as soon as possible while using the least possible network bandwidth. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the AWS CLI to copy all files locally to the S3 bucket.",
      "B": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the Snowball Edge client to transfer data to the device. Return the device so that AWS can import the data into Amazon S3.",
      "C": "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway.",
      "D": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an S3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File Gateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the new file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File Gateway."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家公司使用 NFS 在本地网络附加存储中存储大型视频文件。每个视频文件的大小范围为 1 MB 到 500 GB。总存储量为 70 TB，并且不再增长。该公司决定将视频文件迁移到 Amazon S3。该公司必须尽快迁移视频文件，同时使用最少的网络带宽。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建 S3 存储桶。创建一个具有写入 S3 存储桶权限的 IAM 角色。使用 AWS CLI 将所有文件本地复制到 S3 存储桶。",
      "B": "创建一个 AWS Snowball Edge 作业。在本地接收 Snowball Edge 设备。使用 Snowball Edge 客户端将数据传输到该设备。返回该设备，以便 AWS 可以将数据导入 Amazon S3。",
      "C": "在本地部署一个 S3 文件网关。创建公共服务终端节点以连接到 S3 文件网关。创建一个 S3 存储桶。在 S3 文件网关上创建一个新的 NFS 文件共享。将新的文件共享指向 S3 存储桶。将数据从现有的 NFS 文件共享传输到 S3 文件网关。",
      "D": "在本地网络和 AWS 之间设置 AWS Direct Connect 连接。在本地部署一个 S3 文件网关。创建公共虚拟接口 (VIF) 以连接到 S3 文件网关。创建一个 S3 存储桶。在 S3 文件网关上创建一个新的 NFS 文件共享。将新的文件共享指向 S3 存储桶。将数据从现有的 NFS 文件共享传输到 S3 文件网关。"
    },
    "tags": [
      "Amazon S3",
      "AWS File Gateway",
      "NFS",
      "Data Migration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n考查大规模数据迁移到 S3 的方案选择，侧重考虑迁移速度、网络带宽消耗和实际操作便利性。",
      "why_correct": "Snowball Edge 专为大规模数据迁移设计，适合本地数据量大、迁移时间要求紧迫、且网络带宽受限的场景。该方案通过物理设备运输数据，最大程度减少了网络带宽的消耗，并能快速完成数据迁移。Snowball Edge 客户端简化了数据传输流程，方便用户操作。",
      "why_wrong": "A 选项使用 AWS CLI 通过网络复制数据，对于 70 TB 的数据量来说，迁移时间会非常长，且会消耗大量的网络带宽，不符合尽快迁移的要求。C 选项使用 S3 File Gateway，旨在提供对 S3 的本地文件共享访问，而非高效迁移大量数据。D 选项在 C 选项基础上增加了 Direct Connect，虽然减少了网络延迟，但依然需要通过网络传输数据，效率不如 Snowball Edge，且部署更为复杂。"
    },
    "related_terms": [
      "NFS",
      "Amazon S3",
      "AWS CLI",
      "IAM",
      "AWS Snowball Edge",
      "S3 File Gateway",
      "AWS Direct Connect",
      "S3",
      "VIF"
    ]
  },
  {
    "id": 7,
    "topic": "1",
    "question_en": "A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?",
    "options_en": {
      "A": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
      "B": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
      "C": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
      "D": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues."
    },
    "correct_answer": "D",
    "vote_percentage": "83%",
    "question_cn": "一家公司有一个摄取传入消息的应用程序。 随后，数十个其他应用程序和微服务会快速使用这些消息。消息数量变化很大，有时会突然增加到每秒 100,000 条。该公司希望解耦该解决方案并提高可扩展性。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "将消息持久化到 Amazon Kinesis Data Analytics。配置消费者应用程序来读取和处理这些消息。",
      "B": "在 Auto Scaling 组中的 Amazon EC2 实例上部署摄取应用程序，以根据 CPU 指标扩展 EC2 实例的数量。",
      "C": "将消息写入具有单个分片的 Amazon Kinesis Data Streams。 使用 AWS Lambda 函数预处理消息并将其存储在 Amazon DynamoDB 中。 配置消费者应用程序从 DynamoDB 读取以处理消息。",
      "D": "将消息发布到 Amazon Simple Notification Service (Amazon SNS) 主题，并带有多个 Amazon Simple Queue Service (Amazon SQS) 订阅。 配置消费者应用程序从队列中处理消息。"
    },
    "tags": [
      "Amazon Kinesis",
      "Kinesis Data Analytics",
      "Scalability",
      "Decoupling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 83%），解析仅供参考。】\n\n考查解耦应用、提高可扩展性的消息队列方案。需要考虑高吞吐量、消息数量突增等场景。",
      "why_correct": "Amazon SNS 和 Amazon SQS 结合是典型的解耦和高可扩展性方案。 SNS 负责消息发布，支持扇出，将消息传递给多个 SQS 队列。 SQS 提供了消息的持久化和异步处理，消费者应用程序可以从队列中异步读取消息，应对消息数量的突增。",
      "why_wrong": "选项 A，Amazon Kinesis Data Analytics 侧重于流数据的实时分析，而非解耦消息传递。 选项 B，Auto Scaling 和 EC2 实例用于扩展计算资源，但未解决消息解耦问题，并且难以应对消息数量的快速增长。 选项 C，虽然 Kinesis Data Streams 也可以用于处理流数据，但使用 Lambda 函数预处理并写入 DynamoDB，增加了复杂性，且从 DynamoDB 读取消息效率较低，不适合高吞吐量场景。"
    },
    "related_terms": [
      "Amazon SNS",
      "Amazon SQS",
      "Amazon Kinesis Data Analytics",
      "Auto Scaling",
      "Amazon EC2",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Kinesis Data Streams"
    ]
  },
  {
    "id": 8,
    "topic": "1",
    "question_en": "A company is migrating a distributed application to AWS. The application serves variable workloads. The legacy platform consists of a primary server that coordinates jobs across multiple compute nodes. The company wants to modernize the application with a solution that maximizes resiliency and scalability. How should a solutions architect design the architecture to meet these requirements?",
    "options_en": {
      "A": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling to use scheduled scaling.",
      "B": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. Implement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure EC2 Auto Scaling based on the size of the queue.",
      "C": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the primary server.",
      "D": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a destination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司正在将分布式应用程序迁移到 AWS。该应用程序处理可变的工作负载。传统平台由一个主服务器组成，该服务器协调多个计算节点上的作业。该公司希望使用能够最大程度提高弹性和可扩展性的解决方案来现代化应用程序。解决方案架构师应如何设计架构以满足这些要求？",
    "options_cn": {
      "A": "将 Amazon Simple Queue Service (Amazon SQS) 队列配置为作业的目的地。使用 Amazon EC2 实例实现计算节点，这些实例在 Auto Scaling 组中进行管理。配置 EC2 Auto Scaling 以使用计划扩展。",
      "B": "将 Amazon Simple Queue Service (Amazon SQS) 队列配置为作业的目的地。使用 Amazon EC2 实例实现计算节点，这些实例在 Auto Scaling 组中进行管理。根据队列的大小配置 EC2 Auto Scaling。",
      "C": "使用 Amazon EC2 实例实现主服务器和计算节点，这些实例在 Auto Scaling 组中进行管理。将 AWS CloudTrail 配置为作业的目的地。根据主服务器上的负载配置 EC2 Auto Scaling。",
      "D": "使用 Amazon EC2 实例实现主服务器和计算节点，这些实例在 Auto Scaling 组中进行管理。将 Amazon EventBridge (Amazon CloudWatch Events) 配置为作业的目的地。根据计算节点上的负载配置 EC2 Auto Scaling。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "AWS CloudTrail",
      "Elasticity",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查如何使用 Amazon SQS 和 EC2 Auto Scaling 构建弹性且可扩展的分布式应用程序。",
      "why_correct": "选项 B 提供了最佳的解决方案。使用 Amazon SQS 队列作为作业目的地，允许异步处理，增强了弹性。EC2 Auto Scaling 组根据队列大小自动调整 EC2 实例数量，确保资源与工作负载相匹配，从而实现可扩展性。",
      "why_wrong": "选项 A 使用计划扩展，无法动态响应可变工作负载的变化，弹性较差。选项 C 使用 EC2 实例实现主服务器，存在单点故障风险，且 CloudTrail 并非作业目的地，不适合此类应用。选项 D 使用 EventBridge 作为作业目的地，这不符合题意中应用程序处理作业的要求，且 EC2 Auto Scaling 根据计算节点的负载配置，而不是全局的作业负载，无法有效扩展。"
    },
    "related_terms": [
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon EC2",
      "Auto Scaling",
      "AWS CloudTrail",
      "Amazon EventBridge (Amazon CloudWatch Events)"
    ]
  },
  {
    "id": 9,
    "topic": "1",
    "question_en": "A company is running an SMB file server in its data center. The file server stores large files that are accessed frequently for the first few days after the files are created. After 7 days the files are rarely accessed. The total data size is increasing and is close to the company's total storage capacity. A solutions architect must increase the company's available storage space without losing low-latency access to the most recently accessed files. The solutions architect must also provide file lifecycle management to avoid future storage issues. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
      "B": "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.",
      "C": "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.",
      "D": "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days."
    },
    "correct_answer": "B",
    "vote_percentage": "83%",
    "question_cn": "一家公司在其数据中心运行 SMB 文件服务器。该文件服务器存储大文件，这些文件在创建后的头几天会被频繁访问。7 天后，文件很少被访问。总数据大小正在增加，并且接近公司的总存储容量。一位解决方案架构师必须增加公司可用的存储空间，同时又不失去对最近访问文件的低延迟访问。解决方案架构师还必须提供文件生命周期管理，以避免未来的存储问题。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 将超过 7 天的数据从 SMB 文件服务器复制到 AWS。",
      "B": "创建一个 Amazon S3 File Gateway 来扩展公司的存储空间。创建 S3 生命周期策略，在 7 天后将数据转换为 S3 Glacier Deep Archive。",
      "C": "创建一个 Amazon FSx for Windows File Server 文件系统来扩展公司的存储空间。",
      "D": "在每个用户的计算机上安装一个实用程序以访问 Amazon S3。创建 S3 生命周期策略，在 7 天后将数据转换为 S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "Amazon S3",
      "S3 Lifecycle",
      "S3 Glacier",
      "Amazon FSx",
      "AWS DataSync",
      "S3 File Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 83%），解析仅供参考。】\n\n考查如何通过 AWS 服务扩展存储空间、实现文件生命周期管理，以及满足低延迟访问需求。",
      "why_correct": "Amazon S3 File Gateway 提供了本地缓存，用于低延迟访问最近访问的文件。将数据存储在 Amazon S3 中可以扩展存储容量。结合 S3 生命周期策略，可以在 7 天后将不常访问的文件转换为 S3 Glacier Deep Archive，实现成本优化和文件生命周期管理。",
      "why_wrong": "A 选项仅将数据复制到 AWS，没有本地缓存功能，无法满足低延迟访问需求，并且没有说明如何优化成本。C 选项使用 FSx for Windows File Server 虽然提供了存储空间，但无法实现自动的生命周期管理和成本优化。D 选项要求用户安装实用程序，增加了复杂性，并且没有提供本地缓存机制，同时 S3 Glacier Flexible Retrieval 的访问成本高于 Deep Archive，也并非最佳选择。"
    },
    "related_terms": [
      "SMB",
      "AWS DataSync",
      "Amazon S3",
      "Amazon S3 File Gateway",
      "S3 Glacier Deep Archive",
      "Amazon FSx for Windows File Server",
      "S3 Glacier Flexible Retrieval"
    ]
  },
  {
    "id": 10,
    "topic": "1",
    "question_en": "A company is building an ecommerce web application on AWS. The application sends information about new orders to an Amazon API Gateway REST API to process. The company wants to ensure that orders are processed in the order that they are received. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to the topic to perform processing.",
      "B": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke an AWS Lambda function for processing.",
      "C": "Use an API Gateway authorizer to block any requests while the application processes an order.",
      "D": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon SQS) standard queue when the application receives an order. Configure the SQS standard queue to invoke an AWS Lambda function for processing."
    },
    "correct_answer": "B",
    "vote_percentage": "98%",
    "question_cn": "一家公司正在 AWS 上构建一个电子商务 Web 应用程序。该应用程序将有关新订单的信息发送到 Amazon API Gateway REST API 进行处理。该公司希望确保订单按照接收顺序进行处理。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 API Gateway 集成将消息发布到 Amazon Simple Notification Service (Amazon SNS) 主题，当应用程序收到订单时。将 AWS Lambda 函数订阅到该主题以执行处理。",
      "B": "使用 API Gateway 集成将消息发送到 Amazon Simple Queue Service (Amazon SQS) FIFO 队列，当应用程序收到订单时。配置 SQS FIFO 队列以调用 AWS Lambda 函数进行处理。",
      "C": "使用 API Gateway 授权程序阻止任何请求，同时应用程序处理订单。",
      "D": "使用 API Gateway 集成将消息发送到 Amazon Simple Queue Service (Amazon SQS) 标准队列，当应用程序收到订单时。配置 SQS 标准队列以调用 AWS Lambda 函数进行处理。"
    },
    "tags": [
      "Amazon API Gateway",
      "Amazon SNS",
      "AWS Lambda",
      "Message Ordering"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 98%），解析仅供参考。】\n\n考查如何确保 API Gateway 接收的订单消息按照接收顺序处理，同时利用 AWS 服务进行异步处理。",
      "why_correct": "选项 B 使用 SQS FIFO 队列，FIFO 队列保证消息的顺序，满足订单处理需要顺序的要求。API Gateway 将消息发送到 FIFO 队列，Lambda 函数从 FIFO 队列中获取并处理消息，实现了异步处理，提高了系统的可靠性。SQS 和 Lambda 的结合是处理异步任务的常用模式。",
      "why_wrong": "选项 A 使用 SNS。SNS 用于消息的扇出，无法保证消息的顺序，不符合题目要求。选项 C 使用授权程序阻止请求，这会阻塞对新订单的处理，无法实现异步处理，且不高效。选项 D 使用 SQS 标准队列。标准队列不保证消息的顺序，不符合题目要求。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "Amazon Simple Notification Service (Amazon SNS)",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "FIFO",
      "REST API"
    ]
  },
  {
    "id": 11,
    "topic": "1",
    "question_en": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora database. The EC2 instances connect to the database by using user names and passwords that are stored locally in a file. The company wants to minimize the operational overhead of credential management. What should a solutions architect do to accomplish this goal?",
    "options_en": {
      "A": "Use AWS Secrets Manager. Turn on automatic rotation.",
      "B": "Use AWS Systems Manager Parameter Store. Turn on automatic rotation.",
      "C": "Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management Service (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application to the S3 bucket.",
      "D": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. Attach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS volume. Point the application to the new EBS volume."
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一家公司有一个在 Amazon EC2 实例上运行并使用 Amazon Aurora 数据库的应用程序。 EC2 实例通过使用本地存储在文件中的用户名和密码来连接到数据库。该公司希望最大限度地减少凭据管理的运营开销。解决方案架构师应该怎么做才能实现此目标？",
    "options_cn": {
      "A": "使用 AWS Secrets Manager。 打开自动轮换。",
      "B": "使用 AWS Systems Manager Parameter Store。 打开自动轮换。",
      "C": "创建一个 Amazon S3 存储桶来存储使用 AWS Key Management Service (AWS KMS) 加密密钥加密的对象。 将凭据文件迁移到 S3 存储桶。 将应用程序指向 S3 存储桶。",
      "D": "为每个 EC2 实例创建一个加密的 Amazon Elastic Block Store (Amazon EBS) 卷。 将新的 EBS 卷附加到每个 EC2 实例。 将凭据文件迁移到新的 EBS 卷。 将应用程序指向新的 EBS 卷。"
    },
    "tags": [
      "AWS Systems Manager",
      "Parameter Store",
      "Secrets Management",
      "Amazon EC2",
      "Amazon Aurora",
      "AWS KMS",
      "Amazon S3",
      "Amazon EBS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n考查如何使用 AWS Secrets Manager 管理数据库凭证，以减少运营开销，并实现凭证的自动轮换。",
      "why_correct": "AWS Secrets Manager 专门设计用于安全地存储和轮换秘密信息，如数据库凭证。通过启用自动轮换，Secrets Manager 可以定期更改密码，从而减少安全风险，并减轻手动密码管理的工作量。 这种方法满足了题目中最大限度减少凭据管理运营开销的需求。",
      "why_wrong": "选项 B, AWS Systems Manager Parameter Store 主要用于存储配置数据和参数，虽然可以存储秘密，但其轮换功能不如 Secrets Manager 完善，更适合存储应用程序配置。选项 C 和 D 都涉及文件存储和管理，但它们没有提供凭证轮换的能力，并且管理起来也更加复杂。S3 和 EBS 卷都需要手动管理凭证的生命周期，无法达到减少运营开销的目标。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Aurora",
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)",
      "Amazon Elastic Block Store (Amazon EBS)"
    ]
  },
  {
    "id": 12,
    "topic": "1",
    "question_en": "A global company hosts its web application on Amazon EC2 instances behind an Application Load Balancer (ALB). The web application has static data and dynamic data. The company stores its static data in an Amazon S3 bucket. The company wants to improve performance and reduce latency for the static data and dynamic data. The company is using its own domain name registered with Amazon Route 53. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure Route 53 to route trafic to the CloudFront distribution.",
      "B": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route trafic to the CloudFront distribution.",
      "C": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS Global Accelerator standard accelerator that has the ALB and the CloudFront distribution as endpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom domain name as an endpoint for the web application.",
      "D": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global Accelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. Point one domain name to the CloudFront DNS name for dynamic content. Point the other domain name to the accelerator DNS name for static content. Use the domain names as endpoints for the web application."
    },
    "correct_answer": "A",
    "vote_percentage": "80%",
    "question_cn": "一家全球公司将其 Web 应用程序托管在 Application Load Balancer (ALB) 后的 Amazon EC2 实例上。该 Web 应用程序包含静态数据和动态数据。该公司将其静态数据存储在 Amazon S3 存储桶中。该公司希望提高静态数据和动态数据的性能并减少延迟。该公司正在使用其在 Amazon Route 53 中注册的自己的域名。解决方案架构师应采取什么措施来满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon CloudFront 分发，该分发将 S3 存储桶和 ALB 作为源。配置 Route 53 以将流量路由到 CloudFront 分发。",
      "B": "创建一个 Amazon CloudFront 分发，该分发将 ALB 作为源。创建一个 AWS Global Accelerator 标准加速器，该加速器将 S3 存储桶作为端点。配置 Route 53 以将流量路由到 CloudFront 分发。",
      "C": "创建一个 Amazon CloudFront 分发，该分发将 S3 存储桶作为源。创建一个 AWS Global Accelerator 标准加速器，该加速器将 ALB 和 CloudFront 分发作为端点。创建一个指向加速器 DNS 域名的自定义域名。使用自定义域名作为 Web 应用程序的端点。",
      "D": "创建一个 Amazon CloudFront 分发，该分发将 ALB 作为源。创建一个 AWS Global Accelerator 标准加速器，该加速器将 S3 存储桶作为端点。创建两个域名。将一个域名指向 CloudFront DNS 域名，用于动态内容。将另一个域名指向加速器 DNS 域名，用于静态内容。使用这些域名作为 Web 应用程序的端点。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "Application Load Balancer",
      "AWS Global Accelerator",
      "Amazon Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 80%），解析仅供参考。】\n\n考查如何通过 CloudFront 和 Route 53 优化 Web 应用程序的性能和延迟，并结合 S3 存储静态数据和 ALB 负载均衡动态内容。",
      "why_correct": "选项 A 提供了最直接的解决方案。CloudFront 可以缓存 S3 中的静态内容，减少延迟。同时，CloudFront 可以将 ALB 作为源，加速动态内容。Route 53 配置将用户流量导向 CloudFront，实现最佳的性能和用户体验。",
      "why_wrong": "选项 B 错误，因为 Global Accelerator 主要针对网络层加速，对静态内容缓存效果有限，且将 S3 作为加速器端点无法有效利用 CloudFront 的缓存能力。选项 C 过于复杂，Global Accelerator 无法同时以 CloudFront 和 ALB 作为端点，且不符合常见应用场景。选项 D 同样复杂，需要维护两个域名，增加了管理难度，且将加速器用于静态内容不合适，而且分开域名指向容易造成访问复杂化，不利于用户体验。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "Amazon Route 53",
      "AWS Global Accelerator"
    ]
  },
  {
    "id": 13,
    "topic": "1",
    "question_en": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance activities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases across multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the required Regions. Configure Secrets Manager to rotate the secrets on a schedule.",
      "B": "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. Use multi-Region secret replication for the required Regions. Configure Systems Manager to rotate the secrets on a schedule.",
      "C": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the credentials.",
      "D": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region customer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS Lambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司每月对其 AWS 基础设施进行维护。在这些维护活动期间，该公司需要轮换其跨多个 AWS 区域的 Amazon RDS for MySQL 数据库的凭证。哪种解决方案将以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "将凭证作为秘密存储在 AWS Secrets Manager 中。对所需区域使用多区域秘密复制。配置 Secrets Manager 以按计划轮换秘密。",
      "B": "通过创建安全字符串参数，将凭证作为秘密存储在 AWS Systems Manager 中。对所需区域使用多区域秘密复制。配置 Systems Manager 以按计划轮换秘密。",
      "C": "将凭证存储在已启用服务器端加密 (SSE) 的 Amazon S3 存储桶中。使用 Amazon EventBridge (Amazon CloudWatch Events) 调用 AWS Lambda 函数来轮换凭证。",
      "D": "使用 AWS Key Management Service (AWS KMS) 多区域客户托管密钥将凭证加密为秘密。将秘密存储在 Amazon DynamoDB 全局表中。使用 AWS Lambda 函数从 DynamoDB 检索秘密。使用 RDS API 轮换秘密。"
    },
    "tags": [
      "AWS Secrets Manager",
      "Amazon RDS for MySQL",
      "Multi-region secret replication"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查了在多区域环境中安全管理 RDS 数据库凭证的解决方案。重点考察了 Secrets Manager 的使用，以及与 Systems Manager Parameter Store、S3、Lambda、KMS 和 DynamoDB 的对比，以及多区域复制的能力。此外，还考察了轮换凭证的自动化能力。",
      "why_correct": "AWS Secrets Manager 是管理秘密（如数据库凭证）的推荐服务。通过将凭证作为秘密存储在 Secrets Manager 中，并使用多区域秘密复制，可以确保凭证在多个区域中可用。配置 Secrets Manager 的计划轮换功能，可以实现凭证的自动轮换，满足了题目中以最小运营开销满足需求的要求。Secrets Manager 提供了加密、访问控制和轮换等功能，能够更安全、更便捷地管理秘密。",
      "why_wrong": "选项 B 使用 Systems Manager Parameter Store 来存储凭证，虽然 Parameter Store 也可以存储秘密，但其主要功能是管理配置数据，而 Secrets Manager 专为管理秘密设计，提供了更全面的功能，如自动轮换。选项 C 使用 S3 存储凭证，并使用 EventBridge 和 Lambda 来轮换。这种方案需要自行管理轮换逻辑，增加了运营开销，并且在安全性和便捷性上不如 Secrets Manager。选项 D 使用 KMS、DynamoDB 和 Lambda。这种方案较为复杂，需要额外的配置和管理，相比使用 Secrets Manager 增加了不必要的复杂性，并且 DynamoDB 全局表的配置和维护也增加了开销。"
    },
    "related_terms": [
      "AWS Secrets Manager",
      "AWS Systems Manager",
      "Amazon S3",
      "Lambda",
      "AWS KMS",
      "Amazon DynamoDB",
      "EventBridge",
      "CloudWatch Events",
      "Amazon RDS for MySQL",
      "Multi-region secret replication",
      "RDS API"
    ]
  },
  {
    "id": 14,
    "topic": "1",
    "question_en": "A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance. The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Redshift with a single node for leader and compute functionality.",
      "B": "Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.",
      "C": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.",
      "D": "Use Amazon ElastiCache for Memcached with EC2 Spot Instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其Application Load Balancer后面的Amazon EC2实例上运行电子商务应用程序。这些实例在跨多个可用区的Amazon EC2 Auto Scaling组中运行。Auto Scaling组根据CPU利用率指标进行扩展。电子商务应用程序将事务数据存储在大型EC2实例上托管的MySQL 8.0数据库中。随着应用程序负载的增加，数据库的性能会迅速下降。该应用程序处理的读取请求多于写入事务。该公司希望找到一种解决方案，该方案将自动扩展数据库以满足不可预测的读取工作负载的需求，同时保持高可用性。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用具有单个节点作为领导者和计算功能的Amazon Redshift。",
      "B": "使用单可用区部署的Amazon RDS。配置Amazon RDS以在不同的可用区中添加读取器实例。",
      "C": "使用多可用区部署的Amazon Aurora。使用Aurora Replicas配置Aurora Auto Scaling。",
      "D": "使用带有EC2 Spot实例的Amazon ElastiCache for Memcached。"
    },
    "tags": [
      "Amazon Aurora",
      "Aurora Replicas",
      "Auto Scaling",
      "Amazon RDS",
      "MySQL",
      "ElastiCache",
      "Amazon Redshift",
      "EC2",
      "Application Load Balancer",
      "EC2 Auto Scaling",
      "Availability Zone"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查数据库解决方案的选型，特别是针对读密集型负载的自动扩展和高可用性需求；涉及 Amazon Aurora、Amazon RDS、Amazon ElastiCache 和 Amazon Redshift 的比较。",
      "why_correct": "Amazon Aurora 是一种 MySQL 和 PostgreSQL 兼容的数据库服务，专为云环境设计，提供高性能、高可用性和可扩展性。使用多可用区部署的 Aurora 数据库，可以实现跨可用区的容错能力。Aurora Replicas 是 Aurora 的只读副本，可以分担读取流量。 配置 Aurora Auto Scaling 功能，可以根据读取负载自动增加或减少 Aurora Replicas 的数量，以满足应用程序的读取需求，实现弹性扩展。",
      "why_wrong": "选项 A，Amazon Redshift 是一个数据仓库服务，主要用于分析型查询，而非事务型数据库，无法直接替代 MySQL 数据库。此外，Redshift 的架构不适用于快速的读写操作，且其单一节点无法满足高可用性需求。 选项 B，Amazon RDS 无法自动扩展数据库以满足不可预测的读取工作负载的需求。虽然 RDS 可以配置读取器实例，但其扩容需要手动操作，不如 Aurora 的自动扩展灵活。单可用区的部署也无法满足高可用性需求。 选项 D，Amazon ElastiCache for Memcached 是一个缓存服务，用于加速应用程序的读取性能，不能替代数据库。使用 EC2 Spot 实例的 Memcached 容易受到实例回收的影响，可靠性较低，且无法实现数据库的持久化存储。此外，Memcached 主要用于缓存，不适用于存储事务数据，无法满足数据库的需求。"
    },
    "related_terms": [
      "Amazon Aurora",
      "Auto Scaling",
      "MySQL",
      "Amazon RDS",
      "ElastiCache",
      "Amazon Redshift",
      "EC2",
      "Application Load Balancer",
      "EC2 Auto Scaling",
      "Availability Zone",
      "Aurora Replicas",
      "Memcached"
    ]
  },
  {
    "id": 15,
    "topic": "1",
    "question_en": "A company recently migrated to AWS and wants to implement a solution to protect the trafic that fiows in and out of the production VPC. The company had an inspection server in its on-premises data center. The inspection server performed specific operations such as trafic fiow inspection and trafic filtering. The company wants to have the same functionalities in the AWS Cloud. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon GuardDuty for trafic inspection and trafic filtering in the production VPC.",
      "B": "Use Trafic Mirroring to mirror trafic from the production VPC for trafic inspection and filtering.",
      "C": "Use AWS Network Firewall to create the required rules for trafic inspection and trafic filtering for the production VPC.",
      "D": "Use AWS Firewall Manager to create the required rules for trafic inspection and trafic filtering for the production VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "94%",
    "question_cn": "一家公司最近迁移到 AWS，并希望实施一个解决方案来保护进出生产 VPC 的流量。该公司在其本地数据中心有一个检查服务器。该检查服务器执行特定操作，例如流量流检查和流量过滤。该公司希望在 AWS 云中拥有相同的功能。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon GuardDuty 进行生产 VPC 中的流量检查和流量过滤。",
      "B": "使用 Trafic Mirroring 镜像生产 VPC 中的流量以进行流量检查和过滤。",
      "C": "使用 AWS Network Firewall 为生产 VPC 创建所需的流量检查和流量过滤规则。",
      "D": "使用 AWS Firewall Manager 为生产 VPC 创建所需的流量检查和流量过滤规则。"
    },
    "tags": [
      "AWS Network Firewall",
      "VPC",
      "Traffic inspection",
      "Traffic filtering"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 94%），解析仅供参考。】\n\n考查在 VPC 中实现流量检查和过滤的解决方案；与 AWS Network Firewall、AWS Firewall Manager、Traffic Mirroring 和 GuardDuty 等服务相关。",
      "why_correct": "AWS Network Firewall 提供了全面的网络保护功能，允许您创建规则以检查和过滤 VPC 中的流量。它能够执行流量流检查和流量过滤，满足了题目中对在 AWS 云中拥有相同流量检查和过滤功能的需求。Network Firewall 易于部署和管理，并可以与 VPC 集成，从而控制进出 VPC 的流量。",
      "why_wrong": "A. Amazon GuardDuty 主要用于威胁检测，而不是流量检查和过滤。它分析 CloudTrail 事件、VPC Flow Logs 和 DNS 查询等数据，以识别潜在的安全威胁，与题目要求的流量流检查和过滤功能不符。\nB. Traffic Mirroring 用于将 VPC 内的流量镜像到目标，例如入侵检测系统或流量分析工具，但不直接提供流量过滤功能。虽然它可以用于流量检查，但它需要与第三方工具集成才能执行流量过滤，并且没有提供题目所需的核心功能。\nD. AWS Firewall Manager 用于跨多个账户和应用管理 AWS WAF、AWS Network Firewall 和 Amazon Route 53 Resolver DNS Firewall 的安全策略。它本身不提供流量检查和过滤的功能，而是作为管理工具，用于管理多个防火墙策略。Firewall Manager 需要依赖 AWS WAF 或 Network Firewall 来实现流量检查和过滤，无法单独满足题目的需求。"
    },
    "related_terms": [
      "VPC",
      "Amazon GuardDuty",
      "AWS Firewall Manager",
      "CloudTrail",
      "DNS",
      "AWS WAF",
      "Traffic Mirroring",
      "AWS Network Firewall",
      "VPC Flow Logs",
      "Route 53 Resolver DNS Firewall"
    ]
  },
  {
    "id": 16,
    "topic": "1",
    "question_en": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.",
      "B": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.",
      "C": "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.",
      "D": "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports."
    },
    "correct_answer": "B",
    "vote_percentage": "83%",
    "question_cn": "一家公司在 AWS 上托管数据湖。数据湖包含 Amazon S3 和 Amazon RDS for PostgreSQL 中的数据。该公司需要一个报告解决方案，该解决方案提供数据可视化，并包含数据湖内的所有数据源。只有公司的管理团队才应该完全访问所有可视化内容。公司的其余人员应该只有有限的访问权限。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 Amazon QuickSight 中创建分析。连接所有数据源并创建新的数据集。发布仪表板以可视化数据。与适当的 IAM 角色共享仪表板。",
      "B": "在 Amazon QuickSight 中创建分析。连接所有数据源并创建新的数据集。发布仪表板以可视化数据。与适当的用户和组共享仪表板。",
      "C": "为 Amazon S3 中的数据创建 AWS Glue 表和爬网程序。创建 AWS Glue 提取、转换和加载 (ETL) 作业以生成报告。将报告发布到 Amazon S3。使用 S3 存储桶策略限制对报告的访问。",
      "D": "为 Amazon S3 中的数据创建 AWS Glue 表和爬网程序。使用 Amazon Athena Federated Query 访问 Amazon RDS for PostgreSQL 中的数据。使用 Amazon Athena 生成报告。将报告发布到 Amazon S3。使用 S3 存储桶策略限制对报告的访问。"
    },
    "tags": [
      "QuickSight",
      "IAM",
      "S3",
      "RDS for PostgreSQL",
      "Glue",
      "Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 83%），解析仅供参考。】\n\n考查利用 Amazon QuickSight 进行数据可视化，并通过用户和组管理访问权限的方案。",
      "why_correct": "选项 B 正确地使用了 Amazon QuickSight 进行数据可视化，并支持通过用户和组来控制访问权限。QuickSight 允许连接不同的数据源，并创建仪表板。通过与不同的用户和组共享仪表板，可以实现对不同用户设置不同的访问权限，满足题目要求。",
      "why_wrong": "选项 A 错误在于，它提到了使用 IAM 角色共享仪表板。虽然 IAM 角色可以控制对 QuickSight 的访问，但无法满足对特定用户和组的细粒度访问控制需求，例如需要对不同用户设置不同的数据访问权限。选项 C 和 D 都涉及到使用 AWS Glue 和 Amazon Athena，这对于简单的可视化需求来说过于复杂，且生成的报告存储在 S3 中，QuickSight 提供了更直接的可视化解决方案。"
    },
    "related_terms": [
      "Amazon QuickSight",
      "Amazon S3",
      "Amazon RDS for PostgreSQL",
      "IAM",
      "AWS Glue",
      "ETL",
      "Amazon Athena Federated Query",
      "S3"
    ]
  },
  {
    "id": 17,
    "topic": "1",
    "question_en": "A company is implementing a new business application. The application runs on two Amazon EC2 instances and uses an Amazon S3 bucket for document storage. A solutions architect needs to ensure that the EC2 instances can access the S3 bucket. What should the solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.",
      "B": "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.",
      "C": "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.",
      "D": "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 instances."
    },
    "correct_answer": "A",
    "vote_percentage": "99%",
    "question_cn": "一家公司正在实施一个新的业务应用程序。该应用程序在两个 Amazon EC2 实例上运行，并使用 Amazon S3 存储桶进行文档存储。解决方案架构师需要确保 EC2 实例可以访问 S3 存储桶。 解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "创建一个 IAM 角色，授予对 S3 存储桶的访问权限。将该角色附加到 EC2 实例。",
      "B": "创建一个 IAM 策略，授予对 S3 存储桶的访问权限。将该策略附加到 EC2 实例。",
      "C": "创建一个 IAM 组，授予对 S3 存储桶的访问权限。将该组附加到 EC2 实例。",
      "D": "创建一个 IAM 用户，授予对 S3 存储桶的访问权限。将该用户帐户附加到 EC2 实例。"
    },
    "tags": [
      "EC2",
      "S3",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 99%），解析仅供参考。】\n\n此题考察 EC2 实例访问 S3 存储桶的权限配置。使用 IAM 角色是最佳实践，可以直接将角色附加到 EC2 实例，简化权限管理，避免了凭证泄露的风险。",
      "why_correct": "选项 A 通过创建 IAM 角色并将其附加到 EC2 实例，为 EC2 实例提供了访问 S3 存储桶的权限。这是实现此目的最安全、最有效的方法。",
      "why_wrong": "选项 B 将 IAM 策略附加到 EC2 实例，这是错误的，因为策略需要附加到 IAM 用户或角色。选项 C 描述了 IAM 组的错误使用。选项 D 将 IAM 用户附加到 EC2 实例，不符合 AWS 的最佳实践。"
    },
    "related_terms": [
      "EC2",
      "S3",
      "IAM 角色",
      "IAM 策略",
      "IAM 用户",
      "IAM 组"
    ]
  },
  {
    "id": 18,
    "topic": "1",
    "question_en": "An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket. A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically. Which combination of actions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.",
      "B": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.",
      "C": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.",
      "D": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queu",
      "E": "When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function. E. Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing."
    },
    "correct_answer": "AB",
    "vote_percentage": "99%",
    "question_cn": "一个应用程序开发团队正在设计一个微服务，该微服务会将大图像转换为较小的压缩图像。当用户通过 Web 界面上传图像时，该微服务应将图像存储在 Amazon S3 存储桶中，使用 AWS Lambda 函数处理和压缩图像，并将压缩形式的图像存储在另一个 S3 存储桶中。一个解决方案架构师需要设计一个使用持久、无状态组件自动处理图像的解决方案。哪种操作组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。配置 S3 存储桶，以便在将图像上传到 S3 存储桶时，将通知发送到 SQS 队列。",
      "B": "配置 Lambda 函数，使用 Amazon Simple Queue Service (Amazon SQS) 队列作为调用源。当 SQS 消息成功处理后，删除队列中的消息。",
      "C": "配置 Lambda 函数以监视 S3 存储桶中的新上传。当检测到已上传的图像时，将文件名写入内存中的文本文件，并使用该文本文件跟踪已处理的图像。",
      "D": "启动一个 Amazon EC2 实例来监视 Amazon Simple Queue Service (Amazon SQS) 队列。当项目添加到队列时，在 EC2 实例上的文本文件中记录文件名并调用 Lambda 函数。",
      "E": "配置一个 Amazon EventBridge (Amazon CloudWatch Events) 事件来监视 S3 存储桶。当上传图像时，将警报发送到 Amazon 简单通知服务 (Amazon SNS) 主题，其中包含应用程序所有者的电子邮件地址，以便进一步处理。"
    },
    "tags": [
      "S3",
      "SQS",
      "Lambda",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 99%），解析仅供参考。】\n\n本题考察了使用无状态组件处理上传图像的架构设计。选项 A 和 B 共同构成了一个完整的解决方案。A 使用 SQS 作为触发器，当图像上传到 S3 时触发 SQS 消息，从而实现异步处理。B 中 Lambda 函数从 SQS 队列中获取消息，处理图像，实现了无状态、可扩展的架构。",
      "why_correct": "选项 A 使用 SQS 作为 S3 的事件通知，实现了异步处理图像。此方法能够解耦上传触发和图像处理，保证了系统的弹性和可靠性。",
      "why_wrong": "选项 B 描述了如何配置 Lambda 函数，使其使用 SQS 作为触发源，与选项 A 搭配，构成一个完整的解决方案。选项 C 将文件名写入内存，无法扩展和保证可靠性。选项 D 使用 EC2 监听 SQS，增加了不必要的复杂性，且无法满足持久性需求。选项 E 使用 SNS 发送通知，没有解决图像处理的问题。"
    },
    "related_terms": [
      "S3",
      "SQS",
      "Lambda",
      "EventBridge",
      "SNS"
    ]
  },
  {
    "id": 19,
    "topic": "1",
    "question_en": "A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets. A solutions architect needs to integrate the web application with the appliance to inspect all trafic to the application before the trafic reaches the web server. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a Network Load Balancer in the public subnet of the application's VPC to route the trafic to the appliance for packet inspection.",
      "B": "Create an Application Load Balancer in the public subnet of the application's VPC to route the trafic to the appliance for packet inspection.",
      "C": "Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.",
      "D": "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance."
    },
    "correct_answer": "D",
    "vote_percentage": "82%",
    "question_cn": "一家公司在其 AWS 上部署了一个三层 Web 应用程序。Web 服务器部署在 VPC 的公共子网中。应用程序服务器和数据库服务器部署在同一 VPC 中的私有子网中。该公司从 AWS Marketplace 部署了第三方虚拟防火墙设备，该设备位于一个检查 VPC 中。该设备配置了一个可以接受 IP 数据包的 IP 接口。一个解决方案架构师需要将 Web 应用程序与该设备集成，以便在流量到达 Web 服务器之前检查所有应用程序流量。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "在应用程序的 VPC 的公共子网中创建一个 Network Load Balancer，将流量路由到设备进行数据包检查。",
      "B": "在应用程序的 VPC 的公共子网中创建一个 Application Load Balancer，将流量路由到设备进行数据包检查。",
      "C": "在检查 VPC 中部署一个转接网关。配置路由表以通过转接网关路由传入数据包。",
      "D": "在检查 VPC 中部署一个 Gateway Load Balancer。创建一个 Gateway Load Balancer 端点来接收传入数据包，并将数据包转发到该设备。"
    },
    "tags": [
      "VPC",
      "Network Load Balancer",
      "Application Load Balancer",
      "Gateway Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 82%），解析仅供参考。】\n\n考查 Gateway Load Balancer (GWLB) 的应用，以及如何通过 GWLB 集成第三方虚拟防火墙设备，实现流量检查。",
      "why_correct": "选项 D 正确。Gateway Load Balancer (GWLB) 专为安全设备设计，可以透明地将流量路由到多个设备实例。通过在检查 VPC 中部署 GWLB，并配置 Gateway Load Balancer 端点接收流量，可以确保应用程序流量在到达 Web 服务器之前被第三方防火墙设备检查。这种方案能够以最小的运营开销满足流量检查的需求，并且易于扩展和维护。",
      "why_wrong": "选项 A 错误。Network Load Balancer (NLB) 无法直接用于检查流量，并且 NLB 仅支持 TCP、UDP 和 TLS 协议。选项 B 错误。Application Load Balancer (ALB) 无法用于透明地检查流量，需要对应用程序进行修改才能实现流量转发。选项 C 错误。Transit Gateway (TGW) 更多用于 VPC 之间的网络连接，而非流量检查，使用 TGW 实现流量检查增加了配置复杂性，且无法像 GWLB 一样透明地处理流量。"
    },
    "related_terms": [
      "VPC",
      "Web application",
      "Network Load Balancer",
      "Application Load Balancer",
      "Transit Gateway",
      "Gateway Load Balancer",
      "IP",
      "AWS Marketplace"
    ]
  },
  {
    "id": 20,
    "topic": "1",
    "question_en": "A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance. A solutions architect needs to minimize the time that is required to clone the production data into the test environment. Which solution will meet these requirements?",
    "options_en": {
      "A": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.",
      "B": "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.",
      "C": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.",
      "D": "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment."
    },
    "correct_answer": "D",
    "vote_percentage": "93%",
    "question_cn": "一家公司希望提高其将大量生产数据克隆到相同 AWS 区域中的测试环境中的能力。数据存储在 Amazon EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 卷上。对克隆数据的修改不得影响生产环境。访问此数据的软件需要始终如一的高 I/O 性能。 一位解决方案架构师需要最大限度地减少将生产数据克隆到测试环境所需的时间。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "拍摄生产 EBS 卷的 EBS 快照。 将快照恢复到测试环境中的 EC2 实例存储卷上。",
      "B": "配置生产 EBS 卷以使用 EBS Multi-Attach 功能。 拍摄生产 EBS 卷的 EBS 快照。 将生产 EBS 卷附加到测试环境中的 EC2 实例。",
      "C": "拍摄生产 EBS 卷的 EBS 快照。 创建并初始化新的 EBS 卷。 在从生产 EBS 快照恢复卷之前，将新的 EBS 卷附加到测试环境中的 EC2 实例。",
      "D": "拍摄生产 EBS 卷的 EBS 快照。 在 EBS 快照上打开 EBS 快速快照恢复功能。 将快照恢复到新的 EBS 卷中。 将新的 EBS 卷附加到测试环境中的 EC2 实例。"
    },
    "tags": [
      "EBS",
      "EC2",
      "快照",
      "EBS 快照",
      "EBS Multi-Attach",
      "EBS 快速快照恢复"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 93%），解析仅供参考。】\n\n此题考察将生产数据克隆到测试环境的方案，并需要关注性能和最小化时间。选项 D 使用 EBS 快照和 EBS 快速快照恢复，能够快速创建 EBS 卷，并将其附加到测试环境中的 EC2 实例。快速快照恢复可以显著减少恢复时间。",
      "why_correct": "选项 D 使用了 EBS 快速快照恢复，能够加速快照的创建和恢复过程。这使得快速克隆生产 EBS 卷的数据到测试环境成为可能。",
      "why_wrong": "选项 A 使用常规快照，创建时间较长，无法满足最小化时间的需求。选项 B 使用 EBS Multi-Attach，但并没有直接缩短克隆时间。选项 C 在初始化新的 EBS 卷之前需要附加卷，增加了不必要的步骤，无法满足最小化时间的需求。"
    },
    "related_terms": [
      "EBS",
      "EC2",
      "EBS 快照",
      "快照",
      "EBS Multi-Attach",
      "EBS 快速快照恢复"
    ]
  },
  {
    "id": 21,
    "topic": "1",
    "question_en": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.",
      "B": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website trafic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.",
      "C": "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in trafic. Store the data in Amazon RDS for MySQL.",
      "D": "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司希望在 AWS 上推出一个每日特价网站。每天将有一个产品在 24 小时内促销。该公司希望能够处理每小时数百万个请求，并在高峰时具有毫秒级延迟。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 在不同的 S3 存储桶中托管完整的网站。添加 Amazon CloudFront 发行版。将 S3 存储桶设置为发行版的源。将订单数据存储在 Amazon S3 中。",
      "B": "将完整的网站部署在运行于多个可用区中的 Auto Scaling 组中的 Amazon EC2 实例上。添加 Application Load Balancer (ALB) 来分发网站流量。为后端 API 添加另一个 ALB。将数据存储在 Amazon RDS for MySQL 中。",
      "C": "将完整的应用程序迁移到在容器中运行。在 Amazon Elastic Kubernetes Service (Amazon EKS) 上托管容器。使用 Kubernetes Cluster Autoscaler 来增加和减少 pod 的数量以处理流量突发。将数据存储在 Amazon RDS for MySQL 中。",
      "D": "使用 Amazon S3 存储桶托管网站的静态内容。部署 Amazon CloudFront 发行版。将 S3 存储桶设置为源。使用 Amazon API Gateway 和 AWS Lambda 函数作为后端 API。将数据存储在 Amazon DynamoDB 中。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "EC2",
      "Auto Scaling",
      "RDS",
      "EKS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察了每日特价网站的架构设计，需要考虑高并发、低延迟和最小运营开销。选项 D 结合了 S3 静态内容托管、CloudFront CDN 加速、API Gateway + Lambda 后端 API 和 DynamoDB 数据库，整体架构具备高扩展性和高性能，并且运营成本低。",
      "why_correct": "选项 D 结合了 S3 托管静态内容，使用 CloudFront 提供 CDN 加速，通过 API Gateway 和 Lambda 构建后端 API，并使用 DynamoDB 存储数据。此方案具有高可扩展性、低延迟和最小运营开销的特点，满足了题目要求。",
      "why_wrong": "选项 A 仅使用 S3 和 CloudFront，没有后端 API，无法实现每日特价网站的动态内容。选项 B 使用 EC2、ALB 和 RDS，运营成本较高。选项 C 使用 EKS 和 RDS，增加了复杂性，且运营成本较高。"
    },
    "related_terms": [
      "S3",
      "CloudFront",
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "EC2",
      "Auto Scaling",
      "RDS",
      "EKS"
    ]
  },
  {
    "id": 22,
    "topic": "1",
    "question_en": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files. Which storage option meets these requirements?",
    "options_en": {
      "A": "S3 Standard",
      "B": "S3 Intelligent-Tiering",
      "C": "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "D": "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在使用 Amazon S3 来设计新的数字媒体应用程序的存储架构。媒体文件必须能够应对可用区 (Availability Zone) 的丢失。某些文件会被频繁访问，而其他文件则以不可预测的模式很少被访问。解决方案架构师必须最大限度地降低存储和检索媒体文件的成本。哪种存储选项满足这些要求？",
    "options_cn": {
      "A": "S3 标准",
      "B": "S3 智能分层",
      "C": "S3 标准-不频繁访问 (S3 Standard-IA)",
      "D": "S3 单区-不频繁访问 (S3 One Zone-IA)"
    },
    "tags": [
      "S3",
      "存储类",
      "可用区",
      "成本优化"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 S3 存储类的选择，需要考虑可用区丢失和成本优化。S3 智能分层 (S3 Intelligent-Tiering) 能够根据访问模式自动调整存储层，频繁访问和不频繁访问的文件都适用，同时还能应对可用区丢失。",
      "why_correct": "选项 B S3 智能分层，可以自动将对象移动到访问频率较低的存储层。此选项平衡了成本和可用性，并能应对可用区丢失的情况。",
      "why_wrong": "选项 A S3 标准虽然具有高可用性，但成本较高，不符合题目的成本优化要求。选项 C S3 标准-不频繁访问 (S3 Standard-IA) 成本较低，但如果文件被频繁访问，会产生检索费用。选项 D S3 单区-不频繁访问 (S3 One Zone-IA) 成本最低，但无法应对可用区丢失的情况。"
    },
    "related_terms": [
      "S3",
      "可用区",
      "S3 标准",
      "S3 智能分层",
      "存储类",
      "成本优化",
      "S3 标准-不频繁访问",
      "S3 One Zone-IA"
    ]
  },
  {
    "id": 23,
    "topic": "1",
    "question_en": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely. Which storage solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure S3 Intelligent-Tiering to automatically migrate objects.",
      "B": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.",
      "C": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.",
      "D": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month."
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在使用 Amazon S3 标准存储来存储备份文件。这些文件在一个月内会被频繁访问。然而，在一个月后，这些文件将不再被访问。该公司必须无限期地保留这些文件。哪种存储解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置 S3 Intelligent-Tiering 以自动迁移对象。",
      "B": "创建一个 S3 生命周期配置，将对象从 S3 标准存储转换为 S3 Glacier Deep Archive，时间为 1 个月后。",
      "C": "创建一个 S3 生命周期配置，将对象从 S3 标准存储转换为 S3 标准 - 不频繁访问 (S3 Standard-IA)，时间为 1 个月后。",
      "D": "创建一个 S3 生命周期配置，将对象从 S3 标准存储转换为 S3 单区 - 不频繁访问 (S3 One Zone-IA)，时间为 1 个月后。"
    },
    "tags": [
      "S3",
      "存储类",
      "S3 生命周期",
      "成本优化",
      "S3 Glacier Deep Archive"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n此题考察 S3 生命周期的配置，用于将文件从 S3 标准转换为成本更低的存储类。S3 Glacier Deep Archive 存储类是成本最低的选项，适用于长期归档场景。配置 S3 生命周期规则，在 1 个月后将文件迁移至 S3 Glacier Deep Archive，可以满足成本效益要求。",
      "why_correct": "选项 B 创建 S3 生命周期规则，将对象从 S3 标准存储转换为 S3 Glacier Deep Archive，可以满足成本效益要求。",
      "why_wrong": "选项 A 配置 S3 Intelligent-Tiering 并不能最大程度降低成本。选项 C 虽然成本更低，但S3 标准 - 不频繁访问 (S3 Standard-IA) 成本没有 S3 Glacier Deep Archive 低。选项 D 的 S3 单区 - 不频繁访问 (S3 One Zone-IA) 无法满足高可用性的要求。"
    },
    "related_terms": [
      "S3",
      "S3 标准",
      "S3 Glacier Deep Archive",
      "S3 Intelligent-Tiering",
      "存储类",
      "S3 生命周期",
      "成本优化",
      "S3 标准-不频繁访问",
      "S3 One Zone-IA"
    ]
  },
  {
    "id": 24,
    "topic": "1",
    "question_en": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling. How should the solutions architect generate the information with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
      "B": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.",
      "C": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.",
      "D": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types."
    },
    "correct_answer": "B",
    "vote_percentage": "64%",
    "question_cn": "一家公司在其最新的账单中观察到 Amazon EC2 成本有所增加。计费团队注意到几个 EC2 实例的实例类型出现了不必要的垂直扩展。一位解决方案架构师需要创建一个图表，比较过去 2 个月的 EC2 成本，并进行深入分析以确定垂直扩展的根本原因。解决方案架构师应该如何以最少的运营开销生成信息？",
    "options_cn": {
      "A": "使用 AWS Budgets 创建预算报告，并根据实例类型比较 EC2 成本。",
      "B": "使用 Cost Explorer 的细粒度过滤功能，根据实例类型对 EC2 成本进行深入分析。",
      "C": "使用 AWS Billing and Cost Management 仪表板上的图表，比较过去 2 个月的基于实例类型的 EC2 成本。",
      "D": "使用 AWS Cost and Usage Reports 创建报告并将其发送到 Amazon S3 存储桶。 使用 Amazon QuickSight 并将 Amazon S3 用作源，根据实例类型生成交互式图表。"
    },
    "tags": [
      "EC2",
      "成本优化",
      "Cost Explorer",
      "Billing and Cost Management",
      "QuickSight"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 64%），解析仅供参考。】\n\n考查使用 AWS Cost Explorer 分析 EC2 成本，并找出不必要的垂直扩展的根本原因。",
      "why_correct": "Cost Explorer 允许对成本进行细粒度分析，并提供按实例类型等维度进行过滤的功能，非常适合分析 EC2 成本。通过使用 Cost Explorer，可以轻松创建图表并深入了解过去 2 个月的成本变化。这能帮助架构师快速确定垂直扩展对成本的影响，从而找出根本原因。",
      "why_wrong": "A 选项中的 AWS Budgets 主要用于设置预算和监控成本，而不是进行详细的成本分析。C 选项的 AWS Billing and Cost Management 仪表板虽然提供了成本图表，但其分析功能不如 Cost Explorer 强大，无法进行细粒度的深入分析。D 选项涉及使用 AWS Cost and Usage Reports 和 QuickSight。虽然这也可以实现成本分析，但相比 Cost Explorer，其设置和操作的复杂性更高，运营开销更大，不符合题目的“以最少的运营开销”的要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Cost Explorer",
      "EC2 cost",
      "instance type",
      "AWS Budgets",
      "AWS Billing and Cost Management",
      "AWS Cost and Usage Reports",
      "Amazon S3",
      "Amazon QuickSight"
    ]
  },
  {
    "id": 25,
    "topic": "1",
    "question_en": "A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database. During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort. Which solution will meet these requirements?",
    "options_en": {
      "A": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.",
      "B": "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.",
      "C": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification Service (Amazon SNS).",
      "D": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue."
    },
    "correct_answer": "D",
    "vote_percentage": "99%",
    "question_cn": "一家公司正在设计一个应用程序。该应用程序使用一个 AWS Lambda 函数通过 Amazon API Gateway 接收信息，并将信息存储在 Amazon Aurora PostgreSQL 数据库中。在概念验证阶段，该公司必须大幅增加 Lambda 配额，以处理公司需要加载到数据库中的大量数据。一位解决方案架构师必须推荐一种新设计，以提高可扩展性并最大限度地减少配置工作量。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 Lambda 函数代码重构为在 Amazon EC2 实例上运行的 Apache Tomcat 代码。使用原生 Java Database Connectivity (JDBC) 驱动程序连接数据库。",
      "B": "将平台从 Aurora 更改为 Amazon DynamoDB。预置一个 DynamoDB Accelerator (DAX) 集群。使用 DAX 客户端 SDK 将现有的 DynamoDB API 调用指向 DAX 集群。",
      "C": "设置两个 Lambda 函数。配置一个函数来接收信息。配置另一个函数将信息加载到数据库中。使用 Amazon Simple Notification Service (Amazon SNS) 集成 Lambda 函数。",
      "D": "设置两个 Lambda 函数。配置一个函数来接收信息。配置另一个函数将信息加载到数据库中。使用 Amazon Simple Queue Service (Amazon SQS) 队列集成 Lambda 函数。"
    },
    "tags": [
      "Lambda",
      "API Gateway",
      "Aurora PostgreSQL",
      "SQS",
      "可扩展性"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 99%），解析仅供参考。】\n\n此题考察 Lambda 函数和 Aurora PostgreSQL 数据库的集成设计，需要考虑可扩展性和配置工作量。使用 SQS 队列可以在 Lambda 函数和数据库之间提供异步通信，从而提高可扩展性。通过将信息放入 SQS 队列， Lambda 函数可以快速接收信息，然后异步地将信息加载到数据库中。",
      "why_correct": "选项 D 使用 SQS 队列连接 Lambda 函数，可以异步处理数据，从而提高可扩展性。这是一个常用的设计模式，可以缓解 Lambda 配额限制，并降低配置复杂性。",
      "why_wrong": "选项 A 使用 EC2 实例运行 Java 代码，增加了运营成本，配置工作量较大。选项 B 将 Aurora 数据库替换为 DynamoDB，虽然能提高可扩展性，但修改数据库类型的工作量较大。选项 C 使用 SNS 进行 Lambda 函数的集成，SNS 是发布-订阅模型，不是理想的消息队列。"
    },
    "related_terms": [
      "Lambda",
      "API Gateway",
      "Aurora PostgreSQL",
      "SQS",
      "SNS",
      "可扩展性"
    ]
  },
  {
    "id": 26,
    "topic": "1",
    "question_en": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes. What should a solutions architect do to accomplish this goal?",
    "options_en": {
      "A": "Turn on AWS Config with the appropriate rules.",
      "B": "Turn on AWS Trusted Advisor with the appropriate checks.",
      "C": "Turn on Amazon Inspector with the appropriate assessment template.",
      "D": "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events)."
    },
    "correct_answer": "A",
    "vote_percentage": "98%",
    "question_cn": "一家公司需要审查其 AWS 云部署，以确保其 Amazon S3 存储桶没有未经授权的配置更改。解决方案架构师应该怎么做才能实现此目标？",
    "options_cn": {
      "A": "使用适当的规则打开 AWS Config。",
      "B": "使用适当的检查打开 AWS Trusted Advisor。",
      "C": "使用适当的评估模板打开 Amazon Inspector。",
      "D": "打开 Amazon S3 服务器访问日志记录。配置 Amazon EventBridge (Amazon CloudWatch Events)。"
    },
    "tags": [
      "S3",
      "AWS Config",
      "安全"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 98%），解析仅供参考。】\n\n本题考察了 AWS 环境的安全审计。AWS Config 提供了对 AWS 资源配置更改的持续监控和评估。通过启用 AWS Config 并配置适当的规则，可以检测未经授权的配置更改。",
      "why_correct": "选项 A 使用 AWS Config 监控 S3 存储桶的配置更改，是实现此目标的首选方法。AWS Config 提供了对资源配置的持续监控和评估。",
      "why_wrong": "选项 B 使用 AWS Trusted Advisor 无法监控特定的配置更改。选项 C 使用 Amazon Inspector 无法针对 S3 存储桶的配置更改提供帮助。选项 D 使用 S3 服务器访问日志记录，只能记录访问情况，无法监控配置更改。"
    },
    "related_terms": [
      "S3",
      "AWS Config",
      "AWS Trusted Advisor",
      "Amazon Inspector",
      "安全",
      "S3 服务器访问日志记录"
    ]
  },
  {
    "id": 27,
    "topic": "1",
    "question_en": "A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege. Which solution will meet these requirements?",
    "options_en": {
      "A": "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.",
      "B": "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.",
      "C": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
      "D": "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard."
    },
    "correct_answer": "A",
    "vote_percentage": "74%",
    "question_cn": "一家公司正在启动一个新应用程序，并将应用程序指标显示在 Amazon CloudWatch 仪表板上。该公司的产品经理需要定期间隔访问此仪表板。产品经理没有 AWS 账户。一个解决方案架构师必须通过遵循最小权限原则来向产品经理提供访问权限。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "从 CloudWatch 控制台共享仪表板。输入产品经理的电子邮件地址，并完成共享步骤。为产品经理提供仪表板的可共享链接。",
      "B": "专门为产品经理创建一个 IAM 用户。将 CloudWatchReadOnlyAccess AWS 托管策略附加到该用户。与产品经理共享新的登录凭据。与产品经理共享正确仪表板的浏览器 URL。",
      "C": "为公司的员工创建一个 IAM 用户。将 ViewOnlyAccess AWS 托管策略附加到 IAM 用户。与产品经理共享新的登录凭据。要求产品经理导航到 CloudWatch 控制台，并在“仪表板”部分按名称找到仪表板。",
      "D": "在公共子网中部署一个堡垒服务器。当产品经理需要访问仪表板时，启动服务器并共享 RDP 凭据。在堡垒服务器上，确保浏览器配置为使用具有查看仪表板的适当权限的缓存 AWS 凭据打开仪表板 URL。"
    },
    "tags": [
      "CloudWatch",
      "IAM",
      "最小权限原则"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 74%），解析仅供参考。】\n\n考查 CloudWatch 仪表板的共享方式以及最小权限原则的实现。",
      "why_correct": "选项 A 允许直接从 CloudWatch 控制台共享仪表板。通过电子邮件共享仪表板的链接，产品经理无需 AWS 账户即可访问。这种方式满足了题目中产品经理没有 AWS 账户和最小权限原则的要求，因为产品经理只需要访问仪表板的权限。",
      "why_wrong": "选项 B 涉及创建 IAM 用户并共享凭据，这违背了产品经理没有 AWS 账户的要求。此外，共享登录凭据不符合最小权限原则。选项 C 创建的 IAM 用户也需要登录，这不符合题目要求。此外，让产品经理在 CloudWatch 控制台中查找仪表板增加了复杂性，并非最佳实践。选项 D 涉及部署堡垒服务器，过于复杂，且违背了最小权限原则，产品经理需要使用堡垒服务器的凭据进行登录，并不高效。"
    },
    "related_terms": [
      "Amazon CloudWatch",
      "CloudWatch dashboard",
      "IAM",
      "IAM user",
      "CloudWatchReadOnlyAccess",
      "AWS",
      "ViewOnlyAccess",
      "RDP"
    ]
  },
  {
    "id": 28,
    "topic": "1",
    "question_en": "A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory. Which solution will meet these requirements?",
    "options_en": {
      "A": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
      "B": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
      "C": "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.",
      "D": "Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console."
    },
    "correct_answer": "B",
    "vote_percentage": "78%",
    "question_cn": "一家公司正在将应用程序迁移到 AWS。这些应用程序部署在不同的账户中。该公司使用 AWS Organizations 集中管理这些账户。该公司的安全团队需要在所有公司的账户中使用单点登录 (SSO) 解决方案。该公司必须继续在其本地自管理的 Microsoft Active Directory 中管理用户和组。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "从 AWS SSO 控制台启用 AWS Single Sign-On (AWS SSO)。通过使用 AWS Directory Service for Microsoft Active Directory，创建一个单向林信任或单向域信任来连接公司的自管理 Microsoft Active Directory 与 AWS SSO。",
      "B": "从 AWS SSO 控制台启用 AWS Single Sign-On (AWS SSO)。通过使用 AWS Directory Service for Microsoft Active Directory，创建一个双向林信任来连接公司的自管理 Microsoft Active Directory 与 AWS SSO。",
      "C": "使用 AWS Directory Service。创建与公司自管理 Microsoft Active Directory 的双向信任关系。",
      "D": "在本地部署一个身份提供者 (IdP)。从 AWS SSO 控制台启用 AWS Single Sign-On (AWS SSO)。"
    },
    "tags": [
      "AWS Organizations",
      "AWS SSO",
      "Active Directory",
      "单点登录"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 78%），解析仅供参考。】\n\n考查如何在 AWS Organizations 中通过 AWS SSO 实现与本地 Active Directory 的单点登录集成。",
      "why_correct": "选项 B 提供了最合适的解决方案。它利用 AWS SSO 和 AWS Directory Service for Microsoft Active Directory。通过建立一个双向林信任，允许 AWS 访问本地 Active Directory 中的用户和组信息，并允许用户使用本地凭据登录 AWS 应用程序。",
      "why_wrong": "选项 A 错误在于单向林信任无法实现双向身份验证，意味着 AWS SSO 无法验证来自本地 Active Directory 的用户。选项 C 仅创建了 AWS Directory Service 与本地 Active Directory 之间的信任关系，而没有集成 SSO 解决方案。选项 D 建议在本地部署 IdP，增加了管理复杂性，且未明确说明如何集成本地 IdP 与 AWS SSO，无法满足题目的需求。"
    },
    "related_terms": [
      "AWS SSO",
      "AWS Organizations",
      "Microsoft Active Directory",
      "AWS Directory Service for Microsoft Active Directory",
      "Single Sign-On",
      "IdP",
      "林信任",
      "域信任",
      "双向信任",
      "单向信任"
    ]
  },
  {
    "id": 29,
    "topic": "1",
    "question_en": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions. The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.",
      "B": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.",
      "C": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.",
      "D": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin."
    },
    "correct_answer": "A",
    "vote_percentage": "78%",
    "question_cn": "一家公司提供使用 UDP 连接的互联网语音协议 (VoIP) 服务。该服务由在 Auto Scaling 组中运行的 Amazon EC2 实例组成。该公司在多个 AWS 区域中都有部署。该公司需要将用户路由到延迟最低的区域。该公司还需要区域之间的自动化故障转移。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "部署一个 Network Load Balancer (NLB) 和一个相关的目标组。将目标组与 Auto Scaling 组关联。在每个区域中使用 NLB 作为 AWS Global Accelerator 终端节点。",
      "B": "部署一个 Application Load Balancer (ALB) 和一个相关的目标组。将目标组与 Auto Scaling 组关联。在每个区域中使用 ALB 作为 AWS Global Accelerator 终端节点。",
      "C": "部署一个 Network Load Balancer (NLB) 和一个相关的目标组。将目标组与 Auto Scaling 组关联。创建 Amazon Route 53 延迟记录，该记录指向每个 NLB 的别名。创建一个 Amazon CloudFront 分发，该分发使用延迟记录作为源。",
      "D": "部署一个 Application Load Balancer (ALB) 和一个相关的目标组。将目标组与 Auto Scaling 组关联。创建 Amazon Route 53 加权记录，该记录指向每个 ALB 的别名。部署一个 Amazon CloudFront 分发，该分发使用加权记录作为源。"
    },
    "tags": [
      "Global Accelerator",
      "Route 53",
      "NLB",
      "ALB",
      "故障转移",
      "延迟"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 78%），解析仅供参考。】\n\n考察使用 AWS Global Accelerator 实现 VoIP 服务，并结合 NLB 和 Auto Scaling，实现跨区域的负载均衡和故障转移。",
      "why_correct": "Network Load Balancer (NLB) 适用于 UDP 流量，满足 VoIP 服务的需求。结合 AWS Global Accelerator，可以基于延迟将用户路由到最近的区域。通过将 NLB 关联到 Auto Scaling 组，可以实现自动化的故障转移和弹性伸缩。",
      "why_wrong": "选项 B 错误，Application Load Balancer (ALB) 虽然支持 UDP 流量，但性能不如 NLB。选项 C 错误，虽然 Route 53 延迟记录和 CloudFront 可以实现一定程度的延迟优化，但无法直接与 NLB 集成并实现自动故障转移。选项 D 错误，ALB 并不适合 UDP 流量，并且 Route 53 加权记录无法满足基于延迟的路由需求，CloudFront 的使用也显得多余。"
    },
    "related_terms": [
      "UDP",
      "VoIP",
      "Amazon EC2",
      "Auto Scaling",
      "AWS Global Accelerator",
      "Network Load Balancer (NLB)",
      "Application Load Balancer (ALB)",
      "Amazon Route 53",
      "Amazon CloudFront"
    ]
  },
  {
    "id": 30,
    "topic": "1",
    "question_en": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Stop the DB instance when tests are completed. Restart the DB instance when required.",
      "B": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
      "C": "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.",
      "D": "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required."
    },
    "correct_answer": "C",
    "vote_percentage": "76%",
    "question_cn": "一个开发团队在其通用 Amazon RDS for MySQL 数据库实例上运行每月资源密集型测试，并启用了 Performance Insights。测试每月持续 48 小时，并且是唯一使用该数据库的进程。该团队希望降低运行测试的成本，而不会降低数据库实例的计算和内存属性。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "在测试完成后停止数据库实例。在需要时重新启动数据库实例。",
      "B": "使用带有数据库实例的 Auto Scaling 策略，以便在测试完成后自动扩展。",
      "C": "在测试完成后创建快照。终止数据库实例，并在需要时还原快照。",
      "D": "在测试完成后将数据库实例修改为低容量实例。在需要时再次修改数据库实例。"
    },
    "tags": [
      "RDS",
      "成本优化",
      "快照"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 76%），解析仅供参考。】\n\n此题考察如何优化每月资源密集型测试的成本。 选项 C 使用快照， 在测试完成后创建快照，终止数据库实例，并在需要时还原快照。这样可以最大限度地减少数据库实例运行时间，从而降低成本。",
      "why_correct": "选项 C 使用快照并终止数据库实例，仅在需要时恢复快照，最大限度地减少了数据库实例的运行时间，实现了成本优化。",
      "why_wrong": "选项 A 停止和启动数据库实例，测试过程中会产生费用。选项 B 使用 Auto Scaling 无法降低成本，实例仍然会运行。选项 D 修改数据库实例大小，无法大幅度降低成本。"
    },
    "related_terms": [
      "RDS",
      "快照",
      "成本优化"
    ]
  },
  {
    "id": 31,
    "topic": "1",
    "question_en": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check. What should a solutions architect do to accomplish this?",
    "options_en": {
      "A": "Use AWS Config rules to define and detect resources that are not properly tagged.",
      "B": "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
      "C": "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
      "D": "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code."
    },
    "correct_answer": "A",
    "vote_percentage": "98%",
    "question_cn": "一家公司在 AWS 上托管其 Web 应用程序，希望确保所有 Amazon EC2 实例、Amazon RDS 数据库实例和 Amazon Redshift 集群都配置了标签。该公司希望尽量减少配置和操作此检查的工作量。解决方案架构师应该怎么做才能实现此目的？",
    "options_cn": {
      "A": "使用 AWS Config 规则定义和检测未正确标记的资源。",
      "B": "使用 Cost Explorer 显示未正确标记的资源。手动标记这些资源。",
      "C": "编写 API 调用来检查所有资源是否正确分配了标签。定期在 EC2 实例上运行代码。",
      "D": "编写 API 调用来检查所有资源是否正确分配了标签。通过 Amazon CloudWatch 安排一个 AWS Lambda 函数来定期间隔运行代码。"
    },
    "tags": [
      "AWS Config",
      "Tagging",
      "Amazon EC2",
      "Amazon RDS",
      "Amazon Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 98%），解析仅供参考。】\n\n考察使用 AWS Config 管理和审计资源标签。与资源合规性、自动化检测和修复相关。",
      "why_correct": "AWS Config 允许定义规则来评估 AWS 资源的配置。通过创建自定义或使用预定义的 AWS Config 规则，可以检查资源是否符合特定的标签策略。如果资源不符合规则（例如，缺少必要的标签），AWS Config 将标记为不合规，并提供详细信息。这简化了检查和报告过程，并允许自动化合规性。",
      "why_wrong": "B 选项使用 Cost Explorer，Cost Explorer 主要用于成本分析和可视化，无法直接检测未标记的资源；手动标记不符合题干中“尽量减少配置和操作此检查的工作量”的要求。C 选项需要手动编写和维护 API 调用代码，并在 EC2 实例上运行，增加了配置和维护的复杂性，不如使用 AWS Config 自动化。D 选项虽然使用 Lambda 自动化，但仍然需要手动编写 API 调用代码来检查标签，增加了额外的工作量，不如 AWS Config 方便，并且在部署和维护上也比 AWS Config 复杂。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Amazon Redshift",
      "AWS Config",
      "API",
      "Lambda",
      "Amazon CloudWatch",
      "Cost Explorer"
    ]
  },
  {
    "id": 32,
    "topic": "1",
    "question_en": "A development team needs to host a website that will be accessed by other teams. The website contents consist of HTML, CSS, client-side JavaScript, and images. Which method is the MOST cost-effective for hosting the website?",
    "options_en": {
      "A": "Containerize the website and host it in AWS Fargate.",
      "B": "Create an Amazon S3 bucket and host the website there.",
      "C": "Deploy a web server on an Amazon EC2 instance to host the website.",
      "D": "Configure an Application Load Balancer with an AWS Lambda target that uses the Express.js framework."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个开发团队需要托管一个网站，该网站将被其他团队访问。网站内容包括 HTML、CSS、客户端 JavaScript 和图像。哪种方法对于托管网站来说最具成本效益？",
    "options_cn": {
      "A": "将网站容器化并在 AWS Fargate 中托管。",
      "B": "创建一个 Amazon S3 存储桶并在其中托管网站。",
      "C": "在 Amazon EC2 实例上部署一个 Web 服务器来托管网站。",
      "D": "配置一个 Application Load Balancer，其目标是使用 Express.js 框架的 AWS Lambda。"
    },
    "tags": [
      "Amazon S3",
      "Website Hosting",
      "Cost-Effectiveness"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察在 AWS 上托管静态网站的成本效益。需要考虑各种托管方案的成本、可扩展性和管理复杂性，并对比 Amazon S3、EC2、Lambda 和容器服务的适用场景。",
      "why_correct": "使用 Amazon S3 托管静态网站是最具成本效益的方案。S3 提供了高可用性、可扩展性，并且只需为存储和数据传输付费，无需管理服务器。S3 还可以直接配置为托管静态网站，可以自定义域名，并提供内容分发网络（CDN）集成以提高网站的加载速度。",
      "why_wrong": "选项 A 涉及将网站容器化并在 AWS Fargate 中托管，虽然可以提供灵活的部署和扩展，但对于静态网站而言，Fargate 的计算资源和容器镜像维护会增加不必要的成本和复杂性。选项 C 使用 Amazon EC2 实例部署 Web 服务器，需要管理服务器的操作系统、安全补丁、以及软件的维护，增加了运维成本，并且 EC2 实例的运行费用高于 S3 的存储费用。选项 D 使用 Application Load Balancer 和 AWS Lambda，虽然可以实现动态网站的功能，但对于静态网站而言，Lambda 的执行时间和 ALB 的费用都会增加成本，并且增加了架构的复杂度，不符合题目的成本效益要求。"
    },
    "related_terms": [
      "Amazon S3",
      "EC2",
      "Fargate",
      "Lambda",
      "Application Load Balancer",
      "Website Hosting",
      "HTML",
      "CSS",
      "JavaScript",
      "Express.js"
    ]
  },
  {
    "id": 33,
    "topic": "1",
    "question_en": "A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.",
      "B": "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
      "C": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in Amazon DynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
      "D": "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3."
    },
    "correct_answer": "C",
    "vote_percentage": "86%",
    "question_cn": "一家公司在 AWS 上运行一个在线市场 Web 应用程序。该应用程序在高峰时为数十万用户提供服务。该公司需要一个可扩展的近实时解决方案，以与几个其他内部应用程序共享数百万笔金融交易的详细信息。交易也需要被处理以移除敏感数据，然后再存储在文档数据库中，以实现低延迟检索。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "将交易数据存储到 Amazon DynamoDB 中。在 DynamoDB 中设置一个规则，以便在每次写入时从每笔交易中删除敏感数据。使用 DynamoDB Streams 与其他应用程序共享交易数据。",
      "B": "将交易数据流式传输到 Amazon Kinesis Data Firehose 中，以将数据存储在 Amazon DynamoDB 和 Amazon S3 中。使用 AWS Lambda 集成 Kinesis Data Firehose 来删除敏感数据。其他应用程序可以消费存储在 Amazon S3 中的数据。",
      "C": "将交易数据流式传输到 Amazon Kinesis Data Streams 中。使用 AWS Lambda 集成来从每笔交易中删除敏感数据，然后将交易数据存储在 Amazon DynamoDB 中。其他应用程序可以从 Kinesis 数据流中消费交易数据。",
      "D": "将批处理的交易数据作为文件存储在 Amazon S3 中。使用 AWS Lambda 处理每个文件并删除敏感数据，然后再更新 Amazon S3 中的文件。Lambda 函数随后将数据存储在 Amazon DynamoDB 中。其他应用程序可以消费存储在 Amazon S3 中的交易文件。"
    },
    "tags": [
      "Amazon Kinesis",
      "Kinesis Data Streams",
      "AWS Lambda",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 86%），解析仅供参考。】\n\n本题考查近实时数据处理架构，涉及 Kinesis Data Streams、Lambda、DynamoDB 的集成，以及数据安全与可扩展性的考虑。同时，也考察了对不同数据流服务（Kinesis Data Streams vs. Kinesis Data Firehose）的理解与应用场景对比。",
      "why_correct": "Kinesis Data Streams 能够以近实时的方式处理大规模数据流。方案中，交易数据首先被流式传输到 Kinesis Data Streams，然后通过 Lambda 函数进行处理，实现敏感数据移除。处理后的数据被存储在 DynamoDB 中，满足低延迟检索的需求。其他应用程序可以从 Kinesis Data Streams 中消费数据，满足数据共享需求。这种架构兼顾了近实时性、可扩展性，并支持数据处理。",
      "why_wrong": "选项 A 错误，因为 DynamoDB 本身没有内置的数据清洗功能，通过设置 DynamoDB 规则来删除敏感数据在实现上不可行，且可能影响数据库性能。 DynamoDB Streams 主要用于捕获数据的变更，不适合用于数据清洗。 选项 B 错误，Kinesis Data Firehose 适用于将数据流写入特定目标，如 S3、DynamoDB 等，但其数据处理能力不如 Kinesis Data Streams 灵活。虽然可以使用 Lambda 集成来删除敏感数据，但同时写入 DynamoDB 和 S3 的方案增加了复杂性，且 Kinesis Data Firehose 的数据处理延迟通常高于 Kinesis Data Streams，不完全满足近实时性需求。选项 D 错误，该方案采用批处理的方式处理数据，并不满足近实时处理的需求。虽然 Lambda 可以用来处理 S3 中的文件，但更新 S3 中的文件，再存到 DynamoDB，增加了额外的 I/O 操作，且效率低于方案 C。"
    },
    "related_terms": [
      "Kinesis Data Streams",
      "AWS Lambda",
      "Amazon DynamoDB",
      "DynamoDB Streams",
      "Kinesis Data Firehose",
      "Amazon S3",
      "S3",
      "Amazon Kinesis"
    ]
  },
  {
    "id": 34,
    "topic": "1",
    "question_en": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, and security, the company must track configuration changes on its AWS resources and record a history of API calls made to these resources. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.",
      "B": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.",
      "C": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API calls.",
      "D": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API calls."
    },
    "correct_answer": "B",
    "vote_percentage": "99%",
    "question_cn": "一家公司在 AWS 上托管其多层应用程序。为了合规性、管理、审计和安全性，该公司必须跟踪其 AWS 资源上的配置更改，并记录对这些资源进行的 API 调用的历史记录。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 AWS CloudTrail 跟踪配置更改，使用 AWS Config 记录 API 调用。",
      "B": "使用 AWS Config 跟踪配置更改，使用 AWS CloudTrail 记录 API 调用。",
      "C": "使用 AWS Config 跟踪配置更改，使用 Amazon CloudWatch 记录 API 调用。",
      "D": "使用 AWS CloudTrail 跟踪配置更改，使用 Amazon CloudWatch 记录 API 调用。"
    },
    "tags": [
      "AWS Config",
      "AWS CloudTrail"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 99%），解析仅供参考。】\n\n考查 AWS Config 与 AWS CloudTrail 的作用与区别，以及它们分别适用于哪些场景，重点关注配置更改跟踪和 API 调用日志记录。同时，也考察了对不同 AWS 服务的理解，并要求能够根据题目需求进行选择。此外，还涉及了合规性、管理、审计和安全性等方面的知识点。",
      "why_correct": "AWS Config 提供了配置跟踪的功能，可以捕获资源配置随时间的变化，满足了跟踪配置更改的要求。AWS CloudTrail 记录了 AWS 账户的 API 活动，包括由用户、角色或 AWS 服务发起的 API 调用，满足了记录 API 调用的历史记录的需求。因此，结合这两个服务，可以同时满足合规性、管理、审计和安全性的需求。",
      "why_wrong": {
        "A": "AWS CloudTrail 记录 API 调用，而 AWS Config 跟踪配置更改。该选项颠倒了两者功能，导致无法满足题目要求的，配置更改跟踪和 API 调用记录都应该使用各自的正确服务。",
        "C": "AWS Config 用于跟踪配置更改，与正确选项一致。但是，Amazon CloudWatch 并不直接记录 API 调用历史记录。CloudWatch 主要用于监控和日志记录，但其记录的日志侧重于应用程序和系统级别的指标，无法满足 API 调用记录的需求。",
        "D": "AWS CloudTrail 记录 API 调用，与正确选项一致。但是，AWS CloudTrail 无法跟踪配置更改。该选项无法满足跟踪配置更改的需求，而 CloudTrail 在本题中只能满足 API 调用记录的需求。"
      }
    },
    "related_terms": [
      "AWS Config",
      "AWS CloudTrail",
      "Amazon CloudWatch"
    ]
  },
  {
    "id": 35,
    "topic": "1",
    "question_en": "A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
    "options_en": {
      "A": "Enable Amazon GuardDuty on the account.",
      "B": "Enable Amazon Inspector on the EC2 instances.",
      "C": "Enable AWS Shield and assign Amazon Route 53 to it.",
      "D": "Enable AWS Shield Advanced and assign the ELB to it."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正准备在 AWS 云中发布面向公众的 Web 应用程序。该架构由 VPC 内的 Amazon EC2 实例组成，这些实例位于 Elastic Load Balancer (ELB) 之后。第三方服务用于 DNS。公司的解决方案架构师必须推荐一种解决方案来检测并防御大规模 DDoS 攻击。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "在账户上启用 Amazon GuardDuty。",
      "B": "在 EC2 实例上启用 Amazon Inspector。",
      "C": "启用 AWS Shield 并为其分配 Amazon Route 53。",
      "D": "启用 AWS Shield Advanced 并为其分配 ELB。"
    },
    "tags": [
      "AWS Shield",
      "DDoS",
      "ELB",
      "Route 53",
      "Amazon EC2",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察防御大规模 DDoS 攻击的解决方案选择，与 AWS Shield 和 ELB 的集成相关。",
      "why_correct": "AWS Shield Advanced 专为防御针对 AWS 资源的 DDoS 攻击而设计，提供了高级 DDoS 保护功能。将 Shield Advanced 与 ELB 集成，可以针对 HTTP/HTTPS 流量的 DDoS 攻击提供更精细的保护，并支持自动缓解。它还提供了针对应用程序层攻击的保护，并支持 24x7 的专家支持。",
      "why_wrong": "选项 A，Amazon GuardDuty 主要用于威胁检测，而非 DDoS 攻击的防御，它侧重于恶意活动和账户安全方面的监控，不能直接应对大规模 DDoS 攻击。选项 B，Amazon Inspector 主要用于评估 EC2 实例的安全状况，检测软件漏洞和安全配置，与 DDoS 攻击防御无关。选项 C，虽然 AWS Shield 提供基础 DDoS 保护，但题目要求是防御大规模 DDoS 攻击。基础版 Shield 的保护能力有限，并且虽然可以与 Route 53 配合使用，但无法满足大规模 DDoS 攻击的防御需求。此外，由于架构中使用的是 ELB，Route 53 不是主要入口点，Shield Advanced 针对 ELB 的保护才是更合适的选择。"
    },
    "related_terms": [
      "Amazon EC2",
      "ELB",
      "DDoS",
      "VPC",
      "Amazon GuardDuty",
      "Amazon Inspector",
      "AWS Shield",
      "Route 53",
      "AWS Shield Advanced"
    ]
  },
  {
    "id": 36,
    "topic": "1",
    "question_en": "A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.",
      "B": "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.",
      "C": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.",
      "D": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets."
    },
    "correct_answer": "B",
    "vote_percentage": "56%",
    "question_cn": "一家公司正在 AWS 云中构建应用程序。该应用程序将数据存储在两个 AWS 区域的 Amazon S3 存储桶中。该公司必须使用 AWS Key Management Service (AWS KMS) 客户托管密钥来加密存储在 S3 存储桶中的所有数据。两个 S3 存储桶中的数据都必须使用相同的 KMS 密钥进行加密和解密。数据和密钥必须存储在这两个区域中的每一个区域。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在每个区域创建一个 S3 存储桶。配置 S3 存储桶以使用带 Amazon S3 托管加密密钥 (SSE-S3) 的服务器端加密。配置 S3 存储桶之间的复制。",
      "B": "创建客户托管的多区域 KMS 密钥。在每个区域创建一个 S3 存储桶。配置 S3 存储桶之间的复制。配置应用程序以使用具有客户端加密的 KMS 密钥。",
      "C": "在每个区域创建一个客户托管的 KMS 密钥和一个 S3 存储桶。配置 S3 存储桶以使用带 Amazon S3 托管加密密钥 (SSE-S3) 的服务器端加密。配置 S3 存储桶之间的复制。",
      "D": "在每个区域创建一个客户托管的 KMS 密钥和一个 S3 存储桶。配置 S3 存储桶以使用带 AWS KMS 密钥 (SSE-KMS) 的服务器端加密。配置 S3 存储桶之间的复制。"
    },
    "tags": [
      "Amazon S3",
      "AWS KMS",
      "S3 Replication",
      "SSE-S3",
      "SSE-KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 56%），解析仅供参考。】\n\n考查了使用 KMS 客户托管密钥加密 S3 存储桶数据的方案选择，以及多区域 KMS 密钥的应用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 提供了最低的运营开销，满足了所有需求。它使用客户托管的多区域 KMS 密钥，确保了两个区域使用相同的密钥。通过配置 S3 存储桶之间的复制，实现了数据的跨区域冗余。应用程序使用客户端加密可以灵活地处理密钥。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，因为它没有使用客户托管密钥来满足题目要求。它使用了 SSE-S3，这是一种由 Amazon S3 托管的加密方式。选项 C 错误，因为它要求在每个区域创建单独的 KMS 密钥，这不符合“两个 S3 存储桶中的数据都必须使用相同的 KMS 密钥”的要求。选项 D 错误，虽然它使用了客户托管密钥，但没有使用多区域密钥，这增加了密钥管理的复杂性。而且，SSE-KMS 虽然可以使用客户托管密钥，但客户端加密更灵活，能够控制更多的加密流程。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)",
      "S3",
      "KMS",
      "SSE-S3",
      "SSE-KMS",
      "Multi-region KMS key",
      "Server-Side Encryption",
      "Customer Managed Key",
      "Client-side Encryption"
    ]
  },
  {
    "id": 37,
    "topic": "1",
    "question_en": "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
      "B": "Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
      "C": "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.",
      "D": "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司最近在其 AWS 账户中的 Amazon EC2 实例上启动了各种新的工作负载。该公司需要创建一个策略来远程、安全地访问和管理这些实例。该公司需要实施一个可重复的过程，该过程使用原生的 AWS 服务并遵循 AWS 良好的架构框架。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 EC2 串行控制台直接访问每个实例的终端接口以进行管理。",
      "B": "将适当的 IAM 角色附加到每个现有实例和新实例。使用 AWS Systems Manager Session Manager 建立远程 SSH 会话。",
      "C": "创建一个管理 SSH 密钥对。将公钥加载到每个 EC2 实例中。在公共子网中部署一个堡垒主机，为每个实例的管理提供一个隧道。",
      "D": "建立 AWS Site-to-Site VPN 连接。指示管理员使用其本地的本地机器通过 VPN 隧道使用 SSH 密钥直接连接到实例。"
    },
    "tags": [
      "EC2",
      "IAM",
      "AWS Systems Manager",
      "Session Manager",
      "SSH",
      "VPN",
      "Bastion Host"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考查通过原生 AWS 服务安全、远程访问 EC2 实例。涉及 IAM 角色、AWS Systems Manager Session Manager、SSH 密钥对、堡垒机（Bastion Host）、VPN 等多种访问方式的选型与对比。",
      "why_correct": "AWS Systems Manager Session Manager 提供了基于 IAM 角色的安全、可审计的实例访问方式，无需公网 IP 或 SSH 密钥管理。将 IAM 角色附加到 EC2 实例，允许用户通过 AWS Management Console 或 AWS CLI 安全地建立 SSH 或其他 shell 会话。这种方法简化了访问管理，减少了运营开销，并符合 AWS 良好的架构实践。",
      "why_wrong": "A. 使用 EC2 串行控制台需要配置，并且仅适用于紧急情况下的故障排除，不适用于日常远程管理，且缺乏审计功能。\nC. 创建 SSH 密钥对和堡垒机增加了管理复杂性，需要维护密钥对的安全性和堡垒机的可用性。堡垒机通常部署在公共子网，增加了安全风险。\nD. 建立 VPN 连接虽然安全，但需要配置和维护 VPN 连接，且需要管理员配置本地 SSH 客户端，增加了运营开销。此外，这种方式增加了管理员的配置复杂度和潜在的安全风险。"
    },
    "related_terms": [
      "EC2",
      "IAM",
      "AWS Systems Manager",
      "SSH",
      "VPN",
      "AWS CLI",
      "Amazon EC2",
      "Session Manager",
      "Bastion Host",
      "AWS Management Console"
    ]
  },
  {
    "id": 38,
    "topic": "1",
    "question_en": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for DNS. The website is experiencing increased demand from around the world. The company must decrease latency for users who access the website. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 geolocation routing entries.",
      "B": "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with the S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.",
      "C": "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries to point to the CloudFront distribution.",
      "D": "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the new endpoint."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Amazon S3 上托管静态网站，并使用 Amazon Route 53 进行 DNS。该网站正经历来自世界各地日益增长的需求。公司必须减少用户访问网站的延迟。哪种解决方案以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将包含网站的 S3 存储桶复制到所有 AWS 区域。添加 Route 53 地理位置路由条目。",
      "B": "在 AWS Global Accelerator 中配置加速器。将提供的 IP 地址与 S3 存储桶关联。编辑 Route 53 条目以指向加速器的 IP 地址。",
      "C": "在 S3 存储桶前面添加一个 Amazon CloudFront 分发。编辑 Route 53 条目以指向 CloudFront 分发。",
      "D": "在存储桶上启用 S3 Transfer Acceleration。编辑 Route 53 条目以指向新的终端节点。"
    },
    "tags": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查通过使用 Amazon CloudFront 提高静态网站性能。与 CDN (Content Delivery Network, 内容分发网络) 的选型和 S3 存储桶、Route 53 的结合使用相关。",
      "why_correct": "选项 C 是最佳解决方案。Amazon CloudFront 是一种内容分发网络（CDN），可以缓存 S3 存储桶中的内容，并将内容分发到全球边缘站点，从而减少用户访问网站的延迟。将 Route 53 的 DNS 记录指向 CloudFront 分发，可以确保用户访问到最近的边缘站点，进一步提升性能。这种方式相对经济高效，能够满足题目提出的要求。",
      "why_wrong": "选项 A 成本高且维护复杂。将 S3 存储桶复制到所有 AWS 区域，需要维护多个存储桶副本，增加存储成本和管理复杂性。虽然地理位置路由可以根据用户位置将流量定向到最近的存储桶，但无法保证所有区域都有存储桶可用，从而可能导致性能下降。\n选项 B 成本相对较高，且可能存在过度配置。AWS Global Accelerator 主要用于加速需要静态 IP 地址和全局流量管理的应用程序。虽然可以加速访问 S3，但其价格高于 CloudFront，并且对于简单静态网站来说，Global Accelerator 的功能可能过于强大，增加了不必要的成本。\n选项 D 仅对文件上传下载有加速效果。S3 Transfer Acceleration 主要针对上传和下载到 S3 的数据进行加速，对静态网站的访问延迟优化效果有限，无法有效解决全球用户的访问延迟问题，并且不能完全满足题目的需求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon Route 53",
      "S3",
      "CDN",
      "AWS Global Accelerator",
      "S3 Transfer Acceleration"
    ]
  },
  {
    "id": 39,
    "topic": "1",
    "question_en": "A company maintains a searchable repository of items on its website. The data is stored in an Amazon RDS for MySQL database table that contains more than 10 million rows. The database has 2 TB of General Purpose SSD storage. There are millions of updates against this data every day through the company's website. The company has noticed that some insert operations are taking 10 seconds or longer. The company has determined that the database storage performance is the problem. Which solution addresses this performance issue?",
    "options_en": {
      "A": "Change the storage type to Provisioned IOPS SSD.",
      "B": "Change the DB instance to a memory optimized instance class.",
      "C": "Change the DB instance to a burstable performance instance class.",
      "D": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication."
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一家公司在其网站上维护一个可搜索的物品存储库。数据存储在 Amazon RDS for MySQL 数据库表中，该表包含超过 1000 万行。数据库具有 2 TB 的通用 SSD 存储。每天通过公司的网站对这些数据进行数百万次更新。该公司注意到一些插入操作需要 10 秒或更长时间。该公司已确定数据库存储性能是问题所在。哪个解决方案解决了此性能问题？",
    "options_cn": {
      "A": "将存储类型更改为预置 IOPS SSD。",
      "B": "将数据库实例更改为内存优化实例类。",
      "C": "将数据库实例更改为可突增性能实例类。",
      "D": "使用 MySQL 本机异步复制启用多可用区 RDS 读副本。"
    },
    "tags": [
      "Amazon RDS",
      "RDS for MySQL",
      "Performance Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n本题考察了 RDS for MySQL 数据库的存储性能优化策略，重点关注了存储类型选择对性能的影响。",
      "why_correct": "将存储类型更改为预置 IOPS SSD (Provisioned IOPS SSD) 可以提升数据库的存储性能。预置 IOPS SSD 提供了对 IOPS 的控制，可以针对需要高 I/O 性能的工作负载进行优化，从而减少插入操作的延迟，满足题目对性能提升的需求。",
      "why_wrong": "将数据库实例更改为内存优化实例类 (Memory-optimized instance class) 优化的是内存资源，对存储 I/O 性能的提升有限，不能直接解决存储性能瓶颈。将数据库实例更改为可突增性能实例类 (Burstable Performance instance class) 适用于对性能要求不高的场景，无法满足每天数百万次更新的需求。使用 MySQL 本机异步复制启用多可用区 RDS 读副本 (Multi-AZ RDS Read Replicas) 主要用于提升读取性能和可用性，不能解决写入操作的性能问题。"
    },
    "related_terms": [
      "Amazon RDS for MySQL",
      "MySQL",
      "SSD",
      "Provisioned IOPS SSD",
      "IOPS",
      "Memory-optimized instance class",
      "Burstable Performance instance class",
      "Multi-AZ RDS Read Replicas"
    ]
  },
  {
    "id": 40,
    "topic": "1",
    "question_en": "A company has thousands of edge devices that collectively generate 1 TB of status alerts each day. Each alert is approximately 2 KB in size. A solutions architect needs to implement a solution to ingest and store the alerts for future analysis. The company wants a highly available solution. However, the company needs to minimize costs and does not want to manage additional infrastructure. Additionally, the company wants to keep 14 days of data available for immediate analysis and archive any data older than 14 days. What is the MOST operationally eficient solution that meets these requirements?",
    "options_en": {
      "A": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
      "B": "Launch Amazon EC2 instances across two Availability Zones and place them behind an Elastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will store the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition data to Amazon S3 Glacier after 14 days.",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the Kinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon Elasticsearch Service) cluster to take manual snapshots every day and delete data from the cluster that is older than 14 days.",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the alerts, and set the message retention period to 14 days. Configure consumers to poll the SQS queue, check the age of the message, and analyze the message data as needed. If the message is 14 days old, the consumer should copy the message to an Amazon S3 bucket and delete the message from the SQS queue."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司拥有数千个边缘设备，这些设备每天总共生成 1 TB 的状态警报。每个警报大约 2 KB 大小。一位解决方案架构师需要实施一个解决方案来摄取和存储警报以供将来分析。该公司需要一个高可用性解决方案。但是，该公司需要最大限度地降低成本，并且不想管理额外的基础设施。此外，该公司希望保留 14 天的数据以供立即分析，并归档任何超过 14 天的数据。哪种解决方案在运营上最有效，能够满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Kinesis Data Firehose 传输流来摄取警报。配置 Kinesis Data Firehose 流将警报传输到 Amazon S3 存储桶。设置一个 S3 生命周期配置，在 14 天后将数据转换为 Amazon S3 Glacier。",
      "B": "跨两个可用区启动 Amazon EC2 实例，并将它们置于 Application Load Balancer (ALB) 后面以摄取警报。在 EC2 实例上创建一个脚本，该脚本会将警报存储在 Amazon S3 存储桶中。设置一个 S3 生命周期配置，在 14 天后将数据转换为 Amazon S3 Glacier。",
      "C": "创建一个 Amazon Kinesis Data Firehose 传输流来摄取警报。配置 Kinesis Data Firehose 流将警报传输到 Amazon OpenSearch Service（Amazon Elasticsearch Service）集群。设置 Amazon OpenSearch Service（Amazon Elasticsearch Service）集群，每天拍摄手动快照，并从集群中删除超过 14 天的数据。",
      "D": "创建一个 Amazon Simple Queue Service (Amazon SQS) 标准队列来摄取警报，并将消息保留期设置为 14 天。配置使用者轮询 SQS 队列，检查消息的年龄，并根据需要分析消息数据。如果消息已满 14 天，使用者应将消息复制到 Amazon S3 存储桶，然后从 SQS 队列中删除该消息。"
    },
    "tags": [
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "S3 Lifecycle",
      "Amazon S3 Glacier",
      "Cost Optimization",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n本题考察了高可用性、成本优化、数据摄取和存储方案的选择，以及数据生命周期管理。需要根据业务需求，选择合适的 AWS 服务组合，并理解不同服务在性能、成本和管理复杂性方面的差异。与 S3、Kinesis Data Firehose、EC2、SQS 等服务的选型和对比相关。",
      "why_correct": "Amazon Kinesis Data Firehose 提供了一种完全托管的数据摄取服务，可以可靠地将数据传输到 Amazon S3。将 Kinesis Data Firehose 与 S3 结合使用，能够满足题目中对于高可用性、无需管理基础设施、成本效益的需求。 通过 S3 生命周期配置，可以自动将 14 天后的数据转换为 Amazon S3 Glacier，满足了数据归档的要求。 这种方案在运营上高效，因为它减少了管理开销并降低了成本。",
      "why_wrong": "选项 B 涉及使用 EC2 实例，这需要手动管理服务器，增加了运营复杂性和成本，与题目中“不想管理额外的基础设施”的要求相悖。虽然 ALB 提供了高可用性，但 EC2 实例本身的管理会带来额外的开销。 选项 C 使用 Amazon OpenSearch Service（Amazon Elasticsearch Service）来存储数据， OpenSearch Service 适用于数据分析，但其成本通常高于 S3 存储。此外，每天手动拍摄快照并删除数据的方式不如 S3 的生命周期策略自动化。 选项 D 涉及使用 Amazon SQS，SQS 主要用于消息队列，而不是长期存储大量数据，且消息保留时间有限制（最长 14 天），超过 14 天需要额外逻辑处理，增加了复杂性。 从 SQS 复制到 S3 的方案增加了额外的管理工作量，并且无法有效利用 SQS 消息本身的特性。"
    },
    "related_terms": [
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "S3",
      "Amazon S3 Glacier",
      "Amazon EC2",
      "Amazon SQS",
      "Lambda",
      "EC2",
      "EBS",
      "Application Load Balancer (ALB)",
      "Amazon OpenSearch Service (Amazon Elasticsearch Service)"
    ]
  },
  {
    "id": 41,
    "topic": "1",
    "question_en": "A company's application integrates with multiple software-as-a-service (SaaS) sources for data collection. The company runs Amazon EC2 instances to receive the data and to upload the data to an Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also sends a notification to the user when an upload is complete. The company has noticed slow application performance and wants to improve the performance as much as possible. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
      "B": "Create an Amazon AppFlow fiow to transfer data between each SaaS source and the S3 bucket. Configure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete.",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send output data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud Watch Events) rule to send events when the upload to the S3 bucket is complete. Configure an Amazon Simple Notification Service (Amazon SNS) topic as the second rule's target.",
      "D": "Create a Docker container to use instead of an EC2 instance. Host the containerized application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to the S3 bucket is complete."
    },
    "correct_answer": "B",
    "vote_percentage": "68%",
    "question_cn": "一家公司的应用程序与多个软件即服务 (SaaS) 源集成，用于数据收集。该公司运行 Amazon EC2 实例来接收数据并将数据上传到 Amazon S3 存储桶以进行分析。接收和上传数据的同一 EC2 实例也会在上传完成后向用户发送通知。该公司注意到应用程序性能缓慢，并希望尽可能提高性能。哪种解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Auto Scaling 组，以便 EC2 实例可以横向扩展。配置 S3 事件通知，以便在上传到 S3 存储桶完成后，将事件发送到 Amazon Simple Notification Service (Amazon SNS) 主题。",
      "B": "创建一个 Amazon AppFlow 流，用于在每个 SaaS 源和 S3 存储桶之间传输数据。配置 S3 事件通知，以便在上传到 S3 存储桶完成后，将事件发送到 Amazon Simple Notification Service (Amazon SNS) 主题。",
      "C": "为每个 SaaS 源创建一个 Amazon EventBridge (Amazon CloudWatch Events) 规则，以发送输出数据。将 S3 存储桶配置为该规则的目标。创建第二个 EventBridge (CloudWatch Events) 规则，以便在上传到 S3 存储桶完成后发送事件。将 Amazon Simple Notification Service (Amazon SNS) 主题配置为第二个规则的目标。",
      "D": "创建一个 Docker 容器来代替 EC2 实例。在 Amazon Elastic Container Service (Amazon ECS) 上托管容器化应用程序。配置 Amazon CloudWatch Container Insights，以便在上传到 S3 存储桶完成后，将事件发送到 Amazon Simple Notification Service (Amazon SNS) 主题。"
    },
    "tags": [
      "Amazon S3",
      "Amazon AppFlow",
      "Amazon SNS",
      "Amazon EC2",
      "SaaS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 68%），解析仅供参考。】\n\n考察如何优化 SaaS 数据集成，提高数据传输效率和降低运营开销。本题涉及 Amazon AppFlow 的使用，以及与其他服务（如 Amazon SNS 和 Amazon S3）的集成。",
      "why_correct": "Amazon AppFlow 专门用于安全地在 SaaS 应用程序和 AWS 服务之间传输数据。它支持数据传输的自动化，并提供了预构建的连接器。在这种场景下，AppFlow 可以直接从 SaaS 源提取数据并将其上传到 S3 存储桶，无需 EC2 实例的参与。配置 S3 事件通知可以触发 Amazon SNS，实现上传完成后的通知功能，满足题目的要求，并且能最大程度地减少运营开销。 AppFlow 还提供了数据转换、过滤和掩盖等功能，可以在数据传输过程中进行处理。",
      "why_wrong": "选项 A 错误，因为在 EC2 实例上运行数据传输任务会增加计算资源消耗和管理负担。使用 Auto Scaling 虽然可以横向扩展，但并不能根本性地解决性能问题。选项 C 错误，EventBridge 主要用于事件驱动的架构，不直接用于数据传输。将 S3 存储桶配置为 EventBridge 规则的目标是不合适的，并且配置两个 EventBridge 规则增加了复杂性。选项 D 错误，虽然使用 Docker 和 ECS 可以提高应用程序的灵活性和可伸缩性，但仍然需要在 ECS 容器中运行数据传输任务，增加了运营开销和复杂性，并且 Container Insights 不直接发送事件到 SNS。相比之下，AppFlow 更适合数据集成场景，能够简化流程、降低开销。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon EC2",
      "Amazon SNS",
      "SaaS",
      "Amazon AppFlow",
      "Auto Scaling",
      "Amazon EventBridge",
      "CloudWatch Events",
      "Docker",
      "Amazon ECS",
      "Container Insights"
    ]
  },
  {
    "id": 42,
    "topic": "1",
    "question_en": "A company runs a highly available image-processing application on Amazon EC2 instances in a single VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The EC2 instances do not communicate with each other. However, the EC2 instances download images from Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is concerned about data transfer charges. What is the MOST cost-effective way for the company to avoid Regional data transfer charges?",
    "options_en": {
      "A": "Launch the NAT gateway in each Availability Zone.",
      "B": "Replace the NAT gateway with a NAT instance.",
      "C": "Deploy a gateway VPC endpoint for Amazon S3.",
      "D": "Provision an EC2 Dedicated Host to run the EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "99%",
    "question_cn": "一家公司在单个 VPC 中的 Amazon EC2 实例上运行一个高可用性图像处理应用程序。 EC2 实例在多个可用区中的几个子网内运行。 EC2 实例之间不相互通信。 但是，EC2 实例通过单个 NAT 网关从 Amazon S3 下载图像并将图像上传到 Amazon S3。该公司担心数据传输费用。该公司避免区域数据传输费用的最具成本效益的方法是什么？",
    "options_cn": {
      "A": "在每个可用区启动 NAT 网关。",
      "B": "用 NAT 实例替换 NAT 网关。",
      "C": "为 Amazon S3 部署一个网关 VPC endpoint。",
      "D": "预置一个 EC2 专用主机以运行 EC2 实例。"
    },
    "tags": [
      "Amazon S3",
      "VPC endpoint",
      "NAT Gateway",
      "Data Transfer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 99%），解析仅供参考。】\n\n本题考查了降低 Amazon S3 数据传输成本的方法，与 VPC endpoint、NAT Gateway 以及数据传输费用相关。需要理解不同选项的成本和适用场景。",
      "why_correct": "为 Amazon S3 部署一个网关 VPC endpoint 是最具成本效益的方法。网关 VPC endpoint 允许 EC2 实例通过 VPC 内部的连接访问 S3，而无需经过 NAT 网关。由于数据传输发生在同一区域内，因此不会产生区域数据传输费用。这种方法避免了 NAT 网关的数据处理费用和数据传输费用，从而降低了总成本。",
      "why_wrong": "A. 在每个可用区启动 NAT 网关会增加 NAT 网关的部署和维护成本，并不能解决数据传输费用问题，因为数据仍然需要通过 NAT 网关访问 S3。B. 用 NAT 实例替换 NAT 网关虽然可能降低 NAT 实例的费用，但是仍然需要支付数据传输费用。此外，NAT 实例的可用性不如 NAT 网关。D. 预置一个 EC2 专用主机并不能降低数据传输费用，因为它只影响 EC2 实例的部署方式和计算成本，而与 S3 的数据传输费用无关。专用主机无法解决数据传输费用问题，并且会增加 EC2 实例的部署成本。"
    },
    "related_terms": [
      "Amazon S3",
      "EC2",
      "VPC",
      "NAT Gateway",
      "Availability Zone",
      "VPC endpoint",
      "NAT Instance",
      "Data Transfer",
      "EC2 Dedicated Host"
    ]
  },
  {
    "id": 43,
    "topic": "1",
    "question_en": "A company has an on-premises application that generates a large amount of time-sensitive data that is backed up to Amazon S3. The application has grown and there are user complaints about internet bandwidth limitations. A solutions architect needs to design a long-term solution that allows for both timely backups to Amazon S3 and with minimal impact on internet connectivity for internal users. Which solution meets these requirements?",
    "options_en": {
      "A": "Establish AWS VPN connections and proxy all trafic through a VPC gateway endpoint.",
      "B": "Establish a new AWS Direct Connect connection and direct backup trafic through this new connection.",
      "C": "Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices to AWS each day.",
      "D": "Submit a support ticket through the AWS Management Console. Request the removal of S3 service limits from the account."
    },
    "correct_answer": "B",
    "vote_percentage": "99%",
    "question_cn": "一家公司有一个本地应用程序，该应用程序会生成大量时间敏感的数据，这些数据会被备份到 Amazon S3。应用程序不断增长，用户抱怨互联网带宽限制。一个解决方案架构师需要设计一个长期解决方案，该方案既能及时备份到 Amazon S3，又能最大限度地减少对内部用户互联网连接的影响。以下哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "建立 AWS VPN 连接，并通过 VPC 网关终端节点代理所有流量。",
      "B": "建立一个新的 AWS Direct Connect 连接，并将备份流量定向到这个新连接。",
      "C": "每天订购 AWS Snowball 设备。将数据加载到 Snowball 设备上，然后每天将设备返回到 AWS。",
      "D": "通过 AWS Management Console 提交支持工单。请求从账户中删除 S3 服务限制。"
    },
    "tags": [
      "Amazon S3",
      "AWS Direct Connect",
      "VPN",
      "VPC",
      "Snowball",
      "AWS Snowball",
      "Internet bandwidth"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 99%），解析仅供参考。】\n\n考查如何通过优化网络连接来提高数据备份到 Amazon S3 的效率，并减少对本地网络的影响。涉及网络连接方式的选择，与 VPN、Direct Connect、Snowball 及网络带宽限制相关。",
      "why_correct": "建立一个新的 AWS Direct Connect 连接，并将备份流量定向到这个新连接。Direct Connect 提供了专用的网络连接，绕过了公共互联网，从而提供比互联网更稳定、更快速、更可靠的传输，且不占用内部用户互联网带宽。这满足了题目中既要及时备份，又要减少对内部用户互联网连接的影响的要求。通过 Direct Connect，可以实现对S3的大量数据备份。",
      "why_wrong": "选项 A，建立 AWS VPN 连接，并通过 VPC 网关终端节点代理所有流量。虽然 VPN 提供了加密连接，但仍然依赖于公共互联网。VPC 网关终端节点是为了访问 VPC 内部的服务，与题目的目标无关。选项 C，每天订购 AWS Snowball 设备。将数据加载到 Snowball 设备上，然后每天将设备返回到 AWS。Snowball 适用于大量数据的离线迁移，但每天都依赖物理设备交换，时间成本过高，无法满足时间敏感数据的备份需求。选项 D，通过 AWS Management Console 提交支持工单。请求从账户中删除 S3 服务限制。这种方法无法解决互联网带宽限制问题，并且 S3 服务限制与互联网带宽限制是两个不同的概念，即便解除了 S3 限制，也不能解决网络拥堵的问题。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS Direct Connect",
      "VPN",
      "VPC",
      "Snowball",
      "AWS Snowball",
      "AWS Management Console",
      "Internet bandwidth"
    ]
  },
  {
    "id": 44,
    "topic": "1",
    "question_en": "A company has an Amazon S3 bucket that contains critical data. The company must protect the data from accidental deletion. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Enable versioning on the S3 bucket.",
      "B": "Enable MFA Delete on the S3 bucket.",
      "C": "Create a bucket policy on the S3 bucket.",
      "D": "Enable default encryption on the S3 bucket",
      "E": "Create a lifecycle policy for the objects in the S3 bucket."
    },
    "correct_answer": "AB",
    "vote_percentage": "98%",
    "question_cn": "一家公司有一个包含关键数据的 Amazon S3 存储桶。该公司必须保护数据免受意外删除。 解决方案架构师应该采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 S3 存储桶上启用版本控制。",
      "B": "在 S3 存储桶上启用 MFA Delete。",
      "C": "在 S3 存储桶上创建存储桶策略。",
      "D": "在 S3 存储桶上启用默认加密。",
      "E": "为 S3 存储桶中的对象创建生命周期策略。"
    },
    "tags": [
      "Amazon S3",
      "MFA Delete",
      "S3 Versioning",
      "S3 Bucket Policy",
      "S3 Default Encryption",
      "S3 Lifecycle Policies"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 98%），解析仅供参考。】\n\n考查保护 Amazon S3 存储桶中数据免受意外删除的解决方案。需要选择能够防止数据丢失的配置组合。",
      "why_correct": "选项 A，在 S3 存储桶上启用版本控制，可以保存对象的多个版本，即使对象被删除或覆盖，也能恢复到之前的版本。选项 B，启用 MFA Delete，需要通过多因素身份验证才能永久删除对象，增加了额外的安全层，防止未经授权的删除操作。",
      "why_wrong": "选项 C，创建存储桶策略，主要用于控制对 S3 存储桶的访问权限，而不能直接防止意外删除。选项 D，启用默认加密，主要用于保护存储在 S3 存储桶中的数据的机密性，与防止意外删除无关。选项 E，为 S3 存储桶中的对象创建生命周期策略，主要用于管理对象的存储类或删除对象，虽然可以用于删除对象，但并非主要用于防止意外删除，而且删除动作并非基于安全考虑。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "MFA Delete",
      "Versioning",
      "Bucket policy",
      "Default encryption",
      "Lifecycle policy"
    ]
  },
  {
    "id": 45,
    "topic": "1",
    "question_en": "A company has a data ingestion workfiow that consists of the following: • An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data deliveries • An AWS Lambda function to process the data and record metadata The company observes that the ingestion workfiow fails occasionally because of network connectivity issues. When such a failure occurs, the Lambda function does not ingest the corresponding data unless the company manually reruns the job. Which combination of actions should a solutions architect take to ensure that the Lambda function ingests all data in the future? (Choose two.)",
    "options_en": {
      "A": "Deploy the Lambda function in multiple Availability Zones.",
      "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.",
      "C": "Increase the CPU and memory that are allocated to the Lambda function.",
      "D": "Increase provisioned throughput for the Lambda function",
      "E": "Modify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家公司有一个数据摄取工作流，包括： • 用于关于新数据交付通知的 Amazon Simple Notification Service (Amazon SNS) 主题 • 用于处理数据和记录元数据的 AWS Lambda 函数。该公司观察到，由于网络连接问题，摄取工作流偶尔会失败。当发生此类故障时，除非公司手动重新运行该作业，否则 Lambda 函数不会摄取相应的数据。解决方案架构师应该采取哪些组合操作来确保 Lambda 函数将来摄取所有数据？（选择两个。）",
    "options_cn": {
      "A": "将 Lambda 函数部署在多个可用区。",
      "B": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列，并将其订阅到 SNS 主题。",
      "C": "增加分配给 Lambda 函数的 CPU 和内存。",
      "D": "增加 Lambda 函数的预置吞吐量。",
      "E": "修改 Lambda 函数以从 Amazon Simple Queue Service (Amazon SQS) 队列读取。"
    },
    "tags": [
      "Amazon SNS",
      "Amazon SQS",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n考查如何构建可靠的数据摄取工作流，以应对网络连接问题导致的偶尔失败，并确保数据不丢失；与 SNS、SQS 和 Lambda 的集成，以及 Lambda 函数的配置和触发方式相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：创建一个 Amazon SQS 队列，并将其订阅到 SNS 主题，是解决问题的关键。SNS 将消息发布到 SQS 队列，即使 Lambda 函数暂时不可用或由于网络问题导致处理失败，消息也会保留在 SQS 队列中。Lambda 函数可以稍后从 SQS 队列中读取消息，从而确保数据摄取工作的持久性和可靠性，保证了数据的最终一致性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. 将 Lambda 函数部署在多个可用区并不能解决根本问题。虽然多可用区部署提高了 Lambda 函数的可用性，但如果 Lambda 函数由于网络问题无法及时处理数据，数据仍然会丢失。C. 增加 Lambda 函数的 CPU 和内存，仅能提升处理能力，并不能解决消息丢失的问题，也不能保证消息的持久性。D. 增加 Lambda 函数的预置吞吐量与此场景无关，预置吞吐量通常用于提升 Lambda 函数并发处理能力，并非解决数据丢失问题。E. 修改 Lambda 函数以从 Amazon SQS 队列读取是正确方案的必要步骤，但单独修改并不能解决问题，需要结合选项 B 才能发挥作用。如果仅修改 Lambda 函数而没有 SQS 队列存储消息，数据仍然可能因为网络问题而丢失。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon SNS",
      "Amazon SQS",
      "Lambda",
      "AWS Lambda",
      "SNS"
    ]
  },
  {
    "id": 46,
    "topic": "1",
    "question_en": "A company has an application that provides marketing services to stores. The services are based on previous purchases by store customers. The stores upload transaction data to the company through SFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files can exceed 200 GB in size. Recently, the company discovered that some of the stores have uploaded files that contain personally identifiable information (PII) that should not have been included. The company wants administrators to be alerted if PII is shared again. The company also wants to automate remediation. What should a solutions architect do to meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in the bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain PII.",
      "B": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
      "C": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.",
      "D": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when objects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon SES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the meats that contain PII."
    },
    "correct_answer": "B",
    "vote_percentage": "60%",
    "question_cn": "一家公司有一个为商店提供营销服务的应用程序。这些服务基于商店客户之前的购买情况。商店通过 SFTP 将交易数据上传到该公司，然后处理和分析这些数据以生成新的营销优惠。某些文件的大小可能超过 200 GB。最近，该公司发现一些商店上传的文件包含不应包含的个人身份信息 (PII)。该公司希望在再次共享 PII 时提醒管理员。该公司还希望自动化补救措施。解决方案架构师应该怎么做才能以最少的开发工作量满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 存储桶作为安全传输点。使用 Amazon Inspector 扫描存储桶中的对象。如果对象包含 PII，则触发 S3 生命周期策略以删除包含 PII 的对象。",
      "B": "使用 Amazon S3 存储桶作为安全传输点。使用 Amazon Macie 扫描存储桶中的对象。如果对象包含 PII，请使用 Amazon Simple Notification Service (Amazon SNS) 向管理员发送通知，以删除包含 PII 的对象。",
      "C": "在 AWS Lambda 函数中实现自定义扫描算法。当对象加载到存储桶中时触发该函数。如果对象包含 PII，请使用 Amazon Simple Notification Service (Amazon SNS) 向管理员发送通知，以删除包含 PII 的对象。",
      "D": "在 AWS Lambda 函数中实现自定义扫描算法。当对象加载到存储桶中时触发该函数。如果对象包含 PII，请使用 Amazon Simple Email Service (Amazon SES) 向管理员发送通知，并触发 S3 生命周期策略以删除包含 PII 的对象。"
    },
    "tags": [
      "Amazon S3",
      "Amazon Macie",
      "Amazon SNS",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 60%），解析仅供参考。】\n\n此题考察数据安全和合规性。根据题目要求，需要检测并处理上传到 S3 桶中的 PII 数据。Amazon Macie 是一个数据安全服务，可以自动发现和保护敏感数据，并与 SNS 集成，当检测到 PII 时发送通知，同时使用 S3 生命周期策略进行自动补救。",
      "why_correct": "Amazon Macie 可以扫描 S3 存储桶中的对象，检测 PII。如果检测到 PII，Macie 可以触发 SNS 通知，提醒管理员采取措施。",
      "why_wrong": "选项 A 使用 Amazon Inspector，但 Amazon Inspector 专注于应用程序安全和合规性扫描，而不是 PII 检测。选项 C 和 D 需要使用自定义扫描算法和 Lambda 函数进行 PII 检测，这增加了开发工作量，违背了题目要求。"
    },
    "related_terms": [
      "Amazon S3",
      "PII",
      "Amazon Macie",
      "Amazon SNS",
      "AWS Lambda",
      "S3"
    ]
  },
  {
    "id": 47,
    "topic": "1",
    "question_en": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific AWS Region for an upcoming event that will last 1 week. What should the company do to guarantee the EC2 capacity?",
    "options_en": {
      "A": "Purchase Reserved Instances that specify the Region needed.",
      "B": "Create an On-Demand Capacity Reservation that specifies the Region needed.",
      "C": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.",
      "D": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones needed."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要在特定 AWS 区域的三个特定可用区中保证 Amazon EC2 的容量，用于即将举行的一周活动。该公司应该怎么做才能保证 EC2 容量？",
    "options_cn": {
      "A": "购买指定所需区域的预留实例。",
      "B": "创建指定所需区域的按需容量预留。",
      "C": "购买指定所需区域和三个可用区的预留实例。",
      "D": "创建指定所需区域和三个可用区的按需容量预留。"
    },
    "tags": [
      "Amazon EC2",
      "按需容量预留"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考查如何保证 EC2 的容量。按需容量预留可以在特定的可用区保证 EC2 实例的容量，满足活动期间的资源需求。",
      "why_correct": "按需容量预留允许客户在特定可用区预留 EC2 实例的容量，以满足短期或预期的需求。",
      "why_wrong": "预留实例主要用于降低成本，而不是保证容量。选项 A 和 C 无法保证在特定可用区的容量。选项 B 无法保证在三个可用区的容量。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "按需容量预留",
      "预留实例"
    ]
  },
  {
    "id": 48,
    "topic": "1",
    "question_en": "A company's website uses an Amazon EC2 instance store for its catalog of items. The company wants to make sure that the catalog is highly available and that the catalog is stored in a durable location. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Move the catalog to Amazon ElastiCache for Redis.",
      "B": "Deploy a larger EC2 instance with a larger instance store.",
      "C": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.",
      "D": "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system."
    },
    "correct_answer": "D",
    "vote_percentage": "92%",
    "question_cn": "一家公司的网站使用 Amazon EC2 实例存储来存储其商品目录。该公司希望确保目录具有高可用性，并且目录存储在持久的位置。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将目录移动到 Amazon ElastiCache for Redis。",
      "B": "部署一个更大的 EC2 实例，并使用更大的实例存储。",
      "C": "将目录从实例存储移动到 Amazon S3 Glacier Deep Archive。",
      "D": "将目录移动到 Amazon Elastic File System (Amazon EFS) 文件系统。"
    },
    "tags": [
      "Amazon ElastiCache",
      "Amazon EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 92%），解析仅供参考。】\n\n本题考察高可用性存储。EC2 实例存储是临时的，不具备持久性。ElastiCache for Redis 是一个内存中的数据存储，可以提供快速访问，并实现高可用性。Amazon EFS 是一种可扩展的，持久的，高度可用的文件存储。",
      "why_correct": "Amazon ElastiCache for Redis 可以提供快速的数据访问，并支持数据持久性，满足高可用性的需求。",
      "why_wrong": "B 选项中，实例存储是临时的，不提供持久性。C 选项中，S3 Glacier Deep Archive 主要用于归档存储，不适合用作商品目录。D 选项中，EFS 提供了高可用性，但是相比于 ElastiCache 的缓存加速，其读取性能可能不如 ElastiCache，而且对于商品目录来说，成本较高。"
    },
    "related_terms": [
      "Amazon ElastiCache",
      "ElastiCache for Redis",
      "EC2",
      "Amazon S3",
      "Amazon EFS",
      "EFS",
      "Amazon S3 Glacier Deep Archive"
    ]
  },
  {
    "id": 49,
    "topic": "1",
    "question_en": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 year of the call, but users access the files infrequently after 1 year. The company wants to optimize its solution by giving users the ability to query and retrieve files that are less than 1- year-old as quickly as possible. A delay in retrieving older files is acceptable. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the files from S3 Glacier Instant Retrieval.",
      "B": "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to S3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by using Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier Select.",
      "C": "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each archive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Instant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon S3.",
      "D": "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from Amazon RDS. Retrieve the files from S3 Glacier Deep Archive."
    },
    "correct_answer": "B",
    "vote_percentage": "69%",
    "question_cn": "一家公司按月存储通话记录文件。用户在通话后 1 年内随机访问这些文件，但 1 年后用户很少访问这些文件。该公司希望通过为用户提供尽可能快速地查询和检索不到 1 年的文件来优化其解决方案。检索较旧文件时的延迟是可以接受的。哪种解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将单个文件及其标签存储在 Amazon S3 Glacier Instant Retrieval 中。查询标签以从 S3 Glacier Instant Retrieval 检索文件。",
      "B": "将单个文件存储在 Amazon S3 Intelligent-Tiering 中。使用 S3 生命周期策略在 1 年后将文件移动到 S3 Glacier Flexible Retrieval。使用 Amazon Athena 查询和检索 Amazon S3 中的文件。使用 S3 Glacier Select 查询和检索 S3 Glacier 中的文件。",
      "C": "将单个文件及其标签存储在 Amazon S3 Standard 存储中。将每个存档的搜索元数据存储在 Amazon S3 Standard 存储中。使用 S3 生命周期策略在 1 年后将文件移动到 S3 Glacier Instant Retrieval。通过搜索来自 Amazon S3 的元数据来查询和检索文件。",
      "D": "将单个文件存储在 Amazon S3 Standard 存储中。使用 S3 生命周期策略在 1 年后将文件移动到 S3 Glacier Deep Archive。将搜索元数据存储在 Amazon RDS 中。从 Amazon RDS 查询文件。从 S3 Glacier Deep Archive 检索文件。"
    },
    "tags": [
      "Amazon S3",
      "Amazon S3 Glacier",
      "Amazon Athena",
      "Amazon S3 Glacier Select"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 69%），解析仅供参考。】\n\n考查 S3 存储类的选择和生命周期配置，以及针对不同存储类的查询与检索方案，以满足存储成本和访问性能需求。",
      "why_correct": "选项 B 提供了最具成本效益的解决方案。S3 Intelligent-Tiering 能够自动将频繁访问的文件存储在频繁访问层，1 年后，生命周期策略将不常访问的文件移动到 S3 Glacier Flexible Retrieval。Athena 能够查询 S3 中的数据，而 S3 Glacier Select 则可以查询 Glacier 中的数据，满足了对不同访问频率文件的查询需求。",
      "why_wrong": "选项 A 错误，因为 S3 Glacier Instant Retrieval 虽然检索速度快，但存储成本较高，不适合频繁访问的文件。选项 C 错误，因为 S3 Glacier Instant Retrieval 的存储成本高于 Flexible Retrieval，并且从 S3 Glacier 检索文件的成本也比 S3 Glacier Flexible Retrieval 高。选项 D 错误，因为将文件移动到 S3 Glacier Deep Archive 会导致检索延迟过长，且需要借助 RDS 来索引，增加了复杂性和成本，不符合快速检索不到 1 年内文件的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon S3 Glacier Instant Retrieval",
      "Amazon S3 Glacier Flexible Retrieval",
      "Amazon S3 Intelligent-Tiering",
      "S3 lifecycle policy",
      "Amazon Athena",
      "S3 Glacier Select",
      "Amazon S3 Standard",
      "Amazon S3 Glacier Deep Archive",
      "Amazon RDS"
    ]
  },
  {
    "id": 50,
    "topic": "1",
    "question_en": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The workload is powered by third-party software. The company needs to patch the third-party software on all EC2 instances as quickly as possible to remediate a critical security vulnerability. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an AWS Lambda function to apply the patch to all EC2 instances.",
      "B": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.",
      "C": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.",
      "D": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to all EC2 instances."
    },
    "correct_answer": "D",
    "vote_percentage": "73%",
    "question_cn": "一家公司有一个在 1,000 个 Amazon EC2 Linux 实例上运行的生产工作负载。该工作负载由第三方软件提供支持。该公司需要尽快修补所有 EC2 实例上的第三方软件，以修复关键的安全漏洞。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，将补丁应用于所有 EC2 实例。",
      "B": "配置 AWS Systems Manager Patch Manager 以将补丁应用于所有 EC2 实例。",
      "C": "安排一个 AWS Systems Manager 维护时段，将补丁应用于所有 EC2 实例。",
      "D": "使用 AWS Systems Manager Run Command 运行一个自定义命令，该命令应用补丁到所有 EC2 实例。"
    },
    "tags": [
      "AWS Systems Manager",
      "Amazon EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 73%），解析仅供参考。】\n\n此题考察 AWS Systems Manager 在批量更新 EC2 实例上的第三方软件的功能。使用 Run Command 可以直接在 EC2 实例上运行命令，快速实现补丁更新。",
      "why_correct": "AWS Systems Manager Run Command 允许用户在 EC2 实例上远程执行命令，包括安装补丁，适用于批量更新。",
      "why_wrong": "Lambda 函数虽然可以应用于 EC2 实例，但是不适用于大规模的第三方软件的快速更新。 Patch Manager 用于管理操作系统和应用程序的补丁，不是专门用来更新第三方软件。维护时段虽然可以用于更新，但是不能直接运行自定义命令。"
    },
    "related_terms": [
      "AWS Systems Manager",
      "EC2",
      "AWS Lambda",
      "Patch Manager",
      "Run Command"
    ]
  },
  {
    "id": 51,
    "topic": "1",
    "question_en": "A company is developing an application that provides order shipping statistics for retrieval by a REST API. The company wants to extract the shipping statistics, organize the data into an easy-to-read HTML format, and send the report to several email addresses at the same time every morning. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Configure the application to send the data to Amazon Kinesis Data Firehose.",
      "B": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by email.",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Glue job to query the application's API for the data.",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS Lambda function to query the application's API for the data",
      "E": "Store the application data in Amazon S3. Create an Amazon Simple Notification Service (Amazon SNS) topic as an S3 event destination to send the report by email."
    },
    "correct_answer": "BD",
    "vote_percentage": "69%",
    "question_cn": "一家公司正在开发一个应用程序，该应用程序提供订单发货统计数据，以供 REST API 检索。该公司希望提取发货统计数据，将数据组织成易于阅读的 HTML 格式，并在每天早上同一时间将报告发送到多个电子邮件地址。 解决方案架构师应采取哪些组合步骤来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "配置应用程序将数据发送到 Amazon Kinesis Data Firehose。",
      "B": "使用 Amazon Simple Email Service (Amazon SES) 格式化数据并通过电子邮件发送报告。",
      "C": "创建一个 Amazon EventBridge (Amazon CloudWatch Events) 定时事件，该事件调用 AWS Glue 作业以查询应用程序的 API 获取数据。",
      "D": "创建一个 Amazon EventBridge (Amazon CloudWatch Events) 定时事件，该事件调用 AWS Lambda 函数以查询应用程序的 API 获取数据。",
      "E": "将应用程序数据存储在 Amazon S3 中。 创建一个 Amazon Simple Notification Service (Amazon SNS) 主题作为 S3 事件目的地，通过电子邮件发送报告。"
    },
    "tags": [
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon SES"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 69%），解析仅供参考。】\n\n考查如何使用 AWS 服务自动化生成并发送邮件报告，主要涉及事件触发、数据获取、数据格式化及邮件发送等环节。",
      "why_correct": "选项 B 正确，Amazon SES 可以用于格式化数据并通过电子邮件发送报告，符合题目中邮件发送的需求。选项 D 正确，Amazon EventBridge 的定时事件可以触发 Lambda 函数，Lambda 函数可以调用应用程序的 API 获取数据，满足定时获取数据和数据源的需求。",
      "why_wrong": "选项 A 错误，Amazon Kinesis Data Firehose 主要用于数据流的近实时处理，不适用于此处批量的报告生成和邮件发送。选项 C 错误，虽然 AWS Glue 可以处理数据，但它与定时触发的关联不明确，且直接调用 API 获取数据效率较低。选项 E 错误，将数据存储在 S3 中并不能满足定时获取数据和格式化数据的需求，SNS 触发 S3 事件主要用于响应 S3 对象的变化，与题目需求不符。"
    },
    "related_terms": [
      "Amazon SES",
      "Amazon EventBridge",
      "Amazon CloudWatch Events",
      "AWS Lambda",
      "Amazon Kinesis Data Firehose",
      "AWS Glue",
      "Amazon S3",
      "Amazon SNS",
      "REST API",
      "HTML"
    ]
  },
  {
    "id": 52,
    "topic": "1",
    "question_en": "A company wants to migrate its on-premises application to AWS. The application produces output files that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be stored in a standard file system structure. The company wants a solution that scales automatically. is highly available, and requires minimum operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). Use Amazon S3 for storage.",
      "B": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Block Store (Amazon EBS) for storage.",
      "C": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) for storage.",
      "D": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon Elastic Block Store (Amazon EBS) for storage."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其本地应用程序迁移到 AWS。该应用程序生成的输出文件大小从数十 GB 到数百 TB 不等。应用程序数据必须存储在标准文件系统结构中。该公司希望找到一个可以自动扩展、高度可用且运营开销最少的解决方案。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将应用程序迁移为在 Amazon Elastic Container Service (Amazon ECS) 上以容器形式运行。使用 Amazon S3 进行存储。",
      "B": "将应用程序迁移为在 Amazon Elastic Kubernetes Service (Amazon EKS) 上以容器形式运行。使用 Amazon Elastic Block Store (Amazon EBS) 进行存储。",
      "C": "将应用程序迁移到 Multi-AZ Auto Scaling 组中的 Amazon EC2 实例。使用 Amazon Elastic File System (Amazon EFS) 进行存储。",
      "D": "将应用程序迁移到 Multi-AZ Auto Scaling 组中的 Amazon EC2 实例。使用 Amazon Elastic Block Store (Amazon EBS) 进行存储。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon EFS",
      "Amazon ECS",
      "Amazon EKS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何以最少的运营开销处理大规模数据。使用容器化方案（Amazon ECS 或 Amazon EKS）可以简化应用程序的部署和管理，而 EFS 提供了高可用性和可扩展性。",
      "why_correct": "C 选项：在 Multi-AZ Auto Scaling 组中的 EC2 实例上使用 EFS，可以提供高可用性、自动扩展和持久的存储。符合题干中自动扩展、高可用性和最少运营开销的要求。",
      "why_wrong": "A 选项：Amazon ECS 和 Amazon S3 不支持标准文件系统结构。 B 选项：Amazon EKS 增加了运营开销。D 选项：EBS 提供的存储是块存储，而不是文件系统存储。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EFS",
      "Amazon ECS",
      "Amazon EKS",
      "Multi-AZ",
      "Auto Scaling",
      "Amazon EBS",
      "Amazon S3"
    ]
  },
  {
    "id": 53,
    "topic": "1",
    "question_en": "A company needs to store its accounting records in Amazon S3. The records must be immediately accessible for 1 year and then must be archived for an additional 9 years. No one at the company, including administrative users and root users, can be able to delete the records during the entire 10-year period. The records must be stored with maximum resiliency. Which solution will meet these requirements?",
    "options_en": {
      "A": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny deletion of the records for a period of 10 years.",
      "B": "Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. After 10 years, change the IAM policy to allow deletion.",
      "C": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive after 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.",
      "D": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 years."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要将会计记录存储在 Amazon S3 中。这些记录必须可立即访问 1 年，然后必须存档 9 年。在整个 10 年期间，公司内没有人，包括管理用户和根用户，都能够删除这些记录。这些记录必须以最大的弹性进行存储。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将记录在 S3 Glacier 中存储整个 10 年。使用访问控制策略来拒绝在 10 年内删除记录。",
      "B": "使用 S3 Intelligent-Tiering 存储记录。使用 IAM 策略来拒绝删除记录。10 年后，更改 IAM 策略以允许删除。",
      "C": "使用 S3 生命周期策略在 1 年后将记录从 S3 Standard 转换为 S3 Glacier Deep Archive。在合规模式下使用 S3 Object Lock 锁定 10 年。",
      "D": "使用 S3 生命周期策略在 1 年后将记录从 S3 Standard 转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)。在管理模式下使用 S3 Object Lock 锁定 10 年。"
    },
    "tags": [
      "Amazon S3",
      "S3 Object Lock",
      "S3 Glacier Deep Archive"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查 S3 对象锁定功能在数据合规性方面的应用。S3 Object Lock 提供了防止对象被删除的功能，而 S3 生命周期策略可以在不同存储层之间自动迁移数据。",
      "why_correct": "C 选项：使用 S3 生命周期策略将数据迁移到 S3 Glacier Deep Archive 以降低存储成本。在合规模式下使用 S3 Object Lock 锁定对象 10 年，确保数据不可删除，满足合规性要求。",
      "why_wrong": "A 选项：S3 Glacier 不支持直接使用 Object Lock。B 选项：IAM 策略可以被管理用户或根用户修改，无法满足题目的需求。D 选项：S3 One Zone-IA 虽然成本更低，但其设计并非为了高持久性，并且 Object Lock 管理模式无法满足题目的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "S3 Intelligent-Tiering",
      "IAM",
      "S3 Object Lock",
      "S3 Standard",
      "S3 One Zone-Infrequent Access"
    ]
  },
  {
    "id": 54,
    "topic": "1",
    "question_en": "A company runs multiple Windows workloads on AWS. The company's employees use Windows file shares that are hosted on two Amazon EC2 instances. The file shares synchronize data between themselves and maintain duplicate copies. The company wants a highly available and durable storage solution that preserves how users currently access the files. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.",
      "B": "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.",
      "C": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ configuration. Migrate all the data to FSx for Windows File Server.",
      "D": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ configuration. Migrate all the data to Amazon EFS."
    },
    "correct_answer": "C",
    "vote_percentage": "98%",
    "question_cn": "一家公司在 AWS 上运行多个 Windows 工作负载。 公司的员工使用托管在两个 Amazon EC2 实例上的 Windows 文件共享。 文件共享在它们之间同步数据并维护重复的副本。 公司希望获得一个高可用性和持久的存储解决方案，以保留用户当前访问文件的方式。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将所有数据迁移到 Amazon S3。 设置 IAM 身份验证，供用户访问文件。",
      "B": "设置一个 Amazon S3 File Gateway。 将 S3 File Gateway 挂载在现有的 EC2 实例上。",
      "C": "将文件共享环境扩展到具有 Multi-AZ 配置的 Amazon FSx for Windows File Server。 将所有数据迁移到 FSx for Windows File Server。",
      "D": "将文件共享环境扩展到具有 Multi-AZ 配置的 Amazon Elastic File System (Amazon EFS)。 将所有数据迁移到 Amazon EFS。"
    },
    "tags": [
      "Amazon FSx for Windows File Server",
      "Amazon S3",
      "Amazon EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 98%），解析仅供参考。】\n\n本题考查如何为 Windows 文件共享提供高可用性和持久存储。Amazon FSx for Windows File Server 提供了企业级的文件共享服务，并支持 Multi-AZ 部署，满足高可用性需求。",
      "why_correct": "C 选项：FSx for Windows File Server 提供高可用性和持久的存储，能够满足 Windows 文件共享的需求。Multi-AZ 配置确保了高可用性。",
      "why_wrong": "A 选项：将文件共享迁移到 Amazon S3 需要修改应用程序代码，且 S3 并非设计用于文件共享。B 选项：S3 File Gateway 是一种混合云存储解决方案，不适合作为本地 Windows 文件共享的替代方案。 D 选项：Amazon EFS 并非针对 Windows 文件共享设计，且 EFS 主要为 Linux 操作系统提供文件存储。"
    },
    "related_terms": [
      "Amazon FSx for Windows File Server",
      "Amazon S3",
      "Amazon EFS",
      "Multi-AZ"
    ]
  },
  {
    "id": 55,
    "topic": "1",
    "question_en": "A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture will host applications that use Amazon EC2 instances and Amazon RDS DB instances. The architecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public subnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the private subnets can have access to the RDS databases. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the route table with the database subnets.",
      "B": "Create a security group that denies inbound trafic from the security group that is assigned to instances in the public subnets. Attach the security group to the DB instances.",
      "C": "Create a security group that allows inbound trafic from the security group that is assigned to instances in the private subnets. Attach the security group to the DB instances.",
      "D": "Create a new peering connection between the public subnets and the private subnets. Create a different peering connection between the private subnets and the database subnets."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在开发包含多个子网的 VPC 架构。该架构将托管使用 Amazon EC2 实例和 Amazon RDS 数据库实例的应用程序。该架构由两个可用区中的六个子网组成。每个可用区包含一个公有子网、一个私有子网和一个用于数据库的专用子网。只有在私有子网中运行的 EC2 实例才能访问 RDS 数据库。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个新的路由表，该路由表排除到公有子网 CIDR 块的路由。将该路由表与数据库子网关联。",
      "B": "创建一个安全组，该安全组拒绝来自分配给公有子网中实例的安全组的入站流量。将安全组附加到数据库实例。",
      "C": "创建一个安全组，该安全组允许来自分配给私有子网中实例的安全组的入站流量。将安全组附加到数据库实例。",
      "D": "在公有子网和私有子网之间创建一个新的对等连接。在私有子网和数据库子网之间创建不同的对等连接。"
    },
    "tags": [
      "Amazon VPC",
      "Amazon RDS",
      "安全组"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查 VPC 安全组的配置。安全组可以控制对 EC2 和 RDS 数据库实例的访问。通过配置安全组的入站规则，可以限制只有私有子网中的 EC2 实例才能访问 RDS 数据库。",
      "why_correct": "C 选项：创建安全组，并配置入站规则，允许来自私有子网中的实例的安全组的流量访问 RDS 数据库，从而满足安全要求。",
      "why_wrong": "A 选项：通过路由表控制子网之间的流量，而不是限制对数据库的访问。 B 选项：安全组拒绝来自公有子网的流量，无法确保只有私有子网中的实例可以访问数据库。D 选项：对等连接用于连接 VPC，而不是控制对数据库的访问。"
    },
    "related_terms": [
      "Amazon RDS",
      "EC2",
      "安全组",
      "CIDR",
      "私有子网",
      "Amazon VPC",
      "公有子网",
      "对等连接"
    ]
  },
  {
    "id": 56,
    "topic": "1",
    "question_en": "A company has registered its domain name with Amazon Route 53. The company uses Amazon API Gateway in the ca-central-1 Region as a public interface for its backend microservice APIs. Third-party services consume the APIs securely. The company wants to design its API Gateway URL with the company's domain name and corresponding certificate so that the third-party services can use HTTPS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain Name\" to overwrite the default URL. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM).",
      "B": "Create Route 53 DNS records with the company's domain name. Point the alias record to the Regional API Gateway stage endpoint. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region.",
      "C": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. Configure Route 53 to route trafic to the API Gateway endpoint.",
      "D": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's domain name. Import the public certificate associated with the company's domain name into AWS Certificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. Create Route 53 DNS records with the company's domain name. Point an A record to the company's domain name."
    },
    "correct_answer": "C",
    "vote_percentage": "94%",
    "question_cn": "一家公司已使用 Amazon Route 53 注册了其域名。该公司在其位于 ca-central-1 区域的 Amazon API Gateway 中用作其后端微服务 API 的公共接口。第三方服务安全地使用这些 API。该公司希望使用公司的域名及其相应的证书来设计其 API Gateway URL，以便第三方服务可以使用 HTTPS。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 API Gateway 中创建阶段变量，名称为“Endpoint-URL”，值为“公司域名”以覆盖默认 URL。将与公司域名关联的公共证书导入 AWS Certificate Manager (ACM)。",
      "B": "创建 Route 53 DNS 记录，使用公司的域名。将别名记录指向区域 API Gateway 阶段端点。将与公司域名关联的公共证书导入位于 us-east-1 区域的 AWS Certificate Manager (ACM)。",
      "C": "创建区域 API Gateway 端点。将 API Gateway 端点与公司的域名关联。将与公司域名关联的公共证书导入同一区域的 AWS Certificate Manager (ACM)。将证书附加到 API Gateway 端点。配置 Route 53 将流量路由到 API Gateway 端点。",
      "D": "创建区域 API Gateway 端点。将 API Gateway 端点与公司的域名关联。将与公司域名关联的公共证书导入位于 us-east-1 区域的 AWS Certificate Manager (ACM)。将证书附加到 API Gateway API。创建 Route 53 DNS 记录，使用公司的域名。将 A 记录指向公司的域名。"
    },
    "tags": [
      "Amazon Route 53",
      "Amazon API Gateway",
      "AWS Certificate Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 94%），解析仅供参考。】\n\n考察使用自定义域名和 HTTPS 配置 API Gateway，以及与 Route 53 和 ACM 的集成。",
      "why_correct": "选项 C 提供了完整的解决方案。它创建了区域 API Gateway 端点，并将其与公司的域名关联。将用于 HTTPS 的公共证书导入与 API Gateway 相同的区域的 ACM，并将其附加到 API Gateway 端点。最后，配置 Route 53，将流量路由到 API Gateway 端点，满足了所有要求。",
      "why_wrong": "选项 A 错误，因为阶段变量不能用于覆盖 API Gateway 的 URL，且无法实现 HTTPS 域名绑定。选项 B 错误，因为 ACM 证书必须与 API Gateway 所在的区域相同，us-east-1 区域的 ACM 证书无法在 ca-central-1 区域使用，且需要配置别名记录指向 API Gateway 的端点。选项 D 错误，因为 A 记录指向公司域名而非 API Gateway 端点，且证书的部署位置不正确（需要在同一区域）。"
    },
    "related_terms": [
      "Amazon Route 53",
      "Amazon API Gateway",
      "HTTPS",
      "AWS Certificate Manager (ACM)",
      "DNS",
      "API Gateway endpoint",
      "A record",
      "Alias record"
    ]
  },
  {
    "id": 57,
    "topic": "1",
    "question_en": "A company is running a popular social media website. The website gives users the ability to upload images to share with other users. The company wants to make sure that the images do not contain inappropriate content. The company needs a solution that minimizes development effort. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence predictions.",
      "B": "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence predictions.",
      "C": "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence predictions.",
      "D": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use ground truth to label low-confidence predictions."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在运营一个流行的社交媒体网站。该网站允许用户上传图片与其他人分享。该公司希望确保图片不包含不当内容。该公司需要一个最大限度减少开发工作量的解决方案。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Comprehend 检测不当内容。对低置信度的预测进行人工审核。",
      "B": "使用 Amazon Rekognition 检测不当内容。对低置信度的预测进行人工审核。",
      "C": "使用 Amazon SageMaker 检测不当内容。使用 ground truth 标记低置信度的预测。",
      "D": "使用 AWS Fargate 部署一个自定义机器学习模型来检测不当内容。使用 ground truth 标记低置信度的预测。"
    },
    "tags": [
      "Amazon Rekognition",
      "不当内容检测"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查使用 AWS 服务进行图片内容审核。Amazon Rekognition 可以检测图片中的不当内容，并提供置信度。低置信度的预测需要人工审核。",
      "why_correct": "B 选项：Amazon Rekognition 提供了图像内容分析的功能，可以用来检测不当内容，并提供置信度。对于低置信度的结果，人工审核是必要的。",
      "why_wrong": "A 选项：Amazon Comprehend 主要用于文本分析，不适用于图片内容审核。C 选项：Amazon SageMaker 更多用于自定义机器学习模型的构建。 D 选项：部署自定义模型增加了开发成本。"
    },
    "related_terms": [
      "Amazon Rekognition",
      "Amazon Comprehend",
      "Amazon SageMaker",
      "AWS Fargate"
    ]
  },
  {
    "id": 58,
    "topic": "1",
    "question_en": "A company wants to run its critical applications in containers to meet requirements for scalability and availability. The company prefers to focus on maintenance of the critical applications. The company does not want to be responsible for provisioning and managing the underlying infrastructure that runs the containerized workload. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use Amazon EC2 instances, and install Docker on the instances.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.",
      "C": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
      "D": "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized Amazon Machine Image (AMI)."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在容器中运行其关键应用程序，以满足可扩展性和可用性的要求。该公司倾向于专注于关键应用程序的维护。该公司不想负责配置和管理运行容器化工作负载的底层基础设施。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EC2 实例，并在这些实例上安装 Docker。",
      "B": "在 Amazon EC2 worker 节点上使用 Amazon Elastic Container Service (Amazon ECS)。",
      "C": "使用 AWS Fargate 上的 Amazon Elastic Container Service (Amazon ECS)。",
      "D": "使用来自 Amazon Elastic Container Service (Amazon ECS) 优化 Amazon Machine Image (AMI) 的 Amazon EC2 实例。"
    },
    "tags": [
      "Amazon ECS",
      "AWS Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查容器化应用程序的部署。AWS Fargate 提供了无服务器容器服务，简化了容器的部署和管理。",
      "why_correct": "C 选项：AWS Fargate 提供了无服务器容器计算，无需管理底层基础设施，符合题干中“不想负责配置和管理底层基础设施”的要求。",
      "why_wrong": "A 选项：Amazon EC2 需要管理 EC2 实例。B 选项：Amazon ECS on EC2 需要管理 EC2 worker 节点。D 选项：使用优化 AMI 的 EC2 实例仍然需要管理 EC2 实例。"
    },
    "related_terms": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon EC2"
    ]
  },
  {
    "id": 59,
    "topic": "1",
    "question_en": "A company hosts more than 300 global websites and applications. The company requires a platform to analyze more than 30 TB of clickstream data each day. What should a solutions architect do to transmit and process the clickstream data?",
    "options_en": {
      "A": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon EMR cluster with the data to generate analytics.",
      "B": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an Amazon S3 data lake for Amazon Redshift to use for analysis.",
      "C": "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is added to the S3 bucket. run an AWS Lambda function to process the data for analysis.",
      "D": "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit the data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis."
    },
    "correct_answer": "D",
    "vote_percentage": "90%",
    "question_cn": "一家公司托管了 300 多个全球网站和应用程序。该公司需要一个平台来每天分析超过 30 TB 的点击流数据。解决方案架构师应该怎么做才能传输和处理点击流数据？",
    "options_cn": {
      "A": "设计一个 AWS Data Pipeline 以将数据归档到 Amazon S3 存储桶，并使用数据运行 Amazon EMR 集群以生成分析结果。",
      "B": "创建一个 Amazon EC2 实例的 Auto Scaling 组来处理数据，并将其发送到 Amazon S3 数据湖，供 Amazon Redshift 用于分析。",
      "C": "将数据缓存在 Amazon CloudFront 中。将数据存储在 Amazon S3 存储桶中。当对象添加到 S3 存储桶时，运行一个 AWS Lambda 函数来处理数据以进行分析。",
      "D": "从 Amazon Kinesis Data Streams 收集数据。使用 Amazon Kinesis Data Firehose 将数据传输到 Amazon S3 数据湖。将数据加载到 Amazon Redshift 中进行分析。"
    },
    "tags": [
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 90%），解析仅供参考。】\n\n此题考查点击流数据的处理流程。Kinesis Data Streams 用于实时数据摄取，Kinesis Data Firehose 用于将数据传输到 S3 数据湖，最后 Redshift 用于数据分析。",
      "why_correct": "D 选项：Kinesis Data Streams 用于收集数据，Kinesis Data Firehose 用于将数据传输到 S3 数据湖，再将数据加载到 Amazon Redshift 中进行分析，满足数据处理流程的要求。",
      "why_wrong": "A 选项：AWS Data Pipeline 适合用于数据管道，但与 EMR 集成不如 Kinesis 和 Redshift 集成更适合分析。B 选项：EC2 实例处理数据，不具备可扩展性。C 选项：CloudFront 主要用于内容分发，不适合数据处理；Lambda 函数处理数据增加了复杂度。"
    },
    "related_terms": [
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon Redshift",
      "Amazon S3",
      "Amazon EMR",
      "Amazon EC2",
      "Amazon CloudFront",
      "AWS Lambda",
      "AWS Data Pipeline"
    ]
  },
  {
    "id": 60,
    "topic": "1",
    "question_en": "A company has a website hosted on AWS. The website is behind an Application Load Balancer (ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all requests to the website so that the requests will use HTTPS. What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Update the ALB's network ACL to accept only HTTPS trafic.",
      "B": "Create a rule that replaces the HTTP in the URL with HTTPS.",
      "C": "Create a listener rule on the ALB to redirect HTTP trafic to HTTPS.",
      "D": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI)."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上托管了一个网站。该网站位于 Application Load Balancer (ALB) 后面，该 ALB 配置为单独处理 HTTP 和 HTTPS。该公司希望将所有请求转发到该网站，以便请求使用 HTTPS。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "更新 ALB 的网络 ACL，仅接受 HTTPS 流量。",
      "B": "创建一个规则，将 URL 中的 HTTP 替换为 HTTPS。",
      "C": "在 ALB 上创建一个侦听器规则，将 HTTP 流量重定向到 HTTPS。",
      "D": "将 ALB 替换为配置为使用服务器名称指示 (SNI) 的 Network Load Balancer。"
    },
    "tags": [
      "Application Load Balancer",
      "Amazon Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查如何将 HTTP 请求重定向到 HTTPS。使用 Application Load Balancer，可以在侦听器规则中设置重定向规则。",
      "why_correct": "C 选项：在 ALB 上创建一个侦听器规则，将 HTTP 流量重定向到 HTTPS，可以确保所有请求都使用 HTTPS。",
      "why_wrong": "A 选项：Network ACL 仅控制网络层级的流量，不处理 HTTP 重定向。B 选项：无法替换 URL 中的协议。D 选项：Network Load Balancer 不支持 HTTP/HTTPS 协议的处理和重定向。"
    },
    "related_terms": [
      "Application Load Balancer",
      "HTTPS",
      "HTTP",
      "Network Load Balancer",
      "SNI",
      "网络 ACL",
      "侦听器"
    ]
  },
  {
    "id": 61,
    "topic": "1",
    "question_en": "A company is developing a two-tier web application on AWS. The company's developers have deployed the application on an Amazon EC2 instance that connects directly to a backend Amazon RDS database. The company must not hardcode database credentials in the application. The company must also implement a solution to automatically rotate the database credentials on a regular basis. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and instance metadata at the same time.",
      "B": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use Amazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS credentials and the credentials in the configuration file at the same time. Use S3 Versioning to ensure the ability to fall back to previous values.",
      "C": "Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for the secret. Attach the required permission to the EC2 role to grant access to the secret.",
      "D": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. Turn on automatic rotation for the encrypted parameters. Attach the required permission to the EC2 role to grant access to the encrypted parameters."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上开发一个两层 Web 应用程序。公司的开发人员已将应用程序部署在连接到后端 Amazon RDS 数据库的 Amazon EC2 实例上。公司不得在应用程序中硬编码数据库凭证。公司还必须实施一个解决方案，以定期间隔自动轮换数据库凭证。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将数据库凭证存储在实例元数据中。使用 Amazon EventBridge (Amazon CloudWatch Events) 规则运行一个预定的 AWS Lambda 函数，该函数同时更新 RDS 凭证和实例元数据。",
      "B": "将数据库凭证存储在加密的 Amazon S3 存储桶中的配置文件中。使用 Amazon EventBridge (Amazon CloudWatch Events) 规则运行一个预定的 AWS Lambda 函数，该函数同时更新 RDS 凭证和配置文件中的凭证。使用 S3 版本控制以确保能够回退到先前的值。",
      "C": "将数据库凭证作为密钥存储在 AWS Secrets Manager 中。打开密钥的自动轮换功能。将所需权限附加到 EC2 角色以授予对密钥的访问权限。",
      "D": "将数据库凭证作为加密参数存储在 AWS Systems Manager Parameter Store 中。打开加密参数的自动轮换功能。将所需权限附加到 EC2 角色以授予对加密参数的访问权限。"
    },
    "tags": [
      "RDS",
      "Secrets Manager",
      "EC2",
      "Lambda",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n该问题考察了在 AWS 上安全地管理数据库凭据。使用 AWS Secrets Manager 存储凭据，并结合 Secrets Manager 的自动轮换功能和 EC2 角色授权，是最佳实践。这种方法既满足了安全需求，又减少了运营开销。",
      "why_correct": "选项 C 使用 Secrets Manager 存储和自动轮换数据库凭据，并通过 EC2 角色授予访问权限，是最安全且管理最简便的方案。",
      "why_wrong": "选项 A 和 B 使用实例元数据或 S3 存储凭据，安全性较低，并且手动轮换凭据的流程增加了运营负担。选项 D 使用 Systems Manager Parameter Store，虽然也能存储凭据，但 Secrets Manager 更专门为存储和管理密钥设计，并提供自动轮换功能，更为便捷。"
    },
    "related_terms": [
      "RDS",
      "EC2",
      "Lambda",
      "EventBridge",
      "Secrets Manager",
      "S3",
      "Parameter Store"
    ]
  },
  {
    "id": 62,
    "topic": "1",
    "question_en": "A company is deploying a new public web application to AWS. The application will run behind an Application Load Balancer (ALB). The application needs to be encrypted at the edge with an SSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be rotated each year before the certificate expires. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
      "B": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from the certificate. Apply the certificate to the ALUse the managed renewal feature to automatically rotate the certificate.",
      "C": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate from the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically rotate the certificate.",
      "D": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the ALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the certificate is nearing expiration. Rotate the certificate manually."
    },
    "correct_answer": "D",
    "vote_percentage": "95%",
    "question_cn": "一家公司正在将新的公共 Web 应用程序部署到 AWS。该应用程序将在 Application Load Balancer (ALB) 后面运行。该应用程序需要在边缘使用由外部证书颁发机构 (CA) 颁发的 SSL/TLS 证书进行加密。该证书必须在证书过期之前每年轮换一次。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Certificate Manager (ACM) 颁发 SSL/TLS 证书。将该证书应用于 ALB。使用托管续订功能自动轮换证书。",
      "B": "使用 AWS Certificate Manager (ACM) 颁发 SSL/TLS 证书。从证书导入密钥材料。将该证书应用于 ALB。使用托管续订功能自动轮换证书。",
      "C": "使用 AWS Certificate Manager (ACM) 私有证书颁发机构从根 CA 颁发 SSL/TLS 证书。将该证书应用于 ALB。使用托管续订功能自动轮换证书。",
      "D": "使用 AWS Certificate Manager (ACM) 导入 SSL/TLS 证书。将该证书应用于 ALB。使用 Amazon EventBridge (Amazon CloudWatch Events) 在证书即将到期时发送通知。手动轮换证书。"
    },
    "tags": [
      "ACM",
      "ALB",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 95%），解析仅供参考。】\n\n该问题考查了使用外部 CA 颁发的 SSL/TLS 证书在 AWS 上部署应用程序。需要使用 ACM 导入证书并与 ALB 结合使用，手动轮换证书。ACM 不支持导入证书的自动轮换，需要使用 EventBridge 监控证书到期时间并手动进行轮换。",
      "why_correct": "选项 D 使用 ACM 导入外部 CA 颁发的 SSL/TLS 证书，并将证书应用于 ALB。通过 EventBridge 发送通知提醒手动轮换证书，满足了安全需求和轮换要求。",
      "why_wrong": "选项 A、B、C 试图使用 ACM 的托管续订功能。然而，ACM 的托管续订仅适用于 ACM 颁发的证书，而不适用于导入的证书。选项 C 描述了使用 ACM 私有证书颁发机构（Private CA），这与问题需求不符。"
    },
    "related_terms": [
      "ACM",
      "ALB",
      "EventBridge",
      "SSL/TLS",
      "CA"
    ]
  },
  {
    "id": 63,
    "topic": "1",
    "question_en": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its document management application. The company intends to create a product that converts large .pdf files to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the original files and the converted files. A solutions architect must design a scalable solution to accommodate demand that will grow rapidly over time. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function to convert the files to .jpg format and store them back in Amazon S3.",
      "B": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS Lambda function to convert the files to .jpg format and store them back in DynamoDB.",
      "C": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the EBS store.",
      "D": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in the EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS store."
    },
    "correct_answer": "A",
    "vote_percentage": "99%",
    "question_cn": "一家公司在 AWS 上运行其基础设施，并为其文档管理应用程序注册了 700,000 个用户。该公司计划创建一个将大型 .pdf 文件转换为 .jpg 图像文件的产品。 .pdf 文件平均大小为 5 MB。该公司需要存储原始文件和转换后的文件。解决方案架构师必须设计一个可扩展的解决方案，以适应会随着时间推移而快速增长的需求。哪个解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "将 .pdf 文件保存到 Amazon S3。配置一个 S3 PUT 事件来调用一个 AWS Lambda 函数，以将文件转换为 .jpg 格式，并将它们存储回 Amazon S3。",
      "B": "将 .pdf 文件保存到 Amazon DynamoDB。使用 DynamoDB Streams 功能来调用一个 AWS Lambda 函数，以将文件转换为 .jpg 格式，并将它们存储回 DynamoDB。",
      "C": "将 .pdf 文件上传到包含 Amazon EC2 实例、Amazon Elastic Block Store (Amazon EBS) 存储和一个 Auto Scaling 组的 AWS Elastic Beanstalk 应用程序。在 EC2 实例中使用一个程序将文件转换为 .jpg 格式。将 .pdf 文件和 .jpg 文件保存在 EBS 存储中。",
      "D": "将 .pdf 文件上传到包含 Amazon EC2 实例、Amazon Elastic File System (Amazon EFS) 存储和一个 Auto Scaling 组的 AWS Elastic Beanstalk 应用程序。在 EC2 实例中使用一个程序将文件转换为 .jpg 格式。将 .pdf 文件和 .jpg 文件保存在 EBS 存储中。"
    },
    "tags": [
      "S3",
      "Lambda",
      "EC2",
      "Elastic Beanstalk",
      "EBS",
      "EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 99%），解析仅供参考。】\n\n本题考察了图像转换的解决方案。 需要考虑可扩展性和成本效益。 S3 触发 Lambda 函数的方式成本效益最高，也具备良好的可扩展性。",
      "why_correct": "选项 A 使用 S3 作为存储，并触发 Lambda 函数进行转换。这种方式是事件驱动的，易于扩展，并且成本效益高。",
      "why_wrong": "选项 B 使用 DynamoDB，虽然能触发 Lambda，但 DynamoDB 本身不是存储文件的好选择，成本也相对较高。选项 C 和 D 使用 Elastic Beanstalk 和 EC2 实例，管理复杂性增加，扩展性不如 Lambda，成本也更高。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "EC2",
      "Elastic Beanstalk",
      "EBS",
      "EFS",
      "DynamoDB"
    ]
  },
  {
    "id": 64,
    "topic": "1",
    "question_en": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users and applications interact with the data each day. The company is moving its Windows workloads to AWS. As the company continues this process, the company requires access to AWS and on- premises file storage with minimum latency. The company needs a solution that minimizes operational overhead and requires no significant changes to the existing file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity to AWS. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data to FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on AWS.",
      "B": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to the S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 File Gateway.",
      "C": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to Amazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. depending on each workload's location.",
      "D": "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an Amazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. Configure the cloud workloads to use FSx for Windows File Server on AWS. Configure the on-premises workloads to use the FSx File Gateway."
    },
    "correct_answer": "D",
    "vote_percentage": "78%",
    "question_cn": "一家公司在本地运行的 Windows 文件服务器上拥有超过 5 TB 的文件数据。用户和应用程序每天都与这些数据交互。该公司正在将其 Windows 工作负载迁移到 AWS。随着该公司继续此过程，该公司需要以最小延迟访问 AWS 和本地文件存储。该公司需要一个最大限度地减少运营开销且不需要对其现有文件访问模式进行重大更改的解决方案。该公司使用 AWS Site-to-Site VPN 连接到 AWS。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在 AWS 上部署和配置 Amazon FSx for Windows File Server。将本地文件数据移动到 FSx for Windows File Server。重新配置工作负载以使用 AWS 上的 FSx for Windows File Server。",
      "B": "在本地部署和配置 Amazon S3 File Gateway。将本地文件数据移动到 S3 File Gateway。重新配置本地工作负载和云工作负载以使用 S3 File Gateway。",
      "C": "在本地部署和配置 Amazon S3 File Gateway。将本地文件数据移动到 Amazon S3。重新配置工作负载以根据每个工作负载的位置直接使用 Amazon S3 或 S3 File Gateway。",
      "D": "在 AWS 上部署和配置 Amazon FSx for Windows File Server。在本地部署和配置 Amazon FSx File Gateway。将本地文件数据移动到 FSx File Gateway。配置云工作负载以使用 AWS 上的 FSx for Windows File Server。配置本地工作负载以使用 FSx File Gateway。"
    },
    "tags": [
      "FSx for Windows File Server",
      "S3 File Gateway",
      "Site-to-Site VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 78%），解析仅供参考。】\n\n考查混合云环境下，如何通过最小延迟和运营开销将本地 Windows 文件服务器迁移到 AWS，并保持现有文件访问模式。",
      "why_correct": "选项 D 提供了最佳的混合云解决方案。通过在 AWS 上部署 FSx for Windows File Server，并在本地部署 FSx File Gateway，可以实现本地和云端数据的无缝访问。FSx File Gateway 作为本地缓存，减少了延迟，同时 FSx for Windows File Server 提供了与 Windows 文件服务器兼容的共享文件存储。",
      "why_wrong": "选项 A 仅在 AWS 中部署 FSx for Windows File Server，无法满足本地访问的需求，且会产生额外的延迟。选项 B 采用 S3 File Gateway，虽然支持本地缓存，但与题干中的\"不需要对其现有文件访问模式进行重大更改\"相悖，因为需要更改工作负载。选项 C 仅使用 S3 File Gateway 和 S3，不提供 Windows 文件服务器兼容性，且会影响现有文件访问模式。"
    },
    "related_terms": [
      "Amazon FSx for Windows File Server",
      "AWS Site-to-Site VPN",
      "Amazon S3 File Gateway",
      "Amazon S3",
      "FSx File Gateway",
      "Windows"
    ]
  },
  {
    "id": 65,
    "topic": "1",
    "question_en": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The hospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. The hospital needs to modify the Lambda code to identify protected health information (PHI) in the reports. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the extracted text.",
      "B": "Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the PHI from the extracted text.",
      "C": "Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text.",
      "D": "Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to identify the PHI from the extracted text."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家医院最近使用 Amazon API Gateway 和 AWS Lambda 部署了一个 RESTful API。医院使用 API Gateway 和 Lambda 上传 PDF 和 JPEG 格式的报告。医院需要修改 Lambda 代码以识别报告中的受保护健康信息 (PHI)。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用现有的 Python 库从报告中提取文本，并从提取的文本中识别 PHI。",
      "B": "使用 Amazon Textract 从报告中提取文本。使用 Amazon SageMaker 从提取的文本中识别 PHI。",
      "C": "使用 Amazon Textract 从报告中提取文本。使用 Amazon Comprehend Medical 从提取的文本中识别 PHI。",
      "D": "使用 Amazon Rekognition 从报告中提取文本。使用 Amazon Comprehend Medical 从提取的文本中识别 PHI。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "Textract",
      "Comprehend Medical"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n该问题考察了使用 AWS 服务处理文档并识别 PHI 的方案。 使用 Amazon Textract 从报告中提取文本，然后使用 Amazon Comprehend Medical 从提取的文本中识别 PHI，是最优解。",
      "why_correct": "选项 C 结合使用 Amazon Textract 和 Amazon Comprehend Medical，专门用于处理医疗文档和识别 PHI，是最佳实践。",
      "why_wrong": "选项 A 使用现有的 Python 库，手动实现功能，增加了工作量，并且效果可能不如 AWS 提供的服务。选项 B 和 D 分别使用了 Amazon SageMaker 和 Amazon Rekognition，但它们不是专门为 PHI 识别设计的，无法达到最佳效果。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "Textract",
      "Comprehend Medical",
      "Rekognition",
      "SageMaker",
      "PHI"
    ]
  },
  {
    "id": 66,
    "topic": "1",
    "question_en": "A company has an application that generates a large number of files, each approximately 5 MB in size. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years before they can be deleted. Immediate accessibility is always required as the files contain critical business data that is not easy to reproduce. The files are frequently accessed in the first 30 days of the object creation but are rarely accessed after the first 30 days. Which storage solution is MOST cost-effective?",
    "options_en": {
      "A": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from object creation. Delete the files 4 years after object creation.",
      "B": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.",
      "C": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.",
      "D": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after object creation."
    },
    "correct_answer": "C",
    "vote_percentage": "66%",
    "question_cn": "一家公司有一个应用程序，该应用程序会生成大量文件，每个文件大约 5 MB 大小。这些文件存储在 Amazon S3 中。公司策略要求这些文件在删除之前需要存储 4 年。由于这些文件包含不易再现的关键业务数据，因此始终需要即时访问。文件在创建后的前 30 天内会经常被访问，但在前 30 天后很少被访问。哪种存储解决方案最具成本效益？",
    "options_cn": {
      "A": "创建一个 S3 存储桶生命周期策略，将文件从 S3 Standard 转移到 S3 Glacier，从对象创建后 30 天开始。在对象创建 4 年后删除这些文件。",
      "B": "创建一个 S3 存储桶生命周期策略，将文件从 S3 Standard 转移到 S3 One Zone-Infrequent Access (S3 One Zone-IA)，从对象创建后 30 天开始。在对象创建 4 年后删除这些文件。",
      "C": "创建一个 S3 存储桶生命周期策略，将文件从 S3 Standard 转移到 S3 Standard-Infrequent Access (S3 Standard-IA)，从对象创建后 30 天开始。在对象创建 4 年后删除这些文件。",
      "D": "创建一个 S3 存储桶生命周期策略，将文件从 S3 Standard 转移到 S3 Standard-Infrequent Access (S3 Standard-IA)，从对象创建后 30 天开始。在对象创建 4 年后将文件转移到 S3 Glacier。"
    },
    "tags": [
      "S3",
      "Lifecycle Policy",
      "Glacier",
      "S3 Standard-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 66%），解析仅供参考。】\n\n该问题考查了 S3 存储的成本优化方案。 S3 标准、S3 Standard-IA 和 S3 Glacier 是不同的存储类，用于满足不同的访问频率和存储成本需求。 考虑访问频率和存储时间，选择最佳的生命周期策略。",
      "why_correct": "选项 C 使用 S3 Standard-IA，适用于不经常访问但需要即时访问的文件。30 天后转移到 S3 Standard-IA，4 年后删除，符合题干要求，并且最具成本效益。",
      "why_wrong": "选项 A 使用 S3 Glacier，虽然成本低廉，但恢复时间较长，不符合“始终需要即时访问”的要求。选项 B 使用 S3 One Zone-IA，虽然性能好，但单可用区存储有可用性风险，不推荐。选项 D 最后转移到 S3 Glacier，也与即时访问的要求相悖。"
    },
    "related_terms": [
      "S3",
      "Glacier",
      "S3 Standard-IA",
      "Lifecycle Policy"
    ]
  },
  {
    "id": 67,
    "topic": "1",
    "question_en": "A company hosts an application on multiple Amazon EC2 instances. The application processes messages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message from the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not contain any duplicate messages. What should a solutions architect do to ensure messages are being processed once only?",
    "options_en": {
      "A": "Use the CreateQueue API call to create a new queue.",
      "B": "Use the AddPermission API call to add appropriate permissions.",
      "C": "Use the ReceiveMessage API call to set an appropriate wait time.",
      "D": "Use the ChangeMessageVisibility API call to increase the visibility timeout."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在多个 Amazon EC2 实例上托管一个应用程序。该应用程序处理来自 Amazon SQS 队列的消息，写入 Amazon RDS 表，并从队列中删除消息。RDS 表中偶尔会发现重复的记录。SQS 队列不包含任何重复的消息。解决方案架构师应采取什么措施来确保消息仅被处理一次？",
    "options_cn": {
      "A": "使用 CreateQueue API 调用创建一个新队列。",
      "B": "使用 AddPermission API 调用添加适当的权限。",
      "C": "使用 ReceiveMessage API 调用设置适当的等待时间。",
      "D": "使用 ChangeMessageVisibility API 调用来增加可见性超时。"
    },
    "tags": [
      "SQS",
      "RDS",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n该问题考察了防止 SQS 消息被重复处理的方案。 通过增加消息可见性超时来确保消息只被处理一次，是有效的方法。",
      "why_correct": "选项 D 使用 ChangeMessageVisibility API 调用，增加可见性超时，当 EC2 实例处理消息失败时，消息不会立即被其他实例处理，确保了消息只被处理一次。",
      "why_wrong": "选项 A、B、C 提供了创建和授权队列的方法，但并不能解决重复处理的问题。"
    },
    "related_terms": [
      "SQS",
      "RDS",
      "EC2",
      "API"
    ]
  },
  {
    "id": 68,
    "topic": "1",
    "question_en": "A solutions architect is designing a new hybrid architecture to extend a company's on-premises infrastructure to AWS. The company requires a highly available connection with consistent low latency to an AWS Region. The company needs to minimize costs and is willing to accept slower trafic if the primary connection fails. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup if the primary Direct Connect connection fails.",
      "B": "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN tunnel for private connectivity and as a backup if the primary VPN connection fails.",
      "C": "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect connection to the same Region as a backup if the primary Direct Connect connection fails.",
      "D": "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute from the AWS CLI to automatically create a backup connection if the primary Direct Connect connection fails."
    },
    "correct_answer": "A",
    "vote_percentage": "94%",
    "question_cn": "一位解决方案架构师正在设计一个新的混合架构，以将一家公司的本地基础设施扩展到 AWS。该公司需要一个高可用性、具有一致低延迟的连接到 AWS 区域。该公司需要最大限度地降低成本，并且如果主连接失败，愿意接受较慢的流量。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "预置一个 AWS Direct Connect 连接到某个区域。如果主 Direct Connect 连接失败，则预置一个 VPN 连接作为备份。",
      "B": "预置一个 VPN 隧道连接到某个区域以进行私有连接。预置第二个 VPN 隧道以进行私有连接，并作为备份，以防主 VPN 连接失败。",
      "C": "预置一个 AWS Direct Connect 连接到某个区域。如果主 Direct Connect 连接失败，则预置第二个 Direct Connect 连接到同一区域作为备份。",
      "D": "预置一个 AWS Direct Connect 连接到某个区域。使用来自 AWS CLI 的 Direct Connect 故障转移属性，如果主 Direct Connect 连接失败，则自动创建一个备份连接。"
    },
    "tags": [
      "Direct Connect",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 94%），解析仅供参考。】\n\n该问题考察了混合云架构的连接方案。Direct Connect 提供了比 VPN 更好的性能，但成本也较高，而 VPN 的成本较低。为了实现高可用性和低延迟，需要同时使用 Direct Connect 和 VPN 作为备份连接。",
      "why_correct": "选项 A 提供了一个 Direct Connect 连接作为主链路，并使用 VPN 作为备份链路，兼顾了性能和成本，并且满足了“如果主连接失败，愿意接受较慢的流量”的需求。",
      "why_wrong": "选项 B 仅使用 VPN，虽然满足了高可用性，但延迟较高。选项 C 在同一区域使用两个 Direct Connect 连接，成本较高。选项 D 使用 Direct Connect 故障转移属性，虽然可以实现自动故障转移，但未考虑备份连接。"
    },
    "related_terms": [
      "Direct Connect",
      "VPN"
    ]
  },
  {
    "id": 69,
    "topic": "1",
    "question_en": "A company is running a business-critical web application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an Amazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company wants the application to be highly available with minimum downtime and minimum loss of data. Which solution will meet these requirements with the LEAST operational effort?",
    "options_en": {
      "A": "Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect trafic. Use Aurora PostgreSQL Cross- Region Replication.",
      "B": "Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as Multi-AZ. Configure an Amazon RDS Proxy instance for the database.",
      "C": "Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the database. Recover the database from the snapshots in the event of a failure.",
      "D": "Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application to Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to the database."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司在其Application Load Balancer后面的Amazon EC2实例上运行着一项关键业务Web应用程序。 EC2实例位于一个Auto Scaling组中。该应用程序使用部署在单个可用区中的Amazon Aurora PostgreSQL数据库。公司希望该应用程序具有高可用性，且停机时间最短，数据丢失最少。哪个解决方案将以最少的运营工作量满足这些要求？",
    "options_cn": {
      "A": "将EC2实例放置在不同的AWS区域。使用Amazon Route 53健康检查来重定向流量。使用Aurora PostgreSQL 跨区域复制。",
      "B": "配置Auto Scaling组以使用多个可用区。将数据库配置为Multi-AZ。为数据库配置Amazon RDS Proxy实例。",
      "C": "配置Auto Scaling组以使用一个可用区。每小时拍摄数据库快照。在发生故障时从快照恢复数据库。",
      "D": "配置Auto Scaling组以使用多个AWS区域。将应用程序中的数据写入Amazon S3。使用S3事件通知来启动一个AWS Lambda函数，将数据写入数据库。"
    },
    "tags": [
      "Application Load Balancer",
      "EC2",
      "Auto Scaling",
      "Aurora PostgreSQL",
      "Route 53",
      "RDS Proxy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n本题考查了高可用性数据库和应用程序的配置。 将数据库配置为 Multi-AZ 和 RDS Proxy，可以提供高可用性，减少停机时间，并减少运营工作量。",
      "why_correct": "选项 B 将数据库配置为 Multi-AZ，提供数据库的高可用性，使用 RDS Proxy 可以减少数据库连接开销，提高性能。",
      "why_wrong": "选项 A 跨区域复制数据库，增加了复杂性，且跨区域同步会有延迟，无法保证数据一致性。 选项 C 使用数据库快照进行恢复，恢复时间较长，不符合“停机时间最短，数据丢失最少”的要求。 选项 D 使用 S3，无法保证数据库的事务一致性。"
    },
    "related_terms": [
      "Application Load Balancer",
      "EC2",
      "Auto Scaling",
      "Aurora PostgreSQL",
      "Route 53",
      "RDS Proxy"
    ]
  },
  {
    "id": 70,
    "topic": "1",
    "question_en": "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is configured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web service. The company notices that the NLB is not detecting HTTP errors for the application. These errors require a manual restart of the EC2 instances that run the web service. The company needs to improve the application's availability without writing custom scripts or code. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Enable HTTP health checks on the NLB, supplying the URL of the company's application.",
      "B": "Add a cron job to the EC2 instances to check the local application's logs once each minute. If HTTP errors are detected. the application will restart.",
      "C": "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the URL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.",
      "D": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. Configure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM state."
    },
    "correct_answer": "C",
    "vote_percentage": "88%",
    "question_cn": "一家公司的 HTTP 应用程序位于 Network Load Balancer (NLB) 之后。NLB 的目标组被配置为使用 Amazon EC2 Auto Scaling 组，该组具有多个运行 Web 服务的 EC2 实例。该公司注意到 NLB 未检测到应用程序的 HTTP 错误。这些错误需要手动重启运行 Web 服务的 EC2 实例。该公司需要提高应用程序的可用性，而无需编写自定义脚本或代码。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在 NLB 上启用 HTTP 运行状况检查，提供公司应用程序的 URL。",
      "B": "在 EC2 实例中添加一个 cron 作业，每分钟检查一次本地应用程序的日志。如果检测到 HTTP 错误，应用程序将重新启动。",
      "C": "用 Application Load Balancer 替换 NLB。通过提供公司应用程序的 URL 来启用 HTTP 运行状况检查。配置 Auto Scaling 操作以替换运行状况不佳的实例。",
      "D": "创建一个 Amazon CloudWatch 警报，该警报监控 NLB 的 UnhealthyHostCount 指标。配置 Auto Scaling 操作，以便在警报处于 ALARM 状态时替换运行状况不佳的实例。"
    },
    "tags": [
      "Network Load Balancer",
      "Application Load Balancer",
      "Auto Scaling",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 88%），解析仅供参考。】\n\n该问题考察了提高应用程序可用性的方案。 将 NLB 替换为 ALB，并使用 ALB 的运行状况检查功能，可以自动检测并替换运行状况不佳的实例。无需编写自定义脚本或代码。",
      "why_correct": "选项 C 将 NLB 替换为 ALB，并通过配置 ALB 的运行状况检查和 Auto Scaling，实现了自动检测和替换运行状况不佳的实例，提高了可用性，也减少了运营开销。",
      "why_wrong": "选项 A 在 NLB 上启用 HTTP 运行状况检查，NLB 仅在 TCP 层进行运行状况检查，无法感知应用程序的 HTTP 错误。选项 B 在 EC2 实例中添加 cron 作业，增加了维护工作量。 选项 D 使用 CloudWatch 监控 NLB 的 UnhealthyHostCount 指标，无法解决应用程序的 HTTP 错误，并且配置不够简洁。"
    },
    "related_terms": [
      "Network Load Balancer",
      "Application Load Balancer",
      "Auto Scaling",
      "CloudWatch"
    ]
  },
  {
    "id": 71,
    "topic": "1",
    "question_en": "A company runs a shopping application that uses Amazon DynamoDB to store customer information. In case of data corruption, a solutions architect needs to design a solution that meets a recovery point objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour. What should the solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS Region.",
      "B": "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.",
      "C": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the data from S3 Glacier to DynamoDB.",
      "D": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table every 15 minutes. For RPO recovery, restore the DynamoDB table by using the EBS snapshot."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司运行一个使用 Amazon DynamoDB 存储客户信息的购物应用程序。如果发生数据损坏，解决方案架构师需要设计一个解决方案，以满足 15 分钟的恢复点目标 (RPO) 和 1 小时的恢复时间目标 (RTO)。 解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "配置 DynamoDB 全局表。对于 RPO 恢复，将应用程序指向不同的 AWS 区域。",
      "B": "配置 DynamoDB 的时间点恢复。对于 RPO 恢复，恢复到所需的时间点。",
      "C": "每天将 DynamoDB 数据导出到 Amazon S3 Glacier。对于 RPO 恢复，从 S3 Glacier 将数据导入到 DynamoDB。",
      "D": "每 15 分钟为 DynamoDB 表安排 Amazon Elastic Block Store (Amazon EBS) 快照。对于 RPO 恢复，使用 EBS 快照恢复 DynamoDB 表。"
    },
    "tags": [
      "DynamoDB",
      "RPO",
      "RTO",
      "Point-in-time recovery",
      "Global Tables"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n该问题考察了 DynamoDB 的数据恢复方案。 满足 15 分钟 RPO 和 1 小时 RTO，需要使用 DynamoDB 的时间点恢复 (PITR) 功能。",
      "why_correct": "选项 B 使用 DynamoDB 的时间点恢复 (PITR) 功能，可以恢复到过去 35 天内的任意时间点，满足了 RPO 和 RTO 的要求。",
      "why_wrong": "选项 A 使用全局表，虽然可以实现跨区域的数据复制，但无法满足 15 分钟 RPO 的要求。选项 C 导出到 Glacier 恢复时间较长，不满足 RTO 要求。选项 D 使用 EBS 快照，EBS 快照用于 EC2 实例，而非 DynamoDB 表。"
    },
    "related_terms": [
      "DynamoDB",
      "RPO",
      "RTO",
      "Point-in-time recovery",
      "Global Tables"
    ]
  },
  {
    "id": 72,
    "topic": "1",
    "question_en": "A company runs a photo processing application that needs to frequently upload and download pictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect has noticed an increased cost in data transfer fees and needs to implement a solution to reduce these costs. How can the solutions architect meet this requirement?",
    "options_en": {
      "A": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls through it.",
      "B": "Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the S3 buckets.",
      "C": "Deploy the application into a public subnet and allow it to route through an internet gateway to access the S3 buckets.",
      "D": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access to the S3 buckets."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司运行一个照片处理应用程序，该应用程序需要经常从位于同一 AWS 区域的 Amazon S3 存储桶上传和下载图片。 解决方案架构师注意到数据传输费用增加，需要实施一个解决方案来降低这些成本。 解决方案架构师如何满足此要求？",
    "options_cn": {
      "A": "将 Amazon API Gateway 部署到公有子网中，并调整路由表以通过它路由 S3 调用。",
      "B": "在公有子网中部署一个 NAT Gateway，并附加一个允许访问 S3 存储桶的终端节点策略。",
      "C": "将应用程序部署到公有子网中，并允许其通过互联网网关路由以访问 S3 存储桶。",
      "D": "将 S3 VPC 网关终端节点部署到 VPC 中，并附加一个允许访问 S3 存储桶的终端节点策略。"
    },
    "tags": [
      "S3",
      "VPC Endpoint",
      "API Gateway",
      "NAT Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n该问题考察了优化 S3 数据传输成本的方案。 使用 VPC 终端节点，可以使流量在 VPC 内部传输，避免了流量经过互联网，从而降低了数据传输成本。",
      "why_correct": "选项 D 使用 S3 VPC 网关终端节点，使应用程序能够通过 VPC 内部路由访问 S3，避免了数据传输费用。",
      "why_wrong": "选项 A 使用 API Gateway，会增加额外的开销。选项 B 使用 NAT 网关，虽然可以访问 S3，但数据流量仍需经过互联网网关，并不能降低数据传输成本。 选项 C 使用 Internet Gateway，会产生数据传输费用。"
    },
    "related_terms": [
      "S3",
      "API Gateway",
      "NAT Gateway",
      "VPC Endpoint"
    ]
  },
  {
    "id": 73,
    "topic": "1",
    "question_en": "A company recently launched Linux-based application instances on Amazon EC2 in a private subnet and launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. A solutions architect needs to connect from the on-premises network, through the company's internet connection, to the bastion host, and to the application servers. The solutions architect must make sure that the security groups of all the EC2 instances will allow that access. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Replace the current security group of the bastion host with one that only allows inbound access from the application instances.",
      "B": "Replace the current security group of the bastion host with one that only allows inbound access from the internal IP range for the company.",
      "C": "Replace the current security group of the bastion host with one that only allows inbound access from the external IP range for the company.",
      "D": "Replace the current security group of the application instances with one that allows inbound SSH access from only the private IP address of the bastion host",
      "E": "Replace the current security group of the application instances with one that allows inbound SSH access from only the public IP address of the bastion host."
    },
    "correct_answer": "CD",
    "vote_percentage": "92%",
    "question_cn": "一家公司最近在 VPC 的私有子网中的 Amazon EC2 上启动了基于 Linux 的应用程序实例，并在 VPC 的公有子网中的 Amazon EC2 实例上启动了基于 Linux 的堡垒主机。 解决方案架构师需要通过公司的互联网连接从本地网络连接到堡垒主机和应用程序服务器。 解决方案架构师必须确保所有 EC2 实例的安全组将允许该访问。 解决方案架构师应采取哪些步骤组合以满足这些要求？ （选择两个。）",
    "options_cn": {
      "A": "将堡垒主机的当前安全组替换为仅允许从应用程序实例进行入站访问的安全组。",
      "B": "将堡垒主机的当前安全组替换为仅允许来自公司内部 IP 范围的入站访问的安全组。",
      "C": "将堡垒主机的当前安全组替换为仅允许来自公司外部 IP 范围的入站访问的安全组。",
      "D": "将应用程序实例的当前安全组替换为仅允许来自堡垒主机的私有 IP 地址的入站 SSH 访问的安全组。",
      "E": "将应用程序实例的当前安全组替换为仅允许来自堡垒主机的公共 IP 地址的入站 SSH 访问的安全组。"
    },
    "tags": [
      "EC2",
      "VPC",
      "Security Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 92%），解析仅供参考。】\n\n该问题考察了安全组的配置方案。 为了保证安全性，堡垒机安全组应该只允许来自公司内部 IP 范围的入站访问，应用程序安全组应该只允许来自堡垒机的私有 IP 地址的 SSH 访问。",
      "why_correct": "选项 B 将堡垒主机的当前安全组替换为仅允许来自公司内部 IP 范围的入站访问的安全组。选项 D 将应用程序实例的当前安全组替换为仅允许来自堡垒主机的私有 IP 地址的入站 SSH 访问的安全组，符合安全最佳实践。",
      "why_wrong": "选项 A、C、E 不符合安全最佳实践。"
    },
    "related_terms": [
      "EC2",
      "VPC",
      "Security Group"
    ]
  },
  {
    "id": 74,
    "topic": "1",
    "question_en": "A solutions architect is designing a two-tier web application. The application consists of a public-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of Microsoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the company. How should security groups be configured in this situation? (Choose two.)",
    "options_en": {
      "A": "Configure the security group for the web tier to allow inbound trafic on port 443 from 0.0.0.0/0.",
      "B": "Configure the security group for the web tier to allow outbound trafic on port 443 from 0.0.0.0/0.",
      "C": "Configure the security group for the database tier to allow inbound trafic on port 1433 from the security group for the web tier.",
      "D": "Configure the security group for the database tier to allow outbound trafic on ports 443 and 1433 to the security group for the web tier",
      "E": "Configure the security group for the database tier to allow inbound trafic on ports 443 and 1433 from the security group for the web tier."
    },
    "correct_answer": "AC",
    "vote_percentage": "98%",
    "question_cn": "一位解决方案架构师正在设计一个两层 Web 应用程序。该应用程序包含一个在公有子网中的 Amazon EC2 上托管的面向公众的 Web 层。数据库层包含在私有子网中的 Amazon EC2 上运行的 Microsoft SQL Server。安全性是该公司的首要任务。在这种情况下，应该如何配置安全组？（选择两个。）",
    "options_cn": {
      "A": "配置 Web 层的安全组，允许来自 0.0.0.0/0 的端口 443 的入站流量。",
      "B": "配置 Web 层的安全组，允许来自 0.0.0.0/0 的端口 443 的出站流量。",
      "C": "配置数据库层的安全组，允许来自 Web 层安全组的端口 1433 的入站流量。",
      "D": "配置数据库层的安全组，允许到 Web 层安全组的端口 443 和 1433 的出站流量。",
      "E": "配置数据库层的安全组，允许来自 Web 层安全组的端口 443 和 1433 的入站流量。"
    },
    "tags": [
      "EC2",
      "Security Group",
      "Web Application"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 98%），解析仅供参考。】\n\n该问题考察了安全组的配置方案。Web 层的安全组应该允许来自 0.0.0.0/0 的 HTTPS (443) 入站流量，数据库层的安全组应该允许来自 Web 层安全组的 MySQL (1433) 入站流量。",
      "why_correct": "选项 A 配置 Web 层的安全组，允许来自 0.0.0.0/0 的端口 443 的入站流量。 选项 C 配置数据库层的安全组，允许来自 Web 层安全组的端口 1433 的入站流量。 这两个选项符合安全最佳实践。",
      "why_wrong": "选项 B 允许从 0.0.0.0/0 传出的流量不安全。选项 D 和 E 允许数据库层对 Web 层的端口进行出站流量，这是不必要的。"
    },
    "related_terms": [
      "EC2",
      "Security Group",
      "Web Application"
    ]
  },
  {
    "id": 75,
    "topic": "1",
    "question_en": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve the application's performance. The application consists of application tiers that communicate with each other by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A solutions architect must design a solution that resolves these issues and modernizes the application. Which solution meets these requirements and is the MOST operationally eficient?",
    "options_en": {
      "A": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application layer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between application services.",
      "B": "Use Amazon CloudWatch metrics to analyze the application performance history to determine the servers' peak utilization during the performance failures. Increase the size of the application server's Amazon EC2 instances to meet the peak requirements.",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SNS queue length and scale up and down as required.",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application servers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the SQS queue length and scale up when communication failures are detected."
    },
    "correct_answer": "A",
    "vote_percentage": "81%",
    "question_cn": "一家公司希望将多层应用程序从本地环境迁移到 AWS 云，以提高应用程序的性能。该应用程序包含通过 RESTful 服务相互通信的应用程序层。当某一层过载时，事务将被丢弃。解决方案架构师必须设计一个解决方案来解决这些问题并实现应用程序现代化。哪种解决方案满足这些要求，并且运营效率最高？",
    "options_cn": {
      "A": "使用 Amazon API Gateway 并将事务定向到 AWS Lambda 函数作为应用程序层。使用 Amazon Simple Queue Service (Amazon SQS) 作为应用程序服务之间的通信层。",
      "B": "使用 Amazon CloudWatch 指标来分析应用程序性能历史记录，以确定服务器在性能故障期间的峰值利用率。增加应用程序服务器的 Amazon EC2 实例的大小以满足峰值需求。",
      "C": "使用 Amazon Simple Notification Service (Amazon SNS) 来处理在 Auto Scaling 组中运行在 Amazon EC2 上的应用程序服务器之间的消息传递。使用 Amazon CloudWatch 监控 SNS 队列长度，并根据需要进行扩展和缩减。",
      "D": "使用 Amazon Simple Queue Service (Amazon SQS) 来处理在 Auto Scaling 组中运行在 Amazon EC2 上的应用程序服务器之间的消息传递。使用 Amazon CloudWatch 监控 SQS 队列长度，并在检测到通信故障时进行扩展。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "SQS",
      "Auto Scaling",
      "CloudWatch",
      "SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 81%），解析仅供参考。】\n\n该问题考察了应用程序现代化的方案。 API Gateway 和 Lambda 的组合可以实现无服务器架构， SQS 可以在应用程序层之间提供异步通信，是现代化的最佳实践。",
      "why_correct": "选项 A 使用 API Gateway 和 Lambda 作为应用程序层，并使用 SQS 作为应用程序服务之间的通信层。这是无服务器架构的经典组合，可以提高应用程序的性能和可伸缩性。 SQS 也能解耦各应用程序组件。",
      "why_wrong": "选项 B 仅通过增加 EC2 实例的大小来解决性能问题，扩展性有限。选项 C 使用 SNS，SNS 适用于发布-订阅模式，不适用于队列模式。选项 D 使用 SQS，但扩展策略依赖 CloudWatch 监控 SQS 队列长度。这种方式虽然可行，但不如选项 A 使用 API Gateway 和 Lambda 简洁，且缺少 API Gateway 提供的一些功能。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "SQS",
      "Auto Scaling",
      "CloudWatch",
      "SNS"
    ]
  },
  {
    "id": 76,
    "topic": "1",
    "question_en": "A company receives 10 TB of instrumentation data each day from several machines located at a single factory. The data consists of JSON files stored on a storage area network (SAN) in an on-premises data center located within the factory. The company wants to send this data to Amazon S3 where it can be accessed by several additional systems that provide critical near-real-time analytics. A secure transfer is important because the data is considered sensitive. Which solution offers the MOST reliable data transfer?",
    "options_en": {
      "A": "AWS DataSync over public internet",
      "B": "AWS DataSync over AWS Direct Connect",
      "C": "AWS Database Migration Service (AWS DMS) over public internet",
      "D": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司每天从位于单个工厂的几台机器接收 10 TB 的仪器数据。数据由存储在工厂内本地数据中心存储区域网络 (SAN) 上的 JSON 文件组成。该公司希望将此数据发送到 Amazon S3，以便能够被提供关键近实时分析的几个其他系统访问。由于数据被认为是敏感的，因此安全传输非常重要。哪种解决方案提供最可靠的数据传输？",
    "options_cn": {
      "A": "通过公共互联网使用 AWS DataSync",
      "B": "通过 AWS Direct Connect 使用 AWS DataSync",
      "C": "通过公共互联网使用 AWS Database Migration Service (AWS DMS)",
      "D": "通过 AWS Direct Connect 使用 AWS Database Migration Service (AWS DMS)"
    },
    "tags": [
      "Amazon S3",
      "DataSync",
      "AWS Direct Connect",
      "Data Transfer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何将大量数据安全、可靠地传输到 Amazon S3。涉及 DataSync 和 Direct Connect 的选择，以及安全传输的考虑。",
      "why_correct": "通过 AWS Direct Connect 使用 AWS DataSync 是最可靠的解决方案。Direct Connect 提供专用网络连接，绕过公共互联网，显著提高数据传输的稳定性和速度，降低延迟。DataSync 专为数据传输设计，可以高效地将数据从本地存储传输到 S3，并确保数据的完整性。结合 Direct Connect，可以实现安全、高速、可靠的数据传输，满足题目对敏感数据安全传输的要求。",
      "why_wrong": "选项 A 错误，通过公共互联网使用 AWS DataSync，虽然 DataSync 能够保障数据传输，但通过公共互联网会引入潜在的网络延迟和安全风险，尤其对于敏感数据，安全性无法完全保证，且传输速度可能受到公共互联网带宽的限制。选项 C 错误，通过公共互联网使用 AWS Database Migration Service (AWS DMS) 并非最佳选择，DMS 主要用于数据库迁移，而非大规模文件传输，且通过公共互联网传输敏感数据存在安全隐患。选项 D 错误，虽然 Direct Connect 提升了网络连接的质量，但 DMS 并非为大规模文件传输设计，在性能和效率上不如 DataSync。因此，选项 A 和 C 涉及通过公共互联网传输数据，安全性较差，选项 C 和 D 均未充分利用 DataSync 在数据传输方面的优势。"
    },
    "related_terms": [
      "Amazon S3",
      "DataSync",
      "AWS Direct Connect",
      "AWS DMS",
      "SAN"
    ]
  },
  {
    "id": 77,
    "topic": "1",
    "question_en": "A company needs to configure a real-time data ingestion architecture for its application. The company needs an API, a process that transforms data as the data is streamed, and a storage solution for the data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
      "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop source/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send the data to Amazon S3.",
      "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to send the data to Amazon S3.",
      "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to transform the data. Use AWS Glue to send the data to Amazon S3."
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一家公司需要为其应用程序配置一个实时数据摄取架构。该公司需要一个 API、一个在数据流传输时转换数据的流程以及一个用于存储数据的解决方案。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "部署一个 Amazon EC2 实例来托管一个 API，该 API 将数据发送到 Amazon Kinesis 数据流。创建一个 Amazon Kinesis Data Firehose 交付流，该交付流使用 Kinesis 数据流作为数据源。使用 AWS Lambda 函数转换数据。使用 Kinesis Data Firehose 交付流将数据发送到 Amazon S3。",
      "B": "部署一个 Amazon EC2 实例来托管一个 API，该 API 将数据发送到 AWS Glue。停止在 EC2 实例上进行 源/目标 检查。使用 AWS Glue 转换数据，并将数据发送到 Amazon S3。",
      "C": "配置一个 Amazon API Gateway API，将数据发送到 Amazon Kinesis 数据流。创建一个 Amazon Kinesis Data Firehose 交付流，该交付流使用 Kinesis 数据流作为数据源。使用 AWS Lambda 函数转换数据。使用 Kinesis Data Firehose 交付流将数据发送到 Amazon S3。",
      "D": "配置一个 Amazon API Gateway API，将数据发送到 AWS Glue。使用 AWS Lambda 函数转换数据。使用 AWS Glue 将数据发送到 Amazon S3。"
    },
    "tags": [
      "Amazon Kinesis",
      "Kinesis Data Firehose",
      "AWS Lambda",
      "Amazon S3",
      "Amazon API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n考察实时数据摄取架构设计，以及在低运营成本下，如何选择合适的 AWS 服务来满足 API、数据转换和数据存储的需求。涉及 API Gateway、Kinesis Data Streams、Kinesis Data Firehose、Lambda 和 S3 的选型与搭配。",
      "why_correct": "选项 C 提供了一个完全托管的解决方案，运营开销最低。API Gateway 负责接收数据。数据被发送到 Kinesis Data Streams，然后 Kinesis Data Firehose 从 Kinesis Data Streams 提取数据。Lambda 函数用于转换数据，并且数据最终通过 Kinesis Data Firehose 被写入 Amazon S3。这种架构利用了完全托管的服务，减少了维护和管理的负担，例如无需管理 EC2 实例。",
      "why_wrong": {
        "A": "选项 A 使用 EC2 实例托管 API，增加了运营开销。虽然它使用了 Kinesis Data Streams、Kinesis Data Firehose、Lambda 和 S3，但引入 EC2 意味着需要管理服务器，包括维护、更新和扩展，与题目要求的\"最少运营开销\"相悖。",
        "B": "选项 B 使用 EC2 实例托管 API，同样带来了额外的运营负担。此外，直接使用 AWS Glue 作为实时数据摄取管道通常不是最佳选择，因为 Glue 主要是为 ETL 批量处理设计的。虽然 Glue 也有一些流处理能力，但它的设计和性能更适合批处理作业，而不是低延迟的实时数据摄取。相比之下，Kinesis Data Firehose 专为流数据处理而设计，能提供更优的性能和更低的延迟。",
        "D": "选项 D 缺少 Kinesis 数据流或 Kinesis Data Firehose，这使得它不适合实时数据摄取。API Gateway 接收数据后，需要一个流处理服务来处理数据转换，然后将数据发送到存储。选项 D 使用 Lambda 转换数据，但随后使用 Glue 将数据发送到 S3，这不符合低延迟实时数据处理的需求。Glue 的主要用途是 ETL 作业，并不擅长实时数据流的处理，且 Glue 并非设计为 API Gateway 的原生集成服务。"
      }
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon API Gateway",
      "Kinesis Data Streams",
      "Kinesis Data Firehose",
      "AWS Lambda",
      "Amazon S3",
      "AWS Glue",
      "Amazon Kinesis"
    ]
  },
  {
    "id": 78,
    "topic": "1",
    "question_en": "A company needs to keep user transaction data in an Amazon DynamoDB table. The company must retain the data for 7 years. What is the MOST operationally eficient solution that meets these requirements?",
    "options_en": {
      "A": "Use DynamoDB point-in-time recovery to back up the table continuously.",
      "B": "Use AWS Backup to create backup schedules and retention policies for the table.",
      "C": "Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda function. Configure the Lambda function to back up the table and to store the backup in an Amazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要在 Amazon DynamoDB 表中保留用户交易数据。该公司必须将数据保留 7 年。哪个解决方案在运营上效率最高，并且满足这些要求？",
    "options_cn": {
      "A": "使用 DynamoDB 的时间点恢复功能持续备份表。",
      "B": "使用 AWS Backup 为表创建备份计划和保留策略。",
      "C": "使用 DynamoDB 控制台创建表的按需备份。将备份存储在 Amazon S3 存储桶中。为 S3 存储桶设置 S3 生命周期配置。",
      "D": "创建一个 Amazon EventBridge（Amazon CloudWatch Events）规则来调用一个 AWS Lambda 函数。配置 Lambda 函数以备份表并将备份存储在 Amazon S3 存储桶中。为 S3 存储桶设置 S3 生命周期配置。"
    },
    "tags": [
      "Amazon DynamoDB",
      "AWS Backup",
      "Backup and Restore"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察 DynamoDB 的数据备份与恢复策略，以及运营效率。涉及 AWS Backup、DynamoDB 时间点恢复、S3 生命周期配置等概念的理解和对比。",
      "why_correct": "AWS Backup 是一种集中式的备份服务，可以跨多个 AWS 服务（包括 DynamoDB）创建和管理备份。使用 AWS Backup，可以定义备份计划（包括备份频率和保留策略）。这种方法在运营上效率最高，因为 AWS Backup 提供了自动化的备份和保留管理功能，满足了 7 年的数据保留需求，并且易于管理和监控。",
      "why_wrong": "A. DynamoDB 的时间点恢复 (Point-in-time Recovery, PITR) 仅用于在过去 35 天内恢复表到特定时间点，不适用于长期数据保留。它主要用于数据丢失或损坏时的恢复，而非满足 7 年的数据保留要求。\nC. 通过 DynamoDB 控制台手动创建按需备份，并将备份存储在 Amazon S3 中，需要手动管理和维护备份的生命周期，不如 AWS Backup 自动化。虽然可以结合 S3 生命周期配置实现数据保留，但手动操作增加了运营负担。\nD. 使用 Lambda 函数备份 DynamoDB 表到 S3 是一种自定义的备份方案，需要开发和维护 Lambda 函数。这种方案的复杂性远高于使用 AWS Backup，而且需要手动管理备份的频率、保留策略和监控，在运营上效率较低。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB",
      "AWS Backup",
      "Amazon S3",
      "S3",
      "Lambda",
      "Amazon EventBridge",
      "CloudWatch Events",
      "Point-in-time Recovery, PITR"
    ]
  },
  {
    "id": 79,
    "topic": "1",
    "question_en": "A company is planning to use an Amazon DynamoDB table for data storage. The company is concerned about cost optimization. The table will not be used on most mornings. In the evenings, the read and write trafic will often be unpredictable. When trafic spikes occur, they will happen very quickly. What should a solutions architect recommend?",
    "options_en": {
      "A": "Create a DynamoDB table in on-demand capacity mode.",
      "B": "Create a DynamoDB table with a global secondary index.",
      "C": "Create a DynamoDB table with provisioned capacity and auto scaling.",
      "D": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table."
    },
    "correct_answer": "A",
    "vote_percentage": "79%",
    "question_cn": "一家公司计划使用 Amazon DynamoDB 表进行数据存储。该公司关注成本优化。该表在大多数早晨都不会被使用。在晚上，读写流量通常是不可预测的。当流量高峰出现时，它们会很快发生。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "在按需容量模式下创建 DynamoDB 表。",
      "B": "创建带有全局二级索引的 DynamoDB 表。",
      "C": "创建具有预置容量和自动伸缩的 DynamoDB 表。",
      "D": "在预置容量模式下创建 DynamoDB 表，并将其配置为全局表。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB On-Demand",
      "DynamoDB Auto Scaling",
      "DynamoDB Global Tables"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 79%），解析仅供参考。】\n\n考查 DynamoDB 的容量模式选择；与成本优化、流量模式及 DynamoDB 的特性相关。",
      "why_correct": "按需容量模式 (On-Demand Capacity Mode) 非常适合流量不可预测的应用程序。它会自动处理流量高峰，无需提前规划容量，从而实现成本优化。当流量在晚上出现高峰并快速变化时，按需容量模式能够自动扩展以满足需求。",
      "why_wrong": "B 选项：创建带有全局二级索引的 DynamoDB 表，这增加了存储成本和写入成本，与成本优化的目标相悖。全局二级索引会影响写入性能，当写入量大时，性能会受影响。C 选项：预置容量模式 (Provisioned Capacity Mode) 需要提前设置容量，并手动或通过自动伸缩来调整。由于早晨流量低，晚上流量不可预测，预置容量无法很好地适应这种流量模式，且自动伸缩不能完全解决流量突发的问题。D 选项：在预置容量模式下创建 DynamoDB 表，并将其配置为全局表，全局表主要用于跨区域复制数据，与题目中成本优化和流量不可预测的场景不符，且也会增加存储和数据传输成本，不符合成本优化原则。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB",
      "Auto Scaling",
      "Provisioned Capacity Mode",
      "On-Demand Capacity Mode",
      "Global Tables",
      "Global Secondary Index"
    ]
  },
  {
    "id": 80,
    "topic": "1",
    "question_en": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for help with an application migration initiative. A solutions architect needs ta share an Amazon Machine Image (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is backed by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service (AWS KMS) customer managed key to encrypt EBS volume snapshots. What is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's AWS account?",
    "options_en": {
      "A": "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
      "B": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to allow the MSP Partner's AWS account to use the key.",
      "C": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS account only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for encryption.",
      "D": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS account, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and launch the AMI in the MSP Partner's AWS account."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家公司最近与一家 AWS 托管服务提供商 (MSP) 合作伙伴签订了合同，以帮助进行应用程序迁移计划。一位解决方案架构师需要与 MSP 合作伙伴的 AWS 账户共享来自现有 AWS 账户的 Amazon Machine Image (AMI)。AMI 由 Amazon Elastic Block Store (Amazon EBS) 支持，并使用 AWS Key Management Service (AWS KMS) 客户托管密钥来加密 EBS 卷快照。解决方案架构师与 MSP 合作伙伴的 AWS 账户共享 AMI 的最安全方法是什么？",
    "options_cn": {
      "A": "将加密的 AMI 和快照公开。修改密钥策略以允许 MSP 合作伙伴的 AWS 账户使用该密钥。",
      "B": "修改 AMI 的 launchPermission 属性。仅与 MSP 合作伙伴的 AWS 账户共享 AMI。修改密钥策略以允许 MSP 合作伙伴的 AWS 账户使用该密钥。",
      "C": "修改 AMI 的 launchPermission 属性。仅与 MSP 合作伙伴的 AWS 账户共享 AMI。修改密钥策略以信任由 MSP 合作伙伴拥有的新 KMS 密钥进行加密。",
      "D": "将 AMI 从 源账户 导出到 MSP 合作伙伴的 AWS 账户中的 Amazon S3 存储桶，使用由 MSP 合作伙伴拥有的新 KMS 密钥加密 S3 存储桶。在 MSP 合作伙伴的 AWS 账户中复制并启动 AMI。"
    },
    "tags": [
      "Amazon EC2",
      "AMI",
      "Amazon EBS",
      "AWS KMS",
      "Launch Permissions"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n考查 AMI 的安全共享。涉及到 AMI 的 launchPermission 属性的设置、AWS KMS 密钥策略的配置，以及 AMI 加密和共享的流程。也与 S3 共享、密钥管理等知识点相关。",
      "why_correct": "选项 B 提供了最安全的方法，因为它允许在不公开任何敏感信息的情况下共享 AMI。通过修改 AMI 的 `launchPermission` 属性，可以指定允许哪些 AWS 账户使用该 AMI。同时，修改密钥策略以允许 MSP 合作伙伴的 AWS 账户使用 KMS 客户托管密钥，确保了 MSP 合作伙伴可以解密 EBS 卷快照。这种方法避免了 AMI 或快照的公开，并且授权访问是基于 KMS 密钥和账户的，满足了安全性和访问控制的需求。",
      "why_wrong": "选项 A 错误，因为公开加密的 AMI 和快照违背了安全最佳实践，可能导致未经授权的访问。虽然修改密钥策略允许 MSP 合作伙伴使用该密钥，但公开 AMI 和快照会带来安全风险。选项 C 错误，因为即使修改了 `launchPermission`，让 AMI 仅对 MSP 合作伙伴账户可用，但要求 KMS 密钥策略信任 MSP 合作伙伴拥有的新 KMS 密钥是不必要的，且可能引入密钥管理复杂性，也会导致密钥管理风险，增加安全隐患。选项 D 错误，因为将 AMI 导出到 S3 并使用 MSP 合作伙伴拥有的密钥加密 S3 存储桶，涉及 AMI 的迁移和复制，增加了操作的复杂性。虽然使用了 MSP 合作伙伴的密钥进行加密，但增加了数据传输和管理的复杂性，且并非最直接、最安全的方法。"
    },
    "related_terms": [
      "Amazon EC2",
      "AMI",
      "Amazon EBS",
      "AWS KMS",
      "Amazon S3",
      "EBS",
      "Launch Permissions",
      "MSP"
    ]
  },
  {
    "id": 81,
    "topic": "1",
    "question_en": "A solutions architect is designing the cloud architecture for a new application being deployed on AWS. The process should run in parallel while adding and removing application nodes as needed based on the number of jobs to be processed. The processor application is stateless. The solutions architect must ensure that the application is loosely coupled and the job items are durably stored. Which design should the solutions architect use?",
    "options_en": {
      "A": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on CPU usage.",
      "B": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch configuration that uses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy for the Auto Scaling group to add and remove nodes based on network usage.",
      "C": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of items in the SQS queue.",
      "D": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon Machine Image (AMI) that consists of the processor application. Create a launch template that uses the AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto Scaling group to add and remove nodes based on the number of messages published to the SNS topic."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为在 AWS 上部署的新应用程序设计云架构。该流程应并行运行，并根据要处理的作业数量按需添加和删除应用程序节点。处理器应用程序是无状态的。解决方案架构师必须确保应用程序是松散耦合的，并且作业项被持久存储。解决方案架构师应该使用哪种设计？",
    "options_cn": {
      "A": "创建一个 Amazon SNS 主题来发送需要处理的作业。创建一个包含处理器应用程序的 Amazon 机器映像 (AMI)。创建一个使用 AMI 的启动配置。创建一个使用启动配置的 Auto Scaling 组。将 Auto Scaling 组的缩放策略设置为根据 CPU 使用率添加和删除节点。",
      "B": "创建一个 Amazon SQS 队列来保存需要处理的作业。创建一个包含处理器应用程序的 Amazon 机器映像 (AMI)。创建一个使用 AMI 的启动配置。创建一个使用启动配置的 Auto Scaling 组。将 Auto Scaling 组的缩放策略设置为根据网络使用情况添加和删除节点。",
      "C": "创建一个 Amazon SQS 队列来保存需要处理的作业。创建一个包含处理器应用程序的 Amazon 机器映像 (AMI)。创建一个使用 AMI 的启动模板。创建一个使用启动模板的 Auto Scaling 组。将 Auto Scaling 组的缩放策略设置为根据 SQS 队列中的项目数量添加和删除节点。",
      "D": "创建一个 Amazon SNS 主题来发送需要处理的作业。创建一个包含处理器应用程序的 Amazon 机器映像 (AMI)。创建一个使用 AMI 的启动模板。创建一个使用启动模板的 Auto Scaling 组。将 Auto Scaling 组的缩放策略设置为根据发布到 SNS 主题的消息数量添加和删除节点。"
    },
    "tags": [
      "Amazon SQS",
      "Auto Scaling",
      "Amazon EC2",
      "AMI",
      "Launch Template",
      "Scalability",
      "Loose Coupling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查基于队列的异步任务处理架构设计，以及 Auto Scaling 的配置。重点在于选择合适的队列服务（Amazon SQS），并根据作业负载进行自动伸缩，以满足并行处理、松散耦合和持久存储的需求；也涉及了 AMI、启动模板、Auto Scaling Group 等基础设施的合理配置。与发布/订阅模式（SNS）、基于 CPU/网络利用率的伸缩策略等相关知识点密切关联。",
      "why_correct": "选项 C 提供了最优的解决方案。它使用 Amazon SQS 队列作为作业存储，实现了松散耦合和持久存储的要求。Auto Scaling 组根据 SQS 队列中的消息数量进行伸缩，这直接对应了“根据要处理的作业数量按需添加和删除应用程序节点”的需求。使用启动模板比启动配置更灵活，也更易于版本管理和配置更新，也符合最佳实践。",
      "why_wrong": "选项 A 使用了 Amazon SNS 主题，这更适合发布/订阅模式，而不是存储和持久化作业。此外，根据 CPU 使用率进行伸缩，可能无法直接反映待处理的作业数量，导致伸缩不够及时或过度伸缩。选项 B 使用了 Amazon SQS，但根据网络使用情况进行伸缩，这也不是处理作业数量的直接指标，与题目要求不符。选项 D 使用了 SNS 主题，与选项 A 类似，不适合作业的持久存储；虽然根据 SNS 消息数量伸缩在某些场景下可行，但不如直接根据 SQS 队列中的项目数量更贴合题目需求。使用启动模板本身没错，但是因为使用了 SNS 就不对了。"
    },
    "related_terms": [
      "Amazon SQS",
      "Auto Scaling",
      "Amazon EC2",
      "AMI",
      "Amazon SNS",
      "CPU",
      "EC2",
      "Launch Template",
      "Network",
      "Auto Scaling Group"
    ]
  },
  {
    "id": 82,
    "topic": "1",
    "question_en": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load Balancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's security team must be notified 30 days before the expiration of each certificate. What should a solutions architect recommend to meet this requirement?",
    "options_en": {
      "A": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon SNS) topic every day, beginning 30 days before any certificate will expire.",
      "B": "Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure Amazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon Simple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.",
      "C": "Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon CloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the alarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will expire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda function to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS)."
    },
    "correct_answer": "B",
    "vote_percentage": "52%",
    "question_cn": "一家公司在 AWS 云中托管其 Web 应用程序。该公司配置 Elastic Load Balancer 以使用导入到 AWS Certificate Manager (ACM) 中的证书。该公司的安全团队必须在每个证书到期前 30 天收到通知。解决方案架构师应推荐什么来满足此要求？",
    "options_cn": {
      "A": "在 ACM 中添加一个规则，每天将自定义消息发布到 Amazon Simple Notification Service (Amazon SNS) 主题，从任何证书到期前 30 天开始。",
      "B": "创建一个 AWS Config 规则，检查将在 30 天内到期的证书。配置 Amazon EventBridge (Amazon CloudWatch Events) 通过 Amazon Simple Notification Service (Amazon SNS) 调用自定义警报，当 AWS Config 报告不合规资源时。",
      "C": "使用 AWS Trusted Advisor 检查将在 30 天内到期的证书。创建一个 Amazon CloudWatch 警报，该警报基于 Trusted Advisor 的检查状态更改指标。配置警报以通过 Amazon Simple Notification Service (Amazon SNS) 发送自定义警报。",
      "D": "创建一个 Amazon EventBridge (Amazon CloudWatch Events) 规则，以检测将在 30 天内到期的任何证书。配置该规则以调用一个 AWS Lambda 函数。配置 Lambda 函数以通过 Amazon Simple Notification Service (Amazon SNS) 发送自定义警报。"
    },
    "tags": [
      "AWS Certificate Manager",
      "Amazon SNS",
      "Amazon EventBridge",
      "Lambda",
      "AWS Config",
      "AWS Trusted Advisor",
      "Amazon CloudWatch",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 52%），解析仅供参考。】\n\n考察使用 AWS Config、EventBridge 和 SNS 监控和通知 ACM 证书到期情况。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：AWS Config 可以检测不合规的资源，这里的不合规是指证书将在 30 天内到期。创建 AWS Config 规则，结合 EventBridge 和 SNS，可以实现当证书即将到期时，通过 SNS 发送自定义警报。这种方案能够自动、及时地通知证书到期情况，满足题目的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 的方案需要手动添加规则，不够自动化且不推荐。选项 C 使用 Trusted Advisor 检查，但 Trusted Advisor 的指标不一定能够精确反映证书到期时间，且配置较为复杂。选项 D 使用 EventBridge 直接检测证书到期，需要编写 Lambda 函数进行逻辑处理，不如使用 AWS Config 和 EventBridge 的组合方案简洁高效。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Certificate Manager (ACM)",
      "Elastic Load Balancer",
      "Amazon Simple Notification Service (Amazon SNS)",
      "AWS Config",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "AWS Trusted Advisor",
      "Amazon CloudWatch",
      "AWS Lambda"
    ]
  },
  {
    "id": 83,
    "topic": "1",
    "question_en": "A company's dynamic website is hosted using on-premises servers in the United States. The company is launching its product in Europe, and it wants to optimize site loading times for new European users. The site's backend must remain in the United States. The product is being launched in a few days, and an immediate solution is needed. What should the solutions architect recommend?",
    "options_en": {
      "A": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.",
      "B": "Move the website to Amazon S3. Use Cross-Region Replication between Regions.",
      "C": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.",
      "D": "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的动态网站使用位于美国的本地服务器进行托管。该公司将在欧洲推出其产品，并希望优化新欧洲用户的站点加载时间。该站点的后端必须保留在美国。该产品将在几天内推出，并且需要一个即时解决方案。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "在 us-east-1 启动一个 Amazon EC2 实例并将站点迁移到它。",
      "B": "将网站移至 Amazon S3。使用跨区域复制 between Regions。",
      "C": "使用 Amazon CloudFront，其自定义源指向本地服务器。",
      "D": "使用 Amazon Route 53 地理位置路由策略，指向本地服务器。"
    },
    "tags": [
      "Amazon CloudFront",
      "Content Delivery Network (CDN)",
      "Amazon EC2",
      "Amazon S3",
      "Amazon Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 CDN 优化网站性能；与内容分发、就近访问、源服务器相关。同时考察了不同 AWS 服务在类似场景下的适用性，以及如何满足快速部署的需求。",
      "why_correct": "Amazon CloudFront 是一个内容分发网络（CDN）服务，能够将内容缓存在全球分布的边缘站点上。通过将网站内容缓存在离欧洲用户更近的边缘站点，可以显著减少延迟，提高加载速度。 CloudFront 的自定义源功能允许指定网站的源服务器（在本例中为美国的本地服务器），满足后端保留在美国的要求。此方案实现起来快速，满足题目中即时解决方案的需求。",
      "why_wrong": "选项 A 错误，在 us-east-1 启动一个 Amazon EC2 实例并将站点迁移到它，虽然可以解决部分问题，但是将网站迁移到 us-east-1 无法解决欧洲用户的延迟问题，反而会因为服务器位置变动而影响美国用户的访问速度，不满足优化欧洲用户体验的目标。 选项 B 错误，将网站移至 Amazon S3 并使用跨区域复制。此方案将网站的静态内容复制到欧洲的 S3 Bucket，可以提升访问速度，但是题目中提到“后端必须保留在美国”，跨区域复制无法满足该要求。 选项 D 错误，使用 Amazon Route 53 地理位置路由策略，指向本地服务器。 Route 53 的地理位置路由只能将用户的请求路由到不同的资源，而不能缓存内容。这无法解决欧洲用户访问时，源服务器延迟高的问题。 Route 53 主要用于域名解析，而不是内容分发，不能提升站点加载速度。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "CDN",
      "Amazon EC2",
      "Amazon S3",
      "Route 53",
      "us-east-1",
      "Lambda",
      "EBS",
      "Regions"
    ]
  },
  {
    "id": 84,
    "topic": "1",
    "question_en": "A company wants to reduce the cost of its existing three-tier web architecture. The web, application, and database servers are running on Amazon EC2 instances for the development, test, and production environments. The EC2 instances average 30% CPU utilization during peak hours and 10% CPU utilization during non-peak hours. The production EC2 instances run 24 hours a day. The development and test EC2 instances run for at least 8 hours each day. The company plans to implement automation to stop the development and test EC2 instances when they are not in use. Which EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
      "B": "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the development and test EC2 instances.",
      "C": "Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and test EC2 instances.",
      "D": "Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development and test EC2 instances."
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一家公司希望降低其现有三层 Web 架构的成本。 Web、应用程序和数据库服务器在 Amazon EC2 实例上运行，用于开发、测试和生产环境。 EC2 实例在高峰时段的平均 CPU 利用率为 30%，在非高峰时段的 CPU 利用率为 10%。 生产 EC2 实例每天运行 24 小时。 开发和测试 EC2 实例每天至少运行 8 小时。 该公司计划实施自动化，以便在不使用时停止开发和测试 EC2 实例。 哪种 EC2 实例购买解决方案最符合公司的要求，并且最具成本效益？",
    "options_cn": {
      "A": "对生产 EC2 实例使用 Spot 实例。 对开发和测试 EC2 实例使用预留实例。",
      "B": "对生产 EC2 实例使用预留实例。 对开发和测试 EC2 实例使用按需实例。",
      "C": "对生产 EC2 实例使用 Spot 块。 对开发和测试 EC2 实例使用预留实例。",
      "D": "对生产 EC2 实例使用按需实例。 对开发和测试 EC2 实例使用 Spot 块。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Reserved Instances",
      "EC2 On-Demand Instances",
      "EC2 Spot Instances",
      "EC2 Spot Blocks"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n本题考查 EC2 实例购买选项，主要考察如何在不同环境下选择最具成本效益的方案，并考虑到实例的稳定运行时间。与 EC2 实例的不同购买选项、成本优化、以及弹性伸缩相关。",
      "why_correct": "对于生产环境 EC2 实例，预留实例（Reserved Instances）是最佳选择，因为生产环境需要 24/7 持续运行。预留实例提供了显著的折扣，并且可以提供稳定的容量保证，避免 Spot 实例可能出现的被中断风险。对于开发和测试环境，按需实例（On-Demand Instances）是更合适的选择，因为这些实例并非全天候运行，可以在不需要时停止，按需付费，避免了预留实例的固定成本，并且无需提前预估容量需求。",
      "why_wrong": "选项 A 错误，Spot 实例不适用于生产环境，因为它们可能随时被 AWS 中断，无法满足 24/7 的可用性需求。预留实例的费用比 Spot 实例高，但提供了稳定的容量保障，更适合生产环境。选项 C 错误，Spot Blocks 类似于 Spot 实例，但其运行时间更长，但仍可能被中断，可靠性不如预留实例；预留实例更适合开发和测试环境。选项 D 错误，按需实例费用高于预留实例，不适合生产环境，这将导致不必要的成本支出；Spot 块不适合开发和测试环境，其价格不具有优势，且可能因中断造成影响。"
    },
    "related_terms": [
      "EC2",
      "CPU",
      "EC2 Instances",
      "Reserved Instances",
      "On-Demand Instances",
      "Spot Instances",
      "Spot Blocks"
    ]
  },
  {
    "id": 85,
    "topic": "1",
    "question_en": "A company has a production web application in which users upload documents through a web interface or a mobile app. According to a new regulatory requirement. new documents cannot be modified or deleted after they are stored. What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock enabled.",
      "B": "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive the documents periodically.",
      "C": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an ACL to restrict all access to read-only.",
      "D": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the data by mounting the volume in read-only mode."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个生产型 Web 应用程序，用户可以通过 Web 界面或移动应用程序上传文档。根据一项新的监管要求，新文档在存储后不能被修改或删除。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "将上传的文档存储在启用了 S3 版本控制和 S3 对象锁定的 Amazon S3 存储桶中。",
      "B": "将上传的文档存储在 Amazon S3 存储桶中。配置 S3 生命周期策略以定期间隔归档文档。",
      "C": "将上传的文档存储在启用了 S3 版本控制的 Amazon S3 存储桶中。配置 ACL 以将所有访问权限限制为只读。",
      "D": "将上传的文档存储在 Amazon Elastic File System (Amazon EFS) 卷上。通过以只读模式挂载该卷来访问数据。"
    },
    "tags": [
      "Amazon S3",
      "S3 Versioning",
      "S3 Object Lock",
      "S3 Lifecycle",
      "Amazon EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在满足监管需求下，如何实现存储对象的不可变性，以及 Amazon S3 的版本控制、对象锁定特性。涉及对 S3 存储桶配置的选择和理解，并与 S3 生命周期、EFS 等服务进行对比。",
      "why_correct": "将上传的文档存储在启用了 S3 版本控制和 S3 对象锁定的 Amazon S3 存储桶中，能够满足监管要求。S3 版本控制允许您保留对象的多个版本，并能防止意外删除。S3 对象锁定可以阻止对象在指定的保留期内被删除或覆盖，从而确保数据不可变。",
      "why_wrong": "B 选项不正确，因为 S3 生命周期策略主要用于数据迁移和归档，虽然可以减少对数据的更改，但并不能直接提供不可变性，无法满足监管要求中“不能被修改或删除”的需求。C 选项不正确，仅使用 S3 版本控制，并不能完全防止文档被删除，并且 ACL 更多用于控制访问权限，而不是保证数据不可变性。D 选项不正确，虽然 EFS 支持只读挂载，但 EFS 本身不提供内置的不可变性特性，无法满足“存储后不能被修改或删除”的监管要求，并且 EFS 的设计目标也并非针对对象存储场景。"
    },
    "related_terms": [
      "Amazon S3",
      "ACL",
      "Amazon EFS",
      "EC2",
      "EBS",
      "S3 Versioning",
      "S3 Object Lock",
      "S3 Lifecycle"
    ]
  },
  {
    "id": 86,
    "topic": "1",
    "question_en": "A company has several web servers that need to frequently access a common Amazon RDS MySQL Multi-AZ DB instance. The company wants a secure method for the web servers to connect to the database while meeting a security requirement to rotate user credentials frequently. Which solution meets these requirements?",
    "options_en": {
      "A": "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions to allow the web servers to access AWS Secrets Manager.",
      "B": "Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM permissions to allow the web servers to access OpsCenter.",
      "C": "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM permissions to allow the web servers to retrieve credentials and access the database.",
      "D": "Store the database user credentials in files encrypted with AWS Key Management Service (AWS KMS) on the web server file system. The web server should be able to decrypt the files and access the database."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有多个 Web 服务器，需要频繁访问一个公共 Amazon RDS MySQL 多可用区数据库实例。该公司希望 Web 服务器能够安全地连接到数据库，同时满足定期轮换用户凭证的安全要求。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "将数据库用户凭证存储在 AWS Secrets Manager 中。授予必要的 IAM 权限，以允许 Web 服务器访问 AWS Secrets Manager。",
      "B": "将数据库用户凭证存储在 AWS Systems Manager OpsCenter 中。授予必要的 IAM 权限，以允许 Web 服务器访问 OpsCenter。",
      "C": "将数据库用户凭证存储在安全的 Amazon S3 存储桶中。授予必要的 IAM 权限，以允许 Web 服务器检索凭证并访问数据库。",
      "D": "将数据库用户凭证存储在用 AWS Key Management Service (AWS KMS) 加密的 Web 服务器文件系统中。Web 服务器应该能够解密文件并访问数据库。"
    },
    "tags": [
      "AWS Secrets Manager",
      "Amazon RDS",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查安全存储和管理数据库凭证。与 AWS Secrets Manager、IAM 权限配置、以及数据库连接的安全性相关。",
      "why_correct": "AWS Secrets Manager 是一种专门用于安全存储和管理密码、API 密钥和其他敏感信息的服务。使用 Secrets Manager，可以集中存储数据库凭证，并允许通过 IAM 权限控制对这些凭证的访问。 Secrets Manager 支持凭证轮换，满足了题目中定期轮换用户凭证的要求。 Web 服务器可以通过 IAM 角色访问 Secrets Manager，从而安全地获取数据库凭证，而无需将凭证硬编码到代码或配置文件中。",
      "why_wrong": "选项 B 错误，因为 AWS Systems Manager OpsCenter 主要是用于运维管理，它并非设计用于安全存储和轮换敏感凭证。选项 C 错误，将数据库凭证存储在 Amazon S3 中不符合最佳实践，S3 对象并非专门设计用于安全存储敏感信息，且 S3 访问控制不如 Secrets Manager 细粒度，难以实现凭证轮换。选项 D 错误，将凭证存储在 Web 服务器文件系统中不安全，因为这使得凭证更容易受到攻击，且难以进行轮换。此外，服务器文件系统的加密，密钥管理和轮换也会带来额外复杂性，不如使用 Secrets Manager 方便。"
    },
    "related_terms": [
      "AWS Secrets Manager",
      "Amazon RDS",
      "MySQL",
      "IAM",
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)",
      "Web Server",
      "OpsCenter"
    ]
  },
  {
    "id": 87,
    "topic": "1",
    "question_en": "A company hosts an application on AWS Lambda functions that are invoked by an Amazon API Gateway API. The Lambda functions save customer data to an Amazon Aurora MySQL database. Whenever the company upgrades the database, the Lambda functions fail to establish database connections until the upgrade is complete. The result is that customer data is not recorded for some of the event. A solutions architect needs to design a solution that stores customer data that is created during database upgrades. Which solution will meet these requirements?",
    "options_en": {
      "A": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure the Lambda functions to connect to the RDS proxy.",
      "B": "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the code that stores the customer data in the database.",
      "C": "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the local storage to save the customer data to the database.",
      "D": "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a new Lambda function that polls the queue and stores the customer data in the database."
    },
    "correct_answer": "D",
    "vote_percentage": "61%",
    "question_cn": "一家公司在其由 Amazon API Gateway API 调用的 AWS Lambda 函数上托管一个应用程序。Lambda 函数将客户数据保存到 Amazon Aurora MySQL 数据库。每当公司升级数据库时，Lambda 函数都会在升级完成之前无法建立数据库连接。结果是，某些事件的客户数据未被记录。一位解决方案架构师需要设计一个解决方案来存储在数据库升级期间创建的客户数据。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置一个 Amazon RDS 代理，位于 Lambda 函数和数据库之间。配置 Lambda 函数连接到 RDS 代理。",
      "B": "将 Lambda 函数的运行时增加到最大。在代码中创建一个重试机制，将客户数据存储在数据库中。",
      "C": "将客户数据持久保存到 Lambda 本地存储。配置新的 Lambda 函数来扫描本地存储，以便将客户数据保存到数据库。",
      "D": "将客户数据存储在 Amazon Simple Queue Service (Amazon SQS) FIFO 队列中。创建一个新的 Lambda 函数来轮询队列并将客户数据存储在数据库中。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "AWS Lambda",
      "Amazon Aurora MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 61%），解析仅供参考。】\n\n考察在数据库升级期间，如何设计方案来持久化 Lambda 函数产生的客户数据，保证数据不丢失。",
      "why_correct": "选项 D 提供了使用 Amazon SQS FIFO 队列的解决方案。Lambda 函数将数据发送到 FIFO 队列，保证数据按照接收顺序被处理，即使数据库升级导致连接中断，数据也能被安全存储。另一个 Lambda 函数轮询队列，并将数据最终写入 Aurora MySQL 数据库，保证了数据的持久性和最终一致性。",
      "why_wrong": "选项 A，Amazon RDS Proxy 可以提高数据库连接效率和可用性，但在数据库升级期间，Lambda 函数仍然可能因为连接问题而无法写入数据。选项 B，增加 Lambda 函数的运行时并不能解决数据库连接问题，重试机制可能在连接未恢复时持续失败，导致数据丢失。选项 C，使用 Lambda 本地存储会导致数据在函数执行结束后丢失，不具备持久性，也无法保证数据存储的可靠性，同时增加新的 Lambda 函数扫描本地存储，增加了架构的复杂性。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon Aurora MySQL",
      "Amazon RDS",
      "RDS Proxy",
      "Amazon Simple Queue Service (Amazon SQS)",
      "FIFO"
    ]
  },
  {
    "id": 88,
    "topic": "1",
    "question_en": "A survey company has gathered data for several years from areas in the United States. The company hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company has started to share the data with a European marketing firm that has S3 buckets. The company wants to ensure that its data transfer costs remain as low as possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the Requester Pays feature on the company's S3 bucket.",
      "B": "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's S3 buckets.",
      "C": "Configure cross-account access for the marketing firm so that the marketing firm has access to the company's S3 bucket.",
      "D": "Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the marketing firm's S3 buckets."
    },
    "correct_answer": "B",
    "vote_percentage": "46%",
    "question_cn": "一家调查公司收集了美国多个地区多年来的数据。该公司将数据托管在大小为 3 TB 且不断增长的 Amazon S3 存储桶中。该公司已开始与拥有 S3 存储桶的欧洲营销公司共享数据。该公司希望确保其数据传输成本保持在尽可能低的水平。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在公司的 S3 存储桶上配置 Requester Pays 功能。",
      "B": "配置 S3 跨区域复制，将数据从公司的 S3 存储桶复制到营销公司的其中一个 S3 存储桶。",
      "C": "为营销公司配置跨账户访问权限，以便营销公司可以访问公司的 S3 存储桶。",
      "D": "将公司的 S3 存储桶配置为使用 S3 Intelligent-Tiering。将 S3 存储桶同步到营销公司的其中一个 S3 存储桶。"
    },
    "tags": [
      "Amazon S3",
      "S3 Cross-Region Replication",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 46%），解析仅供参考。】\n\n考查 Amazon S3 的数据传输成本优化策略；与 S3 跨区域复制、访问控制、存储分层以及 Requester Pays 模式的选择相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：配置 S3 跨区域复制 (S3 Cross-Region Replication) 将数据从公司的 S3 存储桶复制到营销公司的其中一个 S3 存储桶，可以使营销公司从地理位置更近的存储桶中下载数据，从而减少数据传输延迟。同时，营销公司从其自己的存储桶中下载数据，数据传输成本由营销公司承担，满足了降低数据传输成本的要求。此方案解决了数据传输成本高昂的问题，并且使营销公司可以更高效地访问数据。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，Requester Pays 功能会使数据访问的成本由请求者（即营销公司）承担，但无法降低数据传输的总体成本，因为数据仍然需要从美国传输到欧洲，这将产生高昂的传输费用。选项 C，配置跨账户访问权限，虽然允许营销公司访问数据，但数据传输成本仍然是由调查公司承担，这与题目要求不符。选项 D，使用 S3 Intelligent-Tiering，仅优化存储成本，并不会解决数据传输的成本问题，而且同步到营销公司的 S3 存储桶也会导致调查公司产生数据传输费用。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3",
      "S3 Cross-Region Replication",
      "Requester Pays"
    ]
  },
  {
    "id": 89,
    "topic": "1",
    "question_en": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket policies to restrict access to audit team IAM user credentials according to the principle of least privilege. Company managers are worried about accidental deletion of documents in the S3 bucket and want a more secure solution. What should a solutions architect do to secure the audit documents?",
    "options_en": {
      "A": "Enable the versioning and MFA Delete features on the S3 bucket.",
      "B": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user account.",
      "C": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action during audit dates.",
      "D": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team IAM user accounts from accessing the KMS key."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon S3 存储其机密的审计文档。 S3 存储桶使用存储桶策略，根据最小权限原则限制对审计团队 IAM 用户凭证的访问。 公司经理担心 S3 存储桶中文件的意外删除，并希望获得更安全的解决方案。 解决方案架构师应该怎么做来保护审计文档？",
    "options_cn": {
      "A": "在 S3 存储桶上打开版本控制和 MFA 删除功能。",
      "B": "在每个审计团队 IAM 用户帐户的 IAM 用户凭证上打开多因素身份验证 (MFA)。",
      "C": "为审计团队的 IAM 用户帐户添加一个 S3 生命周期策略，以在审计日期期间拒绝 s3:DeleteObject 操作。",
      "D": "使用 AWS Key Management Service (AWS KMS) 加密 S3 存储桶，并限制审计团队 IAM 用户帐户访问 KMS 密钥。"
    },
    "tags": [
      "Amazon S3",
      "S3 Versioning",
      "MFA Delete",
      "IAM",
      "S3 Bucket Policy",
      "AWS KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查保护 S3 存储桶中对象免受意外删除的解决方案；与 S3 版本控制、MFA 删除、IAM 用户权限、KMS 加密等相关。",
      "why_correct": "在 S3 存储桶上启用版本控制和 MFA 删除功能是保护审计文档免受意外删除的有效方法。版本控制允许您保留对象的多个版本，即使对象被删除，也可以恢复到之前的版本。 MFA 删除增加了额外的安全层，要求用户在删除对象时提供 MFA 身份验证，从而防止未经授权的删除。",
      "why_wrong": "选项 B 仅在 IAM 用户级别启用了 MFA，但它并未直接保护 S3 存储桶中的对象。即使启用了 MFA，如果用户拥有删除对象的权限，他们仍然可以删除 S3 中的对象。选项 C 使用生命周期策略来限制删除操作，这依赖于 IAM 用户的权限，而 IAM 权限本身可能受到影响。此外，生命周期策略无法防止具有删除权限的用户恶意删除对象。选项 D 侧重于加密，无法直接解决意外删除的问题；虽然 KMS 可以保护数据安全，但它并不能防止用户意外删除对象，也不会保留被删除对象的历史记录。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "IAM",
      "AWS KMS",
      "KMS",
      "S3 Bucket Policy",
      "S3 Versioning",
      "MFA Delete",
      "IAM User"
    ]
  },
  {
    "id": 90,
    "topic": "1",
    "question_en": "A company is using a SQL database to store movie data that is publicly accessible. The database runs on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day to record the number of new movies that have been added to the database. The script must report a final total during business hours. The company's development team notices that the database performance is inadequate for development tasks when the script is running. A solutions architect must recommend a solution to resolve this issue. Which solution will meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Modify the DB instance to be a Multi-AZ deployment.",
      "B": "Create a read replica of the database. Configure the script to query only the read replica.",
      "C": "Instruct the development team to manually export the entries in the database at the end of each day.",
      "D": "Use Amazon ElastiCache to cache the common queries that the script runs against the database."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司正在使用 SQL 数据库来存储可公开访问的电影数据。该数据库运行在 Amazon RDS 单可用区数据库实例上。一个脚本每天以随机间隔运行查询，以记录已添加到数据库中的新电影的数量。该脚本必须在工作时间内报告最终总数。该公司的开发团队注意到，当脚本运行时，数据库性能对于开发任务来说是不够的。解决方案架构师必须推荐一个解决方案来解决这个问题。哪种解决方案将以最少的运营开销满足此要求？",
    "options_cn": {
      "A": "将数据库实例修改为多可用区部署。",
      "B": "创建数据库的只读副本。配置脚本仅查询只读副本。",
      "C": "指示开发团队在每天结束时手动导出数据库中的条目。",
      "D": "使用 Amazon ElastiCache 缓存脚本对数据库运行的常见查询。"
    },
    "tags": [
      "Amazon RDS",
      "Amazon ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考查数据库性能优化；与数据库实例类型、只读副本、缓存策略的选择相关。",
      "why_correct": "使用 Amazon ElastiCache 缓存脚本对数据库运行的常见查询可以提高性能。ElastiCache 通过将查询结果存储在内存中，从而减少了对数据库的请求次数，加快了响应速度。由于脚本的查询是重复性的，使用 ElastiCache 可以显著改善数据库性能，并且运营开销相对较低。",
      "why_wrong": "A. 将数据库实例修改为多可用区部署主要用于提高数据库的可用性和容错能力，而不是解决性能问题。它无法直接改善脚本查询对数据库造成的性能瓶颈。\nB. 创建数据库的只读副本虽然可以减轻主数据库的负载，但创建和维护只读副本需要额外的运营开销。同时，使用只读副本可能需要更改应用程序代码。虽然可以缓解数据库的负载，但无法解决频繁的脚本查询带来的性能问题，且增加了管理复杂性。\nC. 指示开发团队手动导出数据库中的条目会引入额外的人工操作，并且与题干中要求在工作时间内报告最终总数的需求不符。手动导出不是一个自动化的、高效的解决方案，且容易出错，也无法改善数据库的性能。"
    },
    "related_terms": [
      "Amazon RDS",
      "Amazon ElastiCache",
      "SQL",
      "RDS",
      "EC2",
      "EBS"
    ]
  },
  {
    "id": 91,
    "topic": "1",
    "question_en": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications needs to call the Amazon S3 API to store and read objects. According to the company's security regulations, no trafic from the applications is allowed to travel across the internet. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an S3 gateway endpoint.",
      "B": "Create an S3 bucket in a private subnet.",
      "C": "Create an S3 bucket in the same AWS Region as the EC2 instances.",
      "D": "Configure a NAT gateway in the same subnet as the EC2 instances."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有在 VPC 中的 Amazon EC2 实例上运行的应用程序。其中一个应用程序需要调用 Amazon S3 API 来存储和读取对象。根据公司的安全规定，应用程序的流量不允许通过互联网传输。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置一个 S3 网关 VPC endpoint。",
      "B": "在私有子网中创建一个 S3 存储桶。",
      "C": "在与 EC2 实例相同的 AWS 区域中创建一个 S3 存储桶。",
      "D": "在与 EC2 实例相同的子网中配置一个 NAT Gateway。"
    },
    "tags": [
      "VPC",
      "EC2",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n该题考察了在 VPC 环境中，EC2 实例访问 S3 的安全性问题。 选项 A 使用 S3 网关 VPC endpoint，允许 EC2 实例通过私有网络安全访问 S3，且流量不会经过互联网。",
      "why_correct": "S3 网关 VPC endpoint 允许从 VPC 内部通过私有连接访问 S3，符合安全规定。",
      "why_wrong": "B 选项错误，在私有子网创建 S3 存储桶，不能解决从 EC2 访问 S3 的问题。C 选项错误，S3 存储桶与 EC2 实例在同一区域，不涉及网络访问问题。D 选项错误，NAT Gateway 用于允许私有子网的实例访问 Internet，与直接访问 S3 无关。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "S3",
      "NAT Gateway",
      "S3 VPC endpoint"
    ]
  },
  {
    "id": 92,
    "topic": "1",
    "question_en": "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to provide secure access to this bucket from the application tier running on Amazon EC2 instances inside a VPC. Which combination of steps should a solutions architect take to accomplish this? (Choose two.)",
    "options_en": {
      "A": "Configure a VPC gateway endpoint for Amazon S3 within the VPC.",
      "B": "Create a bucket policy to make the objects in the S3 bucket public.",
      "C": "Create a bucket policy that limits access to only the application tier running in the VPC.",
      "D": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instanc",
      "E": "E. Create a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket."
    },
    "correct_answer": "AC",
    "vote_percentage": "88%",
    "question_cn": "一家公司正在将敏感的用户信息存储在 Amazon S3 存储桶中。该公司希望从在 VPC 内的 Amazon EC2 实例上运行的应用层安全地访问此存储桶。 解决方案架构师应采取哪些步骤的组合来实现此目的？（选择两个。）",
    "options_cn": {
      "A": "在 VPC 内为 Amazon S3 配置一个 VPC gateway endpoint。",
      "B": "创建一个桶策略，使 S3 存储桶中的对象公开。",
      "C": "创建一个桶策略，将访问限制为仅在 VPC 中运行的应用层。",
      "D": "创建一个带有 S3 访问策略的 IAM 用户，并将 IAM 凭证复制到 EC2 实例。",
      "E": "创建一个 NAT 实例，并让 EC2 实例使用 NAT 实例来访问 S3 存储桶。"
    },
    "tags": [
      "VPC",
      "S3",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 88%），解析仅供参考。】\n\n本题考察了如何从 VPC 内的 EC2 实例安全地访问 S3 存储桶。 选项 A 和 C 结合使用，通过 VPC gateway endpoint 和桶策略来实现安全访问。",
      "why_correct": "A 选项通过配置 S3 VPC gateway endpoint，保证了 EC2 实例通过 VPC 内部网络访问 S3，避免流量暴露到公网。 C 选项通过桶策略，限制只有在 VPC 中运行的应用层才能访问 S3 存储桶，加强了安全性。",
      "why_wrong": "B 选项错误，桶策略将访问限制为公开，不符合安全要求。D 选项错误，IAM 用户凭证嵌入 EC2 不安全。E 选项错误，NAT 实例提供 Internet 访问，与从 VPC 内安全访问 S3 的目标不符。"
    },
    "related_terms": [
      "VPC",
      "S3",
      "EC2",
      "IAM",
      "VPC gateway endpoint"
    ]
  },
  {
    "id": 93,
    "topic": "1",
    "question_en": "A company runs an on-premises application that is powered by a MySQL database. The company is migrating the application to AWS to increase the application's elasticity and availability. The current architecture shows heavy read activity on the database during times of normal operation. Every 4 hours, the company's development team pulls a full export of the production database to populate a database in the staging environment. During this period, users experience unacceptable application latency. The development team is unable to use the staging environment until the procedure completes. A solutions architect must recommend replacement architecture that alleviates the application latency issue. The replacement architecture also must give the development team the ability to continue using the staging environment without delay. Which solution meets these requirements?",
    "options_en": {
      "A": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility.",
      "B": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to create the staging database on- demand.",
      "C": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the standby instance for the staging database.",
      "D": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate the staging database by implementing a backup and restore process that uses the mysqldump utility."
    },
    "correct_answer": "B",
    "vote_percentage": "91%",
    "question_cn": "一家公司运行一个由 MySQL 数据库支持的本地应用程序。该公司正在将应用程序迁移到 AWS 以提高应用程序的弹性和可用性。当前架构显示在正常运行期间数据库上有大量的读取活动。每 4 小时，公司的开发团队都会提取生产数据库的完整导出，以填充登台环境中的数据库。在此期间，用户会遇到不可接受的应用程序延迟。在过程完成之前，开发团队无法使用登台环境。一个解决方案架构师必须推荐一个能够缓解应用程序延迟问题的替代架构。替代架构还必须使开发团队能够继续使用登台环境而不会延迟。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Aurora MySQL 和 Multi-AZ Aurora 副本进行生产。通过实施使用 mysqldump 实用程序的备份和恢复过程来填充登台数据库。",
      "B": "使用 Amazon Aurora MySQL 和 Multi-AZ Aurora 副本进行生产。使用数据库克隆按需创建登台数据库。",
      "C": "使用 Amazon RDS for MySQL 和 Multi-AZ 部署以及用于生产的读取副本。使用备用实例作为登台数据库。",
      "D": "使用 Amazon RDS for MySQL 和 Multi-AZ 部署以及用于生产的读取副本。通过实施使用 mysqldump 实用程序的备份和恢复过程来填充登台数据库。"
    },
    "tags": [
      "Aurora",
      "MySQL",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 91%），解析仅供参考。】\n\n该题考察了在 MySQL 数据库迁移到 AWS 后，如何解决应用程序的延迟问题以及开发团队继续使用登台环境的需求。 选项 B 使用 Aurora MySQL 数据库克隆功能，可以快速创建登台数据库，从而解决问题。",
      "why_correct": "Aurora 数据库克隆能快速创建登台数据库，且不影响生产环境性能，满足了题目需求。",
      "why_wrong": "A 选项错误，使用 mysqldump 备份和恢复过程会产生延迟。 C 选项错误，使用备用实例作为登台数据库，仍然会影响性能。 D 选项错误，使用 mysqldump 备份和恢复过程会产生延迟。"
    },
    "related_terms": [
      "Aurora",
      "MySQL",
      "RDS",
      "mysqldump",
      "database clone"
    ]
  },
  {
    "id": 94,
    "topic": "1",
    "question_en": "A company is designing an application where users upload small files into Amazon S3. After a user uploads a file, the file requires one-time simple processing to transform the data and save the data in JSON format for later analysis. Each file must be processed as quickly as possible after it is uploaded. Demand will vary. On some days, users will upload a high number of files. On other days, users will upload a few files or no files. Which solution meets these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the data. Store the resulting JSON file in an Amazon Aurora DB cluster.",
      "B": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
      "C": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the resulting JSON file in Amazon DynamoDB.",
      "D": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis Data Streams when a new file is uploaded. Use an AWS Lambda function to consume the event from the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB cluster."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在设计一个应用程序，用户将小文件上传到 Amazon S3。在用户上传文件后，该文件需要一次性简单处理以转换数据，并将数据保存为 JSON 格式，以供以后分析。每个文件必须在上传后尽快处理。需求量会有所不同。在某些日子，用户将上传大量文件。在其他日子，用户将上传少量文件或不上传任何文件。哪个解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon EMR 从 Amazon S3 读取文本文件。运行处理脚本以转换数据。将生成的 JSON 文件存储在 Amazon Aurora 数据库集群中。",
      "B": "配置 Amazon S3 向 Amazon Simple Queue Service (Amazon SQS) 队列发送事件通知。使用 Amazon EC2 实例从队列中读取并处理数据。将生成的 JSON 文件存储在 Amazon DynamoDB 中。",
      "C": "配置 Amazon S3 向 Amazon Simple Queue Service (Amazon SQS) 队列发送事件通知。使用 AWS Lambda 函数从队列中读取并处理数据。将生成的 JSON 文件存储在 Amazon DynamoDB 中。",
      "D": "配置 Amazon EventBridge (Amazon CloudWatch Events) 在上传新文件时向 Amazon Kinesis Data Streams 发送事件。使用 AWS Lambda 函数从流中消费事件并处理数据。将生成的 JSON 文件存储在 Amazon Aurora 数据库集群中。"
    },
    "tags": [
      "S3",
      "SQS",
      "Lambda",
      "DynamoDB",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了使用 AWS 服务处理 S3 上传文件，并实现可伸缩性和低运营成本。 选项 C 结合 S3 事件通知、SQS 和 Lambda，实现了异步处理，并使用 DynamoDB 存储结果。",
      "why_correct": "C 选项采用事件驱动架构，当文件上传到 S3 时触发事件，通过 SQS 队列传递到 Lambda 函数，实现了异步处理，并支持自动伸缩，符合题意。",
      "why_wrong": "A 选项错误，使用 EMR 处理文件，启动时间长，开销大。B 选项错误，使用 EC2 实例维护成本高。D 选项错误，使用 EventBridge、Kinesis Data Streams 处理文件，架构复杂，且不一定比 SQS 效率高。"
    },
    "related_terms": [
      "S3",
      "SQS",
      "Lambda",
      "DynamoDB",
      "EventBridge",
      "EC2",
      "EMR",
      "Kinesis Data Streams"
    ]
  },
  {
    "id": 95,
    "topic": "1",
    "question_en": "An application allows users at a company's headquarters to access product data. The product data is stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application performance slowdown and wants to separate read trafic from write trafic. A solutions architect needs to optimize the application's performance quickly. What should the solutions architect recommend?",
    "options_en": {
      "A": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary Availability Zone.",
      "B": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary Availability Zone.",
      "C": "Create read replicas for the database. Configure the read replicas with half of the compute and storage resources as the source database.",
      "D": "Create read replicas for the database. Configure the read replicas with the same compute and storage resources as the source database."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一个应用程序允许公司总部的用户访问产品数据。产品数据存储在 Amazon RDS MySQL 数据库实例中。运维团队隔离了一个应用程序性能变慢的问题，并希望将读取流量与写入流量分开。一个解决方案架构师需要快速优化应用程序的性能。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "将现有数据库更改为多可用区部署。从主可用区提供读取请求。",
      "B": "将现有数据库更改为多可用区部署。从辅助可用区提供读取请求。",
      "C": "为数据库创建只读副本。将只读副本配置为具有源数据库一半的计算和存储资源。",
      "D": "为数据库创建只读副本。将只读副本配置为与源数据库相同的计算和存储资源。"
    },
    "tags": [
      "RDS",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n该题考察了如何优化 RDS MySQL 数据库的性能，分离读写流量。 选项 D 通过创建只读副本，将读取流量分流，从而提高性能。",
      "why_correct": "D 选项通过创建只读副本，将读请求分流到只读副本，从而提高数据库性能，满足了题目需求。",
      "why_wrong": "A 选项错误，多可用区部署并不能直接分离读写流量。B 选项错误，从辅助可用区提供读取请求，不如配置只读副本。C 选项错误，只读副本计算和存储资源减半，会影响性能。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "只读副本"
    ]
  },
  {
    "id": 96,
    "topic": "1",
    "question_en": "An Amazon EC2 administrator created the following policy associated with an IAM group containing several users: What is the effect of this policy?",
    "question_image": "/data/images/96.png",
    "options_en": {
      "A": "Users can terminate an EC2 instance in any AWS Region except us-east-1.",
      "B": "Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.",
      "C": "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254.",
      "D": "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is 10.100.100.254."
    },
    "correct_answer": "C",
    "vote_percentage": "70%",
    "question_cn": "一位 Amazon EC2 管理员创建了以下与包含多个用户的 IAM 组关联的策略：此策略的效果是什么？",
    "options_cn": {
      "A": "用户可以终止任何 AWS 区域中的 EC2 实例，除了 us-east-1。",
      "B": "用户可以在 us-east-1 区域中终止 IP 地址为 10.100.100.1 的 EC2 实例。",
      "C": "当用户的 源 IP 为 10.100.100.254 时，用户可以在 us-east-1 区域中终止 EC2 实例。",
      "D": "当用户的 源 IP 为 10.100.100.254 时，用户无法在 us-east-1 区域中终止 EC2 实例。"
    },
    "tags": [
      "IAM",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 70%），解析仅供参考。】\n\n根据所提供的策略,正确的效果是:\nC. 当用户的源IP为10.100.100.254时,用户可以终止位于us-east-1区域的EC2实例。\n\n以下是对该策略效果的分析:\n1. 第一条语句允许终止EC2实例,但仅限于来自10.100.100.0/24范围内的IP地址。这包括10.100.100.254。\n2. 第二条语句拒绝在除us-east-1区域之外的任何区域执行所有EC2操作。\n\n综合这些语句:\n• 用户只能在us-east-1区域执行EC2操作。\n• 在us-east-1区域内,只有当用户的源IP在10.100.100.0/24范围内时,才能终止实例。\n\n需要注意的是,该策略通过将操作限制在特定区域和IP范围,遵循了最小权限原则。然而,为了增强安全性:\n• 如有可能,考虑进一步将「资源」字段限制为特定的实例ARN(亚马逊资源名称)。\n• 对敏感操作实施额外条件,如要求进行多因素认证(MFA)。\n• 定期审查和审计策略,以确保它们符合当前的安全要求。\n\n请记住,在将任何策略更改实施到生产环境之前,先在非生产环境中进行全面测试。",
      "why_correct": "C. 当用户的源IP为10.100.100.254时,用户可以终止位于us-east-1区域的EC2实例。",
      "why_wrong": "A/B/D 与策略两条语句的综合效果不符：策略限定仅在 us-east-1 且源 IP 在 10.100.100.0/24 时允许终止实例。"
    },
    "related_terms": [
      "IAM",
      "EC2",
      "IAM 策略"
    ]
  },
  {
    "id": 97,
    "topic": "1",
    "question_en": "A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft Windows shared file storage. The company wants to migrate this workload to the AWS Cloud and is considering various storage options. The storage solution must be highly available and integrated with Active Directory for access control. Which solution will satisfy these requirements?",
    "options_en": {
      "A": "Configure Amazon EFS storage and set the Active Directory domain for authentication.",
      "B": "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.",
      "C": "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.",
      "D": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory domain for authentication."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在本地运行大型 Microsoft SharePoint 部署，需要 Microsoft Windows 共享文件存储。该公司希望将此工作负载迁移到 AWS Cloud，并正在考虑各种存储选项。存储解决方案必须高度可用并与 Active Directory 集成以进行访问控制。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon EFS 存储并为身份验证设置 Active Directory 域。",
      "B": "在两个可用区内的 AWS Storage Gateway 文件网关上创建 SMB 文件共享。",
      "C": "创建一个 Amazon S3 存储桶，并配置 Microsoft Windows Server 将其挂载为卷。",
      "D": "在 AWS 上创建一个 Amazon FSx for Windows File Server 文件系统，并为身份验证设置 Active Directory 域。"
    },
    "tags": [
      "EFS",
      "FSx",
      "Windows",
      "Active Directory"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n该题考察了 Windows 共享文件存储的迁移方案。 选项 D 使用 FSx for Windows File Server，提供高度可用性，并与 Active Directory 集成。",
      "why_correct": "D 选项使用 FSx for Windows File Server，提供完全托管的 Windows 文件服务器，满足高度可用性，并与 Active Directory 集成的要求。",
      "why_wrong": "A 选项错误，EFS 不支持 Windows 文件共享协议。B 选项错误，Storage Gateway 文件网关不直接支持 Windows 文件共享。C 选项错误，挂载 S3 到 Windows Server 不符合题目中对共享文件存储的需求。"
    },
    "related_terms": [
      "EFS",
      "FSx",
      "Windows",
      "Active Directory",
      "Storage Gateway"
    ]
  },
  {
    "id": 98,
    "topic": "1",
    "question_en": "An image-processing company has a web application that users use to upload images. The application uploads the images into an Amazon S3 bucket. The company has set up S3 event notifications to publish the object creation events to an Amazon Simple Queue Service (Amazon SQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function that processes the images and sends the results to users through email. Users report that they are receiving multiple email messages for every uploaded image. A solutions architect determines that SQS messages are invoking the Lambda function more than once, resulting in multiple email messages. What should the solutions architect do to resolve this issue with the LEAST operational overhead?",
    "options_en": {
      "A": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.",
      "B": "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to discard duplicate messages.",
      "C": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the function timeout and the batch window timeout.",
      "D": "Modify the Lambda function to delete each message from the SQS queue immediately after the message is read before processing."
    },
    "correct_answer": "C",
    "vote_percentage": "81%",
    "question_cn": "一家图像处理公司有一个用户用于上传图像的 Web 应用程序。 该应用程序将图像上传到 Amazon S3 存储桶中。该公司设置了 S3 事件通知，将对象创建事件发布到 Amazon Simple Queue Service (Amazon SQS) 标准队列。 SQS 队列充当 AWS Lambda 函数的事件源，该函数处理图像并通过电子邮件将结果发送给用户。用户报告说，他们为每个上传的图像都收到了多封电子邮件。一位解决方案架构师确定 SQS 消息多次调用 Lambda 函数，导致多封电子邮件。 解决方案架构师应该怎么做才能以最少的运营开销解决此问题？",
    "options_cn": {
      "A": "通过将 ReceiveMessage 等待时间增加到 30 秒，在 SQS 队列中设置长轮询。",
      "B": "将 SQS 标准队列更改为 SQS FIFO 队列。使用消息去重 ID 来丢弃重复的消息。",
      "C": "将 SQS 队列中的可见性超时增加到大于函数超时和批处理窗口超时总和的值。",
      "D": "修改 Lambda 函数，以便在处理之前从 SQS 队列中读取消息后立即删除每条消息。"
    },
    "tags": [
      "S3",
      "SQS",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 81%），解析仅供参考。】\n\n本题考察了 SQS 与 Lambda 的集成问题，以及如何解决 Lambda 函数被多次调用的问题。",
      "why_correct": "选项 C 正确。Lambda 函数的可见性超时设置决定了 SQS 消息在 Lambda 函数处理期间保持不可见的时间。如果可见性超时设置过短，Lambda 函数处理时间超过此时间，消息将重新变为可见状态并被再次处理，从而导致重复调用。将可见性超时设置为大于函数超时和批处理窗口超时总和的值，可以确保在 Lambda 函数完成处理之前，消息不会被其他消费者读取。",
      "why_wrong": "选项 A 错误。增加 ReceiveMessage 等待时间（长轮询）可以减少 SQS 消息的数量，但不能解决消息被多次调用的问题。 选项 B 错误。虽然 FIFO 队列可以保证消息的顺序，并使用消息去重 ID 来防止重复消息，但这会引入额外的配置和管理开销，并且并非解决 Lambda 多次调用的最直接方法。 选项 D 错误。在处理前删除消息并不能解决 Lambda 函数被多次调用的根本问题，如果函数在删除消息之前失败，则消息将丢失，并且可能导致其他问题，如漏处理事件。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "Amazon Simple Queue Service",
      "SQS",
      "AWS Lambda",
      "FIFO",
      "ReceiveMessage",
      "visibility timeout",
      "batch window"
    ]
  },
  {
    "id": 99,
    "topic": "1",
    "question_en": "A company is implementing a shared storage solution for a gaming application that is hosted in an on-premises data center. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?",
    "options_en": {
      "A": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
      "B": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.",
      "D": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system."
    },
    "correct_answer": "D",
    "vote_percentage": "94%",
    "question_cn": "一家公司正在为其托管在本地数据中心的视频游戏应用程序实施共享存储解决方案。该公司需要使用Lustre客户端访问数据的能力。该解决方案必须是完全托管的。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Storage Gateway 文件网关。创建使用所需客户端协议的文件共享。将应用程序服务器连接到文件共享。",
      "B": "创建一个 Amazon EC2 Windows 实例。在该实例上安装并配置 Windows 文件共享角色。将应用程序服务器连接到文件共享。",
      "C": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统，并将其配置为支持Lustre。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。",
      "D": "创建一个 Amazon FSx for Lustre 文件系统。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。"
    },
    "tags": [
      "FSx",
      "Lustre",
      "Storage Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 94%），解析仅供参考。】\n\n本题考察了共享存储解决方案的选择，需要支持 Lustre 客户端。 选项 D 使用 Amazon FSx for Lustre 文件系统，满足了要求。",
      "why_correct": "D 选项使用 Amazon FSx for Lustre 文件系统，提供完全托管的 Lustre 文件系统，满足了完全托管和 Lustre 客户端访问的需求。",
      "why_wrong": "A 选项错误，Storage Gateway 文件网关不支持 Lustre 客户端。B 选项错误，EC2 Windows 实例需要自行配置，不符合完全托管的要求。 C 选项错误，EFS 不支持 Lustre 客户端。"
    },
    "related_terms": [
      "FSx",
      "Storage Gateway",
      "EFS",
      "Lustre"
    ]
  },
  {
    "id": 100,
    "topic": "1",
    "question_en": "A company's containerized application runs on an Amazon EC2 instance. The application needs to download security certificates before it can communicate with other business applications. The company wants a highly secure solution to encrypt and decrypt the certificates in near real time. The solution also needs to store data in highly available storage after the data is encrypted. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as needed. Control access to the data by using fine-grained IAM access.",
      "B": "Create an AWS Lambda function that uses the Python cryptography library to receive and perform encryption operations. Store the function in an Amazon S3 bucket.",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon S3.",
      "D": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role to use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block Store (Amazon EBS) volumes."
    },
    "correct_answer": "C",
    "vote_percentage": "79%",
    "question_cn": "一家公司的容器化应用程序在 Amazon EC2 实例上运行。该应用程序需要先下载安全证书，然后才能与其他业务应用程序通信。该公司希望使用高度安全的解决方案，以近乎实时的速度对证书进行加密和解密。该解决方案还需要在数据加密后将数据存储在具有高可用性的存储中。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "为加密证书创建 AWS Secrets Manager 密钥。根据需要手动更新证书。通过使用细粒度的 IAM 访问控制对数据的访问。",
      "B": "创建一个使用 Python 加密库接收和执行加密操作的 AWS Lambda 函数。将该函数存储在 Amazon S3 存储桶中。",
      "C": "创建一个 AWS Key Management Service (AWS KMS) 客户托管密钥。允许 EC2 角色使用 KMS 密钥进行加密操作。将加密数据存储在 Amazon S3 上。",
      "D": "创建一个 AWS Key Management Service (AWS KMS) 客户托管密钥。允许 EC2 角色使用 KMS 密钥进行加密操作。将加密数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷上。"
    },
    "tags": [
      "KMS",
      "EC2",
      "S3",
      "EBS",
      "Secrets Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 79%），解析仅供参考。】\n\n考查使用 AWS KMS 进行数据加密，以及选择合适的存储方案，以满足安全性和可用性需求。",
      "why_correct": "AWS KMS 提供了高度安全的密钥管理服务，可用于加密和解密数据。创建客户托管密钥允许对密钥进行精细的控制。将加密后的数据存储在 Amazon S3 上，S3 具有高可用性和持久性，满足了题目的存储需求。",
      "why_wrong": "选项 A 使用 Secrets Manager 存储证书，虽然安全，但手动更新证书无法满足近乎实时的加密和解密需求。选项 B 使用 Lambda 函数和 Python 加密库，增加了运营开销和复杂性，且未明确存储方案。选项 D 使用 EBS 作为存储，EBS 虽然可以提供加密，但相比 S3，在可用性和可扩展性上不如 S3，且没有说明S3存储方案。此外，EBS 主要用于实例的根卷和数据卷，并不适合长期存储加密数据。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Secrets Manager",
      "IAM",
      "AWS Lambda",
      "Amazon S3",
      "AWS Key Management Service (AWS KMS)",
      "EC2",
      "Amazon Elastic Block Store (Amazon EBS)"
    ]
  },
  {
    "id": 101,
    "topic": "1",
    "question_en": "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use IPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability Zones (AZs) for high availability. An internet gateway is used to provide internet access for the public subnets. The private subnets require access to the internet to allow Amazon EC2 instances to download software updates. What should the solutions architect do to enable Internet access for the private subnets?",
    "options_en": {
      "A": "Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for each AZ that forwards non-VPC trafic to the NAT gateway in its AZ.",
      "B": "Create three NAT instances, one for each private subnet in each AZ. Create a private route table for each AZ that forwards non-VPC trafic to the NAT instance in its AZ.",
      "C": "Create a second internet gateway on one of the private subnets. Update the route table for the private subnets that forward non-VPC trafic to the private internet gateway.",
      "D": "Create an egress-only internet gateway on one of the public subnets. Update the route table for the private subnets that forward non- VPC trafic to the egress-only Internet gateway."
    },
    "correct_answer": "A",
    "vote_percentage": "98%",
    "question_cn": "一位解决方案架构师正在设计一个包含公有子网和私有子网的 VPC。VPC 和子网使用 IPv4 CIDR 块。为了实现高可用性，在三个可用区（AZ）的每个可用区中都有一个公有子网和一个私有子网。一个互联网网关用于为公有子网提供互联网访问。私有子网需要访问互联网以允许 Amazon EC2 实例下载软件更新。解决方案架构师应该怎么做才能为私有子网启用互联网访问？",
    "options_cn": {
      "A": "创建三个 NAT 网关，每个可用区中的每个公有子网一个。为每个可用区创建一个私有路由表，将非 VPC 流量转发到其可用区中的 NAT 网关。",
      "B": "创建三个 NAT 实例，每个可用区中的每个私有子网一个。为每个可用区创建一个私有路由表，将非 VPC 流量转发到其可用区中的 NAT 实例。",
      "C": "在其中一个私有子网上创建一个第二个互联网网关。更新私有子网的路由表，将非 VPC 流量转发到私有互联网网关。",
      "D": "在其中一个公有子网上创建一个仅限出口的互联网网关。更新私有子网的路由表，将非 VPC 流量转发到仅限出口的互联网网关。"
    },
    "tags": [
      "VPC",
      "NAT Gateway",
      "NAT Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 98%），解析仅供参考。】\n\n该题考察了如何为私有子网提供 Internet 访问。 选项 A 通过使用 NAT 网关，为每个可用区内的私有子网提供 Internet 访问。",
      "why_correct": "A 选项通过在每个可用区中部署 NAT 网关，为私有子网提供互联网访问，实现了高可用性，满足题目要求。",
      "why_wrong": "B 选项错误，使用 NAT 实例无法保证高可用性。C 选项错误，在私有子网中创建互联网网关是不正确的，互联网网关应该在公有子网。D 选项错误，出口互联网网关与题目要求不符，不能为私有子网提供互联网访问。"
    },
    "related_terms": [
      "VPC",
      "NAT Gateway",
      "Internet Gateway",
      "NAT Instance"
    ]
  },
  {
    "id": 102,
    "topic": "1",
    "question_en": "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP server that stores its data on an NFS-based file system. The server holds 200 GB of data that needs to be transferred. The server must be hosted on an Amazon EC2 instance that uses an Amazon Elastic File System (Amazon EFS) file system. Which combination of steps should a solutions architect take to automate this task? (Choose two.)",
    "options_en": {
      "A": "Launch the EC2 instance into the same Availability Zone as the EFS file system.",
      "B": "Install an AWS DataSync agent in the on-premises data center.",
      "C": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the data.",
      "D": "Manually use an operating system copy command to push the data to the EC2 instanc",
      "E": "E. Use AWS DataSync to create a suitable location configuration for the on-premises SFTP server."
    },
    "correct_answer": "BE",
    "vote_percentage": "55%",
    "question_cn": "一家公司希望将本地数据中心迁移到 AWS。 数据中心托管一个 SFTP 服务器，该服务器将其数据存储在基于 NFS 的文件系统上。 服务器包含 200 GB 的数据需要传输。 服务器必须托管在 Amazon EC2 实例上，该实例使用 Amazon Elastic File System (Amazon EFS) 文件系统。 解决方案架构师应采取哪些步骤组合来自动化此任务？（选择两项。）",
    "options_cn": {
      "A": "将 EC2 实例启动到与 EFS 文件系统相同的可用区。",
      "B": "在本地数据中心安装一个 AWS DataSync 代理。",
      "C": "在 EC2 实例上创建一个辅助 Amazon Elastic Block Store (Amazon EBS) 卷以存储数据。",
      "D": "手动使用操作系统复制命令将数据推送到 EC2 实例。",
      "E": "使用 AWS DataSync 为本地 SFTP 服务器创建合适的地址配置。"
    },
    "tags": [
      "EC2",
      "EFS",
      "DataSync",
      "SFTP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 55%），解析仅供参考。】\n\n考察使用 AWS DataSync 将本地 SFTP 服务器的数据迁移到 Amazon EFS 的流程。需要配置 DataSync 代理和地址。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：选项 B 和 E 共同构成了使用 AWS DataSync 迁移 SFTP 服务器数据的最佳实践。选项 B 在本地数据中心部署 DataSync 代理，用于数据传输。选项 E 使用 DataSync 配置源地址，使其能够连接到本地 SFTP 服务器，从而开始数据迁移。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 仅启动了 EC2 实例，没有提及数据传输过程。选项 C 引入了额外的 Amazon EBS 卷，这增加了复杂性，且不是最佳实践。选项 D 描述了手动复制数据，这不满足自动化需求，且效率低下。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EFS",
      "SFTP",
      "NFS",
      "AWS DataSync",
      "Amazon EBS"
    ]
  },
  {
    "id": 103,
    "topic": "1",
    "question_en": "A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the same time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the S3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during each run. What should the solutions architect do to prevent AWS Glue from reprocessing old data?",
    "options_en": {
      "A": "Edit the job to use job bookmarks.",
      "B": "Edit the job to delete data after the data is processed.",
      "C": "Edit the job by setting the NumberOfWorkers field to 1.",
      "D": "Use a FindMatches machine learning (ML) transform."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个 AWS Glue extract, transform, and load (ETL) 作业，该作业每天在同一时间运行。该作业处理 Amazon S3 存储桶中的 XML 数据。新数据每天添加到 S3 存储桶。一位解决方案架构师注意到 AWS Glue 每次运行时都在处理所有数据。解决方案架构师应该怎么做才能防止 AWS Glue 重新处理旧数据？",
    "options_cn": {
      "A": "编辑作业以使用作业书签。",
      "B": "编辑作业以在处理完数据后删除数据。",
      "C": "通过将 NumberOfWorkers 字段设置为 1 来编辑作业。",
      "D": "使用 FindMatches 机器学习 (ML) 转换。"
    },
    "tags": [
      "Glue",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n该题考察了如何防止 AWS Glue ETL 作业重复处理旧数据。 选项 A 使用作业书签，可以只处理新的数据，防止重复处理。",
      "why_correct": "A 选项使用作业书签，确保 AWS Glue 只处理新增的 S3 数据。",
      "why_wrong": "B 选项错误，删除数据会导致数据丢失。C 选项错误，设置 NumberOfWorkers 字段不影响数据处理范围。D 选项错误，FindMatches ML 转换不能解决重新处理旧数据的问题。"
    },
    "related_terms": [
      "Glue",
      "S3",
      "作业书签"
    ]
  },
  {
    "id": 104,
    "topic": "1",
    "question_en": "A solutions architect must design a highly available infrastructure for a website. The website is powered by Windows web servers that run on Amazon EC2 instances. The solutions architect must implement a solution that can mitigate a large-scale DDoS attack that originates from thousands of IP addresses. Downtime is not acceptable for the website. Which actions should the solutions architect take to protect the website from such an attack? (Choose two.)",
    "options_en": {
      "A": "Use AWS Shield Advanced to stop the DDoS attack.",
      "B": "Configure Amazon GuardDuty to automatically block the attackers.",
      "C": "Configure the website to use Amazon CloudFront for both static and dynamic content.",
      "D": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs",
      "E": "Use EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to 80% CPU utilization."
    },
    "correct_answer": "AC",
    "vote_percentage": "87%",
    "question_cn": "一位解决方案架构师必须为网站设计一个高可用性基础设施。该网站由在 Amazon EC2 实例上运行的 Windows Web 服务器提供支持。解决方案架构师必须实施一个可以缓解源自数千个 IP 地址的大规模 DDoS 攻击的解决方案。该网站的停机时间是不可接受的。 解决方案架构师应该采取哪些措施来保护网站免受此类攻击？（选择两项。）",
    "options_cn": {
      "A": "使用 AWS Shield Advanced 来阻止 DDoS 攻击。",
      "B": "配置 Amazon GuardDuty 以自动阻止攻击者。",
      "C": "将网站配置为使用 Amazon CloudFront 来提供静态和动态内容。",
      "D": "使用 AWS Lambda 函数自动将攻击者的 IP 地址添加到 VPC 网络 ACL 中。",
      "E": "在具有目标跟踪扩展策略的 Auto Scaling 组中使用 EC2 Spot 实例，该策略设置为 80% 的 CPU 利用率。"
    },
    "tags": [
      "Shield",
      "CloudFront",
      "DDoS",
      "EC2",
      "Auto Scaling",
      "GuardDuty"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 87%），解析仅供参考。】\n\n该题考察了如何保护网站免受大规模 DDoS 攻击。 选项 A 和 C 结合使用，AWS Shield Advanced 和 CloudFront，可以有效缓解 DDoS 攻击，并提供高可用性。",
      "why_correct": "A 选项使用 AWS Shield Advanced，提供高级 DDoS 保护。C 选项使用 CloudFront，提供内容分发和 DDoS 缓解，满足高可用性需求。",
      "why_wrong": "B 选项错误，GuardDuty 不是用来阻止 DDoS 攻击的。 D 选项错误，使用 Lambda 自动添加 IP 到网络 ACL 不能有效防御 DDoS 攻击。 E 选项错误，Auto Scaling 组与 DDoS 攻击防护无关。"
    },
    "related_terms": [
      "CloudFront",
      "DDoS",
      "EC2",
      "Auto Scaling",
      "GuardDuty",
      "Shield"
    ]
  },
  {
    "id": 105,
    "topic": "1",
    "question_en": "A company is preparing to deploy a new serverless workload. A solutions architect must use the principle of least privilege to configure permissions that will be used to run an AWS Lambda function. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function. Which solution meets these requirements?",
    "options_en": {
      "A": "Add an execution role to the function with lambda:InvokeFunction as the action and * as the principal.",
      "B": "Add an execution role to the function with lambda:InvokeFunction as the action and Service: lambda.amazonaws.com as the principal.",
      "C": "Add a resource-based policy to the function with lambda:* as the action and Service: events.amazonaws.com as the principal.",
      "D": "Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: events.amazonaws.com as the principal."
    },
    "correct_answer": "D",
    "vote_percentage": "97%",
    "question_cn": "一家公司正准备部署新的无服务器工作负载。一位解决方案架构师必须使用最小权限原则来配置将用于运行 AWS Lambda 函数的权限。Amazon EventBridge (Amazon CloudWatch Events) 规则将调用该函数。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "为函数添加一个执行角色，其操作为 lambda:InvokeFunction，主体为 *。",
      "B": "为函数添加一个执行角色，其操作为 lambda:InvokeFunction，主体为 Service: lambda.amazonaws.com。",
      "C": "为函数添加一个基于资源的策略，其操作为 lambda:*，主体为 Service: events.amazonaws.com。",
      "D": "为函数添加一个基于资源的策略，其操作为 lambda:InvokeFunction，主体为 Service: events.amazonaws.com。"
    },
    "tags": [
      "Lambda",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 97%），解析仅供参考。】\n\n该题考察了 Lambda 函数的最小权限原则。 选项 D 使用基于资源的策略，将 lambda:InvokeFunction 权限授予 EventBridge 服务，符合最小权限原则。",
      "why_correct": "D 选项使用了基于资源的策略，允许 EventBridge 调用 Lambda 函数，实现了最小权限原则。",
      "why_wrong": "A 选项错误，给所有主体(*) 授权会违反最小权限原则。B 选项错误，使用 service:lambda.amazonaws.com 不正确。C 选项错误，对 lambda:* 操作的授权权限过大，不符合最小权限原则。"
    },
    "related_terms": [
      "Lambda",
      "EventBridge",
      "IAM 策略",
      "最小权限原则"
    ]
  },
  {
    "id": 106,
    "topic": "1",
    "question_en": "A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data must be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must be rotated every year. Which solution meets these requirements and is the MOST operationally eficient?",
    "options_en": {
      "A": "Server-side encryption with customer-provided keys (SSE-C)",
      "B": "Server-side encryption with Amazon S3 managed keys (SSE-S3)",
      "C": "Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation",
      "D": "Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation"
    },
    "correct_answer": "D",
    "vote_percentage": "93%",
    "question_cn": "一家公司正准备在 Amazon S3 中存储机密数据。出于合规性原因，静态数据必须加密。必须记录密钥使用情况以进行审计。密钥必须每年轮换一次。哪个解决方案满足这些要求并且操作效率最高？",
    "options_cn": {
      "A": "使用客户提供的密钥的服务器端加密 (SSE-C)",
      "B": "使用 Amazon S3 托管密钥的服务器端加密 (SSE-S3)",
      "C": "使用 AWS KMS 密钥的服务器端加密 (SSE-KMS)，手动轮换",
      "D": "使用 AWS KMS 密钥的服务器端加密 (SSE-KMS)，自动轮换"
    },
    "tags": [
      "Amazon S3",
      "SSE-KMS",
      "AWS KMS",
      "Data Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 93%），解析仅供参考。】\n\n考察 Amazon S3 静态数据加密方案的选择，以及密钥管理和审计的要求。与 SSE-C、SSE-S3、SSE-KMS 的对比相关。",
      "why_correct": "使用 AWS KMS 密钥的服务器端加密 (SSE-KMS) 满足所有要求。SSE-KMS 提供了密钥管理和审计，并且可以自动轮换密钥。通过 AWS KMS，可以集中管理加密密钥，并实现对密钥使用情况的全面审计。自动轮换功能则简化了密钥管理，符合题目中每年轮换一次的要求。",
      "why_wrong": "选项 A 使用客户提供的密钥的服务器端加密 (SSE-C)，这虽然满足静态数据加密要求，但AWS 不会存储客户的密钥，因此不提供密钥的审计日志，无法满足审计要求。选项 B 使用 Amazon S3 托管密钥的服务器端加密 (SSE-S3)，虽然由 Amazon S3 托管密钥，易于使用，但用户无法控制密钥的轮换，也不提供详细的密钥使用审计日志，无法满足题目中密钥轮换和审计的要求。选项 C 使用 AWS KMS 密钥的服务器端加密 (SSE-KMS)，但要求手动轮换密钥，增加了操作负担，不符合操作效率最高的的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "SSE-C",
      "SSE-S3",
      "SSE-KMS",
      "AWS KMS",
      "Data Encryption"
    ]
  },
  {
    "id": 107,
    "topic": "1",
    "question_en": "A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles during peak operating hours. The company wants to use these data points in its existing analytics platform. A solutions architect must determine the most viable multi-tier option to support this architecture. The data points must be accessible from the REST API. Which action meets these requirements for storing and retrieving location data?",
    "options_en": {
      "A": "Use Amazon Athena with Amazon S3.",
      "B": "Use Amazon API Gateway with AWS Lambda.",
      "C": "Use Amazon QuickSight with Amazon Redshift.",
      "D": "Use Amazon API Gateway with Amazon Kinesis Data Analytics."
    },
    "correct_answer": "B",
    "vote_percentage": "50%",
    "question_cn": "一家自行车共享公司正在开发一个多层架构，以在运营高峰时跟踪其自行车的 GPS 位置。该公司希望在其现有的分析平台中使用这些数据点。解决方案架构师必须确定最可行的多层选项以支持此架构。这些数据点必须可以通过 REST API 访问。哪个操作满足存储和检索位置数据的这些要求？",
    "options_cn": {
      "A": "使用 Amazon Athena 和 Amazon S3。",
      "B": "使用 Amazon API Gateway 和 AWS Lambda。",
      "C": "使用 Amazon QuickSight 和 Amazon Redshift。",
      "D": "使用 Amazon API Gateway 和 Amazon Kinesis Data Analytics。"
    },
    "tags": [
      "Amazon API Gateway",
      "Amazon Kinesis Data Analytics",
      "REST API",
      "Data Storage"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 50%），解析仅供参考。】\n\n考查如何通过 REST API 访问 GPS 位置数据，并选择合适的存储和检索方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：Amazon API Gateway 提供了创建、发布、维护和保护 REST API 的服务，能够满足通过 REST API 访问数据的需求。AWS Lambda 允许运行代码而无需预置或管理服务器，适用于处理和存储 GPS 数据。这种组合提供了可扩展且经济高效的解决方案。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，Amazon Athena 用于分析存储在 Amazon S3 中的数据，不提供直接的 REST API 访问功能。选项 C，Amazon QuickSight 用于数据可视化，Amazon Redshift 是数据仓库，两者均不提供 REST API 访问和实时数据存储的能力。选项 D，Amazon Kinesis Data Analytics 用于对实时数据流进行分析，但它本身不提供 REST API 接口，需要与其他服务结合使用才能满足题目要求，且其主要用途是实时数据分析，而非直接的数据存储和检索。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Athena",
      "Amazon S3",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon QuickSight",
      "Amazon Redshift",
      "Amazon Kinesis Data Analytics",
      "REST API",
      "GPS"
    ]
  },
  {
    "id": 108,
    "topic": "1",
    "question_en": "A company has an automobile sales website that stores its listings in a database on Amazon RDS. When an automobile is sold, the listing needs to be removed from the website and the data must be sent to multiple target systems. Which design should a solutions architect recommend?",
    "options_en": {
      "A": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to consume.",
      "B": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send the information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to consume.",
      "C": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) queue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS Lambda functions to update the targets.",
      "D": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon SNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS Lambda functions to update the targets."
    },
    "correct_answer": "A",
    "vote_percentage": "63%",
    "question_cn": "一家公司有一个汽车销售网站，该网站将其列表存储在 Amazon RDS 上的数据库中。 当汽车售出时，该列表需要从网站上删除，并且数据必须发送到多个目标系统。 解决方案架构师应推荐哪种设计？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，当 Amazon RDS 上的数据库更新时触发，以将信息发送到 Amazon Simple Queue Service (Amazon SQS) 队列，供目标系统使用。",
      "B": "创建一个 AWS Lambda 函数，当 Amazon RDS 上的数据库更新时触发，以将信息发送到 Amazon Simple Queue Service (Amazon SQS) FIFO 队列，供目标系统使用。",
      "C": "订阅 RDS 事件通知，并将 Amazon Simple Queue Service (Amazon SQS) 队列扇出到多个 Amazon Simple Notification Service (Amazon SNS) 主题。 使用 AWS Lambda 函数更新目标系统。",
      "D": "订阅 RDS 事件通知，并将 Amazon Simple Notification Service (Amazon SNS) 主题扇出到多个 Amazon Simple Queue Service (Amazon SQS) 队列。 使用 AWS Lambda 函数更新目标系统。"
    },
    "tags": [
      "Amazon RDS",
      "Amazon SQS",
      "Amazon SNS",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 63%），解析仅供参考。】\n\n考查了如何利用 RDS 数据库更新触发事件，并结合 SQS 和 Lambda 实现数据分发到多个目标系统。",
      "why_correct": "选项 A 提供了一种有效的方案。当 RDS 上的数据库更新时，Lambda 函数被触发，并将更新信息发送到标准 SQS 队列。目标系统可以从该队列中获取消息，进行处理。这种方式解耦了数据库更新和目标系统，并且易于扩展和维护。",
      "why_wrong": "选项 B 错误，因为标准 SQS 队列通常满足要求，除非需要严格的消息顺序，否则无需使用 FIFO 队列。选项 C 和 D 都使用了 SNS。虽然 SNS 可以将消息扇出到多个主题或队列，但直接通过 Lambda 处理更新更为简洁高效，无需引入 SNS 作为中间环节，增加了复杂性。"
    },
    "related_terms": [
      "Amazon RDS",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "FIFO"
    ]
  },
  {
    "id": 109,
    "topic": "1",
    "question_en": "A company needs to store data in Amazon S3 and must prevent the data from being changed. The company wants new objects that are uploaded to Amazon S3 to remain unchangeable for a nonspecific amount of time until the company decides to modify the objects. Only specific users in the company's AWS account can have the ability 10 delete the objects. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.",
      "B": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 years. Use governance mode as the S3 bucket’s default retention mode for new objects.",
      "C": "Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon notification, restore the modified objects from any backup versions that the company has.",
      "D": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the objects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete the objects."
    },
    "correct_answer": "D",
    "vote_percentage": "76%",
    "question_cn": "一家公司需要在 Amazon S3 中存储数据，并且必须防止数据被更改。该公司希望上传到 Amazon S3 的新对象在公司决定修改这些对象之前，保持不可更改的时间，时间长短不确定。只有公司 AWS 账户中的特定用户才能删除这些对象。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 S3 Glacier 保险库。对这些对象应用一次写入多次读取（WORM）保险库锁定策略。",
      "B": "创建一个启用了 S3 对象锁定的 S3 存储桶。启用版本控制。设置 100 年的保留期。将治理模式用作 S3 存储桶中新对象的默认保留模式。",
      "C": "创建一个 S3 存储桶。使用 AWS CloudTrail 跟踪修改这些对象的任何 S3 API 事件。收到通知后，从公司拥有的任何备份版本恢复修改后的对象。",
      "D": "创建一个启用了 S3 对象锁定的 S3 存储桶。启用版本控制。为这些对象添加法律保留。将 s3:PutObjectLegalHold 权限添加到需要删除对象的用户的 IAM 策略中。"
    },
    "tags": [
      "Amazon S3",
      "S3 Object Lock",
      "S3 Versioning",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 76%），解析仅供参考。】\n\n考查 S3 对象锁定（S3 Object Lock）的使用以及与版本控制、IAM 权限的结合。题目要求数据不可变以及特定用户才能删除对象，需要选择合适的 S3 配置来实现。",
      "why_correct": "选项 D 提供了满足需求的解决方案。通过创建启用 S3 对象锁定的 S3 存储桶，并启用版本控制，确保了对象的不可变性。设置法律保留（Legal Hold）可以防止对象被覆盖或删除，直到法律保留被解除。将 `s3:PutObjectLegalHold` 权限添加到需要删除对象的用户的 IAM 策略中，赋予他们解除 Legal Hold 从而删除对象的权限。这同时满足了数据不可变和特定用户才能删除对象的要求。",
      "why_wrong": "选项 A 错误，因为 S3 Glacier 属于归档存储，虽然也提供 WORM 功能，但它不是为频繁访问设计，不适合这种需要灵活修改保留期的场景。选项 B 错误，虽然 S3 对象锁定并设置了治理模式，并设置了 100 年的保留期，虽然能实现不可变，但由于公司的数据保留时间不确定，100 年的固定保留期无法满足需求。选项 C 错误，虽然使用 AWS CloudTrail 跟踪 S3 API 事件，可以检测到对象被修改，但它无法阻止修改，无法满足数据不可变的要求，而且事后恢复的方案会丢失数据变更痕迹，无法满足题目要求的数据不被更改的条件。"
    },
    "related_terms": [
      "Amazon S3",
      "IAM",
      "S3 Glacier",
      "WORM",
      "AWS CloudTrail",
      "S3 Object Lock",
      "S3 Versioning",
      "s3:PutObjectLegalHold",
      "Legal Hold"
    ]
  },
  {
    "id": 110,
    "topic": "1",
    "question_en": "A social media company allows users to upload images to its website. The website runs on Amazon EC2 instances. During upload requests, the website resizes the images to a standard size and stores the resized images in Amazon S3. Users are experiencing slow upload requests to the website. The company needs to reduce coupling within the application and improve website performance. A solutions architect must design the most operationally eficient process for image uploads. Which combination of actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Configure the application to upload images to S3 Glacier.",
      "B": "Configure the web server to upload the original images to Amazon S3.",
      "C": "Configure the application to upload images directly from each user's browser to Amazon S3 through the use of a presigned URL",
      "D": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. Use the function to resize the imag",
      "E": "E. Create an Amazon EventBridge (Amazon CloudWatch Events) rule that invokes an AWS Lambda function on a schedule to resize uploaded images."
    },
    "correct_answer": "BD",
    "vote_percentage": "52%",
    "question_cn": "一家社交媒体公司允许用户向其网站上传图片。该网站运行在 Amazon EC2 实例上。在上传请求期间，网站将图片调整为标准尺寸，并将调整后的图片存储在 Amazon S3 中。用户遇到上传请求到网站的速度变慢的问题。该公司需要减少应用程序内的耦合并提高网站性能。一个解决方案架构师必须设计最高效的图像上传流程。解决方案架构师应该采取哪些操作组合来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "配置应用程序将图片上传到 S3 Glacier。",
      "B": "配置 Web 服务器将原始图片上传到 Amazon S3。",
      "C": "配置应用程序使用预签名 URL，直接从每个用户的浏览器将图片上传到 Amazon S3。",
      "D": "配置 S3 事件通知，以便在上传图片时调用 AWS Lambda 函数。使用该函数调整图片大小。",
      "E": "创建一个 Amazon EventBridge（Amazon CloudWatch Events）规则，该规则在定期间隔调用 AWS Lambda 函数以调整上传的图片大小。"
    },
    "tags": [
      "Amazon S3",
      "EC2",
      "Lambda",
      "EventBridge",
      "S3 pre-signed URL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 52%），解析仅供参考。】\n\n本题考查图片上传流程的优化，以及如何解耦应用。与 S3、Lambda、EventBridge、预签名URL等服务的使用和配置相关，也涉及应用程序架构设计。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BD。理由简述：配置 Web 服务器将原始图片上传到 Amazon S3 是一个合理的步骤。这简化了流程，让 EC2 实例专注于处理 Web 请求，避免了在服务器端进行图片处理。上传原始图片可以保持应用程序的职责清晰，将图片存储与处理分离，提高应用程序的响应速度。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. 将图片上传到 S3 Glacier 不适用。S3 Glacier 专为长期数据归档设计，访问速度慢，不适合用户上传的即时访问的图片。\nC. 配置应用程序使用预签名 URL，直接从浏览器上传图片是正确的，但题目要求选择两项。单独使用预签名 URL 无法处理图片调整大小的需求。\nD. 配置 S3 事件通知，以便在上传图片时调用 Lambda 函数调整大小，是可选方案之一，但仅靠 S3 事件通知，无法直接实现原始图片上传到 S3。本题需要选择两个方案，所以未被选中。\nE. 创建 EventBridge 规则定期调用 Lambda 函数调整图片大小是不高效的。这种方式是异步处理，无法实时响应用户上传，而且周期性的触发也增加了不必要的成本和延迟。这种方式处理图片的延迟性更高，用户体验更差。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Glacier",
      "EC2",
      "Lambda",
      "EventBridge",
      "Web server",
      "S3 pre-signed URL"
    ]
  },
  {
    "id": 111,
    "topic": "1",
    "question_en": "A company recently migrated a message processing system to AWS. The system receives messages into an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a consumer application running on Amazon EC2. The consumer application processes the messages and writes results to a MySQL database running on Amazon EC2. The company wants this application to be highly available with low operational complexity. Which architecture offers the HIGHEST availability?",
    "options_en": {
      "A": "Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.",
      "B": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to another Availability Zone.",
      "C": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an additional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with Multi-AZ enabled.",
      "D": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto Scaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for MySQL with Multi-AZ enabled."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一家公司最近将消息处理系统迁移到了 AWS。该系统接收消息到运行在 Amazon EC2 实例上的 ActiveMQ 队列中。消息由运行在 Amazon EC2 上的消费者应用程序处理。消费者应用程序处理消息并将结果写入运行在 Amazon EC2 上的 MySQL 数据库。公司希望此应用程序具有高可用性，且运营复杂性较低。哪种架构提供最高的可用性？",
    "options_cn": {
      "A": "在另一个可用区添加第二个 ActiveMQ 服务器。在另一个可用区添加一个额外的消费者 EC2 实例。将 MySQL 数据库复制到另一个可用区。",
      "B": "使用 Amazon MQ，在两个可用区配置主动/备用代理。在另一个可用区添加一个额外的消费者 EC2 实例。将 MySQL 数据库复制到另一个可用区。",
      "C": "使用 Amazon MQ，在两个可用区配置主动/备用代理。在另一个可用区添加一个额外的消费者 EC2 实例。使用启用了多可用区的 Amazon RDS for MySQL。",
      "D": "使用 Amazon MQ，在两个可用区配置主动/备用代理。为跨两个可用区的消费者 EC2 实例添加一个 Auto Scaling 组。使用启用了多可用区的 Amazon RDS for MySQL。"
    },
    "tags": [
      "Amazon MQ",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "MySQL",
      "High Availability",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n考查构建高可用消息处理系统的架构。涉及 Amazon MQ 的部署、EC2 实例的扩展和 RDS 的配置，以及它们与可用性的关系。",
      "why_correct": "选项 D 提供了最高可用性，因为它结合了 Amazon MQ 的主动/备用配置，确保消息传递的高可用；使用 Auto Scaling 组管理 EC2 实例，实现消费者应用程序的弹性扩展和故障转移；以及启用 Multi-AZ 的 Amazon RDS for MySQL，实现数据库的高可用和数据冗余。这种架构确保了消息队列、消费者应用程序和数据库在不同可用区中的冗余，从而提高了系统的整体可用性，并减少了手动操作的复杂性。",
      "why_wrong": "选项 A 仅通过在不同可用区部署组件来尝试实现高可用性，但其主要缺陷在于 ActiveMQ 和 MySQL 数据库都需要手动管理复制和故障转移，增加了运营复杂性，且可用性受限于手动操作的响应速度。选项 B 同样使用手动复制 MySQL 的方式，依赖人工干预，可用性不如 RDS 的 Multi-AZ 配置。选项 C 虽然使用了 Amazon MQ 和 Multi-AZ RDS，但没有使用 Auto Scaling 组，当消费者 EC2 实例出现故障时，需要手动干预，增加了恢复时间，降低了可用性。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "Amazon MQ",
      "Auto Scaling",
      "Amazon RDS",
      "Multi-AZ",
      "ActiveMQ"
    ]
  },
  {
    "id": 112,
    "topic": "1",
    "question_en": "A company hosts a containerized web application on a fieet of on-premises servers that process incoming requests. The number of requests is growing quickly. The on-premises servers cannot handle the increased number of requests. The company wants to move the application to AWS with minimum code changes and minimum development effort. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web application with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming requests.",
      "B": "Use two Amazon EC2 instances to host the containerized web application. Use an Application Load Balancer to distribute the incoming requests.",
      "C": "Use AWS Lambda with a new code that uses one of the supported languages. Create multiple Lambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda functions.",
      "D": "Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC cluster that can process the incoming requests at the appropriate scale."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其本地服务器集群上托管一个容器化的 Web 应用程序，该应用程序会处理传入的请求。请求数量正在快速增长。本地服务器无法处理增加的请求数量。该公司希望以最少的代码更改和最少的开发工作量将该应用程序迁移到 AWS。哪种解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Fargate on Amazon Elastic Container Service (Amazon ECS) 运行容器化的 Web 应用程序，并使用 Service Auto Scaling。使用 Application Load Balancer (ALB) 分发传入的请求。",
      "B": "使用两个 Amazon EC2 实例来托管容器化的 Web 应用程序。使用 Application Load Balancer (ALB) 分发传入的请求。",
      "C": "使用 AWS Lambda 和使用其中一种支持语言编写的新代码。创建多个 Lambda 函数以支持负载。使用 Amazon API Gateway 作为 Lambda 函数的入口点。",
      "D": "使用高性能计算 (HPC) 解决方案（例如 AWS ParallelCluster）来建立一个 HPC 集群，该集群可以按适当的规模处理传入的请求。"
    },
    "tags": [
      "Amazon ECS",
      "AWS Fargate",
      "Application Load Balancer",
      "Service Auto Scaling",
      "Amazon EC2",
      "AWS Lambda",
      "Amazon API Gateway",
      "AWS ParallelCluster"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查应用程序容器化迁移到 AWS 的方案选择，侧重于最少代码更改、最少开发工作量和最少运营开销。涉及 Amazon ECS、AWS Fargate、Application Load Balancer、Auto Scaling、Lambda 等服务选型和对比。",
      "why_correct": "AWS Fargate 允许在无需管理服务器的情况下运行容器。结合 Amazon ECS 和 Application Load Balancer，可以轻松迁移容器化 Web 应用程序。Service Auto Scaling 确保根据流量自动扩展容器实例的数量，满足请求数量增长的需求。该方案无需对应用程序代码进行重大更改，且运营开销最低。",
      "why_wrong": "B 方案需要管理 EC2 实例，增加了运营开销，需要用户负责实例的配置、维护、安全更新等。C 方案需要重写代码以适应 Lambda 的运行环境，这违反了“最少代码更改”的要求，且引入了使用 Lambda、API Gateway 的额外复杂度。D 方案，AWS ParallelCluster 主要针对高性能计算场景，用于处理大规模科学计算等任务，与 Web 应用程序的场景不符，且配置和管理 HPC 集群的开销远高于 ECS/Fargate 方案。"
    },
    "related_terms": [
      "Amazon ECS",
      "AWS Fargate",
      "Application Load Balancer",
      "Amazon EC2",
      "AWS Lambda",
      "Amazon API Gateway",
      "EC2",
      "ALB",
      "HPC",
      "Service Auto Scaling",
      "AWS ParallelCluster"
    ]
  },
  {
    "id": 113,
    "topic": "1",
    "question_en": "A company uses 50 TB of data for reporting. The company wants to move this data from on premises to AWS. A custom application in the company’s data center runs a weekly data transformation job. The company plans to pause the application until the data transfer is complete and needs to begin the transfer process as soon as possible. The data center does not have any available network bandwidth for additional workloads. A solutions architect must transfer the data and must configure the transformation job to continue to run in the AWS Cloud. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.",
      "B": "Order an AWS Snowcone device to move the data. Deploy the transformation application to the device.",
      "C": "Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a custom transformation job by using AWS Glue.",
      "D": "Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy the data to the device. Create a new EC2 instance on AWS to run the transformation application."
    },
    "correct_answer": "C",
    "vote_percentage": "71%",
    "question_cn": "一家公司使用 50 TB 的数据进行报告。该公司希望将这些数据从本地迁移到 AWS。该公司数据中心的一个自定义应用程序每周运行一次数据转换作业。该公司计划暂停该应用程序，直到数据传输完成，并且需要尽快开始传输过程。数据中心没有任何可用于额外工作负载的网络带宽。解决方案架构师必须传输数据，并且必须配置转换作业以继续在 AWS 云中运行。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 移动数据。使用 AWS Glue 创建一个自定义转换作业。",
      "B": "订购 AWS Snowcone 设备来移动数据。将转换应用程序部署到该设备。",
      "C": "订购 AWS Snowball Edge 存储优化设备。将数据复制到该设备。使用 AWS Glue 创建一个自定义转换作业。",
      "D": "订购一个包含 Amazon EC2 计算的 AWS Snowball Edge 存储优化设备。将数据复制到该设备。在 AWS 上创建一个新的 EC2 实例以运行转换应用程序。"
    },
    "tags": [
      "AWS Snowball Edge",
      "AWS DataSync",
      "AWS Glue",
      "Amazon EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 71%），解析仅供参考。】\n\n考查本地数据迁移到 AWS 的方案选择，涉及 AWS Snowball Edge、AWS DataSync、AWS Glue 和 Amazon EC2 的对比，以及数据传输速度、运营开销的考量。",
      "why_correct": "选项 C 使用 AWS Snowball Edge 存储优化设备进行数据传输，适用于大量数据的离线迁移场景。将数据复制到 Snowball Edge 设备后，可以将其运回 AWS。同时，使用 AWS Glue 创建自定义转换作业，解决了数据迁移后在 AWS 中进行数据转换的需求。该方案相对而言，运营成本较低，而且避免了在本地环境中进行数据转换，以及在本地缺乏网络带宽的情况下进行数据传输的问题。",
      "why_wrong": "选项 A 使用 AWS DataSync 移动数据，DataSync 适用于在线数据传输，而题目明确指出本地没有足够的网络带宽，所以不适用。选项 B 使用 AWS Snowcone 设备，Snowcone 设备虽然方便携带，但其存储容量有限，并且不适合存储 50 TB 的数据。而且，将应用程序部署到 Snowcone 设备上，会增加管理复杂度和操作开销。选项 D 使用包含 Amazon EC2 计算的 AWS Snowball Edge 设备，虽然解决了数据迁移问题，但需要在 AWS 上创建 EC2 实例来运行转换作业，增加了运营成本，并且配置起来也更复杂，不满足最低运营开销的要求。"
    },
    "related_terms": [
      "AWS DataSync",
      "AWS Glue",
      "Amazon EC2",
      "AWS Snowcone",
      "AWS Snowball Edge"
    ]
  },
  {
    "id": 114,
    "topic": "1",
    "question_en": "A company has created an image analysis application in which users can upload photos and add photo frames to their images. The users upload images and metadata to indicate which photo frames they want to add to their images. The application uses a single Amazon EC2 instance and Amazon DynamoDB to store the metadata. The application is becoming more popular, and the number of users is increasing. The company expects the number of concurrent users to vary significantly depending on the time of day and day of week. The company must ensure that the application can scale to meet the needs of the growing user base. Which solution meats these requirements?",
    "options_en": {
      "A": "Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.",
      "B": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.",
      "C": "Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to store the metadata.",
      "D": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volumes to store the photos and metadata."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司创建了一个图像分析应用程序，用户可以上传照片并向其图像添加相框。用户上传图像和元数据，以指示他们想要添加到图像中的相框。该应用程序使用单个 Amazon EC2 实例和 Amazon DynamoDB 来存储元数据。该应用程序变得越来越受欢迎，用户数量也在增加。该公司预计并发用户数量会因一天中的时间和一周中的日期而异。该公司必须确保该应用程序可以扩展以满足不断增长的用户群的需求。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 AWS Lambda 处理照片。将照片和元数据存储在 DynamoDB 中。",
      "B": "使用 Amazon Kinesis Data Firehose 处理照片，并存储照片和元数据。",
      "C": "使用 AWS Lambda 处理照片。将照片存储在 Amazon S3 中。保留 DynamoDB 来存储元数据。",
      "D": "将 EC2 实例的数量增加到三个。使用预置 IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) 卷来存储照片和元数据。"
    },
    "tags": [
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon S3",
      "Amazon EC2",
      "Scalability",
      "Serverless"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查应用程序的扩展性设计，以及如何存储用户上传的图片和元数据。",
      "why_correct": "选项 C 提供了最可扩展且经济高效的解决方案。使用 AWS Lambda 处理照片，可以自动弹性伸缩以应对并发用户数量的变化。将照片存储在 Amazon S3 中，S3 具有高可用性和可扩展性，可以满足存储需求。DynamoDB 用于存储元数据，可以保证数据的快速访问和管理。",
      "why_wrong": "选项 A 错误在于将照片和元数据都存储在 DynamoDB 中。DynamoDB 主要用于存储元数据，不适合存储大型二进制文件如图片，会影响性能和成本。选项 B 错误，Kinesis Data Firehose 主要用于数据流的摄入和处理，不适合直接存储照片和元数据。选项 D 错误在于，仅增加 EC2 实例数量无法完全解决扩展性问题，并且将照片和元数据存储在 EBS io2 卷上，成本较高，不具备 S3 的弹性和可扩展性。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon EC2",
      "Amazon DynamoDB",
      "Amazon S3",
      "Amazon Kinesis Data Firehose",
      "Amazon EBS",
      "io2"
    ]
  },
  {
    "id": 115,
    "topic": "1",
    "question_en": "A medical records company is hosting an application on Amazon EC2 instances. The application processes customer data files that are stored on Amazon S3. The EC2 instances are hosted in public subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any other network access. A new requirement mandates that the network trafic for file transfers take a private route and not be sent over the internet. Which change to the network architecture should a solutions architect recommend to meet this requirement?",
    "options_en": {
      "A": "Create a NAT gateway. Configure the route table for the public subnets to send trafic to Amazon S3 through the NAT gateway.",
      "B": "Configure the security group for the EC2 instances to restrict outbound trafic so that only trafic to the S3 prefix list is permitted.",
      "C": "Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the endpoint to the route table for the private subnets.",
      "D": "Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route trafic to Amazon S3 over the Direct Connect connection."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家医疗记录公司正在 Amazon EC2 实例上托管一个应用程序。该应用程序处理存储在 Amazon S3 上的客户数据文件。EC2 实例托管在公共子网中。EC2 实例通过互联网访问 Amazon S3，但它们不需要任何其他网络访问。一项新要求规定，文件传输的网络流量必须采取专用路由，而不是通过互联网发送。解决方案架构师应该推荐对网络架构进行哪些更改来满足此要求？",
    "options_cn": {
      "A": "创建一个 NAT Gateway。配置公共子网的路由表，以通过 NAT Gateway 将流量发送到 Amazon S3。",
      "B": "配置 EC2 实例的安全组，以限制出站流量，以便仅允许到 S3 前缀列表的流量。",
      "C": "将 EC2 实例移动到私有子网。为 Amazon S3 创建一个 VPC endpoint，并将该 endpoint 链接到私有子网的路由表。",
      "D": "从 VPC 中删除 internet gateway。设置 AWS Direct Connect 连接，并通过 Direct Connect 连接将流量路由到 Amazon S3。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon S3",
      "VPC endpoint",
      "Private Subnet",
      "Internet Gateway",
      "NAT Gateway",
      "AWS Direct Connect",
      "Security Group",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用私有网络、VPC endpoint 实现 EC2 实例与 Amazon S3 的私有连接；与 VPC 网络架构、安全组配置、Internet Gateway、Direct Connect 的选型相关。",
      "why_correct": "将 EC2 实例移动到私有子网，可以隔离实例的网络访问，使其无法直接通过 Internet Gateway 访问互联网。创建针对 Amazon S3 的 VPC endpoint，将 S3 的流量定向到 AWS 骨干网，实现私有连接，满足了“文件传输的网络流量必须采取专用路由”的需求。VPC endpoint 利用 AWS 内部网络，提高了数据传输的安全性与性能，避免了通过公网进行数据传输。",
      "why_wrong": "A 选项，NAT Gateway 虽然允许私有子网的实例访问互联网，但其主要用途是提供出站互联网访问，并非专用路由。使用 NAT Gateway 仍然涉及通过 Internet Gateway 访问互联网，与题目要求的“专用路由”相悖。B 选项，安全组控制实例的出站流量，但无法实现专用路由。安全组的配置无法改变流量的路由路径，无法满足题目要求。D 选项，虽然 Direct Connect 提供了私有连接，但从 VPC 中删除 Internet Gateway 会导致 EC2 实例失去对互联网的访问能力，而 Direct Connect 的设置和配置需要额外的时间与成本，不一定是最佳实践。如果 EC2 实例还需要访问公网资源，则此方案不适用。而且，仅为了访问 S3 而引入 Direct Connect，显得过于复杂，性价比不高。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "Internet Gateway",
      "NAT Gateway",
      "AWS Direct Connect",
      "VPC",
      "VPC endpoint",
      "Private Subnet",
      "Security Group"
    ]
  },
  {
    "id": 116,
    "topic": "1",
    "question_en": "A company uses a popular content management system (CMS) for its corporate website. However, the required patching and maintenance are burdensome. The company is redesigning its website and wants anew solution. The website will be updated four times a year and does not need to have any dynamic content available. The solution must provide high scalability and enhanced security. Which combination of changes will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Configure Amazon CloudFront in front of the website to use HTTPS functionality.",
      "B": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.",
      "C": "Create and deploy an AWS Lambda function to manage and serve the website content.",
      "D": "Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static website hosting enabled",
      "E": "Create the new website. Deploy the website by using an Auto Scaling group of Amazon EC2 instances behind an Application Load Balancer."
    },
    "correct_answer": "AD",
    "vote_percentage": "85%",
    "question_cn": "一家公司为其企业网站使用流行的内容管理系统（CMS）。然而，所需的补丁和维护工作非常繁重。该公司正在重新设计其网站，并希望采用新的解决方案。该网站每年将更新四次，并且不需要任何动态内容。该解决方案必须提供高可扩展性和增强的安全性。哪两种变更组合将以最少的运营开销满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在网站前面配置 Amazon CloudFront 以使用 HTTPS 功能。",
      "B": "在网站前面部署 AWS WAF Web ACL 以提供 HTTPS 功能。",
      "C": "创建并部署一个 AWS Lambda 函数来管理和提供网站内容。",
      "D": "创建新网站和一个 Amazon S3 存储桶。将网站部署在 S3 存储桶上，并启用静态网站托管。",
      "E": "创建新网站。通过使用 Application Load Balancer 后面的 Amazon EC2 实例的 Auto Scaling 组来部署网站。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "Static Website Hosting",
      "HTTPS",
      "AWS Lambda",
      "Auto Scaling",
      "Amazon EC2",
      "Application Load Balancer",
      "AWS WAF"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 85%），解析仅供参考。】\n\n考查静态网站的部署、可扩展性、安全性和运营开销。与 Amazon CloudFront、Amazon S3、Lambda、EC2 和 ALB 等服务相关，并涉及 HTTPS 协议的配置与使用。",
      "why_correct": "Amazon CloudFront 是一种内容分发网络（CDN）服务，可以缓存静态内容，加速网站访问速度，并提供 HTTPS 功能。为网站配置 CloudFront 可以增强网站的安全性，通过 TLS/SSL 加密传输数据，同时提高可扩展性，减轻源站（S3）的负载。这满足了题目中对高可扩展性、增强安全性的要求，并且运营开销最小。",
      "why_wrong": "选项 B：AWS WAF 主要用于保护 Web 应用程序免受常见的 Web 攻击，虽然可以提供 HTTPS 支持，但本身不是网站托管解决方案，不能直接满足“部署网站”的需求，并且没有提供高可扩展性。 选项 C：使用 Lambda 函数管理和提供网站内容增加了复杂性，并非静态网站的最优解，且运营成本相对较高。选项 D：Amazon S3 的静态网站托管是理想的静态网站托管方案，但单独使用 S3 无法提供 HTTPS。 选项 E：使用 EC2 实例的 Auto Scaling 组和 Application Load Balancer 虽然可以提供高可用性和可扩展性，但对于静态网站而言，部署和维护成本远高于静态网站托管的解决方案，运营开销较高，也不符合题意中“最少的运营开销”的要求。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "HTTPS",
      "Amazon S3",
      "AWS Lambda",
      "Amazon EC2",
      "Application Load Balancer",
      "Auto Scaling",
      "AWS WAF",
      "Static Website Hosting"
    ]
  },
  {
    "id": 117,
    "topic": "1",
    "question_en": "A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy requires the company to store all application logs in Amazon OpenSearch Service (Amazon Elasticsearch Service) in near-real time. Which solution will meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).",
      "B": "Create an AWS Lambda function. Use the log group to invoke the function to write the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service).",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery streams sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the delivery stream's destination.",
      "D": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon Kinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch Service (Amazon Elasticsearch Service)."
    },
    "correct_answer": "A",
    "vote_percentage": "69%",
    "question_cn": "一家公司将其应用程序日志存储在 Amazon CloudWatch Logs 日志组中。一项新策略要求该公司近乎实时地将所有应用程序日志存储在 Amazon OpenSearch Service (Amazon Elasticsearch Service) 中。哪种解决方案以最少的运营开销满足此要求？",
    "options_cn": {
      "A": "配置 CloudWatch Logs 订阅，将日志流式传输到 Amazon OpenSearch Service (Amazon Elasticsearch Service)。",
      "B": "创建 AWS Lambda 函数。使用日志组调用该函数，将日志写入 Amazon OpenSearch Service (Amazon Elasticsearch Service)。",
      "C": "创建 Amazon Kinesis Data Firehose 交付流。将日志组配置为交付流的源。将 Amazon OpenSearch Service (Amazon Elasticsearch Service) 配置为交付流的目的地。",
      "D": "在每个应用程序服务器上安装并配置 Amazon Kinesis Agent，以便将日志传递到 Amazon Kinesis Data Streams。配置 Kinesis Data Streams 以将日志传递到 Amazon OpenSearch Service (Amazon Elasticsearch Service)。"
    },
    "tags": [
      "Amazon CloudWatch Logs",
      "Amazon OpenSearch Service",
      "Kinesis Data Firehose",
      "AWS Lambda",
      "Kinesis Data Streams",
      "Amazon Kinesis Agent"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 69%），解析仅供参考。】\n\n考查 CloudWatch Logs 日志导出到 Amazon OpenSearch Service (Amazon Elasticsearch Service) 的最佳实践，侧重于运营开销的最小化。",
      "why_correct": "CloudWatch Logs 订阅允许近乎实时地将日志流式传输到 Amazon OpenSearch Service (Amazon Elasticsearch Service)。这种方法利用 CloudWatch Logs 的内置功能，无需额外的计算资源或复杂的配置。订阅使用户能够选择所需的日志数据并将其路由到目标服务，从而简化了操作。",
      "why_wrong": "B 选项引入了 Lambda 函数，增加了运营开销和复杂性，需要维护和管理 Lambda 函数代码。C 选项虽然使用了 Kinesis Data Firehose，但相对于直接的 CloudWatch Logs 订阅，增加了配置和管理的复杂性。D 选项需要在每个应用程序服务器上安装和配置 Kinesis Agent，增加了运维负担和潜在的维护成本。"
    },
    "related_terms": [
      "Amazon CloudWatch Logs",
      "Amazon OpenSearch Service",
      "Amazon Elasticsearch Service",
      "AWS Lambda",
      "Amazon Kinesis Data Firehose",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Agent"
    ]
  },
  {
    "id": 118,
    "topic": "1",
    "question_en": "A company is building a web-based application running on Amazon EC2 instances in multiple Availability Zones. The web application will provide access to a repository of text documents totaling about 900 TB in size. The company anticipates that the web application will experience periods of high demand. A solutions architect must ensure that the storage component for the text documents can scale to meet the demand of the application at all times. The company is concerned about the overall cost of the solution. Which storage solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Amazon Elastic Block Store (Amazon EBS)",
      "B": "Amazon Elastic File System (Amazon EFS)",
      "C": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "D": "Amazon S3"
    },
    "correct_answer": "D",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在构建一个基于网络的应用程序，该应用程序在多个可用区中的 Amazon EC2 实例上运行。该 Web 应用程序将提供对大约 900 TB 文本文档存储库的访问。该公司预计 Web 应用程序将经历高需求时期。解决方案架构师必须确保文本文档的存储组件可以扩展以始终满足应用程序的需求。该公司关心解决方案的总体成本。哪种存储解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "Amazon Elastic Block Store (Amazon EBS)",
      "B": "Amazon Elastic File System (Amazon EFS)",
      "C": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "D": "Amazon S3"
    },
    "tags": [
      "Amazon S3",
      "Cost Optimization",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 96%），解析仅供参考。】\n\n考查存储解决方案的选择，侧重于成本效益和可扩展性。涉及到 Amazon S3、Amazon EBS 和 Amazon EFS 的对比，以及它们在不同场景下的适用性。",
      "why_correct": "Amazon S3 是存储大量非结构化数据的最具成本效益的解决方案。它提供高度可扩展性和持久性，能够轻松处理 900 TB 的文本文档存储库。S3 的按需付费模式能够帮助公司控制成本，特别是在流量高峰时期，能够根据实际需求进行弹性扩展，满足应用程序的需求。",
      "why_wrong": "选项 A，Amazon EBS，主要用于 EC2 实例的块级存储。虽然它提供了高性能和持久性，但它不适合存储大量非结构化数据，且成本通常比 Amazon S3 更高，无法满足题目的成本要求。选项 B，Amazon EFS，是一种文件存储服务，适用于需要共享文件系统访问的场景。然而，EFS 的成本通常高于 S3，且针对大容量存储的场景，性价比不如 S3。选项 C，Amazon OpenSearch Service（原 Amazon Elasticsearch Service）是一种用于搜索和分析数据的服务，不适用于存储大型文本文档存储库，并且其设计目的并非为了提供经济高效的大容量存储。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "Amazon EBS",
      "Amazon EFS",
      "Amazon OpenSearch Service",
      "Scalability",
      "Cost Optimization"
    ]
  },
  {
    "id": 119,
    "topic": "1",
    "question_en": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in the us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to protect these API Gateway managed REST APIs across multiple accounts from SQL injection and cross-site scripting attacks. Which solution will meet these requirements with the LEAST amount of administrative effort?",
    "options_en": {
      "A": "Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.",
      "B": "Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.",
      "C": "Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.",
      "D": "Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage."
    },
    "correct_answer": "B",
    "vote_percentage": "69%",
    "question_cn": "一家全球公司正在使用 Amazon API Gateway 为其位于 us-east-1 区域和 ap-southeast-2 区域的忠诚俱乐部用户设计 REST API。 解决方案架构师必须设计一个解决方案，以保护这些 API Gateway 管理的 REST API 跨多个账户免受 SQL 注入和跨站点脚本攻击。 哪种解决方案将以最少的管理工作量满足这些要求？",
    "options_cn": {
      "A": "在两个区域设置 AWS WAF。 将区域 Web ACL 与 API 阶段关联。",
      "B": "在两个区域设置 AWS Firewall Manager。 集中配置 AWS WAF 规则。",
      "C": "在两个区域设置 AWS Shield。 将区域 Web ACL 与 API 阶段关联。",
      "D": "在一个区域设置 AWS Shield。 将区域 Web ACL 与 API 阶段关联。"
    },
    "tags": [
      "Amazon API Gateway",
      "AWS WAF",
      "AWS Shield",
      "AWS Firewall Manager",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 69%），解析仅供参考。】\n\n考查使用 AWS WAF 和 Firewall Manager 保护 API Gateway 管理的 REST API，以应对 SQL 注入和跨站点脚本攻击，并关注管理工作量。",
      "why_correct": "选项 B 提供了最有效和最少管理工作量的解决方案。使用 AWS Firewall Manager 可以集中管理 AWS WAF 规则，并将其部署到多个账户和区域。这简化了配置和维护过程，确保了一致的保护策略。",
      "why_wrong": "选项 A 需要手动配置和管理每个区域的 AWS WAF，增加了管理工作量。选项 C 使用 AWS Shield，它主要用于 DDoS 攻击防护，而非 SQL 注入和 XSS 防护。选项 D 在一个区域设置 Shield 无法满足跨区域的需求，无法为 ap-southeast-2 区域提供保护。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "us-east-1",
      "ap-southeast-2",
      "REST API",
      "SQL injection",
      "cross-site scripting",
      "AWS WAF",
      "AWS Firewall Manager",
      "Web ACL",
      "API stage",
      "AWS Shield",
      "DDoS"
    ]
  },
  {
    "id": 120,
    "topic": "1",
    "question_en": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind a Network Load Balancer (NLB) in the us- west-2 Region. Most of the company's users are located in the United States and Europe. The company wants to improve the performance and availability of the solution. The company launches and configures three EC2 instances in the eu-west-1 Region and adds the EC2 instances as targets for a new NLB. Which solution can the company use to route trafic to all the EC2 instances?",
    "options_en": {
      "A": "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.",
      "B": "Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and eu-west-1. Add the two NLBs as endpoints for the endpoint groups.",
      "C": "Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation routing policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution's origin.",
      "D": "Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 latency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin."
    },
    "correct_answer": "B",
    "vote_percentage": "75%",
    "question_cn": "一家公司在 us-west-2 区域的三个 Amazon EC2 实例上实现了自管理的 DNS 解决方案，这些实例位于一个 Network Load Balancer (NLB) 之后。该公司的大部分用户位于美国和欧洲。该公司希望提高解决方案的性能和可用性。该公司在 eu-west-1 区域启动并配置了三个 EC2 实例，并将这些 EC2 实例添加为新 NLB 的目标。该公司可以使用哪种解决方案来将流量路由到所有 EC2 实例？",
    "options_cn": {
      "A": "创建 Amazon Route 53 地理位置路由策略，将请求路由到两个 NLB 之一。创建 Amazon CloudFront 分配。使用 Route 53 记录作为分配的源。",
      "B": "在 AWS Global Accelerator 中创建标准加速器。在 us-west-2 和 eu-west-1 中创建终端节点组。将两个 NLB 添加为终端节点组的终端节点。",
      "C": "将弹性 IP 地址附加到六个 EC2 实例。创建 Amazon Route 53 地理位置路由策略，将请求路由到六个 EC2 实例之一。创建 Amazon CloudFront 分配。使用 Route 53 记录作为分配的源。",
      "D": "用两个 Application Load Balancer (ALB) 替换两个 NLB。创建 Amazon Route 53 延迟路由策略，将请求路由到两个 ALB 之一。创建 Amazon CloudFront 分配。使用 Route 53 记录作为分配的源。"
    },
    "tags": [
      "Amazon Route 53",
      "Amazon CloudFront",
      "Network Load Balancer",
      "Route 53 Geolocation Routing",
      "EC2",
      "Application Load Balancer",
      "AWS Global Accelerator"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 75%），解析仅供参考。】\n\n考查通过 AWS Global Accelerator 实现跨区域流量路由，以提高 DNS 解决方案的性能和可用性。",
      "why_correct": "AWS Global Accelerator 能够通过其全球网络，将用户流量智能地路由到性能最佳的终端节点。在题干场景中，Global Accelerator 可以创建标准加速器，将us-west-2和eu-west-1区域的NLB添加为终端节点，从而优化流量路由，提高全球用户的访问体验。",
      "why_wrong": "选项A，地理位置路由策略仅根据用户地理位置将流量路由到特定区域，无法实现跨区域的优化加速。选项C，弹性 IP 地址无法提升跨区域流量路由的性能。选项D，ALB 无法提供Global Accelerator的全球加速能力，并且延迟路由策略主要用于基于终端节点的延迟情况进行路由，与题干中的需求不符，也无法实现跨区域优化。"
    },
    "related_terms": [
      "Amazon EC2",
      "Network Load Balancer (NLB)",
      "Amazon Route 53",
      "Amazon CloudFront",
      "AWS Global Accelerator",
      "Application Load Balancer (ALB)",
      "Route 53 Geolocation routing policy",
      "Elastic IP address",
      "Route 53 latency routing policy"
    ]
  },
  {
    "id": 121,
    "topic": "1",
    "question_en": "A company is running an online transaction processing (OLTP) workload on AWS. This workload uses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots are taken from this instance. What should a solutions architect do to ensure the database and snapshots are always encrypted moving forward?",
    "options_en": {
      "A": "Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted snapshot.",
      "B": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots to it. Enable encryption on the DB instance.",
      "C": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) Restore encrypted snapshot to an existing DB instance.",
      "D": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with AWS Key Management Service (AWS KMS) managed keys (SSE-KMS)."
    },
    "correct_answer": "A",
    "vote_percentage": "83%",
    "question_cn": "一家公司正在 AWS 上运行在线事务处理 (OLTP) 工作负载。此工作负载使用位于 Multi-AZ 部署中的未加密 Amazon RDS 数据库实例。每天从该实例拍摄数据库快照。解决方案架构师应采取什么措施来确保数据库和快照在未来始终被加密？",
    "options_cn": {
      "A": "加密最新数据库快照的副本。通过恢复加密的快照来替换现有的数据库实例。",
      "B": "创建一个新的加密 Amazon Elastic Block Store (Amazon EBS) 卷，并将快照复制到其中。在数据库实例上启用加密。",
      "C": "复制快照并使用 AWS Key Management Service (AWS KMS) 启用加密。将加密的快照恢复到现有的数据库实例。",
      "D": "将快照复制到使用带有 AWS Key Management Service (AWS KMS) 托管密钥的服务器端加密 (SSE-KMS) 加密的 Amazon S3 存储桶。"
    },
    "tags": [
      "RDS",
      "Multi-AZ",
      "encryption",
      "AWS KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 83%），解析仅供参考。】\n\n为了确保数据库及其快照在未来始终加密，需要对数据库快照进行加密。选项 A 通过恢复加密的快照，替换现有的数据库实例，可以实现加密。这涉及创建加密的快照副本，然后用加密快照恢复的数据库实例替换现有的未加密实例。",
      "why_correct": "选项 A 通过恢复加密的快照来替换现有的数据库实例，直接解决了题目中对数据库加密的要求，保证了数据库和快照的加密。",
      "why_wrong": "选项 B 错误，因为即使创建新的加密 EBS 卷，并且在数据库实例上启用加密，也不能保证现有的快照也会被加密。选项 C 错误，因为复制快照并使用 KMS 加密无法直接在现有的未加密数据库实例上实现加密。选项 D 错误，虽然将快照复制到 S3 存储桶并使用 KMS 加密是可行的，但无法保证数据库实例的加密。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "快照",
      "AWS KMS",
      "Amazon EBS",
      "Amazon S3",
      "SSE-KMS",
      "database"
    ]
  },
  {
    "id": 122,
    "topic": "1",
    "question_en": "A company wants to build a scalable key management infrastructure to support developers who need to encrypt data in their applications. What should a solutions architect do to reduce the operational burden?",
    "options_en": {
      "A": "Use multi-factor authentication (MFA) to protect the encryption keys.",
      "B": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys.",
      "C": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.",
      "D": "Use an IAM policy to limit the scope of users who have access permissions to protect the encryption keys."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望构建可扩展的密钥管理基础设施，以支持需要在其应用程序中加密数据的开发人员。解决方案架构师应该怎么做以减少运营负担？",
    "options_cn": {
      "A": "使用多因素身份验证 (MFA) 来保护加密密钥。",
      "B": "使用 AWS Key Management Service (AWS KMS) 来保护加密密钥。",
      "C": "使用 AWS Certificate Manager (ACM) 来创建、存储和分配加密密钥。",
      "D": "使用 IAM 策略来限制有权访问权限的用户范围，以保护加密密钥。"
    },
    "tags": [
      "AWS KMS",
      "encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\nAWS KMS 旨在简化密钥管理，提供高度可扩展且安全的密钥管理服务。使用 KMS 可以减少运营负担，因为它负责密钥的创建、存储、管理和审计，从而降低了维护密钥管理基础设施的复杂性。",
      "why_correct": "选项 B 描述了使用 AWS KMS 保护加密密钥，从而减少运营负担，KMS 提供了密钥的集中管理，并简化了密钥生命周期管理。",
      "why_wrong": "选项 A 错误，多因素身份验证 (MFA) 只能增加访问加密密钥的安全性，并不能直接减少运营负担。 选项 C 错误，AWS Certificate Manager (ACM) 用于管理 SSL/TLS 证书，而不是加密密钥。选项 D 错误，IAM 策略限制了访问权限，但没有减少运营负担。"
    },
    "related_terms": [
      "AWS KMS",
      "加密密钥",
      "MFA",
      "AWS Certificate Manager (ACM)",
      "IAM"
    ]
  },
  {
    "id": 123,
    "topic": "1",
    "question_en": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company has its own SSL certificate, which is on each instance to perform SSL termination. There has been an increase in trafic recently, and the operations team determined that SSL encryption and decryption is causing the compute capacity of the web servers to reach their maximum limit. What should a solutions architect do to increase the application's performance?",
    "options_en": {
      "A": "Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on each instance.",
      "B": "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket. Configure the EC2 instances to reference the bucket for SSL termination.",
      "C": "Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and configure it to direct connections to the existing EC2 instances.",
      "D": "Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer with an HTTPS listener that uses the SSL certificate from ACM."
    },
    "correct_answer": "D",
    "vote_percentage": "96%",
    "question_cn": "一家公司在其两个 Amazon EC2 实例上托管动态 Web 应用程序。该公司拥有自己的 SSL 证书，该证书位于每个实例上以执行 SSL 终止。最近流量有所增加，运营团队确定 SSL 加密和解密导致 Web 服务器的计算能力达到其最大限制。解决方案架构师应该怎么做来提高应用程序的性能？",
    "options_cn": {
      "A": "使用 AWS Certificate Manager (ACM) 创建一个新的 SSL 证书。将 ACM 证书安装在每个实例上。",
      "B": "创建一个 Amazon S3 存储桶。将 SSL 证书迁移到 S3 存储桶。配置 EC2 实例以引用该存储桶进行 SSL 终止。",
      "C": "创建另一个 EC2 实例作为代理服务器。将 SSL 证书迁移到新实例，并将其配置为将连接定向到现有的 EC2 实例。",
      "D": "将 SSL 证书导入 AWS Certificate Manager (ACM)。创建一个 Application Load Balancer (ALB)，其中包含一个使用来自 ACM 的 SSL 证书的 HTTPS 侦听器。"
    },
    "tags": [
      "ACM",
      "SSL",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 96%），解析仅供参考。】\n\n为了提高应用程序的性能，需要优化 SSL 终止过程，减轻 Web 服务器的负担。Application Load Balancer (ALB) 可以处理 SSL 终止，并将解密后的流量转发到后端 EC2 实例。结合 ACM 使用，可以简化证书管理和部署。",
      "why_correct": "选项 D 描述了使用 ACM 生成的 SSL 证书配置 Application Load Balancer (ALB) 来处理 HTTPS 流量，可以有效分担 EC2 实例的 SSL 终止工作，提高应用程序性能。",
      "why_wrong": "选项 A 错误，在每个实例上安装 ACM 证书并不能解决性能问题，反而增加了配置工作。 选项 B 错误，将 SSL 证书迁移到 S3 存储桶无法用于 SSL 终止。 选项 C 错误，创建一个代理服务器，也无法解决性能瓶颈。"
    },
    "related_terms": [
      "AWS Certificate Manager (ACM)",
      "SSL",
      "EC2",
      "HTTPS",
      "Application Load Balancer (ALB)",
      "SSL 终止"
    ]
  },
  {
    "id": 124,
    "topic": "1",
    "question_en": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to complete it. The job is stateless in nature, can be started and stopped at any given time with no negative impact, and typically takes upwards of 60 minutes total to complete. The company has asked a solutions architect to design a scalable and cost-effective solution that meets the requirements of the job. What should the solutions architect recommend?",
    "options_en": {
      "A": "Implement EC2 Spot Instances.",
      "B": "Purchase EC2 Reserved Instances.",
      "C": "Implement EC2 On-Demand Instances.",
      "D": "Implement the processing on AWS Lambda."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司有一个高度动态的批处理作业，该作业使用许多 Amazon EC2 实例来完成。该作业本质上是无状态的，可以在任何给定时间启动和停止，没有任何负面影响，并且通常需要 60 分钟以上才能完成。该公司要求解决方案架构师设计一个可扩展且经济高效的解决方案，以满足该作业的要求。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "实施 EC2 Spot Instances。",
      "B": "购买 EC2 Reserved Instances。",
      "C": "实施 EC2 On-Demand Instances。",
      "D": "在 AWS Lambda 上实施处理。"
    },
    "tags": [
      "EC2",
      "Spot Instances",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n对于高度动态、无状态且可以随时启动和停止的批处理作业，Spot Instances 提供了经济高效的计算资源。Spot Instances 提供了比按需实例更低的价格，尤其适合于可以中断的工作负载。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 使用 EC2 Spot Instances，利用了 Spot 实例的经济性，适合于可以中断的无状态批处理作业，可以有效降低成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误， Reserved Instances 适用于有稳定、可预测工作负载的情况，不适用于动态批处理作业。 选项 C 错误，On-Demand Instances 成本较高，不适合批处理作业。 选项 D 错误，Lambda 适用于短时、事件驱动的任务，不适合长时间运行的批处理作业。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "AWS Lambda",
      "Spot Instances",
      "EC2 Reserved Instances",
      "EC2 On-Demand Instances"
    ]
  },
  {
    "id": 125,
    "topic": "1",
    "question_en": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer that sends trafic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. The EC2 instances and the RDS DB instance should not be exposed to the public internet. The EC2 instances require internet access to complete payment processing of orders through a third-party web service. The application must be highly available. Which combination of configuration options will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ DB instance in private subnets.",
      "B": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the private subnets.",
      "C": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability Zones. Deploy an RDS Multi-AZ DB instance in private subnets.",
      "D": "Configure a VPC with two public subnets, two private subnets, and two NAT gateways across two Availability Zones. Deploy an Application Load Balancer in the public subnets."
    },
    "correct_answer": "AD",
    "vote_percentage": "61%",
    "question_cn": "一家公司在 AWS 上运行其两层电子商务网站。Web 层由一个负载均衡器组成，该负载均衡器将流量发送到 Amazon EC2 实例。数据库层使用 Amazon RDS 数据库实例。EC2 实例和 RDS 数据库实例都不应暴露给公共互联网。EC2 实例需要访问互联网以完成订单的支付处理。应用程序必须是高可用的。哪种组合的配置选项将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用自动扩展组在私有子网中启动 EC2 实例。在私有子网中部署 RDS 多可用区数据库实例。",
      "B": "配置具有两个私有子网和两个跨两个可用区的 NAT 网关的 VPC。在私有子网中部署应用程序负载均衡器。",
      "C": "使用自动扩展组在两个可用区的公共子网中启动 EC2 实例。在私有子网中部署 RDS 多可用区数据库实例。",
      "D": "配置具有两个公有子网、两个私有子网与两个 NAT 网关（跨两个可用区）的 VPC；在公有子网中部署应用程序负载均衡器。"
    },
    "tags": [
      "VPC",
      "EC2",
      "RDS",
      "Auto Scaling",
      "NAT Gateway",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 61%），解析仅供参考。】\n\n考察了 VPC、子网、NAT 网关、ELB、EC2 实例和 RDS 数据库实例的配置，以满足高可用性、私有访问和互联网访问需求。",
      "why_correct": "选项 A 满足所有要求。使用自动扩展组在私有子网中启动 EC2 实例确保了实例不暴露于公网，同时提供了高可用性。RDS 多可用区数据库实例在私有子网中的部署保证了数据库的私有访问和高可用性。",
      "why_wrong": "选项 B 错误，因为应用程序负载均衡器不应该部署在私有子网中，其需要公网访问。选项 C 错误，因为将 EC2 实例部署在公共子网中会暴露给公共互联网，这违反了题目要求。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "RDS",
      "Elastic Load Balancer (ELB)",
      "Auto Scaling Group",
      "Private Subnet",
      "Public Subnet",
      "NAT Gateway",
      "Multi-AZ"
    ]
  },
  {
    "id": 126,
    "topic": "1",
    "question_en": "A solutions architect needs to implement a solution to reduce a company's storage costs. All the company's data is in the Amazon S3 Standard storage class. The company must keep all data for at least 25 years. Data from the most recent 2 years must be highly available and immediately retrievable. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.",
      "B": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.",
      "C": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier Deep Archive.",
      "D": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately and to S3 Glacier Deep Archive after 2 years."
    },
    "correct_answer": "B",
    "vote_percentage": "79%",
    "question_cn": "一位解决方案架构师需要实施一个解决方案来降低公司的存储成本。该公司所有的数据都在 Amazon S3 标准存储类中。该公司必须将所有数据保留至少 25 年。最近 2 年的数据必须具有高可用性，并且可以立即检索。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置 S3 生命周期策略，立即将对象转换为 S3 Glacier Deep Archive。",
      "B": "设置 S3 生命周期策略，在 2 年后将对象转换为 S3 Glacier Deep Archive。",
      "C": "使用 S3 Intelligent-Tiering。 激活归档选项以确保数据归档在 S3 Glacier Deep Archive 中。",
      "D": "设置 S3 生命周期策略，立即将对象转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)，并在 2 年后转换为 S3 Glacier Deep Archive。"
    },
    "tags": [
      "S3",
      "lifecycle policy",
      "S3 Glacier Deep Archive"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 79%），解析仅供参考。】\n\n需要设置存储策略以满足数据保留和快速检索的需求。S3 生命周期策略允许根据对象的使用频率和时间，自动将对象迁移到不同的存储类。为了满足成本效益和数据可用性要求，应该将数据迁移到合适的存储层。",
      "why_correct": "选项 B 设置了 S3 生命周期策略，在 2 年后将对象转换为 S3 Glacier Deep Archive，满足了 2 年内数据高可用性、可立即检索，以及长期存储的要求。",
      "why_wrong": "选项 A 错误，立即转换为 S3 Glacier Deep Archive 无法满足近期数据需要高可用性和立即检索的需求。 选项 C 错误，Intelligent-Tiering 不能直接迁移到 Glacier Deep Archive，并且无法满足立即检索的需求。 选项 D 错误，S3 One Zone-IA 无法满足需要高可用性的要求，并且在 2 年后转换为 Glacier Deep Archive 会导致数据检索的复杂性。"
    },
    "related_terms": [
      "S3",
      "S3 Glacier Deep Archive",
      "S3 Intelligent-Tiering",
      "S3 lifecycle policy",
      "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    ]
  },
  {
    "id": 127,
    "topic": "1",
    "question_en": "A media company is evaluating the possibility of moving its systems to the AWS Cloud. The company needs at least 10 TB of storage with the maximum possible I/O performance for video processing, 300 TB of very durable storage for storing media content, and 900 TB of storage to meet requirements for archival media that is not in use anymore. Which set of services should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage",
      "B": "Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 Glacier for archival storage",
      "C": "Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and Amazon S3 for archival storage",
      "D": "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and Amazon S3 Glacier for archival storage"
    },
    "correct_answer": "D",
    "vote_percentage": "62%",
    "question_cn": "一家媒体公司正在评估将其系统迁移到 AWS 云的可能性。该公司需要至少 10 TB 的存储空间，以便为视频处理提供最高的 I/O 性能，300 TB 的非常耐用的存储空间来存储媒体内容，以及 900 TB 的存储空间以满足不再使用的存档媒体的需求。解决方案架构师应该推荐哪一组服务来满足这些需求？",
    "options_cn": {
      "A": "使用 Amazon EBS 以获得最高性能，Amazon S3 用于耐用数据存储，以及 Amazon S3 Glacier 用于存档存储",
      "B": "使用 Amazon EBS 以获得最高性能，Amazon EFS 用于耐用数据存储，以及 Amazon S3 Glacier 用于存档存储",
      "C": "使用 Amazon EC2 实例存储以获得最高性能，Amazon EFS 用于耐用数据存储，以及 Amazon S3 用于存档存储",
      "D": "使用 Amazon EC2 实例存储以获得最高性能，Amazon S3 用于耐用数据存储，以及 Amazon S3 Glacier 用于存档存储"
    },
    "tags": [
      "EBS",
      "S3",
      "EFS",
      "S3 Glacier"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 62%），解析仅供参考。】\n\n考察 AWS 存储服务的选择，包括高性能存储、耐用存储和存档存储的匹配。需要根据不同存储需求选择合适的存储服务。",
      "why_correct": "选项 D 提供了最佳的组合来满足所有存储需求。Amazon EC2 Instance Store 提供了最高的 I/O 性能，适用于视频处理。Amazon S3 提供了高度耐用的存储，适用于媒体内容。Amazon S3 Glacier 提供了低成本的存档存储，适用于不再使用的存档媒体。",
      "why_wrong": "选项 A 错误在于，EBS 并不总是提供最高的 I/O 性能，EC2 Instance Store 通常更快，尤其是在本地存储方面。选项 B 错误在于，EFS 通常不是用于需要高 I/O 性能的场景，而 EC2 Instance Store 更合适。选项 C 错误在于，EFS 并不像 S3 那样耐用。另外，虽然 EC2 Instance Store 提供了高性能，但它不提供数据冗余，这意味着数据如果实例失效就会丢失，不适合存储需要高持久性的内容。因此，选项 D 最合适，因为它结合了所有要求的最佳服务。"
    },
    "related_terms": [
      "Amazon EBS",
      "Amazon S3",
      "Amazon S3 Glacier",
      "Amazon EFS",
      "Amazon EC2 Instance Store",
      "EC2"
    ]
  },
  {
    "id": 128,
    "topic": "1",
    "question_en": "A company wants to run applications in containers in the AWS Cloud. These applications are stateless and can tolerate disruptions within the underlying infrastructure. The company needs a solution that minimizes cost and operational overhead. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
      "B": "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.",
      "C": "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.",
      "D": "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group."
    },
    "correct_answer": "B",
    "vote_percentage": "72%",
    "question_cn": "一家公司希望在 AWS 云中运行容器化应用程序。这些应用程序是无状态的，并且可以容忍底层基础设施中的中断。该公司需要一个能够最大限度地降低成本和运营开销的解决方案。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在 Amazon EC2 Auto Scaling 组中使用 Spot Instances 来运行应用程序容器。",
      "B": "在 Amazon Elastic Kubernetes Service (Amazon EKS) 托管节点组中使用 Spot Instances。",
      "C": "在 Amazon EC2 Auto Scaling 组中使用 On-Demand Instances 来运行应用程序容器。",
      "D": "在 Amazon Elastic Kubernetes Service (Amazon EKS) 托管节点组中使用 On-Demand Instances。"
    },
    "tags": [
      "EC2",
      "Spot Instances",
      "EKS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 72%），解析仅供参考。】\n\n考查在 AWS 上运行容器化应用时，如何通过选择合适的实例类型和容器编排服务，以降低成本和运营开销。",
      "why_correct": "Amazon EKS 提供了托管 Kubernetes 集群，简化了容器的部署和管理。结合 Spot Instances 使用，可以利用闲置的 EC2 容量，从而显著降低成本。无状态应用程序与 Spot Instances 完美契合，因为它们可以容忍底层基础设施的中断，并且能够自动进行重新部署。",
      "why_wrong": "选项 A 在 EC2 Auto Scaling 组中使用 Spot Instances 运行应用程序容器，虽然可以降低成本，但需要手动管理容器的部署和编排，增加了运营开销。选项 C 使用 On-Demand Instances，无法有效降低成本。选项 D 使用 On-Demand Instances，且虽然利用了 EKS，但无法最大限度降低成本，且成本高于使用 Spot Instances 的方案。"
    },
    "related_terms": [
      "Amazon EKS",
      "Spot Instances",
      "Amazon EC2",
      "Auto Scaling",
      "On-Demand Instances",
      "Kubernetes"
    ]
  },
  {
    "id": 129,
    "topic": "1",
    "question_en": "A company is running a multi-tier web application on premises. The web application is containerized and runs on a number of Linux hosts connected to a PostgreSQL database that contains user records. The operational overhead of maintaining the infrastructure and capacity planning is limiting the company's growth. A solutions architect must improve the application's infrastructure. Which combination of actions should the solutions architect take to accomplish this? (Choose two.)",
    "options_en": {
      "A": "Migrate the PostgreSQL database to Amazon Aurora.",
      "B": "Migrate the web application to be hosted on Amazon EC2 instances.",
      "C": "Set up an Amazon CloudFront distribution for the web application content.",
      "D": "Set up Amazon ElastiCache between the web application and the PostgreSQL databas",
      "E": "E. Migrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service (Amazon ECS)."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家公司在其本地部署上运行一个多层 Web 应用程序。该 Web 应用程序已容器化，并在连接到包含用户记录的 PostgreSQL 数据库的多个 Linux 主机上运行。维护基础设施和容量规划的运营开销限制了公司的发展。一位解决方案架构师必须改进应用程序的基础设施。解决方案架构师应采取哪些组合的操作来完成此任务？（选择两个。）",
    "options_cn": {
      "A": "将 PostgreSQL 数据库迁移到 Amazon Aurora。",
      "B": "将 Web 应用程序迁移到托管在 Amazon EC2 实例上。",
      "C": "为 Web 应用程序内容设置 Amazon CloudFront 分发。",
      "D": "在 Web 应用程序和 PostgreSQL 数据库之间设置 Amazon ElastiCache。",
      "E": "将 Web 应用程序迁移到使用 Amazon Elastic Container Service (Amazon ECS) 托管在 AWS Fargate 上。"
    },
    "tags": [
      "Aurora",
      "ECS",
      "ElastiCache",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n为了改进 Web 应用程序的基础设施，需要考虑数据库的选择和 Web 应用程序的部署方式。将数据库迁移到托管服务，例如 Aurora，可以减少运营负担，同时改善性能和可扩展性。使用托管服务可以减少维护成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：选项 A 将 PostgreSQL 数据库迁移到 Amazon Aurora，可以提高数据库的性能、可用性和可扩展性，并减少管理开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，将 Web 应用程序迁移到托管在 Amazon EC2 实例上，没有解决运营开销的问题。 选项 C 错误，设置 Amazon CloudFront 分发可以加速静态内容的访问，但不能解决数据库的瓶颈。 选项 D 错误，在 Web 应用程序和 PostgreSQL 数据库之间设置 Amazon ElastiCache 可以改善数据库的性能，但没有减少运营开销。 选项 E 错误，将 Web 应用程序迁移到使用 Amazon Elastic Container Service (Amazon ECS) 托管在 AWS Fargate 上，虽然减少了运维负担，但没有提升数据库性能。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Aurora",
      "PostgreSQL",
      "Amazon ECS",
      "Amazon EC2",
      "Amazon CloudFront",
      "Amazon ElastiCache"
    ]
  },
  {
    "id": 130,
    "topic": "1",
    "question_en": "An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run in an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application performs best when the CPU utilization of the EC2 instances is at or near 40%. What should a solutions architect do to maintain the desired performance across all instances in the group?",
    "options_en": {
      "A": "Use a simple scaling policy to dynamically scale the Auto Scaling group.",
      "B": "Use a target tracking policy to dynamically scale the Auto Scaling group.",
      "C": "Use an AWS Lambda function ta update the desired Auto Scaling group capacity.",
      "D": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个应用程序在多个可用区运行在 Amazon EC2 实例上。这些实例在 Application Load Balancer 后面的 Amazon EC2 Auto Scaling 组中运行。当 EC2 实例的 CPU 利用率接近或达到 40% 时，该应用程序的性能最佳。解决方案架构师应该怎么做才能在组中的所有实例上保持所需的性能？",
    "options_cn": {
      "A": "使用简单的伸缩策略动态伸缩 Auto Scaling 组。",
      "B": "使用目标跟踪策略动态伸缩 Auto Scaling 组。",
      "C": "使用 AWS Lambda 函数来更新所需的 Auto Scaling 组容量。",
      "D": "使用预定的伸缩操作来伸缩和缩减 Auto Scaling 组。"
    },
    "tags": [
      "Auto Scaling",
      "目标跟踪策略"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n为了保证应用程序的性能，需要根据 CPU 利用率动态伸缩 EC2 实例的数量。目标跟踪策略可以自动调整 Auto Scaling 组的容量，以维持目标 CPU 利用率。",
      "why_correct": "选项 B 使用目标跟踪策略，当 EC2 实例的 CPU 利用率接近或达到 40% 时，该策略可以自动动态伸缩 Auto Scaling 组，维持所需的性能。",
      "why_wrong": "选项 A 错误，简单的伸缩策略无法根据目标 CPU 利用率自动伸缩。 选项 C 错误，使用 AWS Lambda 函数来更新所需的 Auto Scaling 组容量，需要手动配置和维护，并且响应速度较慢。 选项 D 错误，预定的伸缩操作是基于时间表的，不能动态响应 CPU 利用率的变化。"
    },
    "related_terms": [
      "Auto Scaling",
      "AWS Lambda",
      "CPU 利用率",
      "目标跟踪策略"
    ]
  },
  {
    "id": 131,
    "topic": "1",
    "question_en": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. The company wants to serve all the files through an Amazon CloudFront distribution. The company does not want the files to be accessible through direct navigation to the S3 URL. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.",
      "B": "Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to CloudFront.",
      "C": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns the target S3 bucket as the Amazon Resource Name (ARN).",
      "D": "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the S3 bucket permissions so that only the OAI has read permission."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在开发一个文件共享应用程序，该应用程序将使用 Amazon S3 存储桶进行存储。该公司希望通过 Amazon CloudFront 分发来提供所有文件。该公司不希望通过直接导航到 S3 URL 来访问这些文件。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "为每个 S3 存储桶编写单独的策略，仅授予 CloudFront 访问权限的读取权限。",
      "B": "创建一个 IAM 用户。 授予该用户对 S3 存储桶中对象的读取权限。 将该用户分配给 CloudFront。",
      "C": "编写一个 S3 存储桶策略，将 CloudFront 分发 ID 分配为主体，并将目标 S3 存储桶分配为 Amazon 资源名称 (ARN)。",
      "D": "创建源访问身份 (OAI)。 将 OAI 分配给 CloudFront 分发。 配置 S3 存储桶权限，以便只有 OAI 具有读取权限。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "OAI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n为了保护 S3 存储桶中的文件，确保用户只能通过 CloudFront 访问，需要配置适当的访问控制。源访问身份 (OAI) 可以限制对 S3 存储桶的访问，只有 CloudFront 才能访问。",
      "why_correct": "选项 D 创建一个源访问身份 (OAI)。 将 OAI 分配给 CloudFront 分发。 配置 S3 存储桶权限，以便只有 OAI 具有读取权限，这可以确保用户无法直接访问 S3 存储桶中的文件，只能通过 CloudFront 访问。",
      "why_wrong": "选项 A 错误，为每个 S3 存储桶编写单独的策略，虽然可以控制访问权限，但管理起来复杂，而且没有限制用户直接访问。 选项 B 错误，直接授予 IAM 用户访问 S3 存储桶的权限，无法阻止用户直接访问 S3 URL。 选项 C 错误，使用 CloudFront 分发 ID 分配为主体，没有限制用户直接访问。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "IAM",
      "OAI",
      "存储桶策略"
    ]
  },
  {
    "id": 132,
    "topic": "1",
    "question_en": "A company’s website provides users with downloadable historical performance reports. The website needs a solution that will scale to meet the company’s website demands globally. The solution should be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest possible response time. Which combination should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Amazon CloudFront and Amazon S3",
      "B": "AWS Lambda and Amazon DynamoDB",
      "C": "Application Load Balancer with Amazon EC2 Auto Scaling",
      "D": "Amazon Route 53 with internal Application Load Balancers"
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一家公司的网站为用户提供可下载的历史性能报告。该网站需要一个解决方案，以扩展来满足公司网站的全球需求。该解决方案应具有成本效益，限制基础设施资源的配置，并提供最快的响应时间。解决方案架构师应该推荐哪种组合来满足这些要求？",
    "options_cn": {
      "A": "Amazon CloudFront 和 Amazon S3",
      "B": "AWS Lambda 和 Amazon DynamoDB",
      "C": "带有 Amazon EC2 Auto Scaling 的 Application Load Balancer",
      "D": "带有内部 Application Load Balancers 的 Amazon Route 53"
    },
    "tags": [
      "CloudFront",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n为了满足全球用户的需求，并提供最快的响应时间，需要使用内容分发网络（CDN）来缓存和分发内容。Amazon CloudFront 可以缓存内容，并将其分发到全球各地的边缘站点，从而提高网站的性能。",
      "why_correct": "选项 A 使用 Amazon CloudFront 和 Amazon S3。Amazon S3 用于存储可下载的报告，Amazon CloudFront 用于在全球范围内分发这些报告，从而提高性能和用户体验。",
      "why_wrong": "选项 B 错误，AWS Lambda 和 Amazon DynamoDB 不是用来分发静态内容的。 选项 C 错误，Application Load Balancer 主要用于负载均衡，不能直接解决全球分发的问题。 选项 D 错误，Route 53 用于 DNS 解析，也不是用来分发静态内容的。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Application Load Balancer",
      "Amazon Route 53"
    ]
  },
  {
    "id": 133,
    "topic": "1",
    "question_en": "A company runs an Oracle database on premises. As part of the company’s migration to AWS, the company wants to upgrade the database to the most recent available version. The company also wants to set up disaster recovery (DR) for the database. The company needs to minimize the operational overhead for normal operations and DR setup. The company also needs to maintain access to the database's underlying operating system. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different AWS Region.",
      "B": "Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups to replicate the snapshots to another AWS Region.",
      "C": "Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the database in another AWS Region.",
      "D": "Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another Availability Zone."
    },
    "correct_answer": "C",
    "vote_percentage": "49%",
    "question_cn": "一家公司在其本地运行 Oracle 数据库。作为公司迁移到 AWS 的一部分，该公司希望将数据库升级到最新的可用版本。该公司还希望为数据库设置灾难恢复 (DR)。该公司需要最大限度地减少正常运营和 DR 设置的运营开销。该公司还需要保持对数据库底层操作系统的访问。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 Oracle 数据库迁移到 Amazon EC2 实例。设置数据库复制到不同的 AWS 区域。",
      "B": "将 Oracle 数据库迁移到 Amazon RDS for Oracle。激活跨区域自动备份以将快照复制到另一个 AWS 区域。",
      "C": "将 Oracle 数据库迁移到 Amazon RDS Custom for Oracle。在另一个 AWS 区域中为数据库创建一个只读副本。",
      "D": "将 Oracle 数据库迁移到 Amazon RDS for Oracle。在另一个可用区中创建一个备用数据库。"
    },
    "tags": [
      "RDS",
      "Oracle",
      "DR"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 49%），解析仅供参考。】\n\n考查在迁移 Oracle 数据库到 AWS 时，如何在满足运营开销最小化、灾难恢复、以及操作系统访问需求的情况下选择合适的 RDS 解决方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：Amazon RDS Custom for Oracle 允许用户自定义数据库环境和操作系统设置，满足了保留底层操作系统访问的需求。创建一个只读副本可以实现灾难恢复，并减少运营开销，因为 RDS Custom 提供了对数据库的自动化管理。由于是只读副本，可以最大限度地减少管理开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 仅提供了 EC2 实例的解决方案，需要手动管理数据库复制和 DR 方案，增加了运营开销。选项 B 虽然使用 RDS for Oracle，简化了数据库管理，但跨区域自动备份并非是最佳的 DR 解决方案，并且不满足保留操作系统访问的需求。选项 D 使用了 RDS for Oracle，在同一个区域内创建备用数据库，无法满足跨区域的灾难恢复需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Oracle",
      "AWS",
      "Amazon RDS for Oracle",
      "disaster recovery",
      "Amazon RDS Custom for Oracle",
      "Availability Zone",
      "read replica"
    ]
  },
  {
    "id": 134,
    "topic": "1",
    "question_en": "A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing and new data by using SL. The company stores the data in an Amazon S3 bucket. The data requires encryption and must be replicated to a different AWS Region. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.",
      "B": "Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.",
      "C": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon Athena to query the data.",
      "D": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use Amazon RDS to query the data."
    },
    "correct_answer": "C",
    "vote_percentage": "52%",
    "question_cn": "一家公司希望将其应用程序迁移到无服务器解决方案。该无服务器解决方案需要使用 SQL 分析现有和新数据。该公司将数据存储在 Amazon S3 存储桶中。数据需要加密，并且必须复制到不同的 AWS 区域。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 S3 存储桶。将数据加载到新的 S3 存储桶中。使用 S3 跨区域复制 (CRR) 将加密对象复制到另一个区域的 S3 存储桶。使用 AWS KMS 多区域密钥 (SSE-KMS) 进行服务器端加密。使用 Amazon Athena 查询数据。",
      "B": "创建一个新的 S3 存储桶。将数据加载到新的 S3 存储桶中。使用 S3 跨区域复制 (CRR) 将加密对象复制到另一个区域的 S3 存储桶。使用 AWS KMS 多区域密钥 (SSE-KMS) 进行服务器端加密。使用 Amazon RDS 查询数据。",
      "C": "将数据加载到现有的 S3 存储桶中。使用 S3 跨区域复制 (CRR) 将加密对象复制到另一个区域的 S3 存储桶。使用 Amazon S3 托管加密密钥 (SSE-S3) 进行服务器端加密。使用 Amazon Athena 查询数据。",
      "D": "将数据加载到现有的 S3 存储桶中。使用 S3 跨区域复制 (CRR) 将加密对象复制到另一个区域的 S3 存储桶。使用 Amazon S3 托管加密密钥 (SSE-S3) 进行服务器端加密。使用 Amazon RDS 查询数据。"
    },
    "tags": [
      "S3",
      "Athena",
      "KMS",
      "跨区域复制"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 52%），解析仅供参考。】\n\n考察无服务器解决方案、S3 存储、数据加密和跨区域复制以及 Athena 的应用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 提供了最简洁的解决方案。它使用现有的 S3 存储桶，避免了不必要的存储桶创建。通过 S3 CRR 实现跨区域复制，使用 SSE-S3 进行加密，并用 Amazon Athena 查询数据，符合题目的无服务器和 SQL 分析需求，运营开销最低。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误在于需要创建一个新的 S3 存储桶，增加了不必要的步骤。选项 B 错误在于使用了 RDS 查询数据，这与无服务器需求相悖。选项 D 同样错误，因为使用 RDS 并不符合无服务器方案，且需要更多的管理成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "Amazon Athena",
      "S3 CRR",
      "SSE-KMS",
      "AWS KMS",
      "SSE-S3",
      "Amazon RDS"
    ]
  },
  {
    "id": 135,
    "topic": "1",
    "question_en": "A company runs workloads on AWS. The company needs to connect to a service from an external provider. The service is hosted in the provider's VPC. According to the company’s security team, the connectivity must be private and must be restricted to the target service. The connection must be initiated only from the company’s VPC. Which solution will mast these requirements?",
    "options_en": {
      "A": "Create a VPC peering connection between the company's VPC and the provider's VPC. Update the route table to connect to the target service.",
      "B": "Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to the target service.",
      "C": "Create a NAT gateway in a public subnet of the company’s VPUpdate the route table to connect to the target service.",
      "D": "Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to the target service."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行工作负载。该公司需要从外部提供商连接到一项服务。该服务托管在提供商的 VPC 中。根据公司的安全团队的要求，连接必须是私有的，并且必须限制为目标服务。连接必须仅从公司的 VPC 启动。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在公司 VPC 和提供商的 VPC 之间创建 VPC 对等连接。更新路由表以连接到目标服务。",
      "B": "要求提供商在其 VPC 中创建一个虚拟专用网关。使用 AWS PrivateLink 连接到目标服务。",
      "C": "在公司 VPC 的一个公共子网中创建 NAT 网关。更新路由表以连接到目标服务。",
      "D": "要求提供商为目标服务创建 VPC 终端节点。使用 AWS PrivateLink 连接到目标服务。"
    },
    "tags": [
      "VPC",
      "PrivateLink"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n为了确保私有连接，并限制对目标服务的访问，可以使用 AWS PrivateLink。PrivateLink 提供了在不同 VPC 之间建立私有连接的方式，通过 VPC 终端节点实现。",
      "why_correct": "选项 D 要求提供商为目标服务创建 VPC 终端节点。使用 AWS PrivateLink 连接到目标服务，这提供了私有连接，并且限制了访问。",
      "why_wrong": "选项 A 错误，创建 VPC 对等连接，可能无法满足限制访问目标服务的要求。 选项 B 错误，虚拟专用网关与 PrivateLink 不匹配。 选项 C 错误，NAT 网关只能用于从私有子网访问互联网，不能用于连接到提供商的 VPC 中的服务。"
    },
    "related_terms": [
      "VPC",
      "AWS PrivateLink",
      "VPC 对等连接",
      "虚拟专用网关",
      "NAT 网关",
      "VPC 终端节点"
    ]
  },
  {
    "id": 136,
    "topic": "1",
    "question_en": "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The on-premises database must remain online and accessible during the migration. The Aurora database must remain synchronized with the on-premises database. Which combination of actions must a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an ongoing replication task.",
      "B": "Create a database backup of the on-premises database.",
      "C": "Create an AWS Database Migration Service (AWS DMS) replication server.",
      "D": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT)",
      "E": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database synchronization."
    },
    "correct_answer": "AC",
    "vote_percentage": "91%",
    "question_cn": "一家公司正在将其本地 PostgreSQL 数据库迁移到 Amazon Aurora PostgreSQL。在迁移期间，本地数据库必须保持在线状态并可访问。Aurora 数据库必须与本地数据库保持同步。解决方案架构师必须采取哪些组合操作来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "创建一个持续的复制任务。",
      "B": "创建本地数据库的数据库备份。",
      "C": "创建 AWS Database Migration Service (AWS DMS) 复制服务器。",
      "D": "使用 AWS Schema Conversion Tool (AWS SCT) 转换数据库模式。",
      "E": "创建 Amazon EventBridge (Amazon CloudWatch Events) 规则来监控数据库同步。"
    },
    "tags": [
      "AWS DMS",
      "Aurora PostgreSQL",
      "PostgreSQL",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 91%），解析仅供参考。】\n\n考查使用 AWS Database Migration Service (AWS DMS) 进行数据库在线迁移和同步的配置和操作。",
      "why_correct": "选项 A 和 C 共同构成 AWS DMS 迁移方案的核心。创建一个持续的复制任务 (Replication Task) 用于持续将本地数据库的更改复制到 Aurora 数据库，从而保证同步。创建 AWS DMS 复制服务器 (Replication Server) 是运行复制任务的基础设施，负责执行数据复制和转换。",
      "why_wrong": "选项 B 涉及数据库备份，主要用于数据恢复，不能用于持续同步。选项 D 使用 AWS Schema Conversion Tool (AWS SCT) 进行数据库模式转换，这通常是迁移的准备阶段，但并不能保证持续的数据同步。选项 E 使用 Amazon EventBridge 监控数据库同步，这只是一个监控手段，并非执行数据复制的核心步骤。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL",
      "PostgreSQL",
      "AWS Database Migration Service (AWS DMS)",
      "Replication Task",
      "Replication Server",
      "AWS Schema Conversion Tool (AWS SCT)",
      "Amazon EventBridge",
      "Amazon CloudWatch Events"
    ]
  },
  {
    "id": 137,
    "topic": "1",
    "question_en": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the company’s email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.",
      "B": "Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.",
      "C": "Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.",
      "D": "Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司使用 AWS Organizations 为每个业务部门创建专用的 AWS 账户，以根据请求独立管理每个业务部门的账户。根电子邮件收件人错过了发送到其中一个账户的根用户电子邮件地址的通知。该公司希望确保将来不会错过所有通知。未来的通知必须仅限于账户管理员。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将公司的电子邮件服务器配置为将发送到 AWS 账户根用户电子邮件地址的通知电子邮件消息转发给组织中的所有用户。",
      "B": "将所有 AWS 账户根用户电子邮件地址配置为发送给可以响应警报的少数管理员的通讯组列表。在 AWS Organizations 控制台或通过编程方式配置 AWS 账户备用联系人。",
      "C": "配置所有 AWS 账户根用户电子邮件消息以发送给负责监控警报并将这些警报转发给相应组的一个管理员。",
      "D": "配置所有现有 AWS 账户和所有新创建的账户以使用相同的根用户电子邮件地址。在 AWS Organizations 控制台或通过编程方式配置 AWS 账户备用联系人。"
    },
    "tags": [
      "AWS Organizations",
      "IAM",
      "email"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n考查如何配置 AWS 账户的通知，确保管理员收到重要通知，并排除根用户邮箱可能错失通知的情况。",
      "why_correct": "选项 B 提供了最可靠的解决方案。通过使用通讯组列表，确保管理员能够收到通知。同时，配置 AWS 账户备用联系人可以作为额外的保障。这种方法能够解决根用户邮箱可能被忽略的问题，并将通知导向能够及时响应的管理员。",
      "why_wrong": "选项 A 错误，因为将通知转发给组织中的所有用户效率低下，且难以管理。选项 C 错误，因为依赖单个管理员手动转发警报，存在单点故障风险。选项 D 错误，因为使用相同的根用户电子邮件地址会降低安全性，且无法满足仅限于账户管理员的要求，且不解决原题根邮箱错失通知的问题。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS account",
      "root user",
      "email address",
      "alert",
      "AWS Organizations console",
      "alternate contact"
    ]
  },
  {
    "id": 138,
    "topic": "1",
    "question_en": "A company runs its ecommerce application on AWS. Every new order is published as a massage in a RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These messages are processed by a different application that runs on a separate EC2 instance. This application stores the details in a PostgreSQL database on another EC2 instance. All the EC2 instances are in the same Availability Zone. The company needs to redesign its architecture to provide the highest availability with the least operational overhead. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database.",
      "B": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. Create a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.",
      "C": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the database to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.",
      "D": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create another Multi-AZ Auto Scaling group for EC2 instances that host the application. Create a third Multi-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行其电子商务应用程序。每个新订单都会作为消息发布到在单个可用区中的 Amazon EC2 实例上运行的 RabbitMQ 队列中。这些消息由在另一个 EC2 实例上运行的不同应用程序处理。此应用程序将详细信息存储在另一个 EC2 实例上的 PostgreSQL 数据库中。所有 EC2 实例都在同一可用区中。该公司需要重新设计其架构，以提供最高的可用性，同时将运营开销降至最低。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将队列迁移到 Amazon MQ 上的一对冗余（活动/备用）RabbitMQ 实例。为托管应用程序的 EC2 实例创建一个多可用区 Auto Scaling 组。为托管 PostgreSQL 数据库的 EC2 实例创建另一个多可用区 Auto Scaling 组。",
      "B": "将队列迁移到 Amazon MQ 上的一对冗余（活动/备用）RabbitMQ 实例。为托管应用程序的 EC2 实例创建一个多可用区 Auto Scaling 组。将数据库迁移到在 PostgreSQL 的 Amazon RDS 的多可用区部署上运行。",
      "C": "为托管 RabbitMQ 队列的 EC2 实例创建一个多可用区 Auto Scaling 组。为托管应用程序的 EC2 实例创建另一个多可用区 Auto Scaling 组。将数据库迁移到在 PostgreSQL 的 Amazon RDS 的多可用区部署上运行。",
      "D": "为托管 RabbitMQ 队列的 EC2 实例创建一个多可用区 Auto Scaling 组。为托管应用程序的 EC2 实例创建另一个多可用区 Auto Scaling 组。为托管 PostgreSQL 数据库的 EC2 实例创建第三个多可用区 Auto Scaling 组"
    },
    "tags": [
      "Amazon MQ",
      "EC2",
      "Auto Scaling",
      "RDS",
      "PostgreSQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查高可用性架构设计。为了实现高可用性和降低运营开销，需要采用托管服务。RabbitMQ 队列可以迁移到 Amazon MQ，数据库使用 RDS 多可用区部署，应用服务器使用 Auto Scaling。",
      "why_correct": "B 选项正确，采用 Amazon MQ 提供 RabbitMQ 的 HA，使用 RDS 多可用区部署提升数据库 HA，并用 Auto Scaling 实现应用服务器的弹性伸缩，满足了高可用性和低运维需求。",
      "why_wrong": "A 选项虽然也使用 Amazon MQ 和 Auto Scaling，但未考虑 RDS 多可用区部署，数据库仍存在单点故障风险。C 和 D 选项没有使用 Amazon MQ，RabbitMQ 队列的 HA 无法保障。"
    },
    "related_terms": [
      "Amazon MQ",
      "EC2",
      "Auto Scaling",
      "RDS",
      "PostgreSQL",
      "RabbitMQ"
    ]
  },
  {
    "id": 139,
    "topic": "1",
    "question_en": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually reviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the same time to use with Amazon QuickSight. Additional teams are starting to send more files in larger sizes to the initial S3 bucket. The reporting team wants to move the files automatically analysis S3 bucket as the files enter the initial S3 bucket. The reporting team also wants to use AWS Lambda functions to run pattern-matching code on the copied data. In addition, the reporting team wants to send the data files to a pipeline in Amazon SageMaker Pipelines. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
      "B": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule.",
      "C": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. Configure s3:ObjectCreated:Put as the event type.",
      "D": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the rule."
    },
    "correct_answer": "D",
    "vote_percentage": "78%",
    "question_cn": "一个报告团队每天都会在一个 Amazon S3 存储桶中收到文件。报告团队每天都会在同一时间手动审查并将文件从这个初始 S3 存储桶复制到分析 S3 存储桶，以便与 Amazon QuickSight 一起使用。其他团队开始将更多更大体积的文件发送到初始 S3 存储桶。报告团队希望在文件进入初始 S3 存储桶时，自动将文件移动到分析 S3 存储桶。报告团队还希望使用 AWS Lambda 函数对复制的数据运行模式匹配代码。此外，报告团队希望将数据文件发送到 Amazon SageMaker Pipelines 中的管道。解决方案架构师应该怎么做，以最少的运营开销来满足这些要求？",
    "options_cn": {
      "A": "创建一个 Lambda 函数将文件复制到分析 S3 存储桶。为分析 S3 存储桶创建 S3 事件通知。将 Lambda 和 SageMaker Pipelines 配置为事件通知的目的地。将 s3:ObjectCreated:Put 配置为事件类型。",
      "B": "创建一个 Lambda 函数将文件复制到分析 S3 存储桶。将分析 S3 存储桶配置为将事件通知发送到 Amazon EventBridge (Amazon CloudWatch Events)。在 EventBridge (CloudWatch Events) 中配置一个 ObjectCreated 规则。将 Lambda 和 SageMaker Pipelines 配置为该规则的目标。",
      "C": "在 S3 存储桶之间配置 S3 复制。为分析 S3 存储桶创建 S3 事件通知。将 Lambda 和 SageMaker Pipelines 配置为事件通知的目的地。将 s3:ObjectCreated:Put 配置为事件类型。",
      "D": "在 S3 存储桶之间配置 S3 复制。将分析 S3 存储桶配置为将事件通知发送到 Amazon EventBridge (Amazon CloudWatch Events)。在 EventBridge (CloudWatch Events) 中配置一个 ObjectCreated 规则。将 Lambda 和 SageMaker Pipelines 配置为该规则的目标。"
    },
    "tags": [
      "S3",
      "Lambda",
      "EventBridge",
      "SageMaker Pipelines"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 78%），解析仅供参考。】\n\n考察如何使用 S3 复制、S3 事件通知、AWS Lambda 和 Amazon SageMaker Pipelines，以实现自动将文件从一个 S3 存储桶复制到另一个存储桶，并在复制后触发 Lambda 函数和 SageMaker Pipelines。",
      "why_correct": "选项 D 提供了最合适的解决方案。它利用 S3 复制将文件自动复制到分析 S3 存储桶，减少手动操作。然后，它使用 Amazon EventBridge（CloudWatch Events）进行事件驱动，当对象创建时，触发 Lambda 函数执行数据处理，并将数据传递给 SageMaker Pipelines。这种方法自动化程度高，运营开销低。",
      "why_wrong": "选项 A 错误在于，它将事件通知直接配置到分析 S3 存储桶，而 S3 事件通知本身没有路由机制。选项 B 的问题是，它让 Lambda 函数直接复制文件，且没有利用 S3 复制的特性。选项 C 错误在于，S3 复制虽然可以复制文件，但触发 Lambda 和 SageMaker Pipelines 的机制不够灵活，难以实现需求。选项 A 和 C 都没有利用 EventBridge 的强大功能来进行事件路由和处理，从而增加了管理的复杂性。这些选项都需要手动配置和维护，不符合最小化运营开销的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS Lambda",
      "Amazon QuickSight",
      "Amazon SageMaker Pipelines",
      "Amazon EventBridge",
      "CloudWatch Events",
      "s3:ObjectCreated:Put",
      "S3 event notifications",
      "S3 replication"
    ]
  },
  {
    "id": 140,
    "topic": "1",
    "question_en": "A solutions architect needs to help a company optimize the cost of running an application on AWS. The application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within the architecture. The EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic and unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The application front end will run on Fargate, and Lambda will serve the API layer. The front-end utilization and API layer utilization will be predictable over the course of the next year. Which combination of purchasing options will provide the MOST cost-effective solution for hosting this application? (Choose two.)",
    "options_en": {
      "A": "Use Spot Instances for the data ingestion layer",
      "B": "Use On-Demand Instances for the data ingestion layer",
      "C": "Purchase a 1-year Compute Savings Plan for the front end and API layer.",
      "D": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer",
      "E": "Purchase a 1-year EC2 instance Savings Plan for the front end and API layer."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要帮助一家公司优化在 AWS 上运行应用程序的成本。该应用程序将在其架构中使用 Amazon EC2 实例、AWS Fargate 和 AWS Lambda 进行计算。 EC2 实例将运行应用程序的数据摄取层。 EC2 的使用将是零星且不可预测的。在 EC2 实例上运行的工作负载可以随时中断。应用程序前端将在 Fargate 上运行，而 Lambda 将服务于 API 层。 在接下来的一年里，前端利用率和 API 层利用率将是可预测的。哪种购买选项的组合将为托管此应用程序提供最具成本效益的解决方案？（选择两个。）",
    "options_cn": {
      "A": "对数据摄取层使用 Spot 实例",
      "B": "对数据摄取层使用按需实例",
      "C": "为前端和 API 层购买 1 年期计算节省计划。",
      "D": "为数据摄取层购买 1 年期预留实例（全部预付）。",
      "E": "为前端和 API 层购买 1 年期 EC2 实例节省计划。"
    },
    "tags": [
      "EC2",
      "Fargate",
      "Lambda",
      "Spot Instances",
      "Compute Savings Plans"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n本题考查成本优化。对于零星的、可中断的 EC2 工作负载，使用 Spot 实例最具成本效益。对于前端和 API 层利用率可预测的场景，选择计算节省计划。",
      "why_correct": "A 选项正确，对于数据摄取层使用 Spot 实例可以最大程度降低成本。C 选项正确，使用计算节省计划。 ",
      "why_wrong": "B 选项，按需实例成本高于 Spot 实例。D 选项，预留实例不适用于零星使用场景。E 选项，EC2 实例节省计划与 Fargate 和 Lambda 不兼容。"
    },
    "related_terms": [
      "EC2",
      "Fargate",
      "Lambda",
      "Compute Savings Plans",
      "按需实例",
      "预留实例",
      "Spot Instances"
    ]
  },
  {
    "id": 141,
    "topic": "1",
    "question_en": "A company runs a web-based portal that provides users with global breaking news, local alerts, and weather updates. The portal delivers each user a personalized view by using mixture of static and dynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 instance behind an Application Load Balancer (ALB). The company wants the portal to provide this content to its users across the world as quickly as possible. How should a solutions architect design the application to ensure the LEAST amount of latency for all users?",
    "options_en": {
      "A": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and dynamic content by specifying the ALB as an origin.",
      "B": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy to serve all content from the ALB in the closest Region.",
      "C": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static content. Serve the dynamic content directly from the ALB.",
      "D": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing policy to serve all content from the ALB in the closest Region."
    },
    "correct_answer": "A",
    "vote_percentage": "70%",
    "question_cn": "一家公司运营一个基于网络的门户网站，为用户提供全球突发新闻、本地警报和天气更新。该门户网站通过使用静态和动态内容的混合为每个用户提供个性化视图。内容通过 HTTPS 在运行在 Application Load Balancer (ALB) 后的 Amazon EC2 实例上的 API 服务器上提供。该公司希望该门户网站能够尽快向全球用户提供此内容。解决方案架构师应如何设计应用程序，以确保所有用户的延迟时间最短？",
    "options_cn": {
      "A": "将应用程序堆栈部署在单个 AWS 区域中。使用 Amazon CloudFront 通过指定 ALB 作为源来提供所有静态和动态内容。",
      "B": "将应用程序堆栈部署在两个 AWS 区域中。使用 Amazon Route 53 延迟路由策略从最近的区域中的 ALB 提供所有内容。",
      "C": "将应用程序堆栈部署在单个 AWS 区域中。使用 Amazon CloudFront 提供静态内容。直接从 ALB 提供动态内容。",
      "D": "将应用程序堆栈部署在两个 AWS 区域中。使用 Amazon Route 53 地理位置路由策略从最近的区域中的 ALB 提供所有内容。"
    },
    "tags": [
      "CloudFront",
      "ALB",
      "Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 70%），解析仅供参考。】\n\n考察使用 CloudFront 优化全球用户延迟的解决方案。关键在于理解 CloudFront 的内容分发网络（CDN）功能以及 ALB 的作用。",
      "why_correct": "选项 A 采用了 CloudFront 作为 CDN，将静态和动态内容缓存在全球边缘站点。通过 CloudFront，用户可以从离他们最近的边缘站点获取内容，从而降低延迟。ALB 作为源，CloudFront 可以从 ALB 缓存的内容，并根据需要从 ALB 获取最新的动态内容。",
      "why_wrong": "选项 B 涉及多个区域，但延迟路由策略可能无法有效降低所有用户的延迟，因为并非所有用户都能始终访问到最近的区域。选项 C 仅使用 CloudFront 缓存静态内容，动态内容直接从 ALB 提供，这无法充分利用 CloudFront 的 CDN 功能。选项 D 使用地理位置路由策略，但仍然涉及多个区域，并且无法有效解决全球用户的延迟问题。地理位置路由不如 CloudFront 的 CDN 功能能够直接降低延迟。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "HTTPS",
      "Amazon Route 53",
      "CDN",
      "AWS Regions",
      "Geographic Location Routing",
      "Latency Routing"
    ]
  },
  {
    "id": 142,
    "topic": "1",
    "question_en": "A gaming company is designing a highly available architecture. The application runs on a modified Linux kernel and supports only UDP-based trafic. The company needs the front-end tier to provide the best possible user experience. That tier must have low latency, route trafic to the nearest edge location, and provide static IP addresses for entry into the application endpoints. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda for the application in AWS Application Auto Scaling.",
      "B": "Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda for the application in an AWS Application Auto Scaling group.",
      "C": "Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group.",
      "D": "Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon EC2 instances for the application in an EC2 Auto Scaling group."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家游戏公司正在设计一个高可用性架构。该应用程序运行在经过修改的 Linux 内核上，并且仅支持基于 UDP 的流量。该公司需要前端层提供最佳的用户体验。该层必须具有低延迟，将流量路由到最近的边缘位置，并为进入应用程序终端节点的入口提供静态 IP 地址。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Route 53 以将请求转发到 Application Load Balancer。在 AWS Application Auto Scaling 中使用 AWS Lambda 作为应用程序。",
      "B": "配置 Amazon CloudFront 以将请求转发到 Network Load Balancer。在 AWS Application Auto Scaling 组中使用 AWS Lambda 作为应用程序。",
      "C": "配置 AWS Global Accelerator 以将请求转发到 Network Load Balancer。在 EC2 Auto Scaling 组中使用 Amazon EC2 实例作为应用程序。",
      "D": "配置 Amazon API Gateway 以将请求转发到 Application Load Balancer。在 EC2 Auto Scaling 组中使用 Amazon EC2 实例作为应用程序。"
    },
    "tags": [
      "CloudFront",
      "NLB",
      "Global Accelerator",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察高可用性架构和低延迟优化。对于基于 UDP 的应用，Global Accelerator 提供了静态 IP 地址，同时结合 NLB 和 EC2 Auto Scaling 实现高可用性和低延迟。",
      "why_correct": "C 选项正确，Global Accelerator 提供静态 IP 和就近接入，NLB 提供负载均衡，EC2 Auto Scaling 保证了高可用性。",
      "why_wrong": "A 选项，Application Load Balancer 不支持 UDP，Lambda 不适用于这种流量。B 选项，CloudFront 和 Lambda 无法满足低延迟需求。D 选项，API Gateway 不支持 UDP，ALB 也不支持 UDP。"
    },
    "related_terms": [
      "CloudFront",
      "NLB",
      "Global Accelerator",
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "API Gateway"
    ]
  },
  {
    "id": 143,
    "topic": "1",
    "question_en": "A company wants to migrate its existing on-premises monolithic application to AWS. The company wants to keep as much of the front-end code and the backend code as possible. However, the company wants to break the application into smaller applications. A different team will manage each application. The company needs a highly scalable solution that minimizes operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.",
      "B": "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that is integrated with AWS Lambda.",
      "C": "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 instances in an Auto Scaling group as targets.",
      "D": "Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application Load Balancer with Amazon ECS as the target."
    },
    "correct_answer": "D",
    "vote_percentage": "74%",
    "question_cn": "一家公司希望将其现有的本地单体应用程序迁移到 AWS。该公司希望尽可能保留前端代码和后端代码。但是，该公司希望将应用程序分解成更小的应用程序。不同的团队将管理每个应用程序。该公司需要一个高度可扩展的解决方案，以最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Lambda 上托管应用程序。将应用程序与 Amazon API Gateway 集成。",
      "B": "使用 AWS Amplify 托管应用程序。将应用程序连接到与 AWS Lambda 集成的 Amazon API Gateway API。",
      "C": "在 Amazon EC2 实例上托管应用程序。设置一个 Application Load Balancer，并将 Auto Scaling 组中的 EC2 实例作为目标。",
      "D": "在 Amazon Elastic Container Service (Amazon ECS) 上托管应用程序。设置一个 Application Load Balancer，并将 Amazon ECS 作为目标。"
    },
    "tags": [
      "EC2",
      "ECS",
      "Application Load Balancer",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 74%），解析仅供参考。】\n\n此题考查应用程序的容器化和部署。 为了降低运维开销并保持代码复用，采用容器化方案。ECS 配合 ALB 可以简化部署和管理。",
      "why_correct": "D 选项正确，在 ECS 上托管应用程序，使用 ALB 提供负载均衡，简化运维并支持容器化部署。",
      "why_wrong": "A 选项， Lambda 不适合现有应用程序。B 选项，Amplify 适用于前端托管，不适用于复杂应用程序。C 选项，EC2 实例和 ALB 的组合不如 ECS 灵活。"
    },
    "related_terms": [
      "EC2",
      "ECS",
      "Application Load Balancer",
      "Lambda",
      "Amplify"
    ]
  },
  {
    "id": 144,
    "topic": "1",
    "question_en": "A company recently started using Amazon Aurora as the data store for its global ecommerce application. When large reports are run, developers report that the ecommerce application is performing poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the ReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run. What is the MOST cost-effective solution?",
    "options_en": {
      "A": "Migrate the monthly reporting to Amazon Redshift.",
      "B": "Migrate the monthly reporting to an Aurora Replica.",
      "C": "Migrate the Aurora database to a larger instance class.",
      "D": "Increase the Provisioned IOPS on the Aurora instance."
    },
    "correct_answer": "B",
    "vote_percentage": "98%",
    "question_cn": "一家公司最近开始使用 Amazon Aurora 作为其全球电子商务应用程序的数据存储。 当运行大型报告时，开发人员报告说电子商务应用程序的性能很差。 在审查 Amazon CloudWatch 中的指标后，解决方案架构师发现，在每月报告运行时，ReadIOPS 和 CPUUtilizalion 指标会飙升。 哪种解决方案最具成本效益？",
    "options_cn": {
      "A": "将每月报告迁移到 Amazon Redshift。",
      "B": "将每月报告迁移到 Aurora 副本。",
      "C": "将 Aurora 数据库迁移到更大的实例类型。",
      "D": "增加 Aurora 实例上的预置 IOPS。"
    },
    "tags": [
      "Aurora",
      "ReadIOPS",
      "CPUUtilization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 98%），解析仅供参考。】\n\n此题考察数据库性能优化。  当 Aurora 数据库的 CPU 和 ReadIOPS 飙升时，通常是由于读取负载过高。通过 Aurora 副本可以实现读取负载分担。",
      "why_correct": "B 选项正确，将每月报告迁移到 Aurora 副本，可以分担主数据库的读取负载，提高性能。",
      "why_wrong": "A 选项，迁移到 Redshift 成本更高，且不适合这种场景。C 选项，增大实例规格不能根本解决读取负载问题。D 选项，增加 IOPS 成本高，且不能根本解决问题。"
    },
    "related_terms": [
      "Aurora",
      "CPUUtilization",
      "ReadIOPS"
    ]
  },
  {
    "id": 145,
    "topic": "1",
    "question_en": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics software is written in PHP and uses a MySQL database. The analytics software, the web server that provides PHP, and the database server are all hosted on the EC2 instance. The application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load to each EC2 instance.",
      "B": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
      "C": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization surpasses 75%.",
      "D": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web application. Apply the AMI to a launch template. Create an Auto Scaling group with the launch template Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group."
    },
    "correct_answer": "D",
    "vote_percentage": "75%",
    "question_cn": "一家公司在单个 Amazon EC2 按需实例上托管一个网站分析应用程序。分析软件用 PHP 编写，并使用 MySQL 数据库。分析软件、提供 PHP 的 Web 服务器以及数据库服务器都托管在 EC2 实例上。该应用程序在繁忙时段表现出性能下降的迹象，并出现 5xx 错误。该公司需要使应用程序无缝扩展。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon RDS for MySQL 数据库实例。创建 Web 应用程序的 AMI。使用该 AMI 启动第二个 EC2 按需实例。使用 Application Load Balancer 将负载分配到每个 EC2 实例。",
      "B": "将数据库迁移到 Amazon RDS for MySQL 数据库实例。创建 Web 应用程序的 AMI。使用该 AMI 启动第二个 EC2 按需实例。使用 Amazon Route 53 加权路由将负载分配到两个 EC2 实例。",
      "C": "将数据库迁移到 Amazon Aurora MySQL 数据库实例。创建一个 AWS Lambda 函数来停止 EC2 实例并更改实例类型。创建一个 Amazon CloudWatch 警报，以便在 CPU 利用率超过 75% 时调用 Lambda 函数。",
      "D": "将数据库迁移到 Amazon Aurora MySQL 数据库实例。创建 Web 应用程序的 AMI。将 AMI 应用于启动模板。使用启动模板创建一个 Auto Scaling 组。配置启动模板以使用 Spot Fleet。将 Application Load Balancer 附加到 Auto Scaling 组。"
    },
    "tags": [
      "EC2",
      "MySQL",
      "RDS",
      "Aurora",
      "Auto Scaling",
      "Spot Fleet",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 75%），解析仅供参考。】\n\n此题考察应用程序的水平扩展和成本优化。  将数据库迁移到 Aurora MySQL，利用 Auto Scaling 和 Spot Fleet 优化成本和扩展性，并使用 ALB 进行负载均衡。",
      "why_correct": "D 选项正确，将数据库迁移到 Aurora MySQL，使用 AMI、启动模板、Auto Scaling 组和 Spot Fleet 来实现自动扩展和成本优化。 ALB 用于负载分发。",
      "why_wrong": "A 选项，仅使用按需实例，无法自动扩展。B 选项，使用 Route 53 加权路由，扩展性不如 ALB。C 选项，Lambda 无法停止和更改实例类型，不适用。"
    },
    "related_terms": [
      "EC2",
      "MySQL",
      "RDS",
      "Aurora",
      "Auto Scaling",
      "Application Load Balancer",
      "AMI",
      "Spot Fleet"
    ]
  },
  {
    "id": 146,
    "topic": "1",
    "question_en": "A company runs a stateless web application in production on a group of Amazon EC2 On-Demand Instances behind an Application Load Balancer. The application experiences heavy usage during an 8-hour period each business day. Application usage is moderate and steady overnight. Application usage is low during weekends. The company wants to minimize its EC2 costs without affecting the availability of the application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Spot Instances for the entire workload.",
      "B": "Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional capacity that the application needs.",
      "C": "Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional capacity that the application needs.",
      "D": "Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any additional capacity that the application needs."
    },
    "correct_answer": "B",
    "vote_percentage": "64%",
    "question_cn": "一家公司在 Application Load Balancer 后的一组 Amazon EC2 按需实例上运行生产中的无状态 Web 应用程序。该应用程序在每个工作日的 8 小时内经历高负载。应用程序在夜间的负载适中且稳定。应用程序在周末的负载较低。该公司希望在不影响应用程序可用性的情况下，最大限度地降低其 EC2 成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "对整个工作负载使用 Spot 实例。",
      "B": "对基线使用量使用预留实例。对应用程序需要的任何额外容量使用 Spot 实例。",
      "C": "对基线使用量使用按需实例。对应用程序需要的任何额外容量使用 Spot 实例。",
      "D": "对基线使用量使用专用实例。对应用程序需要的任何额外容量使用按需实例。"
    },
    "tags": [
      "EC2",
      "Spot Instances",
      "Reserved Instances",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 64%），解析仅供参考。】\n\n本题考查成本优化。  对于有基线需求和突发需求的应用，结合预留实例和 Spot 实例是最佳选择。",
      "why_correct": "B 选项正确，对基线使用量使用预留实例以降低成本，对突发容量使用 Spot 实例，实现成本效益。",
      "why_wrong": "A 选项，仅使用 Spot 实例可能导致可用性问题。C 选项，仅使用按需实例成本较高。D 选项，专用实例成本最高。"
    },
    "related_terms": [
      "EC2",
      "Application Load Balancer",
      "Spot Instances",
      "Reserved Instances"
    ]
  },
  {
    "id": 147,
    "topic": "1",
    "question_en": "A company needs to retain application log files for a critical application for 10 years. The application team regularly accesses logs from the past month for troubleshooting, but logs older than 1 month are rarely accessed. The application generates more than 10 TB of logs per month. Which storage option meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
      "B": "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive.",
      "C": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old to S3 Glacier Deep Archive.",
      "D": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more than 1 month old to S3 Glacier Deep Archive."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要保留一个关键应用程序的应用程序日志文件 10 年。应用程序团队会定期访问上个月的日志以进行故障排除，但很少访问超过 1 个月的日志。该应用程序每月生成超过 10 TB 的日志。哪个存储选项最经济高效地满足这些要求？",
    "options_cn": {
      "A": "将日志存储在 Amazon S3 中。使用 AWS Backup 将超过 1 个月的日志移动到 S3 Glacier Deep Archive。",
      "B": "将日志存储在 Amazon S3 中。使用 S3 生命周期策略将超过 1 个月的日志移动到 S3 Glacier Deep Archive。",
      "C": "将日志存储在 Amazon CloudWatch Logs 中。使用 AWS Backup 将超过 1 个月的日志移动到 S3 Glacier Deep Archive。",
      "D": "将日志存储在 Amazon CloudWatch Logs 中。使用 Amazon S3 生命周期策略将超过 1 个月的日志移动到 S3 Glacier Deep Archive。"
    },
    "tags": [
      "S3",
      "S3 Glacier Deep Archive",
      "Lifecycle policy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查存储成本优化。  S3 Glacier Deep Archive 提供了最低的存储成本，适用于长期归档。S3 生命周期策略可以自动将旧日志迁移到 Deep Archive。",
      "why_correct": "B 选项正确，使用 S3 生命周期策略将超过 1 个月的日志移动到 S3 Glacier Deep Archive，实现成本优化。",
      "why_wrong": "A 选项，AWS Backup 不适用于此场景。C 选项，CloudWatch Logs 不适合存储 10 年日志。D 选项，CloudWatch Logs 不适合存储 10 年日志，并且其没有生命周期策略。"
    },
    "related_terms": [
      "S3",
      "S3 Glacier Deep Archive",
      "CloudWatch Logs",
      "AWS Backup",
      "Lifecycle policy"
    ]
  },
  {
    "id": 148,
    "topic": "1",
    "question_en": "A company has a data ingestion workfiow that includes the following components: An Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new data deliveries An AWS Lambda function that processes and stores the data The ingestion workfiow occasionally fails because of network connectivity issues. When failure occurs, the corresponding data is not ingested unless the company manually reruns the job. What should a solutions architect do to ensure that all notifications are eventually processed?",
    "options_en": {
      "A": "Configure the Lambda function for deployment across multiple Availability Zones.",
      "B": "Modify the Lambda function's configuration to increase the CPU and memory allocations for the function.",
      "C": "Configure the SNS topic’s retry strategy to increase both the number of retries and the wait time between retries.",
      "D": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. Modify the Lambda function to process messages in the queue."
    },
    "correct_answer": "D",
    "vote_percentage": "88%",
    "question_cn": "一家公司有一个数据摄取工作流程，其中包括以下组件：一个 Amazon Simple Notification Service (Amazon SNS) 主题，用于接收关于新数据交付的通知；一个 AWS Lambda 函数，用于处理和存储数据。摄取工作流程偶尔会因为网络连接问题而失败。当失败发生时，除非公司手动重新运行作业，否则相应的数据不会被摄取。解决方案架构师应该怎么做才能确保所有通知最终都被处理？",
    "options_cn": {
      "A": "为 Lambda 函数配置跨多个可用区的部署。",
      "B": "修改 Lambda 函数的配置以增加该函数的 CPU 和内存分配。",
      "C": "配置 SNS 主题的重试策略，以增加重试次数和重试之间的等待时间。",
      "D": "将一个 Amazon Simple Queue Service (Amazon SQS) 队列配置为失败目标。修改 Lambda 函数以处理队列中的消息。"
    },
    "tags": [
      "SNS",
      "Lambda",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 88%），解析仅供参考。】\n\n此题考查消息处理的可靠性。  为了确保所有消息都被处理，需要考虑容错和重试机制。  使用 SQS 作为失败目标可以实现消息的持久化，从而保证最终一致性。",
      "why_correct": "D 选项正确，将 SQS 队列配置为失败目标，Lambda 函数从 SQS 队列中读取消息，确保消息处理的可靠性。",
      "why_wrong": "A 选项，跨可用区部署不能解决网络连接问题。B 选项，增加 Lambda 资源不能解决根本问题。C 选项，重试策略有限制，不能保证所有消息都被处理。"
    },
    "related_terms": [
      "SNS",
      "Lambda",
      "SQS"
    ]
  },
  {
    "id": 149,
    "topic": "1",
    "question_en": "A company has a service that produces event data. The company wants to use AWS to process the event data as it is received. The data is written in a specific order that must be maintained throughout processing. The company wants to implement a solution that minimizes operational overhead. How should a solutions architect accomplish this?",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an AWS Lambda function to process messages from the queue.",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an AWS Lambda function as a subscriber.",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up an AWS Lambda function to process messages from the queue independently.",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing payloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a subscriber."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一项生成事件数据的服务。该公司希望使用 AWS 在接收到事件数据时对其进行处理。数据的写入顺序必须在整个处理过程中保持不变。该公司希望实施一个最大限度地减少运营开销的解决方案。解决方案架构师应如何实现此目的？",
    "options_cn": {
      "A": "创建一个 Amazon Simple Queue Service (Amazon SQS) FIFO 队列来保存消息。设置一个 AWS Lambda 函数来处理来自队列的消息。",
      "B": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题，以传递包含要处理的有效载荷的通知。将一个 AWS Lambda 函数配置为订阅者。",
      "C": "创建一个 Amazon Simple Queue Service (Amazon SQS) 标准队列来保存消息。设置一个 AWS Lambda 函数来独立处理来自队列的消息。",
      "D": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题，以传递包含要处理的有效载荷的通知。将一个 Amazon Simple Queue Service (Amazon SQS) 队列配置为订阅者。"
    },
    "tags": [
      "SQS",
      "Lambda",
      "SNS",
      "FIFO"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查消息传递和顺序保证。  为了确保数据处理顺序，使用 SQS FIFO 队列。 FIFO 队列确保消息按照发送的顺序被处理。",
      "why_correct": "A 选项正确，使用 SQS FIFO 队列保证消息的顺序，Lambda 函数处理来自队列的消息。",
      "why_wrong": "B 选项，SNS 不能保证消息顺序。C 选项，SQS 标准队列不能保证消息顺序。D 选项，SNS 不能保证消息顺序，并且没有 SQS 队列。"
    },
    "related_terms": [
      "SQS",
      "Lambda",
      "SNS",
      "FIFO"
    ]
  },
  {
    "id": 150,
    "topic": "1",
    "question_en": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As part of the migration design requirements, a solutions architect must implement infrastructure metric alarms. The company does not need to take action if CPU utilization increases to more than 50% for a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS on the disk are high at the same time, the company needs to act as soon as possible. The solutions architect also must reduce false alarms. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create Amazon CloudWatch composite alarms where possible.",
      "B": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.",
      "C": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.",
      "D": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其应用程序从本地服务器迁移到 Amazon EC2 实例。作为迁移设计要求的一部分，解决方案架构师必须实施基础设施指标警报。如果 CPU 利用率在短时间内增加到 50% 以上，该公司不需要采取行动。但是，如果 CPU 利用率增加到 50% 以上，并且磁盘上的读取 IOPS 很高，则该公司需要尽快采取行动。解决方案架构师还必须减少误报。解决方案架构师应如何做才能满足这些要求？",
    "options_cn": {
      "A": "尽可能创建 Amazon CloudWatch 复合警报。",
      "B": "创建 Amazon CloudWatch 仪表板以可视化指标并快速响应问题。",
      "C": "创建 Amazon CloudWatch Synthetics canaries 来监控应用程序并发出警报。",
      "D": "尽可能创建具有多个指标阈值的单个 Amazon CloudWatch 指标警报。"
    },
    "tags": [
      "CloudWatch",
      "Composite Alarm"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察监控和告警。  为了减少误报，需要结合多个指标来定义告警条件。  复合警报可以基于多个指标的组合来触发告警。",
      "why_correct": "A 选项正确，创建 CloudWatch 复合警报可以基于多个指标的组合触发告警，减少误报。",
      "why_wrong": "B 选项，仪表板用于可视化，不能用于触发告警。C 选项，Synthetics 适用于监控应用程序的可用性和响应时间，不适用于此场景。D 选项，单个指标警报无法满足多个条件的需求。"
    },
    "related_terms": [
      "CloudWatch",
      "Composite Alarm"
    ]
  },
  {
    "id": 151,
    "topic": "1",
    "question_en": "A company wants to migrate its on-premises data center to AWS. According to the company's compliance requirements, the company can use only the ap-northeast-3 Region. Company administrators are not permitted to connect VPCs to the internet. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Control Tower to implement data residency guardrails to deny internet access and deny access to all AWS Regions except ap- northeast-3.",
      "B": "Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except ap-northeast-3 in the AWS account settings.",
      "C": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining internet access. Deny access to all AWS Regions except ap-northeast-3.",
      "D": "Create an outbound rule for the network ACL in each VPC to deny all trafic from 0.0.0.0/0. Create an IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3",
      "E": "Use AWS Config to activate managed rules to detect and alert for internet gateways and to detect and alert for new resources deployed outside of ap-northeast-3."
    },
    "correct_answer": "AC",
    "vote_percentage": "78%",
    "question_cn": "一家公司希望将其本地数据中心迁移到 AWS。根据公司的合规性要求，该公司只能使用 ap-northeast-3 区域。公司管理员不允许将 VPC 连接到互联网。哪些解决方案将满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "使用 AWS Control Tower 实施数据驻留防护栏，以拒绝互联网访问，并拒绝访问除 ap-northeast-3 之外的所有 AWS 区域。",
      "B": "使用 AWS WAF 中的规则来阻止互联网访问。在 AWS 账户设置中拒绝访问除 ap-northeast-3 之外的所有 AWS 区域。",
      "C": "使用 AWS Organizations 配置服务控制策略 (SCPS)，以防止 VPC 获得互联网访问权限。拒绝访问除 ap-northeast-3 之外的所有 AWS 区域。",
      "D": "为每个 VPC 中的网络 ACL 创建一个出站规则，以拒绝来自 0.0.0.0/0 的所有流量。为每个用户创建一个 IAM 策略，以防止使用 ap-northeast-3 之外的任何 AWS 区域。",
      "E": "使用 AWS Config 激活托管规则，以检测并警报互联网网关，并检测并警报在 ap-northeast-3 之外部署的新资源。"
    },
    "tags": [
      "AWS Control Tower",
      "VPC",
      "ap-northeast-3",
      "AWS Organizations",
      "SCPS",
      "IAM",
      "WAF",
      "AWS Config"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 78%），解析仅供参考。】\n\n此题考查如何满足区域合规性和互联网访问限制。关键在于使用 AWS Organizations 和服务控制策略 (SCPS) 来限制账户访问区域，并使用网络控制组件来阻止互联网访问。选项 A 和 C 提供了利用这些组件的正确方法。",
      "why_correct": "AWS Control Tower 可以实施数据驻留防护栏，控制访问 AWS 区域。AWS Organizations 的服务控制策略 (SCPS) 可以拒绝对特定区域的访问。两者都符合要求。",
      "why_wrong": "选项 B 错误，因为 AWS WAF 无法从根本上阻止区域访问。选项 C 错误，因为创建 IAM 策略并非最佳实践。选项 D 错误，因为需要配置 SCPS，而不是 IAM 策略，并且 NACL 无法解决区域访问问题。选项 E 错误，因为 AWS Config 无法阻止访问。"
    },
    "related_terms": [
      "AWS Control Tower",
      "VPC",
      "AWS Organizations",
      "IAM",
      "AWS WAF",
      "AWS Config",
      "ap-northeast-3",
      "SCPS"
    ]
  },
  {
    "id": 152,
    "topic": "1",
    "question_en": "A company uses a three-tier web application to provide training to new employees. The application is accessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB instance to store information and wants to minimize costs. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the policy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance.",
      "B": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the data from the cache when the DB instance is stopped. Invalidate the cache after the DB instance is started.",
      "C": "Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach the role to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the desired schedule.",
      "D": "Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge (Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda functions as event targets for the rules."
    },
    "correct_answer": "D",
    "vote_percentage": "80%",
    "question_cn": "一家公司使用三层 Web 应用程序为新员工提供培训。该应用程序每天仅访问 12 小时。该公司正在使用 Amazon RDS for MySQL DB 实例来存储信息，并希望最大限度地降低成本。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "为 AWS Systems Manager Session Manager 配置一个 IAM 策略。为该策略创建一个 IAM 角色。更新该角色的信任关系。设置 DB 实例的自动启动和停止。",
      "B": "创建一个 Amazon ElastiCache for Redis 缓存集群，使用户能够在 DB 实例停止时从缓存中访问数据。在 DB 实例启动后使缓存失效。",
      "C": "启动一个 Amazon EC2 实例。创建一个 IAM 角色，授予对 Amazon RDS 的访问权限。将该角色附加到 EC2 实例。配置一个 cron 作业，以在所需的时间表上启动和停止 EC2 实例。",
      "D": "创建 AWS Lambda 函数来启动和停止 DB 实例。创建 Amazon EventBridge (Amazon CloudWatch Events) 计划规则来调用 Lambda 函数。将 Lambda 函数配置为规则的事件目标。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "ElastiCache",
      "EC2",
      "Lambda",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 80%），解析仅供参考。】\n\n此题考查如何通过自动化启动和停止数据库实例来降低成本。 核心在于使用事件驱动的架构，利用 Lambda 函数和 EventBridge 来实现自动化。选项 D 提供了最直接有效的解决方案。",
      "why_correct": "使用 AWS Lambda 函数启动和停止 RDS 实例，并使用 EventBridge (CloudWatch Events) 计划规则触发 Lambda 函数，是最经济高效且可自动化的方法。",
      "why_wrong": "选项 A 错误，因为使用 Systems Manager Session Manager 与自动启动和停止数据库实例无关。选项 B 错误，因为 ElastiCache 主要用于缓存，无法解决数据库实例的成本优化问题。选项 C 错误，因为 EC2 实例的启动和停止需要手动配置和管理。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "ElastiCache",
      "EC2",
      "Lambda",
      "EventBridge",
      "CloudWatch"
    ]
  },
  {
    "id": 153,
    "topic": "1",
    "question_en": "A company sells ringtones created from clips of popular songs. The files containing the ringtones are stored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, but downloads are infrequent for ringtones older than 90 days. The company needs to save money on storage while keeping the most accessed files readily available for its users. Which action should the company take to meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the objects.",
      "B": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage tier after 90 days.",
      "C": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days.",
      "D": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-1A) after 90 days."
    },
    "correct_answer": "D",
    "vote_percentage": "64%",
    "question_cn": "一家公司销售由流行歌曲片段创建的铃声。包含铃声的文件存储在 Amazon S3 Standard 中，并且大小至少为 128 KB。该公司拥有数百万个文件，但 90 天以上的铃声下载频率较低。该公司需要在节省存储成本的同时，使其用户可以随时访问最常访问的文件。该公司应采取哪项措施以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "为对象的初始存储层配置 S3 Standard-Infrequent Access (S3 Standard-IA) 存储。",
      "B": "将文件移动到 S3 Intelligent-Tiering，并将其配置为在 90 天后将对象移动到更便宜的存储层。",
      "C": "配置 S3 inventory 来管理对象，并在 90 天后将它们移动到 S3 Standard-Infrequent Access (S3 Standard-1A)。",
      "D": "实施一个 S3 生命周期策略，该策略在 90 天后将对象从 S3 Standard 移动到 S3 Standard-Infrequent Access (S3 Standard-1A)。"
    },
    "tags": [
      "S3",
      "S3 Standard-IA",
      "S3 Intelligent-Tiering",
      "S3 lifecycle policy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 64%），解析仅供参考。】\n\n此题考查如何优化 S3 存储成本。 核心在于使用生命周期策略，以自动将不常访问的数据移动到更便宜的存储层。 选项 D 提供了最直接有效的解决方案。",
      "why_correct": "S3 生命周期策略允许根据对象的使用频率自动将对象移动到不同的存储层，从而优化成本。该策略可以设置在 90 天后将对象从 S3 Standard 移动到 S3 Standard-IA。",
      "why_wrong": "选项 A 错误，因为 S3 Standard-IA 适用于访问频率较低的数据，但初始存储层应该是 S3 Standard。选项 B 错误，因为 S3 Intelligent-Tiering 无法根据时间自动移动到更便宜的存储层。选项 C 错误，因为配置 S3 inventory 无法自动迁移存储类型。"
    },
    "related_terms": [
      "S3",
      "S3 Standard-IA",
      "S3 Intelligent-Tiering",
      "S3 lifecycle policy"
    ]
  },
  {
    "id": 154,
    "topic": "1",
    "question_en": "A company needs to save the results from a medical trial to an Amazon S3 repository. The repository must allow a few scientists to add new files and must restrict all other users to read-only access. No users can have the ability to modify or delete any files in the repository. The company must keep every file in the repository for a minimum of 1 year after its creation date. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use S3 Object Lock in governance mode with a legal hold of 1 year.",
      "B": "Use S3 Object Lock in compliance mode with a retention period of 365 days.",
      "C": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 bucket policy to only allow the IAM role.",
      "D": "Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. Configure the function to track the hash of the saved object so that modified objects can be marked accordingly."
    },
    "correct_answer": "B",
    "vote_percentage": "91%",
    "question_cn": "一家公司需要将医疗试验的结果保存到 Amazon S3 存储库中。该存储库必须允许少数科学家添加新文件，并且必须将所有其他用户的访问权限限制为只读。任何用户都不能修改或删除存储库中的任何文件。公司必须将存储库中的每个文件保留至少 1 年的创建日期。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 S3 Object Lock 的治理模式，保留期为 1 年。",
      "B": "使用 S3 Object Lock 的合规模式，保留期为 365 天。",
      "C": "使用 IAM 角色来限制所有用户删除或更改 S3 存储桶中的对象。使用 S3 存储桶策略仅允许 IAM 角色。",
      "D": "配置 S3 存储桶，以便每次添加对象时调用 AWS Lambda 函数。配置该函数以跟踪已保存对象的哈希值，以便可以相应地标记已修改的对象。"
    },
    "tags": [
      "S3",
      "Object Lock",
      "IAM",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 91%），解析仅供参考。】\n\n此题考查如何使用 S3 存储桶保护医疗数据，包括防止修改和删除，以及满足合规性要求。 核心在于使用 S3 Object Lock 来防止对象被修改或删除。",
      "why_correct": "使用 S3 Object Lock 的合规模式可以实现文件的不可变性，确保文件在设定的保留期内无法被修改或删除，符合合规性要求。保留期设置为 365 天。",
      "why_wrong": "选项 A 错误，因为治理模式需要额外的权限管理，不满足所有用户只读的要求。选项 C 错误，因为 IAM 角色无法防止删除。选项 D 错误，因为使用 Lambda 函数跟踪对象哈希值并不能实现防止修改或删除文件的要求。"
    },
    "related_terms": [
      "S3",
      "IAM",
      "Lambda",
      "Object Lock"
    ]
  },
  {
    "id": 155,
    "topic": "1",
    "question_en": "A large media company hosts a web application on AWS. The company wants to start caching confidential media files so that users around the world will have reliable access to the files. The content is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless of where the requests originate geographically. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS DataSync to connect the S3 buckets to the web application.",
      "B": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.",
      "C": "Deploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web application."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家大型媒体公司在 AWS 上托管一个 Web 应用程序。该公司希望开始缓存机密的媒体文件，以便世界各地的用户能够可靠地访问这些文件。内容存储在 Amazon S3 存储桶中。无论请求来自何处，该公司都必须快速交付内容。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 将 S3 存储桶连接到 Web 应用程序。",
      "B": "部署 AWS Global Accelerator 将 S3 存储桶连接到 Web 应用程序。",
      "C": "部署 Amazon CloudFront 将 S3 存储桶连接到 CloudFront 边缘服务器。",
      "D": "使用 Amazon Simple Queue Service (Amazon SQS) 将 S3 存储桶连接到 Web 应用程序。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "DataSync",
      "Global Accelerator",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查如何为媒体公司快速交付内容。 关键在于使用 CDN 服务，以便内容可以从全球边缘位置提供。 选项 C 提供了最有效的解决方案。",
      "why_correct": "Amazon CloudFront 是一种内容分发网络 (CDN) 服务，可以将内容缓存到全球边缘服务器，从而加快内容交付速度。",
      "why_wrong": "选项 A 错误，因为 AWS DataSync 用于数据迁移，而不是内容分发。选项 B 错误，因为 AWS Global Accelerator 用于加速应用程序流量，而不是内容分发。选项 D 错误，因为 Amazon SQS 是一种消息队列服务，与内容分发无关。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "DataSync",
      "Global Accelerator",
      "SQS"
    ]
  },
  {
    "id": 156,
    "topic": "1",
    "question_en": "A company produces batch data that comes from different databases. The company also produces live stream data from network sensors and application APIs. The company needs to consolidate all the data into one place for business analytics. The company needs to process the incoming data and then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and import the data into a business intelligence tool to show key performance indicators (KPIs). Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.",
      "B": "Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.",
      "C": "Create custom AWS Lambda functions to move the individual records from the databases to an Amazon Redshift cluster.",
      "D": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. Load the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters",
      "E": "Use blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. Use AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache Parquet format."
    },
    "correct_answer": "AE",
    "vote_percentage": "89%",
    "question_cn": "一家公司生成来自不同数据库的批量数据。该公司还生成来自网络传感器和应用程序 API 的实时流数据。公司需要将所有数据整合到一个地方进行商业分析。公司需要处理传入的数据，然后将数据分阶段存储在不同的 Amazon S3 存储桶中。团队稍后将运行一次性查询，并将数据导入到商业智能工具中以显示关键绩效指标 (KPI)。哪种步骤组合将以最少的运营开销满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 Amazon Athena 进行一次性查询。使用 Amazon QuickSight 创建 KPI 的仪表板。",
      "B": "使用 Amazon Kinesis Data Analytics 进行一次性查询。使用 Amazon QuickSight 创建 KPI 的仪表板。",
      "C": "创建自定义 AWS Lambda 函数以将单个记录从数据库移动到 Amazon Redshift 集群。",
      "D": "使用 AWS Glue extract, transform, and load (ETL) 作业将数据转换为 JSON 格式。将数据加载到多个 Amazon OpenSearch Service (Amazon Elasticsearch Service) 集群。",
      "E": "使用 AWS Lake Formation 中的蓝图来识别可以摄取到数据湖中的数据。使用 AWS Glue 爬取源，提取数据，并将数据以 Apache Parquet 格式加载到 Amazon S3 中。"
    },
    "tags": [
      "Athena",
      "QuickSight",
      "Kinesis Data Analytics",
      "Glue",
      "Redshift",
      "OpenSearch Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 89%），解析仅供参考。】\n\n考察数据整合、存储、处理及分析的方案选择，侧重于成本效益和运营开销。需要选择能够满足批量数据和实时流数据处理需求，并支持数据分阶段存储和一次性查询的方案。",
      "why_correct": "选项 A 提供了针对一次性查询和 KPI 仪表板的有效方案。Amazon Athena 适合用于对 S3 中的数据进行一次性查询，而 Amazon QuickSight 则可以用来创建 KPI 仪表板，实现数据可视化。选项 E 提供了建立数据湖的方案。AWS Lake Formation 蓝图用于发现和摄取数据，AWS Glue 用于提取、转换和加载数据到 S3 中，以 Apache Parquet 格式存储数据，便于后续分析和数据分阶段存储。",
      "why_wrong": "选项 B 错误，因为 Amazon Kinesis Data Analytics 主要用于处理流数据，不适用于一次性查询数据库数据。选项 C 不适合，因为自定义 AWS Lambda 函数进行数据迁移的方案需要大量手动操作，运营成本高。此外，将数据直接迁移到 Amazon Redshift 集群不符合题目中数据分阶段存储的要求。选项 D 错误，因为将数据加载到 Amazon OpenSearch Service 不符合题目的要求，而且 OpenSearch 并非用于商业分析的一般工具，且不便于数据分阶段存储。"
    },
    "related_terms": [
      "Amazon Athena",
      "Amazon QuickSight",
      "Amazon Kinesis Data Analytics",
      "AWS Lambda",
      "Amazon Redshift",
      "AWS Glue",
      "Amazon OpenSearch Service",
      "AWS Lake Formation",
      "Amazon S3",
      "Apache Parquet"
    ]
  },
  {
    "id": 157,
    "topic": "1",
    "question_en": "A company stores data in an Amazon Aurora PostgreSQL DB cluster. The company must store all the data for 5 years and must delete all the data after 5 years. The company also must indefinitely keep audit logs of actions that are performed within the database. Currently, the company has automated backups configured for Aurora. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Take a manual snapshot of the DB cluster.",
      "B": "Create a lifecycle policy for the automated backups.",
      "C": "Configure automated backup retention for 5 years.",
      "D": "Configure an Amazon CloudWatch Logs export for the DB cluster",
      "E": "Use AWS Backup to take the backups and to keep the backups for 5 years."
    },
    "correct_answer": "DE",
    "vote_percentage": "78%",
    "question_cn": "一家公司将数据存储在 Amazon Aurora PostgreSQL 数据库集群中。该公司必须将所有数据存储 5 年，并在 5 年后删除所有数据。该公司还必须无限期地保留数据库内执行操作的审计日志。目前，该公司已为 Aurora 配置了自动备份。解决方案架构师应采取哪些步骤组合来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "拍摄数据库集群的手动快照。",
      "B": "为自动备份创建生命周期策略。",
      "C": "配置自动备份保留 5 年。",
      "D": "为数据库集群配置 Amazon CloudWatch Logs 导出。",
      "E": "使用 AWS Backup 进行备份，并将备份保留 5 年。"
    },
    "tags": [
      "Aurora PostgreSQL",
      "CloudWatch Logs",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 78%），解析仅供参考。】\n\n考查 Amazon Aurora 数据库数据保留策略的配置，包括数据备份的保留期限和审计日志的持久化存储。",
      "why_correct": "D: 配置 Amazon CloudWatch Logs 导出，可以将数据库的审计日志（例如 PostgreSQL 的审计扩展）发送到 S3，从而实现无限期的日志存储。这满足了题目中对审计日志无限期保留的要求。\nE: 使用 AWS Backup 进行数据库备份，并设置备份保留期为 5 年，满足了对数据存储 5 年的要求，并且 AWS Backup 提供了集中式备份管理和生命周期管理功能。",
      "why_wrong": "A: 拍摄手动快照无法满足 5 年后的数据删除需求，手动快照需要手动管理和删除，无法自动化。手动快照只适合短期的数据恢复需求。\nB: 自动备份的生命周期策略通常无法配置删除审计日志的功能，而且生命周期策略主要管理的是自动备份的保留期。即便可以配置，也与审计日志的无限期保留需求相悖。\nC: 配置自动备份保留 5 年，仅仅满足了数据存储 5 年的需求，但并未考虑审计日志的无限期保留。Aurora 自动备份的保留策略通常无法自定义到只保留审计日志。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL",
      "Amazon CloudWatch Logs",
      "S3",
      "AWS Backup",
      "PostgreSQL"
    ]
  },
  {
    "id": 158,
    "topic": "1",
    "question_en": "A solutions architect is optimizing a website for an upcoming musical event. Videos of the performances will be streamed in real time and then will be available on demand. The event is expected to attract a global online audience. Which service will improve the performance of both the real-time and on-demand streaming?",
    "options_en": {
      "A": "Amazon CloudFront",
      "B": "AWS Global Accelerator",
      "C": "Amazon Route 53",
      "D": "Amazon S3 Transfer Acceleration"
    },
    "correct_answer": "A",
    "vote_percentage": "66%",
    "question_cn": "一位解决方案架构师正在为即将到来的音乐活动优化一个网站。表演视频将进行实时流式传输，然后可供点播。预计此次活动将吸引全球在线观众。哪个服务将提高实时和点播流的性能？",
    "options_cn": {
      "A": "Amazon CloudFront",
      "B": "AWS Global Accelerator",
      "C": "Amazon Route 53",
      "D": "Amazon S3 Transfer Acceleration"
    },
    "tags": [
      "CloudFront",
      "Global Accelerator",
      "Route 53",
      "S3 Transfer Acceleration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 66%），解析仅供参考。】\n\n此题考查如何优化网站性能。 关键在于选择适合的 CDN 服务来加速内容交付。",
      "why_correct": "Amazon CloudFront 是一种内容分发网络 (CDN) 服务，可以将内容缓存到全球边缘服务器，从而加快内容交付速度，提高性能。",
      "why_wrong": "选项 B 错误，因为 AWS Global Accelerator 主要用于加速应用程序流量，而不是内容分发。选项 C 错误，因为 Route 53 是一种 DNS 服务，用于将域名解析到 IP 地址。选项 D 错误，因为 S3 Transfer Acceleration 用于加快 S3 数据的上传速度，而不是内容分发。"
    },
    "related_terms": [
      "CloudFront",
      "Global Accelerator",
      "Route 53",
      "S3 Transfer Acceleration"
    ]
  },
  {
    "id": 159,
    "topic": "1",
    "question_en": "A company is running a publicly accessible serverless application that uses Amazon API Gateway and AWS Lambda. The application’s trafic recently spiked due to fraudulent requests from botnets. Which steps should a solutions architect take to block requests from unauthorized users? (Choose two.)",
    "options_en": {
      "A": "Create a usage plan with an API key that is shared with genuine users only.",
      "B": "Integrate logic within the Lambda function to ignore the requests from fraudulent IP addresses.",
      "C": "Implement an AWS WAF rule to target malicious requests and trigger actions to filter them out.",
      "D": "Convert the existing public API to a private API. Update the DNS records to redirect users to the new API endpoint",
      "E": "Create an IAM role for each user attempting to access the API. A user will assume the role when making the API call."
    },
    "correct_answer": "AC",
    "vote_percentage": "67%",
    "question_cn": "一家公司正在运行一个使用 Amazon API Gateway 和 AWS Lambda 的公开可访问的无服务器应用程序。 由于来自僵尸网络的欺诈性请求，该应用程序的流量最近激增。 解决方案架构师应采取哪些步骤来阻止来自未经授权用户的请求？（选择两项。）",
    "options_cn": {
      "A": "创建一个带有 API 密钥的使用计划，该密钥仅与真实用户共享。",
      "B": "在 Lambda 函数中集成逻辑以忽略来自欺诈性 IP 地址的请求。",
      "C": "实施 AWS WAF 规则以定位恶意请求并触发操作以过滤它们。",
      "D": "将现有的公共 API 转换为私有 API。更新 DNS 记录以将用户重定向到新的 API 终端节点。",
      "E": "为每个尝试访问 API 的用户创建一个 IAM 角色。用户在进行 API 调用时将担任该角色。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "WAF",
      "API Keys",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 67%），解析仅供参考。】\n\n考查如何通过 API Gateway、Lambda 和其他 AWS 服务来缓解针对无服务器应用程序的欺诈性流量攻击。",
      "why_correct": "A. 创建 API 密钥使用计划，并仅与真实用户共享，可以限制未经授权的访问，并能够通过密钥管理对 API 调用进行身份验证和速率限制。C. 实施 AWS WAF 规则可以检测并阻止来自已知恶意来源的流量，例如僵尸网络，并允许对请求进行细粒度的控制和过滤。",
      "why_wrong": "B. 在 Lambda 函数中硬编码 IP 地址过滤逻辑效率低，难以维护和更新，并且容易受到 IP 地址欺骗的攻击。D. 将公共 API 转换为私有 API 会中断现有用户访问，且需要更改 DNS 记录，增加了复杂性，并非优先的防御策略。E. 为每个用户创建 IAM 角色会导致角色数量爆炸式增长，管理成本高昂，且不能直接阻止恶意流量，IAM 角色更多用于权限控制，而非流量过滤。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "API Key",
      "AWS WAF",
      "IAM",
      "DNS",
      "API"
    ]
  },
  {
    "id": 160,
    "topic": "1",
    "question_en": "An ecommerce company hosts its analytics application in the AWS Cloud. The application generates about 300 MB of data each month. The data is stored in JSON format. The company is evaluating a disaster recovery solution to back up the data. The data must be accessible in milliseconds if it is needed, and the data must be kept for 30 days. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "B": "Amazon S3 Glacier",
      "C": "Amazon S3 Standard",
      "D": "Amazon RDS for PostgreSQL"
    },
    "correct_answer": "C",
    "vote_percentage": "89%",
    "question_cn": "一家电子商务公司在 AWS 云中托管其分析应用程序。该应用程序每个月生成大约 300 MB 的数据。数据以 JSON 格式存储。该公司正在评估灾难恢复解决方案以备份数据。如果需要，必须在毫秒内访问数据，并且必须将数据保留 30 天。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "Amazon OpenSearch Service (Amazon Elasticsearch Service)",
      "B": "Amazon S3 Glacier",
      "C": "Amazon S3 Standard",
      "D": "Amazon RDS for PostgreSQL"
    },
    "tags": [
      "OpenSearch Service",
      "S3 Glacier",
      "S3 Standard",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 89%），解析仅供参考。】\n\n此题考查如何为电子商务应用程序选择灾难恢复解决方案。 关键在于选择满足毫秒级数据访问和 30 天数据保留要求的存储服务，同时兼顾成本效益。",
      "why_correct": "Amazon S3 Standard 提供了高性能的访问速度和较低的存储成本，满足毫秒级数据访问和 30 天数据保留的要求。",
      "why_wrong": "选项 A 错误，因为 Amazon OpenSearch Service 主要用于日志分析和搜索，不适合作为主要的灾难恢复解决方案。选项 B 错误，因为 Amazon S3 Glacier 适用于长期归档，访问速度较慢。选项 D 错误，因为 Amazon RDS for PostgreSQL 是一种数据库服务，不适合用于快速访问的备份存储。"
    },
    "related_terms": [
      "OpenSearch Service",
      "S3 Glacier",
      "RDS",
      "S3 Standard"
    ]
  },
  {
    "id": 161,
    "topic": "1",
    "question_en": "A company has a small Python application that processes JSON documents and outputs the results to an on-premises SQL database. The application runs thousands of times each day. The company wants to move the application to the AWS Cloud. The company needs a highly available solution that maximizes scalability and minimizes operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Place the JSON documents in an Amazon S3 bucket. Run the Python code on multiple Amazon EC2 instances to process the documents. Store the results in an Amazon Aurora DB cluster.",
      "B": "Place the JSON documents in an Amazon S3 bucket. Create an AWS Lambda function that runs the Python code to process the documents as they arrive in the S3 bucket. Store the results in an Amazon Aurora DB cluster.",
      "C": "Place the JSON documents in an Amazon Elastic Block Store (Amazon EBS) volume. Use the EBS Multi-Attach feature to attach the volume to multiple Amazon EC2 instances. Run the Python code on the EC2 instances to process the documents. Store the results on an Amazon RDS DB instance.",
      "D": "Place the JSON documents in an Amazon Simple Queue Service (Amazon SQS) queue as messages. Deploy the Python code as a container on an Amazon Elastic Container Service (Amazon ECS) cluster that is configured with the Amazon EC2 launch type. Use the container to process the SQS messages. Store the results on an Amazon RDS DB instance."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司有一个小型 Python 应用程序，该应用程序处理 JSON 文档并将结果输出到本地 SQL 数据库。该应用程序每天运行数千次。该公司希望将该应用程序迁移到 AWS 云。该公司需要一个高可用性解决方案，该解决方案可最大限度地提高可扩展性并最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 JSON 文档放置在 Amazon S3 存储桶中。在多个 Amazon EC2 实例上运行 Python 代码以处理文档。将结果存储在 Amazon Aurora 数据库集群中。",
      "B": "将 JSON 文档放置在 Amazon S3 存储桶中。创建一个 AWS Lambda 函数，该函数运行 Python 代码以在 JSON 文档到达 S3 存储桶时对其进行处理。将结果存储在 Amazon Aurora 数据库集群中。",
      "C": "将 JSON 文档放置在 Amazon Elastic Block Store (Amazon EBS) 卷中。使用 EBS 多重附加功能将卷附加到多个 Amazon EC2 实例。在 EC2 实例上运行 Python 代码以处理文档。将结果存储在 Amazon RDS 数据库实例上。",
      "D": "将 JSON 文档作为消息放置在 Amazon Simple Queue Service (Amazon SQS) 队列中。将 Python 代码作为容器部署在配置了 Amazon EC2 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群上。使用容器处理 SQS 消息。将结果存储在 Amazon RDS 数据库实例上。"
    },
    "tags": [
      "EC2",
      "Lambda",
      "S3",
      "Aurora",
      "SQS",
      "ECS",
      "EBS",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考察使用 AWS 服务构建高可用性、可扩展且运营开销低的应用程序的方案选择。重点考察 S3、Lambda、Aurora 数据库的集成使用。",
      "why_correct": "选项 B 提供了最佳的解决方案。当 JSON 文档上传到 S3 时，Lambda 函数会被触发，实现事件驱动的、无服务器的、可自动扩展的计算。Aurora 数据库提供高可用性和可扩展性，适合存储处理结果。这种组合方案减少了运营开销，实现了高可用性和可扩展性。",
      "why_wrong": "选项 A 涉及 EC2 实例，需要手动管理，增加了运营开销。虽然 Aurora 提供高可用性，但 EC2 实例的管理和扩展不如 Lambda 方便。选项 C 使用 EBS 多重附加，EBS 卷的共享方式不适合高频写入和高并发，并且增加了管理复杂性。RDS 数据库不如 Aurora 数据库具备高可用性和可扩展性。选项 D 使用 SQS、ECS 和 RDS，增加了方案复杂性，且涉及容器管理和消息队列，不如 Lambda 的事件驱动方式简洁高效。"
    },
    "related_terms": [
      "Amazon S3",
      "Python",
      "Amazon Aurora",
      "AWS Lambda",
      "Amazon EC2",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon RDS",
      "Amazon SQS",
      "Amazon Elastic Container Service (Amazon ECS)"
    ]
  },
  {
    "id": 162,
    "topic": "1",
    "question_en": "A company wants to use high performance computing (HPC) infrastructure on AWS for financial risk modeling. The company’s HPC workloads run on Linux. Each HPC workfiow runs on hundreds of Amazon EC2 Spot Instances, is short-lived, and generates thousands of output files that are ultimately stored in persistent storage for analytics and long-term future use. The company seeks a cloud storage solution that permits the copying of on-premises data to long-term persistent storage to make data available for processing by all EC2 instances. The solution should also be a high performance file system that is integrated with persistent storage to read and write datasets and output files. Which combination of AWS services meets these requirements?",
    "options_en": {
      "A": "Amazon FSx for Lustre integrated with Amazon S3",
      "B": "Amazon FSx for Windows File Server integrated with Amazon S3",
      "C": "Amazon S3 Glacier integrated with Amazon Elastic Block Store (Amazon EBS)",
      "D": "Amazon S3 bucket with a VPC endpoint integrated with an Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) volume"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在 AWS 上使用高性能计算 (HPC) 基础设施进行金融风险建模。该公司的 HPC 工作负载运行在 Linux 上。每个 HPC 工作流程都在数百个 Amazon EC2 Spot 实例上运行，生命周期很短，并生成数千个输出文件，这些文件最终存储在持久性存储中，用于分析和长期未来使用。该公司寻求一种云存储解决方案，该解决方案允许将本地数据复制到长期持久性存储中，以便所有 EC2 实例都可以处理数据。该解决方案还应该是一个高性能文件系统，该文件系统与持久性存储集成，用于读取和写入数据集和输出文件。哪种 AWS 服务组合符合这些要求？",
    "options_cn": {
      "A": "Amazon FSx for Lustre 与 Amazon S3 集成",
      "B": "Amazon FSx for Windows File Server 与 Amazon S3 集成",
      "C": "Amazon S3 Glacier 与 Amazon Elastic Block Store (Amazon EBS) 集成",
      "D": "带有 VPC endpoint 的 Amazon S3 存储桶与 Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp2) 卷集成"
    },
    "tags": [
      "FSx for Lustre",
      "S3",
      "FSx for Windows File Server",
      "S3 Glacier",
      "EBS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查如何为 HPC 工作负载选择存储解决方案。 关键在于选择高性能文件系统，并与持久性存储集成。 选项 A 提供了最符合要求的解决方案。",
      "why_correct": "Amazon FSx for Lustre 是一种高性能文件系统，可以与 Amazon S3 集成，实现本地数据复制到长期持久性存储中，满足读取和写入数据集和输出文件的需求。",
      "why_wrong": "选项 B 错误，因为 FSx for Windows File Server 性能不如 FSx for Lustre。选项 C 错误，因为 S3 Glacier 不适合用于快速读取和写入数据集。选项 D 错误，因为 EBS 的性能无法满足 HPC 工作负载的要求。"
    },
    "related_terms": [
      "FSx for Lustre",
      "S3",
      "FSx for Windows File Server",
      "S3 Glacier",
      "EBS"
    ]
  },
  {
    "id": 163,
    "topic": "1",
    "question_en": "A company is building a containerized application on premises and decides to move the application to AWS. The application will have thousands of users soon after it is deployed. The company is unsure how to manage the deployment of containers at scale. The company needs to deploy the containerized application in a highly available architecture that minimizes operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the AWS Fargate launch type to run the containers. Use target tracking to scale automatically based on demand.",
      "B": "Store container images in an Amazon Elastic Container Registry (Amazon ECR) repository. Use an Amazon Elastic Container Service (Amazon ECS) cluster with the Amazon EC2 launch type to run the containers. Use target tracking to scale automatically based on demand.",
      "C": "Store container images in a repository that runs on an Amazon EC2 instance. Run the containers on EC2 instances that are spread across multiple Availability Zones. Monitor the average CPU utilization in Amazon CloudWatch. Launch new EC2 instances as needed.",
      "D": "Create an Amazon EC2 Amazon Machine Image (AMI) that contains the container image. Launch EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon CloudWatch alarm to scale out EC2 instances when the average CPU utilization threshold is breached."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在本地构建一个容器化应用程序，并决定将该应用程序迁移到 AWS。该应用程序在部署后不久将拥有数千名用户。该公司不确定如何大规模管理容器的部署。该公司需要在高可用性架构中部署容器化应用程序，以最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将容器镜像存储在 Amazon Elastic Container Registry (Amazon ECR) 存储库中。使用带有 AWS Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群来运行容器。使用目标跟踪根据需求自动扩展。",
      "B": "将容器镜像存储在 Amazon Elastic Container Registry (Amazon ECR) 存储库中。使用带有 Amazon EC2 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群来运行容器。使用目标跟踪根据需求自动扩展。",
      "C": "将容器镜像存储在运行于 Amazon EC2 实例上的存储库中。在分布于多个可用区的 EC2 实例上运行容器。在 Amazon CloudWatch 中监控平均 CPU 利用率。根据需要启动新的 EC2 实例。",
      "D": "创建一个包含容器镜像的 Amazon EC2 Amazon Machine Image (AMI)。在跨多个可用区的 Auto Scaling 组中启动 EC2 实例。使用 Amazon CloudWatch 警报在超过平均 CPU 利用率阈值时扩展 EC2 实例。"
    },
    "tags": [
      "ECR",
      "ECS",
      "Fargate",
      "EC2",
      "CloudWatch",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在 AWS 上大规模部署和管理容器化应用程序的方案，包括镜像存储、容器编排、启动类型、扩展策略等。",
      "why_correct": "选项 A 采用了 AWS Fargate，这是一种无服务器的计算引擎，用于运行容器。它无需管理服务器，简化了运营开销。结合 Amazon ECS 和目标跟踪的自动扩展功能，可以满足高可用性需求并根据用户量进行弹性伸缩。",
      "why_wrong": "选项 B 采用 Amazon EC2 启动类型，虽然可以实现高可用性，但需要用户管理 EC2 实例，增加了运营负担。选项 C 使用 EC2 实例上的存储库和手动扩展，增加了运维复杂性，且可用性和扩展能力不如 ECS。选项 D 使用 AMI，不够灵活，维护成本高，且自动扩展策略基于 CPU 利用率，效率不如目标跟踪。"
    },
    "related_terms": [
      "Amazon Elastic Container Registry (Amazon ECR)",
      "Amazon Elastic Container Service (Amazon ECS)",
      "AWS Fargate",
      "Amazon EC2",
      "Amazon CloudWatch",
      "Amazon Machine Image (AMI)",
      "Auto Scaling"
    ]
  },
  {
    "id": 164,
    "topic": "1",
    "question_en": "A company has two applications: a sender application that sends messages with payloads to be processed and a processing application intended to receive the messages with payloads. The company wants to implement an AWS service to handle messages between the two applications. The sender application can send about 1,000 messages each hour. The messages may take up to 2 days to be processed: If the messages fail to process, they must be retained so that they do not impact the processing of any remaining messages. Which solution meets these requirements and is the MOST operationally eficient?",
    "options_en": {
      "A": "Set up an Amazon EC2 instance running a Redis database. Configure both applications to use the instance. Store, process, and delete the messages, respectively.",
      "B": "Use an Amazon Kinesis data stream to receive the messages from the sender application. Integrate the processing application with the Kinesis Client Library (KCL).",
      "C": "Integrate the sender and processor applications with an Amazon Simple Queue Service (Amazon SQS) queue. Configure a dead-letter queue to collect the messages that failed to process.",
      "D": "Subscribe the processing application to an Amazon Simple Notification Service (Amazon SNS) topic to receive notifications to process. Integrate the sender application to write to the SNS topic. С"
    },
    "correct_answer": "C",
    "vote_percentage": "90%",
    "question_cn": "一家公司有两个应用程序：一个发送应用程序，用于发送包含待处理负载的消息；一个处理应用程序，用于接收带有负载的消息。该公司希望实施一个 AWS 服务来处理这两个应用程序之间的消息。发送应用程序每小时可以发送大约 1,000 条消息。消息可能需要长达 2 天的时间来处理：如果消息处理失败，则必须保留它们，以免影响剩余消息的处理。哪种解决方案满足这些要求并且在运营上最有效？",
    "options_cn": {
      "A": "设置一个运行 Redis 数据库的 Amazon EC2 实例。将两个应用程序配置为使用该实例。分别存储、处理和删除消息。",
      "B": "使用 Amazon Kinesis 数据流接收来自发送应用程序的消息。将处理应用程序与 Kinesis Client Library (KCL) 集成。",
      "C": "将发送和处理应用程序与 Amazon Simple Queue Service (Amazon SQS) 队列集成。配置一个死信队列来收集处理失败的消息。",
      "D": "将处理应用程序订阅到 Amazon Simple Notification Service (Amazon SNS) 主题，以接收要处理的通知。将发送应用程序集成到 SNS 主题中进行写入。"
    },
    "tags": [
      "EC2",
      "Redis",
      "Kinesis",
      "SQS",
      "SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 90%），解析仅供参考。】\n\n此题考查如何设计消息传递架构。 关键在于选择可靠的消息队列服务，并处理消息失败情况。 选项 C 提供了最有效和最可靠的解决方案。",
      "why_correct": "使用 SQS 队列进行消息传递，配置死信队列来收集处理失败的消息，可以保证消息的可靠传递和处理，并提供消息失败的重试机制。",
      "why_wrong": "选项 A 错误，EC2 运行的 Redis 数据库无法保证消息的持久性和可靠性。选项 B 错误， Kinesis 数据流主要用于流式数据处理，不适合消息队列。选项 D 错误，SNS 主要用于发布-订阅模式，消息处理失败无法处理。"
    },
    "related_terms": [
      "EC2",
      "Redis",
      "Kinesis",
      "SQS",
      "SNS"
    ]
  },
  {
    "id": 165,
    "topic": "1",
    "question_en": "A solutions architect must design a solution that uses Amazon CloudFront with an Amazon S3 origin to store a static website. The company’s security policy requires that all website trafic be inspected by AWS WAF. How should the solutions architect comply with these requirements?",
    "options_en": {
      "A": "Configure an S3 bucket policy to accept requests coming from the AWS WAF Amazon Resource Name (ARN) only.",
      "B": "Configure Amazon CloudFront to forward all incoming requests to AWS WAF before requesting content from the S3 origin.",
      "C": "Configure a security group that allows Amazon CloudFront IP addresses to access Amazon S3 only. Associate AWS WAF to CloudFront.",
      "D": "Configure Amazon CloudFront and Amazon S3 to use an origin access identity (OAI) to restrict access to the S3 bucket. Enable AWS WAF on the distribution."
    },
    "correct_answer": "D",
    "vote_percentage": "58%",
    "question_cn": "一个解决方案架构师必须设计一个使用 Amazon CloudFront 和 Amazon S3 源来存储静态网站的解决方案。该公司的安全策略要求所有网站流量都必须由 AWS WAF 检查。解决方案架构师应如何符合这些要求？",
    "options_cn": {
      "A": "配置 S3 存储桶策略，仅接受来自 AWS WAF Amazon 资源名称 (ARN) 的请求。",
      "B": "配置 Amazon CloudFront 在从 S3 源请求内容之前将所有传入请求转发到 AWS WAF。",
      "C": "配置一个安全组，允许 Amazon CloudFront IP 地址仅访问 Amazon S3。将 AWS WAF 与 CloudFront 关联。",
      "D": "配置 Amazon CloudFront 和 Amazon S3 使用源访问身份 (OAI) 来限制对 S3 存储桶的访问。在分发上打开 AWS WAF。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "WAF",
      "OAI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 58%），解析仅供参考。】\n\n此题考查如何保护静态网站的安全性。 关键在于配置 CloudFront 和 WAF，以确保所有流量都经过 WAF 检查。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：使用 OAI 来限制对 S3 存储桶的访问，并在 CloudFront 分发上启用 AWS WAF，可以确保所有流量都经过 WAF 检查，满足安全要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，因为 S3 存储桶策略无法单独确保所有流量都经过 WAF。选项 B 错误，因为 CloudFront 无法将所有请求直接转发到 WAF。选项 C 错误，因为安全组无法确保所有流量都经过 WAF 检查。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "WAF",
      "OAI"
    ]
  },
  {
    "id": 166,
    "topic": "1",
    "question_en": "Organizers for a global event want to put daily reports online as static HTML pages. The pages are expected to generate millions of views from users around the world. The files are stored in an Amazon S3 bucket. A solutions architect has been asked to design an eficient and effective solution. Which action should the solutions architect take to accomplish this?",
    "options_en": {
      "A": "Generate presigned URLs for the files.",
      "B": "Use cross-Region replication to all Regions.",
      "C": "Use the geoproximity feature of Amazon Route 53.",
      "D": "Use Amazon CloudFront with the S3 bucket as its origin."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "全球活动的组织者希望将每日报告作为静态 HTML 页面发布到线上。预计这些页面将从世界各地的用户那里获得数百万次浏览量。这些文件存储在 Amazon S3 存储桶中。一位解决方案架构师被要求设计一个高效且有效的解决方案。解决方案架构师应采取以下哪项措施来完成此任务？",
    "options_cn": {
      "A": "为文件生成预签名 URL。",
      "B": "使用 跨区域复制 到所有区域。",
      "C": "使用 Amazon Route 53 的地理位置临近功能。",
      "D": "将 Amazon CloudFront 与 S3 存储桶用作其源。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "Content Delivery Network (CDN)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查利用 CDN 服务 Amazon CloudFront 优化静态内容的访问性能。与 S3 存储桶的配置、CDN 的特性、以及全球用户访问场景下的优化策略相关。",
      "why_correct": "选项 D 正确。使用 Amazon CloudFront 作为 CDN，以 Amazon S3 存储桶作为源，是最适合此场景的解决方案。CloudFront 缓存静态 HTML 页面到全球分布的边缘站点，从而缩短用户访问文件的延迟。CloudFront 还能处理高并发流量，实现高效的内容分发，满足数百万次浏览量的需求，并可以有效降低源站的负载。",
      "why_wrong": "选项 A 错误。预签名 URL 适用于对 S3 存储桶中的特定对象进行受限访问，通常用于临时共享私有内容。虽然可以解决权限问题，但无法优化全球用户的访问延迟和带宽消耗，不适合大规模的静态内容分发。选项 B 错误。跨区域复制 (Cross-Region Replication, CRR) 主要用于数据冗余和灾难恢复，将数据复制到其他区域，并不能直接提高访问性能，反而会增加存储成本和复制延迟。选项 C 错误。Amazon Route 53 的地理位置临近功能（Geolocation routing）用于根据用户所在地理位置将流量路由到不同的资源，主要用于流量管理和多区域应用部署，与优化静态内容分发、提高访问速度的场景不直接相关。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "S3",
      "CDN",
      "Route 53",
      "Cross-Region Replication"
    ]
  },
  {
    "id": 167,
    "topic": "1",
    "question_en": "A company runs a production application on a fieet of Amazon EC2 instances. The application reads the data from an Amazon SQS queue and processes the messages in parallel. The message volume is unpredictable and often has intermittent trafic. This application should continually process messages without any downtime. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use Spot Instances exclusively to handle the maximum capacity required.",
      "B": "Use Reserved Instances exclusively to handle the maximum capacity required.",
      "C": "Use Reserved Instances for the baseline capacity and use Spot Instances to handle additional capacity.",
      "D": "Use Reserved Instances for the baseline capacity and use On-Demand Instances to handle additional capacity."
    },
    "correct_answer": "D",
    "vote_percentage": "53%",
    "question_cn": "一家公司在 Amazon EC2 实例集群上运行生产应用程序。该应用程序从 Amazon SQS 队列读取数据并并行处理消息。消息量不可预测，并且经常出现间歇性流量。此应用程序应持续处理消息，且没有任何停机时间。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "完全使用 Spot 实例来处理所需的最大容量。",
      "B": "完全使用预留实例来处理所需的最大容量。",
      "C": "使用预留实例处理基线容量，并使用 Spot 实例处理额外的容量。",
      "D": "使用预留实例处理基线容量，并使用按需实例处理额外的容量。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Spot Instances",
      "EC2 Reserved Instances",
      "Amazon SQS",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 53%），解析仅供参考。】\n\n考查 EC2 实例的弹性伸缩能力，以及如何结合不同 EC2 实例类型（包括按需实例、预留实例和 Spot 实例）满足可变负载下的成本效益需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 提供了最经济高效的解决方案。预留实例能够为应用程序提供基线容量，降低长期运行成本。按需实例则能灵活地处理流量高峰，保证应用程序的持续运行，避免停机时间，并根据实际负载进行弹性伸缩。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 仅使用 Spot 实例，Spot 实例的波动性可能导致处理中断，不满足应用程序持续运行的需求。选项 B 仅使用预留实例，无法处理不可预测的流量峰值，导致容量不足。选项 C 结合了预留实例和 Spot 实例，虽然在成本上有所优化，但 Spot 实例的波动性仍然可能导致处理中断，可靠性不如按需实例。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 instances",
      "Amazon SQS",
      "Spot Instances",
      "Reserved Instances",
      "On-Demand Instances"
    ]
  },
  {
    "id": 168,
    "topic": "1",
    "question_en": "A security team wants to limit access to specific services or actions in all of the team’s AWS accounts. All accounts belong to a large organization in AWS Organizations. The solution must be scalable and there must be a single point where permissions can be maintained. What should a solutions architect do to accomplish this?",
    "options_en": {
      "A": "Create an ACL to provide access to the services or actions.",
      "B": "Create a security group to allow accounts and attach it to user groups.",
      "C": "Create cross-account roles in each account to deny access to the services or actions.",
      "D": "Create a service control policy in the root organizational unit to deny access to the services or actions."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "安全团队希望限制其所有 AWS 账户中对特定服务或操作的访问。所有账户都属于 AWS Organizations 中的一个大型组织。解决方案必须具有可扩展性，并且必须有一个可以维护权限的单点。解决方案架构师应该怎么做才能实现此目的？",
    "options_cn": {
      "A": "创建 ACL 以提供对服务或操作的访问权限。",
      "B": "创建安全组以允许账户并将其附加到用户组。",
      "C": "在每个账户中创建跨账户角色以拒绝访问服务或操作。",
      "D": "在根组织单元中创建服务控制策略以拒绝访问服务或操作。"
    },
    "tags": [
      "AWS Organizations",
      "Service Control Policies (SCPs)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查 AWS Organizations 中限制服务访问的策略实施。与 IAM 权限管理、账户结构、可扩展性设计相关。",
      "why_correct": "在根组织单元（Root OU）中创建 Service Control Policies (SCPs) 能够集中管理整个组织的服务访问权限。SCPs 是一种组织级别的策略，可以应用于组织中的所有账户或特定的组织单元（OU）。通过 SCPs，安全团队可以定义允许或拒绝的操作，从而限制对特定服务或操作的访问。这种方法具有可扩展性，因为策略应用于整个组织，并且权限维护在单点。",
      "why_wrong": "选项 A，ACL（Access Control Lists）通常用于控制 Amazon S3 存储桶的访问权限，不适用于整个 AWS 账户的服务访问限制，且管理粒度较小，不具备可扩展性，不满足题目要求。选项 B，安全组（Security Groups）用于控制 EC2 实例的网络流量，而不是控制对 AWS 服务的访问，与题目需求无关。选项 C，在每个账户中创建跨账户角色来拒绝访问，虽然也能达到拒绝的目的，但管理成本高，不具备单点维护的特性，不符合题目的可扩展性和集中管理的要求，管理复杂，且容易出错。"
    },
    "related_terms": [
      "AWS Organizations",
      "IAM",
      "ACL",
      "Amazon S3",
      "EC2",
      "OU",
      "Service Control Policies (SCPs)",
      "Security Groups"
    ]
  },
  {
    "id": 169,
    "topic": "1",
    "question_en": "A company is concerned about the security of its public web application due to recent web attacks. The application uses an Application Load Balancer (ALB). A solutions architect must reduce the risk of DDoS attacks against the application. What should the solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Add an Amazon Inspector agent to the ALB.",
      "B": "Configure Amazon Macie to prevent attacks.",
      "C": "Enable AWS Shield Advanced to prevent attacks.",
      "D": "Configure Amazon GuardDuty to monitor the ALB."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司担心其公共 Web 应用程序的安全，因为最近发生了网络攻击。该应用程序使用一个 Application Load Balancer (ALB)。一位解决方案架构师必须降低针对该应用程序的 DDoS 攻击风险。 解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "将 Amazon Inspector agent 添加到 ALB。",
      "B": "配置 Amazon Macie 以防止攻击。",
      "C": "启用 AWS Shield Advanced 以防止攻击。",
      "D": "配置 Amazon GuardDuty 来监控 ALB。"
    },
    "tags": [
      "AWS Shield Advanced",
      "DDoS Protection",
      "ALB",
      "Application Load Balancer",
      "DDoS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查针对 Web 应用程序的 DDoS 攻击防护方案，需要选择能够有效降低 DDoS 攻击风险的 AWS 服务。这与选择合适的安全服务、理解 DDoS 攻击的特点以及 AWS 各项安全服务的适用场景相关。",
      "why_correct": "启用 AWS Shield Advanced 可以提供针对 DDoS 攻击的全面防护。Shield Advanced 结合了基本的 DDoS 保护（自动启用）和高级保护功能，包括应用程序层攻击检测和缓解、主动监控以及 24x7 的 AWS 专家支持。这使其成为针对 Web 应用程序的首选方案，尤其是当应用程序使用 Application Load Balancer 时，因为它可以无缝集成。",
      "why_wrong": {
        "A": "Amazon Inspector 主要用于评估 EC2 实例的安全性，通过分析配置、漏洞等来识别安全问题。它不直接提供 DDoS 攻击防护，因此无法满足题目的需求。",
        "B": "Amazon Macie 侧重于数据安全，通过使用机器学习来发现和保护存储在 Amazon S3 中的敏感数据。它与 DDoS 攻击防护无关，无法解决此问题。",
        "D": "Amazon GuardDuty 是一种威胁检测服务，可以持续监控恶意活动和未经授权的行为。虽然 GuardDuty 可以检测到潜在的安全威胁，但它本身并不能直接缓解 DDoS 攻击，而是用于监控和告警，需要配合其他的安全措施（如 AWS Shield）进行防御。"
      }
    },
    "related_terms": [
      "Application Load Balancer",
      "ALB",
      "AWS Shield Advanced",
      "Amazon Inspector",
      "Amazon Macie",
      "Amazon GuardDuty",
      "DDoS",
      "EC2",
      "S3"
    ]
  },
  {
    "id": 170,
    "topic": "1",
    "question_en": "A company’s web application is running on Amazon EC2 instances behind an Application Load Balancer. The company recently changed its policy, which now requires the application to be accessed from one specific country only. Which configuration will meet this requirement?",
    "options_en": {
      "A": "Configure the security group for the EC2 instances.",
      "B": "Configure the security group on the Application Load Balancer.",
      "C": "Configure AWS WAF on the Application Load Balancer in a VPC.",
      "D": "Configure the network ACL for the subnet that contains the EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的 Web 应用程序正在 Application Load Balancer 后面的 Amazon EC2 实例上运行。该公司最近更改了其策略，现在要求仅从一个特定国家/地区访问该应用程序。哪种配置将满足此要求？",
    "options_cn": {
      "A": "配置 EC2 实例的安全组。",
      "B": "在 Application Load Balancer 上配置安全组。",
      "C": "在 VPC 中的 Application Load Balancer 上配置 AWS WAF。",
      "D": "配置包含 EC2 实例的子网的网络 ACL。"
    },
    "tags": [
      "Application Load Balancer",
      "AWS WAF",
      "Network ACL",
      "Security Group",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS WAF 进行地理位置限制。与安全组、Network ACL 的选型与区别相关。",
      "why_correct": "在 Application Load Balancer 上配置 AWS WAF 是满足需求的最佳方法。AWS WAF 允许您基于多种条件（包括地理位置）来控制对 Web 应用程序的访问。通过创建规则，您可以允许或阻止来自特定国家/地区的请求，从而满足公司仅从一个特定国家/地区访问应用程序的要求。",
      "why_wrong": "A. 配置 EC2 实例的安全组无法实现地理位置限制。安全组主要用于控制实例级别的入站和出站流量，基于 IP 地址、协议和端口，而不是地理位置。B. 在 Application Load Balancer 上配置安全组也无法实现地理位置限制，安全组作用于实例级别或负载均衡器级别，并不能识别用户的地理位置。D. 配置包含 EC2 实例的子网的网络 ACL 无法有效地实现地理位置限制。Network ACL 主要用于控制子网级别的入站和出站流量，虽然可以基于 IP 地址进行限制，但配置和维护起来较为复杂，且不如 AWS WAF 灵活，并且无法直接实现基于地理位置的访问控制。"
    },
    "related_terms": [
      "Application Load Balancer",
      "EC2",
      "AWS WAF",
      "VPC",
      "Network ACL",
      "Security Group"
    ]
  },
  {
    "id": 171,
    "topic": "1",
    "question_en": "A company provides an API to its users that automates inquiries for tax computations based on item prices. The company experiences a larger number of inquiries during the holiday season only that cause slower response times. A solutions architect needs to design a solution that is scalable and elastic. What should the solutions architect do to accomplish this?",
    "options_en": {
      "A": "Provide an API hosted on an Amazon EC2 instance. The EC2 instance performs the required computations when the API request is made.",
      "B": "Design a REST API using Amazon API Gateway that accepts the item names. API Gateway passes item names to AWS Lambda for tax computations.",
      "C": "Create an Application Load Balancer that has two Amazon EC2 instances behind it. The EC2 instances will compute the tax on the received item names.",
      "D": "Design a REST API using Amazon API Gateway that connects with an API hosted on an Amazon EC2 instance. API Gateway accepts and passes the item names to the EC2 instance for tax computations."
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一家公司为其用户提供一个 API，该 API 基于商品价格自动计算税务查询。该公司仅在假日季期间经历了大量查询，导致响应时间变慢。一位解决方案架构师需要设计一个可扩展且弹性的解决方案。该解决方案架构师应该怎么做才能实现此目标？",
    "options_cn": {
      "A": "提供一个托管在 Amazon EC2 实例上的 API。当发出 API 请求时，EC2 实例执行所需的计算。",
      "B": "使用 Amazon API Gateway 设计一个 REST API，该 API 接受商品名称。API Gateway 将商品名称传递给 AWS Lambda 进行税务计算。",
      "C": "创建一个 Application Load Balancer，其后有两个 Amazon EC2 实例。EC2 实例将计算收到的商品名称的税款。",
      "D": "使用 Amazon API Gateway 设计一个 REST API，该 API 连接到托管在 Amazon EC2 实例上的 API。API Gateway 接受并将商品名称传递给 EC2 实例进行税务计算。"
    },
    "tags": [
      "Amazon API Gateway",
      "Amazon EC2",
      "REST API",
      "AWS Lambda",
      "Application Load Balancer",
      "Scalability",
      "Elasticity"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n考查如何设计一个可扩展且弹性的 API 解决方案，以应对高峰期的请求量。",
      "why_correct": "选项 B 提供了最适合的解决方案。API Gateway 可以轻松处理大量 API 请求，并自动扩展以适应流量高峰。Lambda 函数提供无服务器计算，按需执行税务计算，进一步提高了可扩展性和弹性。这种组合能够高效地处理假日季的查询量。",
      "why_wrong": "选项 A 不可扩展，EC2 实例需要手动管理和扩展，无法快速响应流量高峰。选项 C 也使用了 EC2 实例，同样存在扩展问题，而且 Application Load Balancer 虽然可以进行负载均衡，但无法解决实例的扩展瓶颈。选项 D 虽然使用了 API Gateway，但依赖于 EC2 实例，限制了解决方案的弹性和可扩展性，并且增加了维护成本。"
    },
    "related_terms": [
      "Amazon EC2",
      "API",
      "REST API",
      "Amazon API Gateway",
      "AWS Lambda",
      "Application Load Balancer"
    ]
  },
  {
    "id": 172,
    "topic": "1",
    "question_en": "A solutions architect is creating a new Amazon CloudFront distribution for an application. Some of the information submitted by users is sensitive. The application uses HTTPS but needs another layer of security. The sensitive information should.be protected throughout the entire application stack, and access to the information should be restricted to certain applications. Which action should the solutions architect take?",
    "options_en": {
      "A": "Configure a CloudFront signed URL.",
      "B": "Configure a CloudFront signed cookie.",
      "C": "Configure a CloudFront field-level encryption profile.",
      "D": "Configure CloudFront and set the Origin Protocol Policy setting to HTTPS Only for the Viewer Protocol Policy."
    },
    "correct_answer": "C",
    "vote_percentage": "77%",
    "question_cn": "一位解决方案架构师正在为应用程序创建新的 Amazon CloudFront 分发。用户提交的一些信息是敏感的。该应用程序使用 HTTPS，但需要另一层安全性。 整个应用程序堆栈都应保护敏感信息，并且应限制对信息的访问，仅限某些应用程序。 解决方案架构师应采取哪项措施？",
    "options_cn": {
      "A": "配置 CloudFront 签名 URL。",
      "B": "配置 CloudFront 签名 cookie。",
      "C": "配置 CloudFront 字段级加密配置文件。",
      "D": "配置 CloudFront，并将源协议策略设置设置为“仅 HTTPS”以实现查看器协议策略。"
    },
    "tags": [
      "Amazon CloudFront",
      "CloudFront Signed URLs",
      "HTTPS",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 77%），解析仅供参考。】\n\n考查 CloudFront 保护敏感信息的配置，包括访问控制和加密。",
      "why_correct": "配置 CloudFront 字段级加密配置文件（Field-Level Encryption configuration）允许在用户提交数据时，在边缘位置加密特定数据字段。这确保了敏感信息在传输到源服务器之前就已经加密，满足了安全需求，限制了对信息的访问。",
      "why_wrong": "CloudFront 签名 URL 和签名 Cookie 侧重于限制对整个内容的访问，而不是保护内容内的特定敏感字段。它们主要用于控制用户是否可以访问内容，而非加密内容中的特定部分。源协议策略“仅 HTTPS”仅仅确保了安全传输，而不能提供额外的加密或访问控制来保护敏感信息。"
    },
    "related_terms": [
      "CloudFront",
      "HTTPS",
      "Field-Level Encryption configuration",
      "CloudFront signed URL",
      "CloudFront signed cookie",
      "Origin protocol policy"
    ]
  },
  {
    "id": 173,
    "topic": "1",
    "question_en": "A gaming company hosts a browser-based application on AWS. The users of the application consume a large number of videos and images that are stored in Amazon S3. This content is the same for all users. The application has increased in popularity, and millions of users worldwide accessing these media files. The company wants to provide the files to the users while reducing the load on the origin. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Deploy an AWS Global Accelerator accelerator in front of the web servers.",
      "B": "Deploy an Amazon CloudFront web distribution in front of the S3 bucket.",
      "C": "Deploy an Amazon ElastiCache for Redis instance in front of the web servers.",
      "D": "Deploy an Amazon ElastiCache for Memcached instance in front of the web servers."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家游戏公司在 AWS 上托管一个基于浏览器的应用程序。应用程序的用户会消耗大量存储在 Amazon S3 中的视频和图像。这些内容对所有用户都是相同的。该应用程序越来越受欢迎，全球有数百万用户访问这些媒体文件。该公司希望向用户提供文件，同时减少源的负载。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "在 Web 服务器前面部署一个 AWS Global Accelerator 加速器。",
      "B": "在 S3 存储桶前面部署一个 Amazon CloudFront Web 分发。",
      "C": "在 Web 服务器前面部署一个 Amazon ElastiCache for Redis 实例。",
      "D": "在 Web 服务器前面部署一个 Amazon ElastiCache for Memcached 实例。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "Content Delivery Network (CDN)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考查利用 CDN 服务优化静态内容分发；与 CloudFront、S3、Global Accelerator 的功能与选型相关。",
      "why_correct": "Amazon CloudFront 是一个内容分发网络（CDN）服务，能够将内容缓存到全球各地的边缘站点。对于静态内容（如视频和图像）的分发，CloudFront 是一个经济高效的解决方案。将 CloudFront 部署在 S3 存储桶前面，可以缓存这些静态内容，减轻 S3 存储桶的负载，并缩短用户访问文件的延迟，尤其是在全球用户访问的情况下。",
      "why_wrong": "选项 A，AWS Global Accelerator 主要优化 TCP 和 UDP 流量的性能，通过将流量路由到最佳 AWS 区域来提高性能，但它不是为内容缓存设计的，因此不如 CloudFront 经济高效地处理静态内容的分发。选项 C 和 D，Amazon ElastiCache 是一个内存缓存服务，用于加速数据库查询或其他动态内容的访问，不适合缓存 S3 中的静态媒体文件，而且成本相对较高。ElastiCache 部署在 Web 服务器前面，不能像 CloudFront 一样在全球范围内缓存内容，从而无法有效降低源站负载和用户访问延迟。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "AWS Global Accelerator",
      "Amazon ElastiCache for Redis",
      "CDN",
      "Amazon ElastiCache for Memcached",
      "Web Server"
    ]
  },
  {
    "id": 174,
    "topic": "1",
    "question_en": "A company has a multi-tier application that runs six front-end web servers in an Amazon EC2 Auto Scaling group in a single Availability Zone behind an Application Load Balancer (ALB). A solutions architect needs to modify the infrastructure to be highly available without modifying the application. Which architecture should the solutions architect choose that provides high availability?",
    "options_en": {
      "A": "Create an Auto Scaling group that uses three instances across each of two Regions.",
      "B": "Modify the Auto Scaling group to use three instances across each of two Availability Zones.",
      "C": "Create an Auto Scaling template that can be used to quickly create more instances in another Region.",
      "D": "Change the ALB in front of the Amazon EC2 instances in a round-robin configuration to balance trafic to the web tier."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个多层应用程序，该应用程序在单个可用区中的 Amazon EC2 Auto Scaling 组中运行六个前端 Web 服务器，这些服务器位于 Application Load Balancer (ALB) 之后。解决方案架构师需要修改基础设施以实现高可用性，而无需修改应用程序。解决方案架构师应该选择哪种架构来提供高可用性？",
    "options_cn": {
      "A": "创建一个 Auto Scaling 组，该组在两个区域中的每个区域使用三个实例。",
      "B": "修改 Auto Scaling 组以使用在两个可用区中的每个可用区中使用三个实例。",
      "C": "创建一个 Auto Scaling 模板，该模板可用于在另一个区域中快速创建更多实例。",
      "D": "将 Amazon EC2 实例前面的 ALB 更改为循环配置，以平衡 Web 层流量。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer (ALB)",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察在现有应用架构下实现高可用性的方案选择，与 Auto Scaling 组的配置、可用区和区域的概念密切相关。",
      "why_correct": "修改 Auto Scaling 组以跨越两个可用区部署实例，每个可用区部署三个实例，是实现高可用性的有效方法。 这种方法利用了 AWS 的基础设施优势，通过在不同可用区部署实例，确保即使一个可用区发生故障，应用仍然可以继续运行。 ALB 自动将流量路由到健康的实例，从而实现自动故障转移。",
      "why_wrong": "A 选项创建跨区域的 Auto Scaling 组，虽然可以实现更高的可用性，但涉及跨区域的数据传输和部署，会引入更高的复杂性和潜在的延迟，且与题干中“无需修改应用程序”的约束冲突。C 选项创建 Auto Scaling 模板，可以加速实例的部署，但它本身并不能提供高可用性，它只是一个部署工具。D 选项更改 ALB 的配置为循环，这并不能真正解决可用区级别的故障问题，无法提供高可用性，因为它仍然在单个可用区内进行流量分配，如果可用区不可用，整个应用程序将受到影响。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Availability Zone",
      "EC2",
      "ALB",
      "Application Load Balancer (ALB)",
      "Region"
    ]
  },
  {
    "id": 175,
    "topic": "1",
    "question_en": "An ecommerce company has an order-processing application that uses Amazon API Gateway and an AWS Lambda function. The application stores data in an Amazon Aurora PostgreSQL database. During a recent sales event, a sudden surge in customer orders occurred. Some customers experienced timeouts, and the application did not process the orders of those customers. A solutions architect determined that the CPU utilization and memory utilization were high on the database because of a large number of open connections. The solutions architect needs to prevent the timeout errors while making the least possible changes to the application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure provisioned concurrency for the Lambda function. Modify the database to be a global database in multiple AWS Regions.",
      "B": "Use Amazon RDS Proxy to create a proxy for the database. Modify the Lambda function to use the RDS Proxy endpoint instead of the database endpoint.",
      "C": "Create a read replica for the database in a different AWS Region. Use query string parameters in API Gateway to route trafic to the read replica.",
      "D": "Migrate the data from Aurora PostgreSQL to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS). Modify the Lambda function to use the DynamoDB table."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司有一个订单处理应用程序，该应用程序使用 Amazon API Gateway 和一个 AWS Lambda 函数。该应用程序将数据存储在 Amazon Aurora PostgreSQL 数据库中。在最近的一次促销活动中，客户订单突然激增。一些客户遇到了超时，并且该应用程序没有处理这些客户的订单。一位解决方案架构师确定，由于大量打开的连接，数据库的 CPU 利用率和内存利用率很高。解决方案架构师需要防止超时错误，同时对应用程序进行尽可能少的更改。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Lambda 函数配置预置并发。将数据库修改为多个 AWS 区域中的全局数据库。",
      "B": "使用 Amazon RDS Proxy 为数据库创建代理。修改 Lambda 函数以使用 RDS Proxy 终端节点而不是数据库终端节点。",
      "C": "在不同的 AWS 区域中为数据库创建一个只读副本。使用 API Gateway 中的查询字符串参数将流量路由到只读副本。",
      "D": "使用 AWS Database Migration Service (AWS DMS) 将数据从 Aurora PostgreSQL 迁移到 Amazon DynamoDB。修改 Lambda 函数以使用 DynamoDB 表。"
    },
    "tags": [
      "Amazon RDS Proxy",
      "AWS Lambda",
      "Amazon Aurora PostgreSQL",
      "API Gateway",
      "Database Connection Management"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何解决数据库连接瓶颈问题，尤其是在 Lambda 函数和 Aurora PostgreSQL 数据库的场景下。涉及到 RDS Proxy 的使用，以及与 API Gateway、Lambda 函数、数据库连接管理相关的知识。",
      "why_correct": "Amazon RDS Proxy 是一个完全托管的数据库代理，可以帮助提高数据库应用程序的可扩展性、应用程序弹性并提高安全性。通过 RDS Proxy，Lambda 函数可以复用数据库连接，减少数据库连接数，从而缓解数据库连接耗尽的问题，提高 CPU 和内存利用率。 修改 Lambda 函数以使用 RDS Proxy 终端节点，可以最大程度地减少对应用程序的更改，同时有效地解决数据库过载的问题，防止超时错误。",
      "why_wrong": "A.  为 Lambda 函数配置预置并发可以提高函数处理请求的能力，但并不能解决数据库连接数量过多导致的问题。将数据库修改为全局数据库会增加复杂性，并且与解决数据库连接问题无直接关系。C. 创建只读副本可以提高读取性能，但无法解决写操作的性能问题，也无法解决 Lambda 函数连接数据库的瓶颈。通过 API Gateway 中的查询字符串参数将流量路由到只读副本，不能解决写入问题，反而引入数据一致性问题。D.  将 Aurora PostgreSQL 迁移到 DynamoDB 会涉及数据库技术的重大变更，需要修改 Lambda 函数以使用 DynamoDB，这与“对应用程序进行尽可能少的更改”的要求相悖。而且，DynamoDB 适用于 key-value 和文档存储，不一定能满足电子商务应用程序的订单处理需求。 同时，迁移过程复杂，耗时较长，并不是快速解决数据库连接问题的有效方案。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon Aurora PostgreSQL",
      "Amazon RDS Proxy",
      "DynamoDB",
      "AWS DMS",
      "RDS",
      "EC2",
      "EBS",
      "Pre-provisioned Concurrency"
    ]
  },
  {
    "id": 176,
    "topic": "1",
    "question_en": "An application runs on Amazon EC2 instances in private subnets. The application needs to access an Amazon DynamoDB table. What is the MOST secure way to access the table while ensuring that the trafic does not leave the AWS network?",
    "options_en": {
      "A": "Use a VPC endpoint for DynamoDB.",
      "B": "Use a NAT gateway in a public subnet.",
      "C": "Use a NAT instance in a private subnet.",
      "D": "Use the internet gateway attached to the VPC."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一个应用程序在私有子网中的 Amazon EC2 实例上运行。该应用程序需要访问 Amazon DynamoDB 表。在确保流量不会离开 AWS 网络的情况下，访问该表的最安全方式是什么？",
    "options_cn": {
      "A": "为 DynamoDB 使用 VPC endpoint。",
      "B": "在公有子网中使用 NAT gateway。",
      "C": "在私有子网中使用 NAT 实例。",
      "D": "使用连接到 VPC 的 internet gateway。"
    },
    "tags": [
      "Amazon DynamoDB",
      "VPC",
      "VPC Endpoint",
      "NAT Gateway",
      "NAT Instance",
      "Internet Gateway",
      "Amazon EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在VPC环境下访问DynamoDB，并保证流量不离开AWS网络的安全方案选择。",
      "why_correct": "VPC endpoint for DynamoDB 允许私有子网中的EC2实例通过私有连接访问DynamoDB，而无需通过Internet。流量在 AWS 网络内部传输，提高了安全性和性能。这种方式避免了使用 Internet Gateway 或 NAT 设备。",
      "why_wrong": "B、C选项都涉及 NAT 网关或 NAT 实例，虽然允许私有子网访问 Internet，但会使流量离开 AWS 网络，增加了安全风险，且增加了运维成本。D选项使用 Internet Gateway 直接将流量暴露到 Internet，违反了题目中“确保流量不会离开 AWS 网络”的要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "VPC",
      "DynamoDB",
      "VPC endpoint",
      "NAT gateway",
      "NAT instance",
      "Internet gateway"
    ]
  },
  {
    "id": 177,
    "topic": "1",
    "question_en": "An entertainment company is using Amazon DynamoDB to store media metadata. The application is read intensive and experiencing delays. The company does not have staff to handle additional operational overhead and needs to improve the performance eficiency of DynamoDB without reconfiguring the application. What should a solutions architect recommend to meet this requirement?",
    "options_en": {
      "A": "Use Amazon ElastiCache for Redis.",
      "B": "Use Amazon DynamoDB Accelerator (DAX).",
      "C": "Replicate data by using DynamoDB global tables.",
      "D": "Use Amazon ElastiCache for Memcached with Auto Discovery enabled."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家娱乐公司正在使用 Amazon DynamoDB 存储媒体元数据。该应用程序是读取密集型的，并且遇到了延迟。该公司没有人员来处理额外的运营开销，并且需要提高 DynamoDB 的性能效率，而无需重新配置应用程序。解决方案架构师应该推荐什么来满足此要求？",
    "options_cn": {
      "A": "使用 Amazon ElastiCache for Redis。",
      "B": "使用 Amazon DynamoDB Accelerator (DAX)。",
      "C": "使用 DynamoDB 全局表复制数据。",
      "D": "使用启用了自动发现的 Amazon ElastiCache for Memcached。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DAX",
      "Caching",
      "ElastiCache",
      "Global Tables"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 DynamoDB 性能优化方案；与 DAX、ElastiCache (Redis, Memcached)、DynamoDB Global Tables 的选型与对比相关。",
      "why_correct": "Amazon DynamoDB Accelerator (DAX) 是一个 DynamoDB 的缓存服务，可以帮助读取密集型应用程序提高性能。 DAX 可以在 DynamoDB 之前缓存数据，从而减少对 DynamoDB 的读取请求次数，降低延迟。由于题目要求“无需重新配置应用程序”，DAX 与 DynamoDB 兼容，只需进行少量配置即可实现，满足了题目需求。",
      "why_wrong": "A. Amazon ElastiCache for Redis 是一个缓存服务，但它需要应用程序进行修改，才能将数据写入和读取 Redis。这与题目“无需重新配置应用程序”的要求不符。\nC. DynamoDB 全局表用于跨多个 AWS 区域复制 DynamoDB 表，主要用于提高应用程序的可用性和降低延迟（针对跨区域用户）。它并不能直接提高单个区域内的读取性能，与题目要求的“提高 DynamoDB 的性能效率”不符。\nD. Amazon ElastiCache for Memcached 类似于 ElastiCache for Redis，需要修改应用程序才能使用，与题目要求“无需重新配置应用程序”的要求不符。Memcached 也不像 DAX 那样专门针对 DynamoDB 进行优化。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DAX",
      "ElastiCache for Redis",
      "ElastiCache for Memcached",
      "Global Tables"
    ]
  },
  {
    "id": 178,
    "topic": "1",
    "question_en": "A company’s infrastructure consists of Amazon EC2 instances and an Amazon RDS DB instance in a single AWS Region. The company wants to back up its data in a separate Region. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Backup to copy EC2 backups and RDS backups to the separate Region.",
      "B": "Use Amazon Data Lifecycle Manager (Amazon DLM) to copy EC2 backups and RDS backups to the separate Region.",
      "C": "Create Amazon Machine Images (AMIs) of the EC2 instances. Copy the AMIs to the separate Region. Create a read replica for the RDS DB instance in the separate Region.",
      "D": "Create Amazon Elastic Block Store (Amazon EBS) snapshots. Copy the EBS snapshots to the separate Region. Create RDS snapshots. Export the RDS snapshots to Amazon S3. Configure S3 Cross-Region Replication (CRR) to the separate Region."
    },
    "correct_answer": "A",
    "vote_percentage": "97%",
    "question_cn": "一家公司的基础设施包括单个 AWS 区域中的 Amazon EC2 实例和 Amazon RDS 数据库实例。该公司希望将其数据备份到另一个区域。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Backup 将 EC2 备份和 RDS 备份复制到单独的区域。",
      "B": "使用 Amazon Data Lifecycle Manager (Amazon DLM) 将 EC2 备份和 RDS 备份复制到单独的区域。",
      "C": "创建 EC2 实例的 Amazon Machine Images (AMI)。将 AMI 复制到单独的区域。在单独的区域中为 RDS 数据库实例创建只读副本。",
      "D": "创建 Amazon Elastic Block Store (Amazon EBS) 快照。将 EBS 快照复制到单独的区域。创建 RDS 快照。将 RDS 快照导出到 Amazon S3。配置 S3 跨区域复制 (CRR) 到单独的区域。"
    },
    "tags": [
      "AWS Backup",
      "Amazon EC2",
      "Amazon RDS",
      "Cross-Region Replication"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 97%），解析仅供参考。】\n\n本题考查在 AWS 上备份和复制数据，特别是 EC2 实例和 RDS 数据库到不同区域的解决方案，以及如何以最小的运营开销实现。涉及 AWS Backup 的使用、数据复制策略以及与其他服务（如 DLM、AMI、EBS 快照、RDS 快照、S3 跨区域复制）的对比。",
      "why_correct": "AWS Backup 是一种集中式备份服务，支持多种 AWS 服务的备份和恢复，包括 EC2 和 RDS。使用 AWS Backup，可以轻松配置跨区域备份，将 EC2 实例的备份和 RDS 数据库的备份复制到另一个区域。这种方法简化了备份管理，减少了手动操作，并降低了运营开销。它提供了集中管理、自动化备份和跨区域复制的功能，满足了题目要求的以最少运营开销满足需求的条件。",
      "why_wrong": "选项 B 错误，因为 Amazon Data Lifecycle Manager (DLM) 主要用于管理 Amazon EBS 快照的生命周期，无法直接支持 RDS 数据库的备份复制到不同区域。虽然 DLM 可以创建和管理 EBS 快照，但它并不提供全面的跨区域备份解决方案，尤其是在同时备份 EC2 实例和 RDS 数据库时，操作复杂度增加。选项 C 错误，虽然可以通过 AMI 复制和创建 RDS 只读副本实现跨区域备份，但这种方法不如 AWS Backup 自动化，并且在 RDS 上创建只读副本会带来额外的管理和成本，不如 AWS Backup 提供的备份管理方案简单高效。选项 D 错误，虽然 EBS 快照和 RDS 快照都可以进行跨区域复制，但这种方法需要手动操作，例如导出 RDS 快照到 S3 并配置 S3 跨区域复制。相比 AWS Backup，这种方法需要更多的配置和管理，增加了运营开销和复杂性，且涉及了多个服务，增加了出错的可能性。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "AWS Backup",
      "AMI",
      "Amazon Elastic Block Store",
      "Amazon EBS",
      "S3",
      "CRR",
      "RDS",
      "EC2",
      "Amazon Data Lifecycle Manager",
      "Amazon DLM",
      "Amazon Machine Images",
      "Cross-Region Replication"
    ]
  },
  {
    "id": 179,
    "topic": "1",
    "question_en": "A solutions architect needs to securely store a database user name and password that an application uses to access an Amazon RDS DB instance. The application that accesses the database runs on an Amazon EC2 instance. The solutions architect wants to create a secure parameter in AWS Systems Manager Parameter Store. What should the solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Create an IAM role that has read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM role to the EC2 instance.",
      "B": "Create an IAM policy that allows read access to the Parameter Store parameter. Allow Decrypt access to an AWS Key Management Service (AWS KMS) key that is used to encrypt the parameter. Assign this IAM policy to the EC2 instance.",
      "C": "Create an IAM trust relationship between the Parameter Store parameter and the EC2 instance. Specify Amazon RDS as a principal in the trust policy.",
      "D": "Create an IAM trust relationship between the DB instance and the EC2 instance. Specify Systems Manager as a principal in the trust policy."
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一名解决方案架构师需要安全地存储一个应用程序用来访问 Amazon RDS 数据库实例的数据库用户名和密码。访问数据库的应用程序在 Amazon EC2 实例上运行。解决方案架构师希望在 AWS Systems Manager Parameter Store 中创建一个安全参数。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "创建一个 IAM 角色，该角色具有对 Parameter Store 参数的读取权限。允许解密访问用于加密该参数的 AWS Key Management Service (AWS KMS) 密钥。将此 IAM 角色分配给 EC2 实例。",
      "B": "创建一个 IAM 策略，该策略允许对 Parameter Store 参数的读取权限。允许解密访问用于加密该参数的 AWS Key Management Service (AWS KMS) 密钥。将此 IAM 策略分配给 EC2 实例。",
      "C": "在 Parameter Store 参数和 EC2 实例之间创建 IAM 信任关系。在信任策略中指定 Amazon RDS 作为委托人。",
      "D": "在数据库实例和 EC2 实例之间创建 IAM 信任关系。在信任策略中指定 Systems Manager 作为委托人。"
    },
    "tags": [
      "AWS Systems Manager",
      "Parameter Store",
      "IAM",
      "EC2",
      "IAM Role",
      "AWS KMS",
      "IAM Policy",
      "IAM Trust Relationship"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n考查如何在 EC2 实例上安全地访问 Parameter Store 中的秘密信息，涉及 IAM 角色、IAM 策略和 KMS 密钥的使用。需要关注 IAM 授权的范围和机制，以及如何将权限授予 EC2 实例。",
      "why_correct": "正确答案是创建一个 IAM 角色，并将其分配给 EC2 实例。该 IAM 角色需要具有读取 Parameter Store 参数的权限，并被允许解密用于加密该参数的 KMS 密钥。由于 EC2 实例上的应用程序需要访问 RDS 数据库的凭据，而这些凭据存储在 Parameter Store 中。通过将 IAM 角色分配给 EC2 实例，应用程序在 EC2 实例上运行时，就可以使用与该角色关联的凭据安全地访问 Parameter Store。",
      "why_wrong": "选项 B 错误，因为将 IAM 策略直接分配给 EC2 实例是错误的。IAM 策略应用于 IAM 用户、组或角色，而不是 EC2 实例本身。EC2 实例通过 IAM 角色获得访问权限。选项 C 和 D 错误，因为 IAM 信任关系用于定义谁可以扮演 IAM 角色，或者 AWS 服务代表客户执行操作。本题的应用场景是 EC2 实例需要访问 Parameter Store 中的秘密信息，不涉及扮演角色，也不需要设置 AWS 服务作为委托人。因此，C 和 D 的方向都错误，与安全访问 Parameter Store 数据库凭据的需求不匹配。"
    },
    "related_terms": [
      "AWS Systems Manager",
      "Parameter Store",
      "IAM",
      "EC2",
      "RDS",
      "AWS KMS",
      "IAM Role",
      "IAM Policy",
      "IAM Trust Relationship"
    ]
  },
  {
    "id": 180,
    "topic": "1",
    "question_en": "A company is designing a cloud communications platform that is driven by APIs. The application is hosted on Amazon EC2 instances behind a Network Load Balancer (NLB). The company uses Amazon API Gateway to provide external users with access to the application through APIs. The company wants to protect the platform against web exploits like SQL injection and also wants to detect and mitigate large, sophisticated DDoS attacks. Which combination of solutions provides the MOST protection? (Choose two.)",
    "options_en": {
      "A": "Use AWS WAF to protect the NLB.",
      "B": "Use AWS Shield Advanced with the NLB.",
      "C": "Use AWS WAF to protect Amazon API Gateway.",
      "D": "Use Amazon GuardDuty with AWS Shield Standard",
      "E": "Use AWS Shield Standard with Amazon API Gateway."
    },
    "correct_answer": "BC",
    "vote_percentage": "93%",
    "question_cn": "一家公司正在设计一个由 API 驱动的云通信平台。该应用程序托管在 Network Load Balancer (NLB) 后面的 Amazon EC2 实例上。该公司使用 Amazon API Gateway 通过 API 为外部用户提供对应用程序的访问。该公司希望保护平台免受 SQL 注入等 Web 攻击，并检测和缓解大型、复杂的 DDoS 攻击。哪种解决方案组合提供了最大的保护？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS WAF 保护 NLB。",
      "B": "将 AWS Shield Advanced 与 NLB 一起使用。",
      "C": "使用 AWS WAF 保护 Amazon API Gateway。",
      "D": "将 Amazon GuardDuty 与 AWS Shield Standard 一起使用。",
      "E": "将 AWS Shield Standard 与 Amazon API Gateway 一起使用。"
    },
    "tags": [
      "AWS WAF",
      "Amazon API Gateway",
      "Network Load Balancer",
      "Amazon EC2",
      "AWS Shield Advanced",
      "DDoS",
      "SQL injection",
      "Amazon GuardDuty",
      "AWS Shield Standard"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 93%），解析仅供参考。】\n\n本题考查 Web 应用程序的保护，主要涉及 DDoS 攻击和 SQL 注入等安全威胁的防御。与 AWS WAF、AWS Shield、API Gateway、Network Load Balancer 的组合使用有关。",
      "why_correct": "AWS Shield Advanced 专为防御大型、复杂的 DDoS 攻击而设计，提供了高级的保护功能。将 Shield Advanced 与 Network Load Balancer (NLB) 一起使用，可以直接保护 NLB 及其后的 EC2 实例免受 DDoS 攻击，符合题目的要求。",
      "why_wrong": "A. 使用 AWS WAF 保护 NLB 提供了针对 Web 攻击的保护，但没有涵盖应对大型 DDoS 攻击的全面防护。C. 使用 AWS WAF 保护 Amazon API Gateway 提供了对 API 流量的保护，但 API Gateway 不直接与 NLB 关联，无法直接保护 NLB 后面的 EC2 实例。D. Amazon GuardDuty 主要用于威胁检测，无法直接缓解 DDoS 攻击，且与 AWS Shield Standard 提供的保护有限。E. AWS Shield Standard 提供了基本的 DDoS 保护，但无法满足抵御大型、复杂的 DDoS 攻击的需求，而 API Gateway 本身不直接受到 DDoS 攻击。同时，该选项未保护 NLB 后的 EC2 实例。"
    },
    "related_terms": [
      "AWS WAF",
      "Amazon API Gateway",
      "Network Load Balancer",
      "Amazon EC2",
      "AWS Shield Advanced",
      "DDoS",
      "Amazon GuardDuty",
      "NLB",
      "SQL injection",
      "AWS Shield Standard"
    ]
  },
  {
    "id": 181,
    "topic": "1",
    "question_en": "A company has a legacy data processing application that runs on Amazon EC2 instances. Data is processed sequentially, but the order of results does not matter. The application uses a monolithic architecture. The only way that the company can scale the application to meet increased demand is to increase the size of the instances. The company’s developers have decided to rewrite the application to use a microservices architecture on Amazon Elastic Container Service (Amazon ECS). What should a solutions architect recommend for communication between the microservices?",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Add code to the data producers, and send data to the queue. Add code to the data consumers to process data from the queue.",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Add code to the data producers, and publish notifications to the topic. Add code to the data consumers to subscribe to the topic.",
      "C": "Create an AWS Lambda function to pass messages. Add code to the data producers to call the Lambda function with a data object. Add code to the data consumers to receive a data object that is passed from the Lambda function.",
      "D": "Create an Amazon DynamoDB table. Enable DynamoDB Streams. Add code to the data producers to insert data into the table. Add code to the data consumers to use the DynamoDB Streams API to detect new table entries and retrieve the data."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司有一个在 Amazon EC2 实例上运行的传统数据处理应用程序。数据被顺序处理，但结果的顺序无关紧要。该应用程序使用单体架构。该公司扩展应用程序以满足不断增长的需求的唯一方法是增加实例的大小。该公司的开发人员已决定重写该应用程序，以便在 Amazon Elastic Container Service (Amazon ECS) 上使用微服务架构。解决方案架构师应该为微服务之间的通信推荐什么？",
    "options_cn": {
      "A": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。将代码添加到数据生成者，并将数据发送到队列。将代码添加到数据消费者以处理来自队列的数据。",
      "B": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题。将代码添加到数据生成者，并将通知发布到该主题。将代码添加到数据消费者以订阅该主题。",
      "C": "创建一个 AWS Lambda 函数来传递消息。将代码添加到数据生成者以使用数据对象调用 Lambda 函数。将代码添加到数据消费者以接收从 Lambda 函数传递的数据对象。",
      "D": "创建一个 Amazon DynamoDB 表。打开 DynamoDB Streams。将代码添加到数据生成者以将数据插入表中。将代码添加到数据消费者以使用 DynamoDB Streams API 检测新的表条目并检索数据。"
    },
    "tags": [
      "Amazon SQS",
      "Amazon ECS",
      "Microservices"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n考查在微服务架构中，服务间通信的推荐方案，以及与解耦、异步处理相关。涉及到 Amazon SQS、Amazon SNS、AWS Lambda 和 Amazon DynamoDB 的选型与对比。",
      "why_correct": "Amazon SQS 是一种完全托管的消息队列服务，非常适合微服务之间的异步通信。通过使用 SQS 队列，数据生成者（生产者）可以将数据放入队列，数据消费者（消费者）从队列中获取数据进行处理。这种方式实现了生产者和消费者的解耦，提高系统的可伸缩性和弹性。在微服务架构中，这种异步通信模式尤其重要，因为它允许各个服务独立扩展和更新。",
      "why_wrong": "选项 B (Amazon SNS) 适用于发布/订阅模式，主要用于扇出（fan-out）场景，一个消息发布到主题，多个订阅者会收到该消息。虽然 SNS 也可以用于服务间通信，但它更适合于通知场景，而不是可靠的数据传递和处理。选项 C (AWS Lambda) 用于传递消息，引入了额外的复杂性，需要创建和管理 Lambda 函数，并且 Lambda 函数可能存在冷启动延迟，不适用于需要高吞吐量的场景。选项 D (Amazon DynamoDB) 和 DynamoDB Streams 主要用于数据库变更的实时通知，而非服务间可靠的消息传递。虽然 DynamoDB Streams 可以用于触发 Lambda 函数，但这不是服务间通信的最佳实践，相比 SQS，其复杂性更高，且不具备消息队列的解耦特性。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon ECS",
      "Amazon SQS",
      "Amazon SNS",
      "AWS Lambda",
      "Amazon DynamoDB",
      "DynamoDB Streams",
      "Microservices"
    ]
  },
  {
    "id": 182,
    "topic": "1",
    "question_en": "A company wants to migrate its MySQL database from on premises to AWS. The company recently experienced a database outage that significantly impacted the business. To ensure this does not happen again, the company wants a reliable database solution on AWS that minimizes data loss and stores every transaction on at least two nodes. Which solution meets these requirements?",
    "options_en": {
      "A": "Create an Amazon RDS DB instance with synchronous replication to three nodes in three Availability Zones.",
      "B": "Create an Amazon RDS MySQL DB instance with Multi-AZ functionality enabled to synchronously replicate the data.",
      "C": "Create an Amazon RDS MySQL DB instance and then create a read replica in a separate AWS Region that synchronously replicates the data.",
      "D": "Create an Amazon EC2 instance with a MySQL engine installed that triggers an AWS Lambda function to synchronously replicate the data to an Amazon RDS MySQL DB instance."
    },
    "correct_answer": "B",
    "vote_percentage": "97%",
    "question_cn": "一家公司希望将其 MySQL 数据库从本地迁移到 AWS。该公司最近经历了数据库中断，这对业务造成了重大影响。为了确保这种情况不再发生，该公司希望在 AWS 上获得一个可靠的数据库解决方案，该方案最大限度地减少数据丢失，并在至少两个节点上存储每笔交易。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon RDS 数据库实例，并与三个可用区中的三个节点进行同步复制。",
      "B": "创建一个启用了多可用区功能的 Amazon RDS MySQL 数据库实例，以同步复制数据。",
      "C": "创建一个 Amazon RDS MySQL 数据库实例，然后在另一个 AWS 区域中创建一个只读副本，该副本同步复制数据。",
      "D": "创建一个安装了 MySQL 引擎的 Amazon EC2 实例，该实例触发一个 AWS Lambda 函数，将数据同步复制到 Amazon RDS MySQL 数据库实例。"
    },
    "tags": [
      "Amazon RDS",
      "MySQL",
      "Multi-AZ",
      "High Availability",
      "Disaster Recovery"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 97%），解析仅供参考。】\n\n考查 Amazon RDS 的高可用性和灾难恢复能力，以及如何通过多可用区部署和数据复制来保证数据库的可靠性。涉及 RDS 的部署策略、数据同步复制机制，以及与数据库中断恢复相关的知识。",
      "why_correct": "选项 B 提供了最可靠的解决方案。启用 Multi-AZ 功能的 Amazon RDS MySQL 数据库实例会自动在不同的可用区（Availability Zone, AZ）中创建备用数据库实例。数据在主实例和备用实例之间通过同步复制进行复制，确保了数据冗余和高可用性。当主实例发生故障时，RDS 会自动故障转移到备用实例，从而最大限度地减少停机时间并满足业务对数据丢失的最小化需求。",
      "why_wrong": "选项 A 错误，因为尽管使用三个可用区中的三个节点进行了同步复制，但它没有明确说明是否为 RDS 实例。如果不是 RDS，需要手动管理数据库，包括配置、维护和故障转移，增加了复杂性，不符合题目的“可靠的数据库解决方案”要求。选项 C 错误，虽然在另一个 AWS 区域中创建只读副本可以提供灾难恢复能力，但只读副本通常是异步复制，无法满足“在至少两个节点上存储每笔交易”的需求，并且跨区域的复制延迟可能导致数据丢失。选项 D 错误，虽然使用 EC2 实例和 Lambda 函数可以将数据复制到 RDS，但这是一种自定义解决方案，增加了管理和维护的复杂性，不如 RDS 内置的 Multi-AZ 功能可靠和高效。这种方案也增加了停机风险，且无法保证同步复制，不满足最小化数据丢失的要求。"
    },
    "related_terms": [
      "Amazon RDS",
      "MySQL",
      "Multi-AZ",
      "Amazon EC2",
      "AWS Lambda",
      "Availability Zone",
      "RDS"
    ]
  },
  {
    "id": 183,
    "topic": "1",
    "question_en": "A company is building a new dynamic ordering website. The company wants to minimize server maintenance and patching. The website must be highly available and must scale read and write capacity as quickly as possible to meet changes in user demand. Which solution will meet these requirements?",
    "options_en": {
      "A": "Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon DynamoDB with on-demand capacity for the database. Configure Amazon CloudFront to deliver the website content.",
      "B": "Host static content in Amazon S3. Host dynamic content by using Amazon API Gateway and AWS Lambda. Use Amazon Aurora with Aurora Auto Scaling for the database. Configure Amazon CloudFront to deliver the website content.",
      "C": "Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute trafic. Use Amazon DynamoDB with provisioned write capacity for the database.",
      "D": "Host all the website content on Amazon EC2 instances. Create an Auto Scaling group to scale the EC2 instances. Use an Application Load Balancer to distribute trafic. Use Amazon Aurora with Aurora Auto Scaling for the database."
    },
    "correct_answer": "A",
    "vote_percentage": "94%",
    "question_cn": "一家公司正在构建一个新的动态订购网站。该公司希望最大限度地减少服务器维护和补丁。该网站必须具有高可用性，并且必须尽可能快地扩展读写容量，以满足用户需求的变化。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将静态内容托管在 Amazon S3 中。 使用 Amazon API Gateway 和 AWS Lambda 托管动态内容。 使用具有按需容量的 Amazon DynamoDB 作为数据库。 配置 Amazon CloudFront 交付网站内容。",
      "B": "将静态内容托管在 Amazon S3 中。 使用 Amazon API Gateway 和 AWS Lambda 托管动态内容。 使用具有 Aurora 自动缩放功能的 Amazon Aurora 作为数据库。 配置 Amazon CloudFront 交付网站内容。",
      "C": "将所有网站内容托管在 Amazon EC2 实例上。 创建一个 Auto Scaling 组来扩展 EC2 实例。 使用 Application Load Balancer 分配流量。 使用具有预置写入容量的 Amazon DynamoDB 作为数据库。",
      "D": "将所有网站内容托管在 Amazon EC2 实例上。 创建一个 Auto Scaling 组来扩展 EC2 实例。 使用 Application Load Balancer 分配流量。 使用具有 Aurora 自动缩放功能的 Amazon Aurora 作为数据库。"
    },
    "tags": [
      "Amazon S3",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon CloudFront",
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 94%），解析仅供参考。】\n\n本题考查无服务器（serverless）架构和可扩展性。重点考察如何选择合适的 AWS 服务，以实现高可用性、快速扩展和最小化服务器维护。涉及 Amazon S3、Amazon API Gateway、AWS Lambda、Amazon DynamoDB、Amazon CloudFront 等服务的组合使用，以及与 Amazon EC2、Amazon Aurora 等传统服务的对比。",
      "why_correct": "选项 A 采用了完全无服务器的架构。将静态内容托管在 Amazon S3 上，利用其高可用性和耐久性。使用 Amazon API Gateway 和 AWS Lambda 处理动态内容，免去了服务器维护的负担，并能根据流量自动扩展。Amazon DynamoDB 提供按需容量，可以根据实际负载动态调整，满足读写容量快速扩展的需求。Amazon CloudFront 用于内容分发，进一步提升网站的性能。",
      "why_wrong": "选项 B 虽然使用了 Amazon S3、Amazon API Gateway、AWS Lambda 和 Amazon CloudFront，但在数据库的选择上使用了 Amazon Aurora。虽然 Amazon Aurora 提供了自动伸缩功能，但与 DynamoDB 相比，仍然需要维护数据库实例，增加了复杂性。选项 C 和 D 都基于 Amazon EC2，需要维护服务器，不符合题目中“最大限度地减少服务器维护和补丁”的要求。此外，选项 C 使用了预置写入容量的 DynamoDB，无法完全匹配按需扩展的需求；选项 D 使用了 Aurora，虽然支持自动伸缩，但依旧需要服务器维护，且成本可能高于无服务器方案。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon CloudFront",
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon Aurora"
    ]
  },
  {
    "id": 184,
    "topic": "1",
    "question_en": "A company has an AWS account used for software engineering. The AWS account has access to the company’s on-premises data center through a pair of AWS Direct Connect connections. All non-VPC trafic routes to the virtual private gateway. A development team recently created an AWS Lambda function through the console. The development team needs to allow the function to access a database that runs in a private subnet in the company’s data center. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the Lambda function to run in the VPC with the appropriate security group.",
      "B": "Set up a VPN connection from AWS to the data center. Route the trafic from the Lambda function through the VPN.",
      "C": "Update the route tables in the VPC to allow the Lambda function to access the on-premises data center through Direct Connect.",
      "D": "Create an Elastic IP address. Configure the Lambda function to send trafic through the Elastic IP address without an elastic network interface."
    },
    "correct_answer": "A",
    "vote_percentage": "73%",
    "question_cn": "一家公司有一个用于软件工程的 AWS 账户。该 AWS 账户通过一对 AWS Direct Connect 连接访问公司本地数据中心。所有非 VPC 流量路由到虚拟私有网关。一个开发团队最近通过控制台创建了一个 AWS Lambda 函数。开发团队需要允许该函数访问在公司数据中心内的私有子网中运行的数据库。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 Lambda 函数配置为在 VPC 中运行，并使用适当的安全组。",
      "B": "设置从 AWS 到数据中心的 VPN 连接。将来自 Lambda 函数的流量通过 VPN 路由。",
      "C": "更新 VPC 中的路由表，以允许 Lambda 函数通过 Direct Connect 访问本地数据中心。",
      "D": "创建一个弹性 IP 地址。将 Lambda 函数配置为通过弹性 IP 地址发送流量，而无需弹性网络接口。"
    },
    "tags": [
      "AWS Lambda",
      "VPC",
      "AWS Direct Connect",
      "Routing Tables",
      "Virtual Private Gateway (VGW)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 73%），解析仅供参考。】\n\n考查 Lambda 函数访问 VPC 内资源的能力，以及使用安全组控制访问。同时，考查 Direct Connect 连接下的网络配置。",
      "why_correct": "将 Lambda 函数配置在 VPC 中允许它访问 VPC 内的资源，如数据库。安全组提供了细粒度的网络访问控制，允许开发团队精确定义 Lambda 函数可以访问的数据库端口和 IP 范围，从而满足安全要求。这种配置方式能够直接利用现有的 Direct Connect 连接和虚拟私有网关（Virtual Private Gateway）。",
      "why_wrong": "B 选项引入了 VPN 连接，而题目中已经存在 Direct Connect，因此这是一种不必要的复杂化，且可能导致性能下降。C 选项只更新路由表不够，因为 Lambda 函数本身需要配置在 VPC 内才能访问。D 选项使用 Elastic IP 地址是不必要的，并且无法直接允许 Lambda 函数访问 VPC 内部资源，并且 Elastic IP 地址主要用于互联网流量，而不是在 VPC 内部访问资源。"
    },
    "related_terms": [
      "AWS Lambda",
      "VPC",
      "Security Group",
      "AWS Direct Connect",
      "Virtual Private Gateway",
      "VPN",
      "Elastic IP Address",
      "Elastic Network Interface"
    ]
  },
  {
    "id": 185,
    "topic": "1",
    "question_en": "A company runs an application using Amazon ECS. The application creates resized versions of an original image and then makes Amazon S3 API calls to store the resized images in Amazon S3. How can a solutions architect ensure that the application has permission to access Amazon S3?",
    "options_en": {
      "A": "Update the S3 role in AWS IAM to allow read/write access from Amazon ECS, and then relaunch the container.",
      "B": "Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task definition.",
      "C": "Create a security group that allows access from Amazon ECS to Amazon S3, and update the launch configuration used by the ECS cluster.",
      "D": "Create an IAM user with S3 permissions, and then relaunch the Amazon EC2 instances for the ECS cluster while logged in as this account."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon ECS 运行应用程序。该应用程序创建原始图像的调整大小版本，然后进行 Amazon S3 API 调用，将调整大小的图像存储在 Amazon S3 中。解决方案架构师如何确保应用程序有权访问 Amazon S3？",
    "options_cn": {
      "A": "在 AWS IAM 中更新 S3 角色，以允许从 Amazon ECS 进行读/写访问，然后重新启动容器。",
      "B": "创建一个具有 S3 权限的 IAM 角色，然后在任务定义中将该角色指定为 taskRoleArn。",
      "C": "创建一个安全组，允许从 Amazon ECS 访问 Amazon S3，并更新 ECS 集群使用的启动配置。",
      "D": "创建一个具有 S3 权限的 IAM 用户，然后在以该帐户登录的情况下重新启动 ECS 集群的 Amazon EC2 实例。"
    },
    "tags": [
      "Amazon ECS",
      "IAM",
      "Amazon S3",
      "Task Role",
      "Security Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 ECS 任务访问 S3 的权限配置。与 IAM 角色、任务定义、安全组的配置有关。",
      "why_correct": "创建一个具有 S3 权限的 IAM 角色，在 ECS 任务定义中指定该角色为 `taskRoleArn`，是最佳实践。通过这种方式，ECS 任务将使用该 IAM 角色来访问 S3。IAM 角色管理权限，允许细粒度的访问控制，且无需在应用程序代码中嵌入 AWS 凭证，提高了安全性。",
      "why_wrong": "选项 A 错误，直接在 IAM 中更新 S3 角色并不能确保 ECS 任务使用该角色访问 S3。即使更新 IAM 角色，ECS 容器仍然需要正确的配置才能使用该角色；选项 C 错误，安全组控制网络层面的访问，无法提供对 S3 的授权。安全组允许 ECS 集群的实例访问 S3，但无法授权 ECS 任务本身访问 S3。选项 D 错误，创建 IAM 用户并将凭证嵌入到 EC2 实例中不安全，不符合最佳实践。这需要管理用户凭证，并可能导致凭证泄露。此外，使用 IAM 角色而不是 IAM 用户是 AWS 推荐的安全实践。"
    },
    "related_terms": [
      "Amazon ECS",
      "IAM",
      "Amazon S3",
      "EC2",
      "Task Role",
      "Security Group",
      "Task Definition"
    ]
  },
  {
    "id": 186,
    "topic": "1",
    "question_en": "A company has a Windows-based application that must be migrated to AWS. The application requires the use of a shared Windows file system attached to multiple Amazon EC2 Windows instances that are deployed across multiple Availability Zone: What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Configure AWS Storage Gateway in volume gateway mode. Mount the volume to each Windows instance.",
      "B": "Configure Amazon FSx for Windows File Server. Mount the Amazon FSx file system to each Windows instance.",
      "C": "Configure a file system by using Amazon Elastic File System (Amazon EFS). Mount the EFS file system to each Windows instance.",
      "D": "Configure an Amazon Elastic Block Store (Amazon EBS) volume with the required size. Attach each EC2 instance to the volume. Mount the file system within the volume to each Windows instance."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个基于 Windows 的应用程序，该应用程序必须迁移到 AWS。该应用程序需要使用连接到多个 Amazon EC2 Windows 实例的共享 Windows 文件系统，这些实例部署在多个可用区。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "在卷网关模式下配置 AWS Storage Gateway。将卷挂载到每个 Windows 实例。",
      "B": "配置 Amazon FSx for Windows File Server。将 Amazon FSx 文件系统挂载到每个 Windows 实例。",
      "C": "使用 Amazon Elastic File System (Amazon EFS) 配置一个文件系统。将 EFS 文件系统挂载到每个 Windows 实例。",
      "D": "配置具有所需大小的 Amazon Elastic Block Store (Amazon EBS) 卷。将每个 EC2 实例连接到该卷。将卷中的文件系统挂载到每个 Windows 实例。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon FSx for Windows File Server",
      "Storage Gateway",
      "Amazon EFS",
      "Amazon EBS",
      "Windows"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查共享文件系统的选择；与 Amazon EC2 上的 Windows 应用程序迁移、可用区和性能需求相关。",
      "why_correct": "Amazon FSx for Windows File Server 专为 Windows 应用程序设计，提供完全托管的 Windows 文件服务器。它提供高性能、可伸缩的共享文件存储，适用于连接到多个 EC2 Windows 实例的场景。 FSx for Windows File Server 支持 SMB 协议，与 Windows 应用程序的兼容性最好。它也支持跨可用区的部署，满足可用性需求。",
      "why_wrong": {
        "A": "AWS Storage Gateway 适用于混合云场景，用于连接本地环境和 AWS 云。 在卷网关模式下，它不适用于多个 EC2 实例直接共享文件。它更侧重于本地数据与云端 S3 的同步，而非多个 EC2 实例间的直接共享。因此，此选项与题目需求不匹配。",
        "C": "Amazon EFS 主要设计用于 Linux 实例的共享文件存储，虽然理论上也可以用于 Windows，但性能和兼容性不如 FSx for Windows File Server，而且不是针对 Windows 文件服务器的需求而优化的，故不推荐。",
        "D": "Amazon EBS 卷是块存储设备，设计用于单个 EC2 实例。虽然可以将其附加到多个实例（但一次只有一个），但这并非共享文件系统的正确方法，容易导致数据一致性问题，并且不提供内置的文件共享功能。多个实例同时访问一个 EBS 卷会产生数据损坏的风险。EBS 的可用区部署能力与题目需求不完全匹配。"
      }
    },
    "related_terms": [
      "Amazon EC2",
      "Windows",
      "Amazon FSx for Windows File Server",
      "Storage Gateway",
      "Amazon EFS",
      "Amazon EBS",
      "SMB",
      "EC2",
      "S3"
    ]
  },
  {
    "id": 187,
    "topic": "1",
    "question_en": "A company is developing an ecommerce application that will consist of a load-balanced front end, a container-based application, and a relational database. A solutions architect needs to create a highly available solution that operates with as little manual intervention as possible. Which solutions meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon RDS DB instance in Multi-AZ mode.",
      "B": "Create an Amazon RDS DB instance and one or more replicas in another Availability Zone.",
      "C": "Create an Amazon EC2 instance-based Docker cluster to handle the dynamic application load.",
      "D": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with a Fargate launch type to handle the dynamic application load",
      "E": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type to handle the dynamic application load."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在开发一个电子商务应用程序，该应用程序将包括一个负载均衡的前端、一个基于容器的应用程序和一个关系数据库。解决方案架构师需要创建一个高可用性解决方案，该方案尽可能减少手动干预。哪些解决方案符合这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 Multi-AZ 模式下创建 Amazon RDS 数据库实例。",
      "B": "在另一个可用区中创建 Amazon RDS 数据库实例和一个或多个副本。",
      "C": "创建一个基于 Amazon EC2 实例的 Docker 集群来处理动态应用程序负载。",
      "D": "创建一个使用 Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群来处理动态应用程序负载。",
      "E": "创建一个使用 Amazon EC2 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群来处理动态应用程序负载。"
    },
    "tags": [
      "Amazon RDS",
      "Multi-AZ",
      "Amazon EC2",
      "Amazon ECS",
      "Fargate",
      "High Availability",
      "Docker"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n考查构建高可用性架构的方案；与 RDS 的 Multi-AZ 部署、ECS 的 Fargate 和 EC2 启动类型选型相关，以及负载均衡、数据库和容器化应用的高可用性设计。",
      "why_correct": "在 Multi-AZ 模式下创建 Amazon RDS 数据库实例，能够提供高可用性和故障转移。当主数据库实例所在可用区发生故障时，RDS 会自动将故障转移到备用实例，无需手动干预，满足了题目中尽可能减少手动干预的要求。 Multi-AZ 部署提供了高可用性，同时 RDS 自动管理数据库的复制和故障转移过程，简化了运维。",
      "why_wrong": "选项 B 描述了创建 RDS 数据库实例及其副本的场景，但是创建副本并不能直接实现高可用性，因为需要手动配置和管理故障转移，不满足“减少手动干预”的要求。选项 C 描述了使用 EC2 实例的 Docker 集群，需要手动管理 EC2 实例、安装 Docker 以及维护 Docker 集群，增加了手动操作的复杂性，不如 Fargate 或 RDS 的 Multi-AZ 方案自动化程度高。选项 D 描述了使用 Fargate 启动类型的 ECS 集群，Fargate 是一种无服务器计算引擎，可以自动管理容器的部署和扩展，符合高可用性和自动化要求，所以 D 是一个正确的备选项，但是题目要求选择两个方案。选项 E 描述了使用 EC2 启动类型的 ECS 集群，需要手动管理 EC2 实例，维护集群，不符合高可用性和自动化需求，因此不符合题目要求。EC2 启动类型需要用户管理底层的 EC2 实例，相较于 Fargate，增加了运维的复杂性。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "EC2",
      "Docker",
      "Amazon ECS",
      "Fargate",
      "High Availability"
    ]
  },
  {
    "id": 188,
    "topic": "1",
    "question_en": "A company uses Amazon S3 as its data lake. The company has a new partner that must use SFTP to upload data files. A solutions architect needs to implement a highly available SFTP solution that minimizes operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Transfer Family to configure an SFTP-enabled server with a publicly accessible endpoint. Choose the S3 data lake as the destination.",
      "B": "Use Amazon S3 File Gateway as an SFTP server. Expose the S3 File Gateway endpoint URL to the new partner. Share the S3 File Gateway endpoint with the new partner.",
      "C": "Launch an Amazon EC2 instance in a private subnet in a VPInstruct the new partner to upload files to the EC2 instance by using a VPN. Run a cron job script, on the EC2 instance to upload files to the S3 data lake.",
      "D": "Launch Amazon EC2 instances in a private subnet in a VPC. Place a Network Load Balancer (NLB) in front of the EC2 instances. Create an SFTP listener port for the NLB. Share the NLB hostname with the new partner. Run a cron job script on the EC2 instances to upload files to the S3 data lake."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon S3 作为其数据湖。该公司有一个新合作伙伴，必须使用 SFTP 上传数据文件。一位解决方案架构师需要实施一个高可用性的 SFTP 解决方案，以最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Transfer Family 配置一个支持 SFTP 的服务器，该服务器具有公开可访问的终端节点。选择 S3 数据湖作为目标。",
      "B": "使用 Amazon S3 File Gateway 作为 SFTP 服务器。将 S3 File Gateway 终端节点 URL 暴露给新合作伙伴。与新合作伙伴共享 S3 File Gateway 终端节点。",
      "C": "在 VPC 的私有子网中启动一个 Amazon EC2 实例。指示新合作伙伴通过 VPN 将文件上传到 EC2 实例。在 EC2 实例上运行一个 cron job 脚本，将文件上传到 S3 数据湖。",
      "D": "在 VPC 的私有子网中启动 Amazon EC2 实例。在 EC2 实例前面放置一个 Network Load Balancer (NLB)。为 NLB 创建一个 SFTP 监听端口。与新合作伙伴共享 NLB 主机名。在 EC2 实例上运行一个 cron job 脚本，将文件上传到 S3 数据湖。"
    },
    "tags": [
      "AWS Transfer Family",
      "SFTP",
      "Amazon S3",
      "EC2",
      "Network Load Balancer (NLB)",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查如何通过 SFTP 安全地将数据上传到 Amazon S3 数据湖，并实现高可用性及最小运营开销。",
      "why_correct": "AWS Transfer Family 提供了对 SFTP 的全面支持，并且易于配置。通过配置一个公开可访问的终端节点，合作伙伴可以直接使用 SFTP 协议上传文件到 S3。这种方案简化了设置和管理，实现了高可用性，并最大限度地减少了运营开销。",
      "why_wrong": "选项 B 涉及 Amazon S3 File Gateway，它主要用于本地存储和 S3 之间的文件传输，并非专为 SFTP 入站连接设计。选项 C 涉及手动配置 EC2 实例和 VPN，以及编写脚本进行数据同步，增加了复杂性和运营负担，不符合题目的最小运营开销要求。选项 D 涉及使用 Network Load Balancer (NLB) 和 EC2 实例，增加了复杂性，需要额外的管理工作，也不符合最小运营开销的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "SFTP",
      "AWS Transfer Family",
      "S3 File Gateway",
      "Amazon EC2",
      "VPC",
      "VPN",
      "Network Load Balancer (NLB)"
    ]
  },
  {
    "id": 189,
    "topic": "1",
    "question_en": "A company needs to store contract documents. A contract lasts for 5 years. During the 5-year period, the company must ensure that the documents cannot be overwritten or deleted. The company needs to encrypt the documents at rest and rotate the encryption keys automatically every year. Which combination of steps should a solutions architect take to meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Store the documents in Amazon S3. Use S3 Object Lock in governance mode.",
      "B": "Store the documents in Amazon S3. Use S3 Object Lock in compliance mode.",
      "C": "Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure key rotation.",
      "D": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer managed keys. Configure key rotation",
      "E": "Use server-side encryption with AWS Key Management Service (AWS KMS) customer provided (imported) keys. Configure key rotation."
    },
    "correct_answer": "BD",
    "vote_percentage": "77%",
    "question_cn": "一家公司需要存储合同文件。一份合同有效期为 5 年。在 5 年期间，公司必须确保文件不能被覆盖或删除。该公司需要在静态时加密文件，并每年自动轮换加密密钥。解决方案架构师应该采取哪些组合步骤，以最少的运营开销来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将文件存储在 Amazon S3 中。在管理模式下使用 S3 Object Lock。",
      "B": "将文件存储在 Amazon S3 中。在合规模式下使用 S3 Object Lock。",
      "C": "使用带有 Amazon S3 托管加密密钥 (SSE-S3) 的服务器端加密。配置密钥轮换。",
      "D": "使用带有 AWS Key Management Service (AWS KMS) 客户托管密钥的服务器端加密。配置密钥轮换。",
      "E": "使用带有 AWS Key Management Service (AWS KMS) 客户提供（导入）密钥的服务器端加密。配置密钥轮换。"
    },
    "tags": [
      "Amazon S3",
      "SSE-S3",
      "AWS KMS",
      "S3 Object Lock"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 77%），解析仅供参考。】\n\n考察使用 S3 Object Lock 实现不可变存储，以及使用 KMS 进行静态加密和密钥轮换。",
      "why_correct": "选项 B 正确，因为合规模式下的 S3 Object Lock 确保对象在指定保留期内无法被删除或覆盖，满足了合同文件不可篡改的需求。选项 D 正确，通过使用 KMS 客户托管密钥进行服务器端加密，并配置密钥轮换，可以满足加密需求，同时实现密钥的自动更新，减少了运维开销。",
      "why_wrong": "选项 A 错误，管理模式下的 S3 Object Lock 允许有权限的用户覆盖或删除对象，不满足不可变性要求。选项 C 错误，SSE-S3 使用 S3 托管密钥，无法实现密钥轮换，不满足每年轮换密钥的要求。选项 E 错误，使用客户提供（导入）密钥会增加密钥管理复杂性，且无法直接满足轮换需求，增加了运维开销。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Object Lock",
      "AWS KMS",
      "SSE-S3"
    ]
  },
  {
    "id": 190,
    "topic": "1",
    "question_en": "A company has a web application that is based on Java and PHP. The company plans to move the application from on premises to AWS. The company needs the ability to test new site features frequently. The company also needs a highly available and managed solution that requires minimum operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon S3 bucket. Enable static web hosting on the S3 bucket. Upload the static content to the S3 bucket. Use AWS Lambda to process all dynamic content.",
      "B": "Deploy the web application to an AWS Elastic Beanstalk environment. Use URL swapping to switch between multiple Elastic Beanstalk environments for feature testing.",
      "C": "Deploy the web application to Amazon EC2 instances that are configured with Java and PHP. Use Auto Scaling groups and an Application Load Balancer to manage the website’s availability.",
      "D": "Containerize the web application. Deploy the web application to Amazon EC2 instances. Use the AWS Load Balancer Controller to dynamically route trafic between containers that contain the new site features for testing."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家公司有一个基于 Java 和 PHP 的 Web 应用程序。该公司计划将该应用程序从本地环境迁移到 AWS。该公司需要频繁测试新站点功能的能力。该公司还需要一个高可用且托管的解决方案，该解决方案需要最少的运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon S3 存储桶。在 S3 存储桶上启用静态网站托管。将静态内容上传到 S3 存储桶。使用 AWS Lambda 处理所有动态内容。",
      "B": "将 Web 应用程序部署到 AWS Elastic Beanstalk 环境。使用 URL 交换在多个 Elastic Beanstalk 环境之间切换以进行功能测试。",
      "C": "将 Web 应用程序部署到配置了 Java 和 PHP 的 Amazon EC2 实例。使用 Auto Scaling 组和 Application Load Balancer 来管理网站的可用性。",
      "D": "容器化 Web 应用程序。将 Web 应用程序部署到 Amazon EC2 实例。使用 AWS Load Balancer Controller 动态路由流量，在包含新站点功能的容器之间进行测试。"
    },
    "tags": [
      "Amazon EC2",
      "AWS Load Balancer",
      "Amazon ECS",
      "Containerization",
      "High Availability",
      "Testing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n考察了 Web 应用程序迁移到 AWS 时的部署方案选择，以及如何进行功能测试、高可用性、和运营开销的权衡。",
      "why_correct": "Elastic Beanstalk 是一种托管服务，简化了 Web 应用程序的部署和管理。它支持 Java 和 PHP，满足了应用程序的技术需求。URL 交换功能允许在多个环境之间切换，方便进行功能测试，并提供高可用性。",
      "why_wrong": "选项 A 使用 S3 进行静态网站托管，无法满足对动态内容的 Java 和 PHP 应用程序的需求。选项 C 使用 EC2 实例，需要手动配置和管理，运营开销较高。选项 D 容器化应用程序部署到 EC2 实例也增加了运营负担，并且使用 Load Balancer Controller 的方案较为复杂，不适合题干中“最小的运营开销”的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS Lambda",
      "AWS Elastic Beanstalk",
      "Elastic Beanstalk",
      "Java",
      "PHP",
      "Amazon EC2",
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "AWS Load Balancer Controller"
    ]
  },
  {
    "id": 191,
    "topic": "1",
    "question_en": "A company has an ordering application that stores customer information in Amazon RDS for MySQL. During regular business hours, employees run one-time queries for reporting purposes. Timeouts are occurring during order processing because the reporting queries are taking a long time to run. The company needs to eliminate the timeouts without preventing employees from performing queries. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a read replica. Move reporting queries to the read replica.",
      "B": "Create a read replica. Distribute the ordering application to the primary DB instance and the read replica.",
      "C": "Migrate the ordering application to Amazon DynamoDB with on-demand capacity.",
      "D": "Schedule the reporting queries for non-peak hours."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个订购应用程序，将客户信息存储在 Amazon RDS for MySQL 中。 在正常工作时间，员工运行一次性查询以进行报告。 由于报告查询的运行时间过长，订单处理期间会发生超时。 公司需要消除超时，同时不阻止员工执行查询。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个读取副本。 将报告查询移动到读取副本。",
      "B": "创建一个读取副本。 将订购应用程序分发到主数据库实例和读取副本。",
      "C": "将订购应用程序迁移到 Amazon DynamoDB 并使用按需容量。",
      "D": "将报告查询安排在非高峰时段进行。"
    },
    "tags": [
      "Amazon RDS",
      "RDS for MySQL",
      "Read Replica",
      "Database Performance",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查 RDS for MySQL 数据库的性能优化，特别是针对长时间运行的查询和避免影响在线交易的解决方案。",
      "why_correct": "创建一个 Amazon RDS for MySQL 读取副本，允许将只读查询（如报告查询）定向到读取副本。 这将负载从主数据库实例中卸载，从而消除超时并确保订单处理期间的性能。读取副本提供了数据一致性，满足了题目的要求，避免影响员工执行查询。",
      "why_wrong": "选项 B 错误，因为将订购应用程序分发到主数据库实例和读取副本会引入数据同步问题和潜在的一致性问题，这并非本题的优化目标。选项 C 错误，将数据库迁移到 DynamoDB 并使用按需容量是一种架构上的重大改变，不直接解决长时间运行的查询问题，且可能导致数据库的重新设计。选项 D 错误，虽然安排查询在非高峰时段进行可以缓解问题，但并未从根本上解决长时间运行查询对订单处理的影响，且无法满足员工随时查询的需求。"
    },
    "related_terms": [
      "Amazon RDS for MySQL",
      "read replica",
      "Amazon DynamoDB",
      "MySQL"
    ]
  },
  {
    "id": 192,
    "topic": "1",
    "question_en": "A hospital wants to create digital copies for its large collection of historical written records. The hospital will continue to add hundreds of new documents each day. The hospital’s data team will scan the documents and will upload the documents to the AWS Cloud. A solutions architect must implement a solution to analyze the documents, extract the medical information, and store the documents so that an application can run SQL queries on the data. The solution must maximize scalability and operational eficiency. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Write the document information to an Amazon EC2 instance that runs a MySQL database.",
      "B": "Write the document information to an Amazon S3 bucket. Use Amazon Athena to query the data.",
      "C": "Create an Auto Scaling group of Amazon EC2 instances to run a custom application that processes the scanned files and extracts the medical information.",
      "D": "Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Rekognition to convert the documents to raw text. Use Amazon Transcribe Medical to detect and extract relevant medical information from the text",
      "E": "Create an AWS Lambda function that runs when new documents are uploaded. Use Amazon Textract to convert the documents to raw text. Use Amazon Comprehend Medical to detect and extract relevant medical information from the text."
    },
    "correct_answer": "BE",
    "vote_percentage": "100%",
    "question_cn": "一家医院希望为其大量历史书面记录创建数字副本。 医院将继续每天添加数百份新文档。 医院的数据团队将扫描这些文档，并将文档上传到 AWS 云。 解决方案架构师必须实施一个解决方案来分析文档、提取医疗信息并存储文档，以便应用程序可以在数据上运行 SQL 查询。 该解决方案必须最大限度地提高可扩展性和运营效率。 解决方案架构师应该采取哪些组合步骤来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将文档信息写入运行 MySQL 数据库的 Amazon EC2 实例。",
      "B": "将文档信息写入 Amazon S3 存储桶。 使用 Amazon Athena 查询数据。",
      "C": "创建一个 Amazon EC2 实例的 Auto Scaling 组，以运行一个自定义应用程序来处理扫描的文件并提取医疗信息。",
      "D": "创建一个 AWS Lambda 函数，该函数在新文档上传时运行。 使用 Amazon Rekognition 将文档转换为原始文本。 使用 Amazon Transcribe Medical 从文本中检测并提取相关的医疗信息。",
      "E": "创建一个 AWS Lambda 函数，该函数在新文档上传时运行。 使用 Amazon Textract 将文档转换为原始文本。 使用 Amazon Comprehend Medical 从文本中检测并提取相关的医疗信息。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon S3",
      "Amazon Athena",
      "AWS Lambda",
      "Amazon Rekognition",
      "Amazon Transcribe Medical",
      "Amazon Textract",
      "Amazon Comprehend Medical",
      "MySQL",
      "EC2 Auto Scaling Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 100%），解析仅供参考。】\n\n考查文档处理、医疗信息提取、数据存储和查询的可扩展性与运营效率，以及 Lambda、S3、Athena、Textract 和 Comprehend Medical 等服务的应用。",
      "why_correct": "选项 B 和 E 提供了符合要求的组合。选项 B 使用 S3 存储桶存储文档信息，并利用 Athena 查询数据，满足可扩展性和运营效率的需求。选项 E 使用 Lambda 触发 Textract 和 Comprehend Medical，实现自动化的医疗信息提取和分析，同样具有可扩展性和高效率。",
      "why_wrong": "选项 A 涉及使用 EC2 实例运行 MySQL 数据库，不利于可扩展性，且运维成本高。选项 C 虽然使用了 Auto Scaling 组，但手动创建和维护自定义应用程序，运营效率较低。选项 D 使用 Rekognition 进行文档转换，而 Rekognition 并非主要用于文本提取，相比 Textract 效率较低，且会增加额外开销。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon Athena",
      "Amazon EC2",
      "Auto Scaling",
      "AWS Lambda",
      "Amazon Rekognition",
      "Amazon Transcribe Medical",
      "Amazon Textract",
      "Amazon Comprehend Medical",
      "MySQL"
    ]
  },
  {
    "id": 193,
    "topic": "1",
    "question_en": "A company is running a batch application on Amazon EC2 instances. The application consists of a backend with multiple Amazon RDS databases. The application is causing a high number of reads on the databases. A solutions architect must reduce the number of database reads while ensuring high availability. What should the solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Add Amazon RDS read replicas.",
      "B": "Use Amazon ElastiCache for Redis.",
      "C": "Use Amazon Route 53 DNS caching",
      "D": "Use Amazon ElastiCache for Memcached."
    },
    "correct_answer": "B",
    "vote_percentage": "53%",
    "question_cn": "一家公司在 Amazon EC2 实例上运行一个批处理应用程序。该应用程序由具有多个 Amazon RDS 数据库的后端组成。该应用程序导致数据库上的读取次数很高。解决方案架构师必须减少数据库读取次数，同时确保高可用性。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "添加 Amazon RDS 副本读取器。",
      "B": "使用 Amazon ElastiCache for Redis。",
      "C": "使用 Amazon Route 53 DNS 缓存",
      "D": "使用 Amazon ElastiCache for Memcached。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Read Replicas",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 53%），解析仅供参考。】\n\n考查如何通过缓存机制优化 RDS 数据库的读取性能，并保证高可用性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：使用 Amazon ElastiCache for Redis 作为缓存解决方案，可以缓存频繁读取的数据，从而减少对 RDS 数据库的读取压力。Redis 支持主从复制，可以提供高可用性。ElastiCache 可以缓存数据库查询结果，显著提高应用程序的性能。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，添加 Amazon RDS 副本读取器，虽然可以分担数据库的读取负载，但无法缓存数据，并不能直接减少数据库的读取次数。选项 C，Amazon Route 53 DNS 缓存，与减少数据库读取次数无关，主要用于缓存 DNS 记录，优化域名解析。选项 D，使用 Amazon ElastiCache for Memcached，Memcached 同样可以缓存数据，但 Redis 提供了更丰富的数据结构和特性，更适合复杂场景，并且 Memcached 在高可用性方面的配置相对 Redis 更复杂。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Amazon ElastiCache for Redis",
      "Amazon Route 53",
      "Amazon ElastiCache for Memcached",
      "RDS",
      "EC2",
      "ElastiCache for Redis",
      "ElastiCache for Memcached",
      "DNS"
    ]
  },
  {
    "id": 194,
    "topic": "1",
    "question_en": "A company needs to run a critical application on AWS. The company needs to use Amazon EC2 for the application’s database. The database must be highly available and must fail over automatically if a disruptive event occurs. Which solution will meet these requirements?",
    "options_en": {
      "A": "Launch two EC2 instances, each in a different Availability Zone in the same AWS Region. Install the database on both EC2 instances. Configure the EC2 instances as a cluster. Set up database replication.",
      "B": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use AWS CloudFormation to automate provisioning of the EC2 instance if a disruptive event occurs.",
      "C": "Launch two EC2 instances, each in a different AWS Region. Install the database on both EC2 instances. Set up database replication. Fail over the database to a second Region.",
      "D": "Launch an EC2 instance in an Availability Zone. Install the database on the EC2 instance. Use an Amazon Machine Image (AMI) to back up the data. Use EC2 automatic recovery to recover the instance if a disruptive event occurs."
    },
    "correct_answer": "A",
    "vote_percentage": "55%",
    "question_cn": "一家公司需要在 AWS 上运行一个关键应用程序。该公司需要使用 Amazon EC2 作为应用程序的数据库。数据库必须具有高可用性，并且如果发生中断事件，必须自动故障转移。以下哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在同一 AWS 区域的不同可用区中启动两个 EC2 实例。在两个 EC2 实例上安装数据库。将 EC2 实例配置为集群。设置数据库复制。",
      "B": "在一个可用区中启动一个 EC2 实例。在 EC2 实例上安装数据库。使用 Amazon Machine Image (AMI) 备份数据。使用 AWS CloudFormation 在发生中断事件时自动预置 EC2 实例。",
      "C": "在不同的 AWS 区域中启动两个 EC2 实例。在两个 EC2 实例上安装数据库。设置数据库复制。将数据库故障转移到第二个区域。",
      "D": "在一个可用区中启动一个 EC2 实例。在 EC2 实例上安装数据库。使用 Amazon Machine Image (AMI) 备份数据。使用 EC2 自动恢复来恢复实例，如果发生中断事件。"
    },
    "tags": [
      "Amazon EC2",
      "High Availability",
      "Disaster Recovery",
      "Database Replication",
      "AWS CloudFormation",
      "EC2 Auto Recovery"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 55%），解析仅供参考。】\n\n考查在 EC2 上部署数据库，并实现高可用性和自动故障转移的解决方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 描述了在同一 AWS 区域的不同可用区中部署数据库集群的方案。通过在不同可用区部署 EC2 实例，可以实现高可用性。配置数据库复制，可以确保数据在实例之间同步。当一个实例发生故障时，数据库会自动故障转移到另一个实例，满足自动故障转移的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 仅在一个可用区中部署了 EC2 实例，无法实现高可用性。虽然使用 AMI 备份数据和 CloudFormation 预置实例可以恢复或重建实例，但不能提供自动故障转移。选项 C 在不同 AWS 区域中部署实例会增加延迟，且故障转移到另一个区域的时间更长，不如在同一区域内的可用区之间故障转移。选项 D 同样只在一个可用区中部署实例，无法满足高可用性。EC2 自动恢复仅在硬件层面发生故障时才能恢复，无法应对数据库层面的故障。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Availability Zone",
      "Amazon Machine Image (AMI)",
      "AWS CloudFormation",
      "EC2 Auto Recovery"
    ]
  },
  {
    "id": 195,
    "topic": "1",
    "question_en": "A company’s order system sends requests from clients to Amazon EC2 instances. The EC2 instances process the orders and then store the orders in a database on Amazon RDS. Users report that they must reprocess orders when the system fails. The company wants a resilient solution that can process orders automatically if a system outage occurs. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Move the EC2 instances into an Auto Scaling group. Create an Amazon EventBridge (Amazon CloudWatch Events) rule to target an Amazon Elastic Container Service (Amazon ECS) task.",
      "B": "Move the EC2 instances into an Auto Scaling group behind an Application Load Balancer (ALB). Update the order system to send messages to the ALB endpoint.",
      "C": "Move the EC2 instances into an Auto Scaling group. Configure the order system to send messages to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the EC2 instances to consume messages from the queue.",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function, and subscribe the function to the SNS topic. Configure the order system to send messages to the SNS topic. Send a command to the EC2 instances to process the messages by using AWS Systems Manager Run Command."
    },
    "correct_answer": "C",
    "vote_percentage": "95%",
    "question_cn": "一家公司的订单系统将来自客户端的请求发送到 Amazon EC2 实例。EC2 实例处理订单，然后将订单存储在 Amazon RDS 上的数据库中。用户报告说，当系统发生故障时，他们必须重新处理订单。该公司希望有一个弹性的解决方案，可以在系统中断时自动处理订单。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将 EC2 实例移入 Auto Scaling 组。创建一个 Amazon EventBridge (Amazon CloudWatch Events) 规则，以目标 Amazon Elastic Container Service (Amazon ECS) 任务。",
      "B": "将 EC2 实例移入 Application Load Balancer (ALB) 后面的 Auto Scaling 组。更新订单系统以将消息发送到 ALB 终端节点。",
      "C": "将 EC2 实例移入 Auto Scaling 组。配置订单系统将消息发送到 Amazon Simple Queue Service (Amazon SQS) 队列。配置 EC2 实例以使用队列中的消息。",
      "D": "创建 Amazon Simple Notification Service (Amazon SNS) 主题。创建一个 AWS Lambda 函数，并将该函数订阅到 SNS 主题。配置订单系统以将消息发送到 SNS 主题。使用 AWS Systems Manager Run Command 将命令发送到 EC2 实例以处理消息。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "Amazon SNS",
      "AWS Lambda",
      "AWS Systems Manager",
      "Amazon SQS",
      "Amazon ECS",
      "Amazon EventBridge",
      "Application Load Balancer (ALB)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 95%），解析仅供参考。】\n\n考查了如何设计弹性的订单处理系统，在 EC2 实例故障时实现订单的自动处理。",
      "why_correct": "选项 C 提供了最直接、可靠的弹性解决方案。将订单系统消息放入 SQS 队列，实现了消息的解耦和异步处理，确保了即使 EC2 实例出现故障，消息也能保留在队列中等待处理。Auto Scaling 组可以根据负载自动扩展 EC2 实例，从而确保了处理能力的弹性。",
      "why_wrong": "选项 A 错误，因为 ECS 任务与 EC2 实例没有直接联系，无法自动处理 EC2 实例故障时的订单重处理需求。选项 B 错误，ALB 仅用于负载均衡，无法提供订单的持久化和重试机制，不能解决故障场景下的订单处理问题。选项 D 错误，虽然 SNS 和 Lambda 提供了消息传递和处理机制，但通过 Systems Manager Run Command 将命令发送到 EC2 实例处理消息的方式不够可靠，且效率较低，难以应对高并发场景。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Auto Scaling",
      "Amazon EventBridge",
      "Amazon CloudWatch Events",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Application Load Balancer (ALB)",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "AWS Lambda",
      "AWS Systems Manager Run Command"
    ]
  },
  {
    "id": 196,
    "topic": "1",
    "question_en": "A company runs an application on a large fieet of Amazon EC2 instances. The application reads and writes entries into an Amazon DynamoDB table. The size of the DynamoDB table continuously grows, but the application needs only data from the last 30 days. The company needs a solution that minimizes cost and development effort. Which solution meets these requirements?",
    "options_en": {
      "A": "Use an AWS CloudFormation template to deploy the complete solution. Redeploy the CloudFormation stack every 30 days, and delete the original stack.",
      "B": "Use an EC2 instance that runs a monitoring application from AWS Marketplace. Configure the monitoring application to use Amazon DynamoDB Streams to store the timestamp when a new item is created in the table. Use a script that runs on the EC2 instance to delete items that have a timestamp that is older than 30 days.",
      "C": "Configure Amazon DynamoDB Streams to invoke an AWS Lambda function when a new item is created in the table. Configure the Lambda function to delete items in the table that are older than 30 days.",
      "D": "Extend the application to add an attribute that has a value of the current timestamp plus 30 days to each new item that is created in the table. Configure DynamoDB to use the attribute as the TTL attribute."
    },
    "correct_answer": "D",
    "vote_percentage": "92%",
    "question_cn": "一家公司在其大型 Amazon EC2 实例集群上运行一个应用程序。该应用程序在 Amazon DynamoDB 表中读取和写入条目。DynamoDB 表的大小持续增长，但应用程序只需要最近 30 天的数据。公司需要一个最大限度地降低成本和开发工作量的解决方案。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 AWS CloudFormation 模板部署完整的解决方案。每 30 天重新部署 CloudFormation 堆栈，并删除原始堆栈。",
      "B": "使用一个 EC2 实例，该实例运行来自 AWS Marketplace 的监控应用程序。配置监控应用程序使用 Amazon DynamoDB Streams 来存储新项目在表中创建的时间戳。使用在 EC2 实例上运行的脚本删除时间戳超过 30 天的项目。",
      "C": "配置 Amazon DynamoDB Streams 以在表中创建新项目时调用 AWS Lambda 函数。配置 Lambda 函数以删除表中超过 30 天的项目。",
      "D": "扩展应用程序，为每个在新表中创建的新项目添加一个属性，该属性的值为当前时间戳加上 30 天。配置 DynamoDB 将该属性用作 TTL 属性。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB TTL",
      "AWS Lambda",
      "DynamoDB Streams",
      "Amazon EC2",
      "AWS CloudFormation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 92%），解析仅供参考。】\n\n考查如何通过配置 DynamoDB 的 TTL 功能来自动删除旧数据，以降低成本和开发工作量。涉及 DynamoDB 数据过期、成本优化、以及与其他服务的整合。方案的选择需要考虑数据维护的自动化程度、复杂性以及成本效益。",
      "why_correct": "选项 D 提供了最高效且成本最低的解决方案。通过在应用程序中为每个新创建的 DynamoDB 条目添加一个 TTL 属性（其值为当前时间戳加上 30 天），然后配置 DynamoDB 将该属性用作 TTL 属性，DynamoDB 将自动删除超过 30 天的条目。这种方法无需额外的 EC2 实例、Lambda 函数或复杂的 CloudFormation 模板，最大限度地减少了开发工作量并降低了成本，利用了 DynamoDB 内置的 TTL 功能。",
      "why_wrong": "选项 A 涉及每 30 天重新部署整个 CloudFormation 堆栈以达到数据清理的目的，这会导致服务中断和不必要的成本。这种方案复杂且低效，不符合“最大限度地降低成本和开发工作量”的要求。\n选项 B 依赖于一个 EC2 实例和第三方监控应用程序，这增加了运维复杂度和成本，并且可能导致单点故障。使用 EC2 实例进行数据处理不如 DynamoDB 内置 TTL 功能高效，并且增加了管理开销。\n选项 C 涉及使用 DynamoDB Streams 和 Lambda 函数来实现数据删除，这比使用 TTL 更复杂，并且会产生额外的 Lambda 调用成本。虽然这种方法可行，但它不如 TTL 直接和高效，增加了开发工作量和潜在的运行成本。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB",
      "DynamoDB Streams",
      "AWS Lambda",
      "EC2",
      "CloudFormation",
      "TTL"
    ]
  },
  {
    "id": 197,
    "topic": "1",
    "question_en": "A company has a Microsoft .NET application that runs on an on-premises Windows Server. The application stores data by using an Oracle Database Standard Edition server. The company is planning a migration to AWS and wants to minimize development changes while moving the application. The AWS application environment should be highly available. Which combination of actions should the company take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Refactor the application as serverless with AWS Lambda functions running .NET Core.",
      "B": "Rehost the application in AWS Elastic Beanstalk with the .NET platform in a Multi-AZ deployment.",
      "C": "Replatform the application to run on Amazon EC2 with the Amazon Linux Amazon Machine Image (AMI).",
      "D": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Amazon DynamoDB in a Multi-AZ deployment",
      "E": "Use AWS Database Migration Service (AWS DMS) to migrate from the Oracle database to Oracle on Amazon RDS in a Multi-AZ deployment."
    },
    "correct_answer": "BE",
    "vote_percentage": "98%",
    "question_cn": "一家公司有一个在本地 Windows Server 上运行的 Microsoft .NET 应用程序。该应用程序使用 Oracle Database Standard Edition 服务器存储数据。该公司计划迁移到 AWS，并希望在移动应用程序时尽量减少开发更改。AWS 应用程序环境应具有高可用性。该公司应采取哪些组合操作来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "将应用程序重构为无服务器架构，使用运行 .NET Core 的 AWS Lambda 函数。",
      "B": "在 AWS Elastic Beanstalk 中重新托管应用程序，使用 .NET 平台进行 Multi-AZ 部署。",
      "C": "将应用程序重新平台化，使其在 Amazon EC2 上运行，使用 Amazon Linux Amazon 机器映像 (AMI)。",
      "D": "使用 AWS 数据库迁移服务 (AWS DMS) 将数据从 Oracle 数据库迁移到 Multi-AZ 部署中的 Amazon DynamoDB。",
      "E": "使用 AWS 数据库迁移服务 (AWS DMS) 将数据从 Oracle 数据库迁移到 Multi-AZ 部署中的 Amazon RDS 上的 Oracle。"
    },
    "tags": [
      "AWS Elastic Beanstalk",
      "Amazon EC2",
      "Multi-AZ",
      "Amazon RDS",
      "AWS DMS",
      "Amazon DynamoDB",
      "AWS Lambda",
      ".NET"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 98%），解析仅供参考。】\n\n考察将本地.NET应用程序迁移到AWS，并实现高可用性的场景，需要考虑应用程序兼容性和数据库迁移方案。",
      "why_correct": "选项B正确，Elastic Beanstalk 允许在 Multi-AZ 部署.NET应用程序，最大程度地减少代码更改。选项E正确，AWS DMS 支持将数据从Oracle迁移到Amazon RDS for Oracle，也支持Multi-AZ部署，从而满足了数据存储的高可用性需求。",
      "why_wrong": "选项A错误，将应用程序重构为无服务器架构，改变了应用程序的架构，需要大量的代码更改。选项C错误，将应用程序平台化到EC2上，虽然可行，但 Amazon Linux AMI 不支持直接运行 .NET Framework 应用程序，而且没有直接提及高可用性的配置。选项D错误，虽然 AWS DMS 可以将数据迁移到DynamoDB，但DynamoDB 不兼容 Oracle 数据模型，需要大幅度更改应用程序的数据库访问代码，不符合最小化开发更改的要求。"
    },
    "related_terms": [
      "AWS",
      "Windows Server",
      ".NET",
      "Oracle Database Standard Edition",
      "AWS Lambda",
      ".NET Core",
      "AWS Elastic Beanstalk",
      "Multi-AZ",
      "Amazon EC2",
      "Amazon Linux",
      "AMI",
      "AWS DMS",
      "Amazon DynamoDB",
      "Amazon RDS",
      "Oracle"
    ]
  },
  {
    "id": 198,
    "topic": "1",
    "question_en": "A company runs a containerized application on a Kubernetes cluster in an on-premises data center. The company is using a MongoDB database for data storage. The company wants to migrate some of these environments to AWS, but no code changes or deployment method changes are possible at this time. The company needs a solution that minimizes operational overhead. Which solution meets these requirements?",
    "options_en": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes for compute and MongoDB on EC2 for data storage.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute and Amazon DynamoDB for data storage",
      "C": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes for compute and Amazon DynamoDB for data storage.",
      "D": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute and Amazon DocumentDB (with MongoDB compatibility) for data storage."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其本地数据中心内的 Kubernetes 集群上运行容器化应用程序。该公司使用 MongoDB 数据库进行数据存储。该公司希望将其中一些环境迁移到 AWS，但目前无法进行代码更改或部署方法更改。该公司需要一个解决方案，以最大限度地减少运营开销。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic Container Service (Amazon ECS) 和 Amazon EC2 工作节点进行计算，并使用 EC2 上的 MongoDB 进行数据存储。",
      "B": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 进行计算，并使用 Amazon DynamoDB 进行数据存储",
      "C": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 Amazon EC2 工作节点进行计算，并使用 Amazon DynamoDB 进行数据存储。",
      "D": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 AWS Fargate 进行计算，并使用 Amazon DocumentDB（兼容 MongoDB）进行数据存储。"
    },
    "tags": [
      "Amazon EKS",
      "AWS Fargate",
      "Amazon DocumentDB",
      "MongoDB",
      "Amazon EC2",
      "Amazon ECS",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查无服务器容器化部署及托管数据库服务选型，以及如何最小化运营开销。需要理解 EKS 和 ECS 的区别，以及 Fargate、EC2 的适用场景，并根据题目条件进行选择。同时，需要结合 MongoDB 数据库的特性选择合适的 AWS 数据库服务。",
      "why_correct": "选项 D 正确。该方案使用了 Amazon Elastic Kubernetes Service (EKS) 和 AWS Fargate。EKS 提供了 Kubernetes 容器编排服务，符合题目的容器化应用场景，并且能够兼容已有的 Kubernetes 集群。Fargate 是一种无服务器计算引擎，用于运行容器，无需管理 EC2 实例，从而减少了运营开销。此外，Amazon DocumentDB 兼容 MongoDB，可以直接替换现有的 MongoDB 数据库，且是托管数据库服务，进一步降低了运营复杂性，同时无需进行代码变更。 因此，该方案完全满足了题目中『最大限度地减少运营开销，且无法进行代码更改或部署方法更改』的要求。",
      "why_wrong": "选项 A 错误。选项 A 使用了 Amazon ECS 和 Amazon EC2。虽然 ECS 也是容器编排服务，但题目已经明确现有环境是 Kubernetes 集群，迁移到 ECS 需要对部署方法进行变更，不满足题目要求。选项 A 在 EC2 上部署 MongoDB 数据库，需要用户管理服务器，增加了运营开销。选项 B 错误。选项 B 使用 ECS 和 DynamoDB。虽然 ECS 和 Fargate 也能满足容器化部署和减少运营开销的要求，但 DynamoDB 并非 MongoDB 的直接替代品，需要代码层面进行修改才能适应。选项 C 错误。选项 C 使用 EKS 和 DynamoDB。虽然 EKS 可以满足 Kubernetes 环境的迁移需求，但 DynamoDB 同样不是 MongoDB 的直接替代品，并且与题目中现有的 MongoDB 数据存储不兼容，同样需要代码变更。而且，MongoDB 在 EC2 上自建，增加了运营成本。"
    },
    "related_terms": [
      "Amazon EKS",
      "AWS Fargate",
      "MongoDB",
      "Amazon EC2",
      "Amazon ECS",
      "Amazon DynamoDB",
      "Kubernetes",
      "EC2",
      "Amazon DocumentDB",
      "Container"
    ]
  },
  {
    "id": 199,
    "topic": "1",
    "question_en": "A telemarketing company is designing its customer call center functionality on AWS. The company needs a solution that provides multiple speaker recognition and generates transcript files. The company wants to query the transcript files to analyze the business patterns. The transcript files must be stored for 7 years for auditing purposes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use machine learning models for transcript file analysis.",
      "B": "Use Amazon Transcribe for multiple speaker recognition. Use Amazon Athena for transcript file analysis.",
      "C": "Use Amazon Translate for multiple speaker recognition. Store the transcript files in Amazon Redshift. Use SQL queries for transcript file analysis.",
      "D": "Use Amazon Rekognition for multiple speaker recognition. Store the transcript files in Amazon S3. Use Amazon Textract for transcript file analysis."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家电话营销公司正在 AWS 上设计其客户呼叫中心功能。该公司需要一个可以提供多发言人识别并生成转录文件的解决方案。该公司希望查询转录文件以分析业务模式。转录文件必须存储 7 年以供审计之用。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Rekognition 进行多发言人识别。将转录文件存储在 Amazon S3 中。使用机器学习模型进行转录文件分析。",
      "B": "使用 Amazon Transcribe 进行多发言人识别。使用 Amazon Athena 进行转录文件分析。",
      "C": "使用 Amazon Translate 进行多发言人识别。将转录文件存储在 Amazon Redshift 中。使用 SQL 查询进行转录文件分析。",
      "D": "使用 Amazon Rekognition 进行多发言人识别。将转录文件存储在 Amazon S3 中。使用 Amazon Textract 进行转录文件分析。"
    },
    "tags": [
      "Amazon Transcribe",
      "Amazon S3",
      "Amazon Athena",
      "Amazon Redshift",
      "SQL",
      "Amazon Rekognition",
      "Amazon Textract",
      "Amazon Translate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n考查如何选择 AWS 服务来实现多发言人识别、转录文件分析和长期存储需求。",
      "why_correct": "Amazon Transcribe 提供了多发言人识别功能，可以直接满足需求。转录文件可以存储在 S3 中，并且可以使用 Amazon Athena 来查询和分析转录文件，Athena 允许使用 SQL 查询对存储在 S3 中的数据进行分析，非常符合题目的分析业务模式需求。",
      "why_wrong": "选项 A 使用 Amazon Rekognition 进行多发言人识别，Rekognition 主要用于图像和视频分析，不适用于音频转录。选项 C 使用 Amazon Translate 进行多发言人识别，Translate 用于翻译，不具备转录功能。选项 D 使用 Amazon Rekognition 进行多发言人识别，与 A 类似，不适用音频转录；Textract 主要用于从文档中提取文本，而不是用于分析转录文件。"
    },
    "related_terms": [
      "Amazon Rekognition",
      "Amazon S3",
      "Amazon Transcribe",
      "Amazon Athena",
      "Amazon Translate",
      "Amazon Redshift",
      "SQL",
      "Amazon Textract"
    ]
  },
  {
    "id": 200,
    "topic": "1",
    "question_en": "A company hosts its application on AWS. The company uses Amazon Cognito to manage users. When users log in to the application, the application fetches required data from Amazon DynamoDB by using a REST API that is hosted in Amazon API Gateway. The company wants an AWS managed solution that will control access to the REST API to reduce development efforts. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure an AWS Lambda function to be an authorizer in API Gateway to validate which user made the request.",
      "B": "For each user, create and assign an API key that must be sent with each request. Validate the key by using an AWS Lambda function.",
      "C": "Send the user’s email address in the header with every request. Invoke an AWS Lambda function to validate that the user with that email address has proper access.",
      "D": "Configure an Amazon Cognito user pool authorizer in API Gateway to allow Amazon Cognito to validate each request."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一家公司在 AWS 上托管其应用程序。该公司使用 Amazon Cognito 管理用户。当用户登录到应用程序时，应用程序使用托管在 Amazon API Gateway 中的 REST API 从 Amazon DynamoDB 获取所需数据。该公司希望使用 AWS 托管解决方案来控制对 REST API 的访问，以减少开发工作量。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "配置一个 AWS Lambda 函数作为 API Gateway 中的授权方，以验证哪个用户发出了请求。",
      "B": "为每个用户创建并分配一个 API 密钥，该密钥必须随每个请求一起发送。使用 AWS Lambda 函数验证该密钥。",
      "C": "在每个请求的标头中发送用户的电子邮件地址。调用一个 AWS Lambda 函数来验证具有该电子邮件地址的用户是否具有适当的访问权限。",
      "D": "在 API Gateway 中配置 Amazon Cognito 用户池授权方，以允许 Amazon Cognito 验证每个请求。"
    },
    "tags": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon Cognito",
      "Amazon DynamoDB",
      "Authorization",
      "Authentication"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n考查使用 API Gateway 与 Cognito 集成的授权机制，以实现对 API 的访问控制，并减少运营开销。",
      "why_correct": "选项 D 利用了 Amazon Cognito 用户池授权方，这是 API Gateway 提供的原生功能，用于验证来自 Cognito 用户的请求。这种集成简化了身份验证流程，减少了开发和维护成本，因为 API Gateway 自动处理了身份验证，无需编写自定义 Lambda 函数。",
      "why_wrong": "选项 A 涉及使用 Lambda 授权方，需要编写额外的代码来实现自定义的身份验证逻辑，增加了开发和运营成本。选项 B 使用 API 密钥，虽然可以实现访问控制，但需要手动创建、管理和分配密钥，并且也需要使用 Lambda 函数进行验证，增加了运营开销。选项 C 涉及传递用户的电子邮件地址并在 Lambda 中进行验证，这需要开发自定义的授权逻辑，并且在安全性方面不如 Cognito 集成的授权方。"
    },
    "related_terms": [
      "Amazon Cognito",
      "API Gateway",
      "REST API",
      "Amazon DynamoDB",
      "AWS Lambda",
      "API Key",
      "Cognito User Pool Authorizer"
    ]
  },
  {
    "id": 201,
    "topic": "1",
    "question_en": "A company is developing a marketing communications service that targets mobile app users. The company needs to send confirmation messages with Short Message Service (SMS) to its users. The users must be able to reply to the SMS messages. The company must store the responses for a year for analysis. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an Amazon Connect contact fiow to send the SMS messages. Use AWS Lambda to process the responses.",
      "B": "Build an Amazon Pinpoint journey. Configure Amazon Pinpoint to send events to an Amazon Kinesis data stream for analysis and archiving.",
      "C": "Use Amazon Simple Queue Service (Amazon SQS) to distribute the SMS messages. Use AWS Lambda to process the responses.",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) FIFO topic. Subscribe an Amazon Kinesis data stream to the SNS topic for analysis and archiving."
    },
    "correct_answer": "B",
    "vote_percentage": "87%",
    "question_cn": "一家公司正在开发一个针对移动应用程序用户的营销传播服务。该公司需要使用短消息服务 (SMS) 向其用户发送确认消息。用户必须能够回复 SMS 消息。该公司必须将回复存储一年以供分析。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Connect 联络流程来发送 SMS 消息。 使用 AWS Lambda 处理回复。",
      "B": "构建一个 Amazon Pinpoint 旅程。配置 Amazon Pinpoint 将事件发送到 Amazon Kinesis 数据流以进行分析和归档。",
      "C": "使用 Amazon Simple Queue Service (Amazon SQS) 分发 SMS 消息。使用 AWS Lambda 处理回复。",
      "D": "创建一个 Amazon Simple Notification Service (Amazon SNS) FIFO 主题。将 Amazon Kinesis 数据流订阅到 SNS 主题以进行分析和归档。"
    },
    "tags": [
      "Amazon Connect",
      "AWS Lambda",
      "SMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 87%），解析仅供参考。】\n\n考察使用 AWS 服务构建 SMS 消息发送和回复处理，以及数据存储和分析的方案。",
      "why_correct": "Amazon Pinpoint 专门用于营销传播，提供了发送 SMS 消息和处理回复的功能。构建旅程可以满足发送消息的需求，配置事件发送到 Amazon Kinesis 数据流可以实现数据存储和分析，符合题目中对回复存储一年的要求。",
      "why_wrong": "选项 A 使用 Amazon Connect 发送 SMS 消息，但 Connect 主要用于客户服务和呼叫中心，不适合大规模营销传播，且该方案缺少存储回复信息的直接途径。选项 C 使用 Amazon SQS 分发消息，SQS 本身不具备发送 SMS 的能力，需要其他服务配合，且该方案缺少存储回复信息的直接途径。选项 D 使用 Amazon SNS FIFO 主题发送 SMS，SNS 主要用于发布-订阅模式，而非处理双向消息交互，而且该方案缺少处理回复的机制。"
    },
    "related_terms": [
      "Amazon Pinpoint",
      "SMS",
      "Amazon Kinesis Data Streams",
      "Amazon Connect",
      "AWS Lambda",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "FIFO",
      "AWS"
    ]
  },
  {
    "id": 202,
    "topic": "1",
    "question_en": "A company is planning to move its data to an Amazon S3 bucket. The data must be encrypted when it is stored in the S3 bucket. Additionally, the encryption key must be automatically rotated every year. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Move the data to the S3 bucket. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.",
      "B": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket’s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket.",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket’s default encryption behavior to use the customer managed KMS key. Move the data to the S3 bucket. Manually rotate the KMS key every year.",
      "D": "Encrypt the data with customer key material before moving the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation."
    },
    "correct_answer": "B",
    "vote_percentage": "55%",
    "question_cn": "一家公司计划将其数据移动到 Amazon S3 存储桶。 数据在存储在 S3 存储桶中时必须进行加密。 此外，加密密钥必须每年自动轮换。 哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将数据移动到 S3 存储桶。 使用具有 Amazon S3 托管加密密钥 (SSE-S3) 的服务器端加密。 使用 SSE-S3 加密密钥的内置密钥轮换行为。",
      "B": "创建 AWS Key Management Service (AWS KMS) 客户托管密钥。 启用自动密钥轮换。 将 S3 存储桶的默认加密行为设置为使用客户托管 KMS 密钥。 将数据移动到 S3 存储桶。",
      "C": "创建 AWS Key Management Service (AWS KMS) 客户托管密钥。 将 S3 存储桶的默认加密行为设置为使用客户托管 KMS 密钥。 将数据移动到 S3 存储桶。 每年手动轮换 KMS 密钥。",
      "D": "在将数据移动到 S3 存储桶之前，使用客户密钥材料对数据进行加密。 创建一个没有密钥材料的 AWS Key Management Service (AWS KMS) 密钥。 将客户密钥材料导入 KMS 密钥。 启用自动密钥轮换。"
    },
    "tags": [
      "Amazon S3",
      "AWS KMS",
      "SSE-KMS",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 55%），解析仅供参考。】\n\n考察使用 AWS KMS 进行 S3 存储桶数据加密，并实现密钥自动轮换。涉及 S3 存储桶的默认加密配置、KMS 客户托管密钥的创建及使用。同时也考察了不同的加密选项（SSE-S3，SSE-KMS）的理解和适用场景。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选择 B 正确，因为它满足了题目的所有要求。该方案创建 AWS KMS 客户托管密钥，启用自动密钥轮换，并将 S3 存储桶的默认加密设置为使用客户托管 KMS 密钥。这样，所有存储在存储桶中的对象都会使用 KMS 密钥进行加密，并且 KMS 密钥将自动轮换，无需手动干预，符合最小运营开销的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，因为 SSE-S3 采用 Amazon S3 托管加密密钥，不提供密钥轮换配置，不满足题目对密钥每年轮换的需求。选项 C 错误，虽然它使用了客户托管 KMS 密钥，但手动轮换 KMS 密钥不符合题目中“最小运营开销”的要求。选项 D 错误，虽然创建了 KMS 密钥并开启了自动轮换，但在将数据导入 KMS 密钥后，题目的条件“数据在存储在 S3 存储桶中时必须进行加密”无法满足，因为该方案是将数据加密后再上传到S3，无法通过配置实现S3的默认加密。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS KMS",
      "SSE-S3",
      "SSE-KMS",
      "Encryption",
      "Key Rotation"
    ]
  },
  {
    "id": 203,
    "topic": "1",
    "question_en": "The customers of a finance company request appointments with financial advisors by sending text messages. A web application that runs on Amazon EC2 instances accepts the appointment requests. The text messages are published to an Amazon Simple Queue Service (Amazon SQS) queue through the web application. Another application that runs on EC2 instances then sends meeting invitations and meeting confirmation email messages to the customers. After successful scheduling, this application stores the meeting information in an Amazon DynamoDB database. As the company expands, customers report that their meeting invitations are taking longer to arrive. What should a solutions architect recommend to resolve this issue?",
    "options_en": {
      "A": "Add a DynamoDB Accelerator (DAX) cluster in front of the DynamoDB database.",
      "B": "Add an Amazon API Gateway API in front of the web application that accepts the appointment requests.",
      "C": "Add an Amazon CloudFront distribution. Set the origin as the web application that accepts the appointment requests.",
      "D": "Add an Auto Scaling group for the application that sends meeting invitations. Configure the Auto Scaling group to scale based on the depth of the SQS queue."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家金融公司的客户通过发送短信来预约理财顾问。一个运行在 Amazon EC2 实例上的 Web 应用程序接受预约请求。短信通过该 Web 应用程序发布到 Amazon Simple Queue Service (Amazon SQS) 队列。另一个运行在 EC2 实例上的应用程序然后向客户发送会议邀请和会议确认电子邮件。成功安排后，该应用程序将会议信息存储在 Amazon DynamoDB 数据库中。随着公司的发展，客户报告说他们的会议邀请需要更长时间才能到达。解决方案架构师应该推荐什么来解决这个问题？",
    "options_cn": {
      "A": "在 DynamoDB 数据库前面添加一个 DynamoDB Accelerator (DAX) 集群。",
      "B": "在接受预约请求的 Web 应用程序前面添加一个 Amazon API Gateway API。",
      "C": "添加一个 Amazon CloudFront 分发。将源设置为接受预约请求的 Web 应用程序。",
      "D": "为发送会议邀请的应用程序添加一个 Auto Scaling 组。配置 Auto Scaling 组以根据 SQS 队列的深度进行扩展。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon SQS",
      "Auto Scaling",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查应用的可扩展性及弹性，特别是针对 SQS 队列深度进行自动扩展的方案选择。这与 Auto Scaling、SQS、EC2 实例的协同工作方式相关。",
      "why_correct": "为发送会议邀请的应用程序添加一个 Auto Scaling 组，并配置其根据 SQS 队列深度进行扩展是解决问题的最佳方案。当 SQS 队列中的消息数量增加时，表明待处理的会议邀请数量增加，Auto Scaling 组可以自动增加 EC2 实例的数量，从而加快会议邀请的发送速度，解决客户反馈的邀请延迟问题。这种方案实现了水平扩展，能够动态调整资源以适应负载的变化，保证了服务的可用性和响应速度。",
      "why_wrong": "A. 在 DynamoDB 数据库前面添加一个 DynamoDB Accelerator (DAX) 集群，可以提高 DynamoDB 数据的读取性能，但与会议邀请的发送延迟问题无关，无法解决客户遇到的问题。\nB. 在接受预约请求的 Web 应用程序前面添加一个 Amazon API Gateway API，API Gateway 主要用于处理 API 请求，不会直接影响会议邀请的发送速度，无法解决问题。API Gateway 主要涉及 API 管理，与消息发送和处理流程无关。\nC. 添加一个 Amazon CloudFront 分发，将源设置为接受预约请求的 Web 应用程序，CloudFront 用于加速静态内容的传输，并不会影响邮件发送的速度，且和 SQS 的消息处理机制无关，无法解决问题。 CloudFront 主要用于 CDN 加速，而本题的关键在于加速邮件发送服务。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon SQS",
      "Auto Scaling",
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon API Gateway",
      "Amazon CloudFront",
      "SQS"
    ]
  },
  {
    "id": 204,
    "topic": "1",
    "question_en": "An online retail company has more than 50 million active customers and receives more than 25,000 orders each day. The company collects purchase data for customers and stores this data in Amazon S3. Additional customer data is stored in Amazon RDS. The company wants to make all the data available to various teams so that the teams can perform analytics. The solution must provide the ability to manage fine-grained permissions for the data and must minimize operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the purchase data to write directly to Amazon RDS. Use RDS access controls to limit access.",
      "B": "Schedule an AWS Lambda function to periodically copy data from Amazon RDS to Amazon S3. Create an AWS Glue crawler. Use Amazon Athena to query the data. Use S3 policies to limit access.",
      "C": "Create a data lake by using AWS Lake Formation. Create an AWS Glue JDBC connection to Amazon RDS. Register the S3 bucket in Lake Formation. Use Lake Formation access controls to limit access.",
      "D": "Create an Amazon Redshift cluster. Schedule an AWS Lambda function to periodically copy data from Amazon S3 and Amazon RDS to Amazon Redshift. Use Amazon Redshift access controls to limit access."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家在线零售公司拥有超过 5000 万活跃客户，每天收到超过 25000 份订单。该公司为客户收集购买数据，并将这些数据存储在 Amazon S3 中。其他客户数据存储在 Amazon RDS 中。该公司希望向各个团队提供所有数据，以便团队可以执行分析。该解决方案必须提供对数据的细粒度权限管理能力，并且必须最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将购买数据迁移到直接写入 Amazon RDS。使用 RDS 访问控制来限制访问。",
      "B": "安排一个 AWS Lambda 函数，用于定期间隔将数据从 Amazon RDS 复制到 Amazon S3。创建一个 AWS Glue 爬虫。使用 Amazon Athena 查询数据。使用 S3 策略来限制访问。",
      "C": "使用 AWS Lake Formation 创建一个数据湖。创建与 Amazon RDS 的 AWS Glue JDBC 连接。在 Lake Formation 中注册 S3 存储桶。使用 Lake Formation 访问控制来限制访问。",
      "D": "创建一个 Amazon Redshift 集群。安排一个 AWS Lambda 函数，用于定期间隔将数据从 Amazon S3 和 Amazon RDS 复制到 Amazon Redshift。使用 Amazon Redshift 访问控制来限制访问。"
    },
    "tags": [
      "Amazon S3",
      "Amazon RDS",
      "Amazon Redshift",
      "AWS Lambda",
      "AWS Glue",
      "AWS Lake Formation",
      "Amazon Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在数据湖场景下，如何结合 Amazon S3、Amazon RDS 和数据访问控制，实现高效的数据分析和细粒度的权限管理，并最大限度地减少运营开销。",
      "why_correct": "AWS Lake Formation 提供了集中式的数据湖管理服务，可以简化数据的摄取、清理、转换和安全访问。通过 Lake Formation，可以创建与 Amazon RDS 的 JDBC 连接，注册 S3 存储桶，实现对数据的细粒度访问控制，满足题目中对权限管理和运营开销的要求。Lake Formation 简化了数据湖的构建和管理，使得团队可以专注于数据分析，而不是底层的基础设施管理。",
      "why_wrong": "选项 A 尝试将大量购买数据直接写入 RDS，会严重影响 RDS 的性能，而且 RDS 不适合存储大量非结构化数据，扩展性差。选项 B 虽然使用了 S3 和 Athena，但是数据复制由 Lambda 函数执行，需要手动管理，并且 Glue 爬虫可能无法完美处理 RDS 的数据结构，影响数据分析。选项 D 使用 Redshift，需要维护一个独立的集群，运维成本较高，而且数据需要从 S3 和 RDS 复制到 Redshift，增加了额外的复杂性和延迟，不符合最小化运营开销的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon RDS",
      "AWS Lambda",
      "AWS Glue",
      "Amazon Athena",
      "Amazon Redshift",
      "AWS Lake Formation",
      "JDBC"
    ]
  },
  {
    "id": 205,
    "topic": "1",
    "question_en": "A company hosts a marketing website in an on-premises data center. The website consists of static documents and runs on a single server. An administrator updates the website content infrequently and uses an SFTP client to upload new documents. The company decides to host its website on AWS and to use Amazon CloudFront. The company’s solutions architect creates a CloudFront distribution. The solutions architect must design the most cost-effective and resilient architecture for website hosting to serve as the CloudFront origin. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a virtual server by using Amazon Lightsail. Configure the web server in the Lightsail instance. Upload website content by using an SFTP client.",
      "B": "Create an AWS Auto Scaling group for Amazon EC2 instances. Use an Application Load Balancer. Upload website content by using an SFTP client.",
      "C": "Create a private Amazon S3 bucket. Use an S3 bucket policy to allow access from a CloudFront origin access identity (OAI). Upload website content by using the AWS CLI.",
      "D": "Create a public Amazon S3 bucket. Configure AWS Transfer for SFTP. Configure the S3 bucket for website hosting. Upload website content by using the SFTP client."
    },
    "correct_answer": "C",
    "vote_percentage": "76%",
    "question_cn": "一家公司在其本地数据中心托管一个营销网站。该网站由静态文档组成，并在单个服务器上运行。管理员不经常更新网站内容，并使用 SFTP 客户端上传新文档。该公司决定在 AWS 上托管其网站并使用 Amazon CloudFront。该公司的解决方案架构师创建了一个 CloudFront 分发。解决方案架构师必须为网站托管设计最具成本效益和弹性的架构，以充当 CloudFront 源。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Lightsail 创建一个虚拟服务器。在 Lightsail 实例中配置 Web 服务器。使用 SFTP 客户端上传网站内容。",
      "B": "为 Amazon EC2 实例创建一个 AWS Auto Scaling 组。使用 Application Load Balancer。使用 SFTP 客户端上传网站内容。",
      "C": "创建一个私有的 Amazon S3 存储桶。使用 S3 存储桶策略允许从 CloudFront 源访问身份 (OAI) 进行访问。使用 AWS CLI 上传网站内容。",
      "D": "创建一个公共的 Amazon S3 存储桶。配置 AWS Transfer for SFTP。配置 S3 存储桶以用于网站托管。使用 SFTP 客户端上传网站内容。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "S3",
      "S3 Bucket Policy",
      "CloudFront Origin Access Identity (OAI)",
      "AWS CLI",
      "Website Hosting"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 76%），解析仅供参考。】\n\n考查利用 Amazon CloudFront 托管静态网站的架构设计；与成本效益、弹性、安全性、上传方式相关。",
      "why_correct": "选项 C 提供了最具成本效益和弹性的解决方案。 使用 Amazon S3 作为 CloudFront 的源，可以实现高度的可用性和可扩展性。通过创建私有 S3 存储桶，可以保证内容的安全。 使用 CloudFront Origin Access Identity (OAI) 限制对 S3 桶的访问，仅允许 CloudFront 访问。使用 AWS CLI 上传内容，可以实现自动化和集成，并且比 SFTP 更具效率。",
      "why_wrong": "选项 A 错误，因为 Amazon Lightsail 在弹性和可扩展性方面不如 S3，并且需要手动维护服务器，这增加了运维成本和复杂性。 选项 B 错误，虽然 EC2 和 Auto Scaling 提供了弹性，但相比于 S3，维护成本更高，且需要 Application Load Balancer，增加了整体复杂度和费用，上传方式也相对复杂。 选项 D 错误，虽然使用公共 S3 桶和 AWS Transfer for SFTP 允许通过 SFTP 上传内容，但这比通过 AWS CLI 上传内容更不具效率，使用公共桶存在安全风险，除非进行更复杂的安全配置，而且需要额外配置 AWS Transfer for SFTP 服务，增加了复杂性和成本。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "S3",
      "Lightsail",
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "AWS CLI",
      "S3 Bucket Policy",
      "CloudFront Origin Access Identity (OAI)",
      "AWS Transfer for SFTP"
    ]
  },
  {
    "id": 206,
    "topic": "1",
    "question_en": "A company wants to manage Amazon Machine Images (AMIs). The company currently copies AMIs to the same AWS Region where the AMIs were created. The company needs to design an application that captures AWS API calls and sends alerts whenever the Amazon EC2 CreateImage API operation is called within the company’s account. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Lambda function to query AWS CloudTrail logs and to send an alert when a CreateImage API call is detected.",
      "B": "Configure AWS CloudTrail with an Amazon Simple Notification Service (Amazon SNS) notification that occurs when updated logs are sent to Amazon S3. Use Amazon Athena to create a new table and to query on CreateImage when an API call is detected.",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the CreateImage API call. Configure the target as an Amazon Simple Notification Service (Amazon SNS) topic to send an alert when a CreateImage API call is detected.",
      "D": "Configure an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a target for AWS CloudTrail logs. Create an AWS Lambda function to send an alert to an Amazon Simple Notification Service (Amazon SNS) topic when a CreateImage API call is detected."
    },
    "correct_answer": "C",
    "vote_percentage": "72%",
    "question_cn": "一家公司希望管理 Amazon Machine Images (AMIs)。该公司目前将 AMIs 复制到创建 AMIs 的相同 AWS 区域。该公司需要设计一个应用程序，该应用程序捕获 AWS API 调用，并在公司帐户内调用 Amazon EC2 CreateImage API 操作时发送警报。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数来查询 AWS CloudTrail 日志，并在检测到 CreateImage API 调用时发送警报。",
      "B": "使用 Amazon Simple Notification Service (Amazon SNS) 通知配置 AWS CloudTrail，该通知发生在将更新的日志发送到 Amazon S3 时。使用 Amazon Athena 创建一个新表，并在检测到 API 调用时查询 CreateImage。",
      "C": "为 CreateImage API 调用创建一个 Amazon EventBridge (Amazon CloudWatch Events) 规则。将目标配置为 Amazon Simple Notification Service (Amazon SNS) 主题，以便在检测到 CreateImage API 调用时发送警报。",
      "D": "将 Amazon Simple Queue Service (Amazon SQS) FIFO 队列配置为 AWS CloudTrail 日志的目标。创建一个 AWS Lambda 函数，以便在检测到 CreateImage API 调用时将警报发送到 Amazon Simple Notification Service (Amazon SNS) 主题。"
    },
    "tags": [
      "Amazon EC2",
      "CloudTrail",
      "Amazon CloudWatch Events",
      "Amazon SNS",
      "Amazon SQS",
      "Lambda",
      "Amazon Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 72%），解析仅供参考。】\n\n考查如何使用 Amazon EventBridge 监控 AWS API 调用，并触发告警通知。",
      "why_correct": "选项 C 使用 Amazon EventBridge 规则直接监听 CreateImage API 调用。EventBridge 规则可以配置 SNS 目标，当检测到匹配事件时，直接触发 SNS 通知，无需额外的复杂配置，运营开销最低，满足题目的要求。",
      "why_wrong": "选项 A 需要 Lambda 函数来查询 CloudTrail 日志，增加了代码编写和维护成本，且查询日志的效率不如 EventBridge 规则直接触发。选项 B 需要使用 Athena 查询 S3 中的 CloudTrail 日志，查询效率较低，并且配置复杂。选项 D 使用 SQS 和 Lambda，需要额外配置 SQS 队列、Lambda 函数处理消息，以及两者之间的交互，增加了复杂性，不符合最少运营开销的要求。"
    },
    "related_terms": [
      "Amazon Machine Images (AMIs)",
      "Amazon EC2 CreateImage API",
      "AWS Lambda",
      "AWS CloudTrail",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon S3",
      "Amazon Athena",
      "Amazon EventBridge (Amazon CloudWatch Events)",
      "Amazon Simple Queue Service (Amazon SQS)",
      "FIFO"
    ]
  },
  {
    "id": 207,
    "topic": "1",
    "question_en": "A company owns an asynchronous API that is used to ingest user requests and, based on the request type, dispatch requests to the appropriate microservice for processing. The company is using Amazon API Gateway to deploy the API front end, and an AWS Lambda function that invokes Amazon DynamoDB to store user requests before dispatching them to the processing microservices. The company provisioned as much DynamoDB throughput as its budget allows, but the company is still experiencing availability issues and is losing user requests. What should a solutions architect do to address this issue without impacting existing users?",
    "options_en": {
      "A": "Add throttling on the API Gateway with server-side throttling limits.",
      "B": "Use DynamoDB Accelerator (DAX) and Lambda to buffer writes to DynamoDB.",
      "C": "Create a secondary index in DynamoDB for the table with the user requests.",
      "D": "Use the Amazon Simple Queue Service (Amazon SQS) queue and Lambda to buffer writes to DynamoDB."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一家公司拥有一个异步 API，用于接收用户请求，并根据请求类型，将请求分发到适当的微服务进行处理。该公司正在使用 Amazon API Gateway 部署 API 前端，并使用一个 AWS Lambda 函数来调用 Amazon DynamoDB，以在将用户请求分发到处理微服务之前存储它们。该公司已根据其预算配置了尽可能多的 DynamoDB 吞吐量，但该公司仍然遇到可用性问题，并且正在丢失用户请求。 解决方案架构师应该怎么做才能解决此问题，而不会影响现有用户？",
    "options_cn": {
      "A": "在 API Gateway 上添加限制，并使用服务器端限制。 ",
      "B": "使用 DynamoDB Accelerator (DAX) 和 Lambda 缓冲对 DynamoDB 的写入。 ",
      "C": "为包含用户请求的表在 DynamoDB 中创建二级索引。",
      "D": "使用 Amazon Simple Queue Service (Amazon SQS) 队列和 Lambda 来缓冲对 DynamoDB 的写入。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon SQS",
      "Lambda",
      "API Gateway",
      "Availability",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n本题考查如何解决 DynamoDB 写入瓶颈导致的可用性问题，同时不能影响现有用户。与 DynamoDB 吞吐量管理、异步处理、队列服务、以及 Lambda 函数的合理应用相关。",
      "why_correct": "选项 D 使用 Amazon SQS 队列和 Lambda 来缓冲对 DynamoDB 的写入。通过将用户请求放入 SQS 队列，Lambda 函数可以异步地从队列中读取请求，然后将它们写入 DynamoDB。SQS 提供了高可用性和可扩展性，可以处理突发的请求流量。Lambda 函数可以根据需要自动扩展，从而确保 DynamoDB 不会被过载。这种方法可以有效地解耦 API Gateway 和 DynamoDB，提高系统的可用性和可靠性，并且在 DynamoDB 写入发生问题时，请求会保留在队列中，不会丢失。",
      "why_wrong": "选项 A，在 API Gateway 上添加限制并使用服务器端限制，只能限制 API Gateway 接收的请求数量，无法解决 DynamoDB 写入瓶颈的问题。虽然可以避免因过载导致的服务不可用，但会影响用户体验，因为某些用户请求可能会被拒绝。\n选项 B，使用 DynamoDB Accelerator (DAX) 来加速读取，对于提高读取性能有帮助，但无法解决 DynamoDB 写入瓶颈的问题。而且，DAX 主要用于缓存读取结果，对写入性能提升有限。使用 Lambda 缓冲对 DynamoDB 的写入，虽然可以一定程度缓解写入压力，但无法保证可靠性，一旦 Lambda 函数失败，数据仍然可能丢失。\n选项 C，为包含用户请求的表在 DynamoDB 中创建二级索引，主要用于提高查询效率，与解决写入瓶颈无关。创建二级索引会增加写入的开销，反而可能加剧 DynamoDB 的吞吐量问题，进一步导致可用性问题。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "Lambda",
      "Amazon DynamoDB",
      "DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon SQS",
      "SQS"
    ]
  },
  {
    "id": 208,
    "topic": "1",
    "question_en": "A company needs to move data from an Amazon EC2 instance to an Amazon S3 bucket. The company must ensure that no API calls and no data are routed through public internet routes. Only the EC2 instance can have access to upload data to the S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an interface VPC endpoint for Amazon S3 in the subnet where the EC2 instance is located. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.",
      "B": "Create a gateway VPC endpoint for Amazon S3 in the Availability Zone where the EC2 instance is located. Attach appropriate security groups to the endpoint. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.",
      "C": "Run the nslookup tool from inside the EC2 instance to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access.",
      "D": "Use the AWS provided, publicly available ip-ranges.json file to obtain the private IP address of the S3 bucket’s service API endpoint. Create a route in the VPC route table to provide the EC2 instance with access to the S3 bucket. Attach a resource policy to the S3 bucket to only allow the EC2 instance’s IAM role for access."
    },
    "correct_answer": "A",
    "vote_percentage": "57%",
    "question_cn": "一家公司需要将数据从 Amazon EC2 实例移动到 Amazon S3 存储桶。该公司必须确保没有 API 调用和数据通过公共互联网路由。只有 EC2 实例可以访问以将数据上传到 S3 存储桶。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 EC2 实例所在的子网中为 Amazon S3 创建一个接口 VPC endpoint。将资源策略附加到 S3 存储桶，仅允许 EC2 实例的 IAM 角色进行访问。",
      "B": "在 EC2 实例所在的可用区中为 Amazon S3 创建一个网关 VPC endpoint。将适当的安全组附加到 endpoint。将资源策略附加到 S3 存储桶，仅允许 EC2 实例的 IAM 角色进行访问。",
      "C": "从 EC2 实例内部运行 nslookup 工具，以获取 S3 存储桶服务的 API endpoint 的私有 IP 地址。在 VPC 路由表中创建一条路由，为 EC2 实例提供对 S3 存储桶的访问权限。将资源策略附加到 S3 存储桶，仅允许 EC2 实例的 IAM 角色进行访问。",
      "D": "使用 AWS 提供的公开可用的 ip-ranges.json 文件来获取 S3 存储桶服务的 API endpoint 的私有 IP 地址。在 VPC 路由表中创建一条路由，为 EC2 实例提供对 S3 存储桶的访问权限。将资源策略附加到 S3 存储桶，仅允许 EC2 实例的 IAM 角色进行访问。"
    },
    "tags": [
      "Amazon S3",
      "VPC Endpoint",
      "Gateway VPC Endpoint",
      "Interface VPC Endpoint",
      "IAM",
      "EC2",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 57%），解析仅供参考。】\n\n考查通过 VPC endpoint 安全地将 EC2 实例的数据上传到 S3，并限制 API 调用通过公共互联网路由。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：接口 VPC endpoint 允许从 VPC 私有访问 Amazon S3，避免数据通过公共互联网传输。选项 A 创建了接口 VPC endpoint，并结合 S3 存储桶的资源策略，确保只有 EC2 实例的 IAM 角色可以访问 S3，满足所有安全需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 使用网关 VPC endpoint，仅支持对 S3 的访问，而题目要求是不能通过公共互联网进行 API 调用。选项 B 仅仅在可用区内创建，没有解决通过公网访问的问题。选项 C 和 D 尝试通过获取 S3 服务的私有 IP 地址，手动配置路由表，但这种方法不可靠，因为 S3 的 IP 地址可能发生变化，而且无法完全规避公网访问，也不够安全。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "VPC endpoint",
      "IAM",
      "API",
      "VPC",
      "nslookup",
      "ip-ranges.json",
      "IAM role"
    ]
  },
  {
    "id": 209,
    "topic": "1",
    "question_en": "A solutions architect is designing the architecture of a new application being deployed to the AWS Cloud. The application will run on Amazon EC2 On-Demand Instances and will automatically scale across multiple Availability Zones. The EC2 instances will scale up and down frequently throughout the day. An Application Load Balancer (ALB) will handle the load distribution. The architecture needs to support distributed session data management. The company is willing to make changes to code if needed. What should the solutions architect do to ensure that the architecture supports distributed session data management?",
    "options_en": {
      "A": "Use Amazon ElastiCache to manage and store session data.",
      "B": "Use session afinity (sticky sessions) of the ALB to manage session data.",
      "C": "Use Session Manager from AWS Systems Manager to manage the session.",
      "D": "Use the GetSessionToken API operation in AWS Security Token Service (AWS STS) to manage the session."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在设计部署到 AWS 云的新应用程序的架构。该应用程序将在 Amazon EC2 按需实例上运行，并将自动跨多个可用区扩展。 EC2 实例将在一天中频繁地上下扩展。一个 Application Load Balancer (ALB) 将处理负载分配。该架构需要支持分布式会话数据管理。如果需要，公司愿意更改代码。解决方案架构师应该怎么做才能确保架构支持分布式会话数据管理？",
    "options_cn": {
      "A": "使用 Amazon ElastiCache 管理和存储会话数据。",
      "B": "使用 ALB 的会话亲和性（粘性会话）来管理会话数据。",
      "C": "使用 AWS Systems Manager 中的 Session Manager 来管理会话。",
      "D": "使用 AWS Security Token Service (AWS STS) 中的 GetSessionToken API 操作来管理会话。"
    },
    "tags": [
      "Amazon ElastiCache",
      "Application Load Balancer",
      "Amazon EC2",
      "Distributed session data management"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查分布式会话数据管理方案的选择，以及与 EC2 自动扩展、Application Load Balancer 结合的场景。也与 ElastiCache、会话亲和性、AWS Systems Manager 和 AWS STS 相关。",
      "why_correct": "使用 Amazon ElastiCache 可以有效地管理和存储分布式会话数据。ElastiCache 提供了 Redis 和 Memcached 等缓存引擎，能够提供快速的读写性能，满足应用程序对会话数据的快速访问需求。ElastiCache 作为一个独立的服务，与 EC2 实例解耦，避免了单点故障，可以实现高可用性和可扩展性。这种方式适用于 EC2 自动伸缩的场景，因为会话数据存储在 ElastiCache 中，而不是 EC2 实例本地，实例的上下伸缩不会导致会话数据丢失。",
      "why_wrong": "选项 B 错误，因为使用 ALB 的会话亲和性（粘性会话）仅能将来自特定客户端的请求定向到特定的 EC2 实例。虽然可以暂时解决会话保持问题，但并不适用于 EC2 实例频繁伸缩的场景，因为当实例销毁时，会话数据将丢失。选项 C 错误，AWS Systems Manager 中的 Session Manager 主要用于远程管理 EC2 实例的会话，如 SSH 连接，而不是用于管理和存储应用程序的会话数据。选项 D 错误， AWS Security Token Service (AWS STS) 用于创建临时的、有时间限制的访问凭证，主要用于安全访问 AWS 资源，与应用程序的会话管理无关，无法满足分布式会话数据的存储和管理需求。"
    },
    "related_terms": [
      "Amazon ElastiCache",
      "Redis",
      "Application Load Balancer",
      "ALB",
      "Amazon EC2",
      "EC2",
      "AWS Systems Manager",
      "AWS Security Token Service",
      "AWS STS",
      "Memcached",
      "Session Manager",
      "GetSessionToken"
    ]
  },
  {
    "id": 210,
    "topic": "1",
    "question_en": "A company offers a food delivery service that is growing rapidly. Because of the growth, the company’s order processing system is experiencing scaling problems during peak trafic hours. The current architecture includes the following: • A group of Amazon EC2 instances that run in an Amazon EC2 Auto Scaling group to collect orders from the application • Another group of EC2 instances that run in an Amazon EC2 Auto Scaling group to fulfill orders The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale properly during peak trafic hours. The solution must optimize utilization of the company’s AWS resources. Which solution meets these requirements?",
    "options_en": {
      "A": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure each Auto Scaling group’s minimum capacity according to peak workload values.",
      "B": "Use Amazon CloudWatch metrics to monitor the CPU of each instance in the Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic that creates additional Auto Scaling groups on demand.",
      "C": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Scale the Auto Scaling groups based on notifications that the queues send.",
      "D": "Provision two Amazon Simple Queue Service (Amazon SQS) queues: one for order collection and another for order fulfillment. Configure the EC2 instances to poll their respective queue. Create a metric based on a backlog per instance calculation. Scale the Auto Scaling groups based on this metric."
    },
    "correct_answer": "D",
    "vote_percentage": "88%",
    "question_cn": "一家公司提供快速增长的食品配送服务。由于业务增长，该公司订单处理系统在高峰时段遇到了扩展问题。当前的架构包括以下内容： • 一组在 Amazon EC2 Auto Scaling 组中运行的 Amazon EC2 实例，用于从应用程序收集订单 • 另一组在 Amazon EC2 Auto Scaling 组中运行的 EC2 实例，用于履行订单 订单收集过程很快，但订单履行过程可能需要更长时间。由于扩展事件，数据不能丢失。一个解决方案架构师必须确保订单收集过程和订单履行过程都能够在高峰时段正确扩展。该解决方案必须优化公司 AWS 资源的利用率。哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch 指标来监控 Auto Scaling 组中每个实例的 CPU。根据峰值工作负载值配置每个 Auto Scaling 组的最小容量。",
      "B": "使用 Amazon CloudWatch 指标来监控 Auto Scaling 组中每个实例的 CPU。配置 CloudWatch 告警以调用 Amazon Simple Notification Service (Amazon SNS) 主题，该主题按需创建额外的 Auto Scaling 组。",
      "C": "预置两个 Amazon Simple Queue Service (Amazon SQS) 队列：一个用于订单收集，另一个用于订单履行。配置 EC2 实例来轮询它们各自的队列。根据队列发送的通知扩展 Auto Scaling 组。",
      "D": "预置两个 Amazon Simple Queue Service (Amazon SQS) 队列：一个用于订单收集，另一个用于订单履行。配置 EC2 实例来轮询它们各自的队列。基于每个实例的积压计算创建一个指标。根据此指标扩展 Auto Scaling 组。"
    },
    "tags": [
      "Amazon EC2 Auto Scaling",
      "Amazon SQS",
      "Queue",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 88%），解析仅供参考。】\n\n考察如何利用 Amazon SQS 队列实现异步处理，并根据队列消息积压量进行 Auto Scaling，从而解决高峰时段的扩展问题。",
      "why_correct": "选项 D 使用两个 SQS 队列，一个用于订单收集，一个用于订单履行，实现了异步解耦。EC2 实例从队列中轮询消息，并根据每个实例的积压量创建一个自定义指标，用于触发 Auto Scaling。这确保了订单收集和履行过程都能根据需求动态扩展，并优化 AWS 资源的利用率。",
      "why_wrong": "选项 A 仅基于 CPU 指标进行扩展，无法直接反映订单处理的积压情况，可能导致扩展不及时或过度扩展。选项 B 使用 SNS 主题触发新的 Auto Scaling 组创建，增加了复杂性和延迟，且未提供基于队列积压的精确扩展机制。选项 C 基于队列通知扩展 Auto Scaling 组，但未说明如何准确衡量积压量，从而无法实现精细化的弹性伸缩。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon CloudWatch",
      "CPU",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Queue Service (Amazon SQS)"
    ]
  },
  {
    "id": 211,
    "topic": "1",
    "question_en": "A company hosts multiple production applications. One of the applications consists of resources from Amazon EC2, AWS Lambda, Amazon RDS, Amazon Simple Notification Service (Amazon SNS), and Amazon Simple Queue Service (Amazon SQS) across multiple AWS Regions. All company resources are tagged with a tag name of “application” and a value that corresponds to each application. A solutions architect must provide the quickest solution for identifying all of the tagged components. Which solution meets these requirements?",
    "options_en": {
      "A": "Use AWS CloudTrail to generate a list of resources with the application tag.",
      "B": "Use the AWS CLI to query each service across all Regions to report the tagged components.",
      "C": "Run a query in Amazon CloudWatch Logs Insights to report on the components with the application tag.",
      "D": "Run a query with the AWS Resource Groups Tag Editor to report on the resources globally with the application tag."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管多个生产应用程序。其中一个应用程序包含来自 Amazon EC2、AWS Lambda、Amazon RDS、Amazon Simple Notification Service (Amazon SNS) 和 Amazon Simple Queue Service (Amazon SQS) 的资源，这些资源分布在多个 AWS 区域。所有公司资源都使用标签名称“application”进行标记，其值对应于每个应用程序。解决方案架构师必须提供最快的解决方案来识别所有标记的组件。哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 AWS CloudTrail 生成带有 application 标签的资源列表。",
      "B": "使用 AWS CLI 查询所有区域中的每个服务，以报告标记的组件。",
      "C": "在 Amazon CloudWatch Logs Insights 中运行查询，以报告带有 application 标签的组件。",
      "D": "使用 AWS Resource Groups Tag Editor 运行查询，以全局报告带有 application 标签的资源。"
    },
    "tags": [
      "Resource Groups",
      "Tag Editor"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查如何快速识别跨多个 AWS 区域的资源，以及使用资源标签进行资源管理的场景。这题与 AWS Resource Groups 和 AWS Tag Editor 的功能特性相关。",
      "why_correct": "AWS Resource Groups Tag Editor 允许您使用标签来跨多个 AWS 区域组织和管理 AWS 资源。通过 Tag Editor，您可以搜索、添加和删除标签，这些操作可以跨多个资源类型和 AWS 区域进行。在这种情况下，可以使用 Tag Editor 快速识别所有具有“application”标签的组件，并全局报告它们。",
      "why_wrong": "A. AWS CloudTrail 主要用于审计 AWS 账户的活动，记录 API 调用。虽然 CloudTrail 可以记录资源创建和修改事件，但它不是设计用来直接查询特定标签的资源的。使用 CloudTrail 生成标签列表会涉及复杂的事件过滤和解析，效率不如直接使用 Tag Editor。B. 使用 AWS CLI 查询多个区域中的每个服务以获取资源信息需要编写复杂的脚本，并针对每个服务和区域进行调用，这不仅耗时，而且容易出错。C. Amazon CloudWatch Logs Insights 主要用于分析日志数据，无法直接查询跨服务的资源标签。虽然可以使用 CloudWatch Logs Insights 分析与资源相关的日志，但它并非为识别具有特定标签的资源而设计。"
    },
    "related_terms": [
      "EC2",
      "Lambda",
      "RDS",
      "SNS",
      "SQS",
      "AWS CloudTrail",
      "AWS CLI",
      "Amazon S3",
      "AWS",
      "CloudWatch Logs Insights",
      "Resource Groups Tag Editor"
    ]
  },
  {
    "id": 212,
    "topic": "1",
    "question_en": "A company needs to export its database once a day to Amazon S3 for other teams to access. The exported object size varies between 2 GB and 5 GB. The S3 access pattern for the data is variable and changes rapidly. The data must be immediately available and must remain accessible for up to 3 months. The company needs the most cost-effective solution that will not increase retrieval time. Which S3 storage class should the company use to meet these requirements?",
    "options_en": {
      "A": "S3 Intelligent-Tiering",
      "B": "S3 Glacier Instant Retrieval",
      "C": "S3 Standard",
      "D": "S3 Standard-Infrequent Access (S3 Standard-IA)"
    },
    "correct_answer": "A",
    "vote_percentage": "75%",
    "question_cn": "一家公司需要每天将数据库导出到 Amazon S3，供其他团队访问。 导出的对象大小在 2 GB 到 5 GB 之间变化。 数据的 S3 访问模式是可变的，并且变化迅速。 数据必须立即可用，并且必须保持可访问长达 3 个月。该公司需要最具成本效益的解决方案，且不会增加检索时间。该公司应该使用哪个 S3 存储类来满足这些要求？",
    "options_cn": {
      "A": "S3 Intelligent-Tiering",
      "B": "S3 Glacier Instant Retrieval",
      "C": "S3 Standard",
      "D": "S3 Standard-Infrequent Access (S3 Standard-IA)"
    },
    "tags": [
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier Instant Retrieval"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 75%），解析仅供参考。】\n\n考查 S3 存储类选型，侧重考虑成本效益、数据访问模式、检索时间和数据保留期等因素。这需要理解不同 S3 存储类的特性，并结合题目的具体需求进行选择。",
      "why_correct": "S3 Intelligent-Tiering 能够自动将对象存储在四种访问层中：频繁访问、不频繁访问、归档访问和深度归档访问，从而优化存储成本。它适用于访问模式未知的、或访问模式频繁变化的数据。由于题目中数据的访问模式可变且变化迅速，S3 Intelligent-Tiering 能够自动适应访问模式的变化，满足数据的立即可用性和 3 个月的保留期，同时最大限度地降低存储成本。这项服务的另一个好处是它无需任何检索成本，保证了检索时间。",
      "why_wrong": {
        "B": "S3 Glacier Instant Retrieval 专为长期归档数据而设计，虽然它提供了快速检索，但其主要目的是存储不经常访问的数据。对于需要立即可用的数据，且访问模式可变的情况，S3 Glacier Instant Retrieval 不适用。此外，虽然其检索时间较短，但成本相对较高。",
        "C": "S3 Standard 提供了高可用性和高持久性，但其存储成本较高，对于冷数据而言，这并不具备成本效益。题目的需求是数据需要保持可访问长达 3 个月，结合成本效益的考量，S3 Standard 并非最佳选择，S3 Intelligent-Tiering 可以更有效地管理成本。",
        "D": "S3 Standard-Infrequent Access (S3 Standard-IA) 存储类的价格低于 S3 Standard，但其检索成本较高，并且最小对象存储时长为 30 天。虽然它在成本上比 S3 Standard 更有优势，但是它不适用于可变访问模式的数据。由于数据访问模式可变，频繁的检索会导致高昂的检索费用，不符合最具成本效益的要求。"
      }
    },
    "related_terms": [
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3 Standard-IA",
      "S3 Glacier Instant Retrieval",
      "S3 Standard"
    ]
  },
  {
    "id": 213,
    "topic": "1",
    "question_en": "A company is developing a new mobile app. The company must implement proper trafic filtering to protect its Application Load Balancer (ALB) against common application-level attacks, such as cross-site scripting or SQL injection. The company has minimal infrastructure and operational staff. The company needs to reduce its share of the responsibility in managing, updating, and securing servers for its AWS environment. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Configure AWS WAF rules and associate them with the ALB.",
      "B": "Deploy the application using Amazon S3 with public hosting enabled.",
      "C": "Deploy AWS Shield Advanced and add the ALB as a protected resource.",
      "D": "Create a new ALB that directs trafic to an Amazon EC2 instance running a third-party firewall, which then passes the trafic to the current ALB."
    },
    "correct_answer": "A",
    "vote_percentage": "72%",
    "question_cn": "一家公司正在开发一个新的移动应用程序。该公司必须实施适当的流量过滤，以保护其 Application Load Balancer (ALB) 免受常见的应用程序级攻击，例如跨站点脚本或 SQL 注入。该公司拥有最少的基础设施和运营人员。该公司需要减少其在管理、更新和保护其 AWS 环境服务器方面的责任份额。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "配置 AWS WAF 规则并将它们与 ALB 关联。",
      "B": "使用启用公共托管的 Amazon S3 部署应用程序。",
      "C": "部署 AWS Shield Advanced 并将 ALB 添加为受保护的资源。",
      "D": "创建一个新的 ALB，将流量定向到运行第三方防火墙的 Amazon EC2 实例，然后该实例将流量传递给当前的 ALB。"
    },
    "tags": [
      "AWS WAF",
      "Application Load Balancer",
      "ALB",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 72%），解析仅供参考。】\n\n考查如何保护 Application Load Balancer (ALB) 免受常见的应用程序级攻击，以及如何在最小化运营负担的前提下选择合适的安全解决方案。与 AWS WAF、AWS Shield Advanced、Amazon S3 和第三方防火墙的选型/对比相关。",
      "why_correct": "配置 AWS WAF 规则并将它们与 ALB 关联是最佳解决方案。AWS WAF 是一项 Web 应用程序防火墙服务，可以帮助保护您的 Web 应用程序免受常见的 Web 攻击，如跨站点脚本 (XSS)、SQL 注入和跨站请求伪造 (CSRF)。通过将 WAF 规则与 ALB 关联，所有通过 ALB 的流量都将经过 WAF 的检查，从而实现对应用程序的保护。",
      "why_wrong": "选项 B 错误，因为使用 Amazon S3 部署应用程序无法提供流量过滤和安全保护，并且与题目中的 ALB 保护需求不符。选项 C 错误，因为 AWS Shield Advanced 提供了 DDoS 保护，而题目的需求是应用程序级的攻击防护，虽然 Shield Advanced 也提供了基础的 Web 应用程序保护，但其主要功能侧重于应对 DDoS 攻击，并不直接满足对 XSS 或 SQL 注入的防护需求，而且它需要额外的成本。选项 D 错误，因为创建运行第三方防火墙的 EC2 实例会增加基础设施的复杂性和管理负担，与题目中减少管理责任的要求相悖，并且增加了运营成本。同时，这种方案也容易导致单点故障，降低系统的可靠性。"
    },
    "related_terms": [
      "AWS WAF",
      "ALB",
      "Amazon S3",
      "EC2",
      "Application Load Balancer",
      "AWS Shield Advanced",
      "DDoS",
      "XSS",
      "SQL injection"
    ]
  },
  {
    "id": 214,
    "topic": "1",
    "question_en": "A company’s reporting system delivers hundreds of .csv files to an Amazon S3 bucket each day. The company must convert these files to Apache Parquet format and must store the files in a transformed data bucket. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Create an Amazon EMR cluster with Apache Spark installed. Write a Spark application to transform the data. Use EMR File System (EMRFS) to write files to the transformed data bucket.",
      "B": "Create an AWS Glue crawler to discover the data. Create an AWS Glue extract, transform, and load (ETL) job to transform the data. Specify the transformed data bucket in the output step.",
      "C": "Use AWS Batch to create a job definition with Bash syntax to transform the data and output the data to the transformed data bucket. Use the job definition to submit a job. Specify an array job as the job type.",
      "D": "Create an AWS Lambda function to transform the data and output the data to the transformed data bucket. Configure an event notification for the S3 bucket. Specify the Lambda function as the destination for the event notification."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司的报告系统每天向 Amazon S3 存储桶交付数百个 .csv 文件。该公司必须将这些文件转换为 Apache Parquet 格式，并且必须将文件存储在转换后的数据存储桶中。哪种解决方案将以最少的开发工作量满足这些要求？",
    "options_cn": {
      "A": "创建一个安装了 Apache Spark 的 Amazon EMR 集群。编写一个 Spark 应用程序来转换数据。使用 EMR File System (EMRFS) 将文件写入转换后的数据存储桶。",
      "B": "创建一个 AWS Glue 爬虫来发现数据。创建一个 AWS Glue 提取、转换和加载 (ETL) 作业来转换数据。在输出步骤中指定转换后的数据存储桶。",
      "C": "使用 AWS Batch 创建一个具有 Bash 语法的作业定义来转换数据，并将数据输出到转换后的数据存储桶。使用作业定义提交一个作业。将数组作业指定为作业类型。",
      "D": "创建一个 AWS Lambda 函数来转换数据并将数据输出到转换后的数据存储桶。为 S3 存储桶配置事件通知。将 Lambda 函数指定为事件通知的目标。"
    },
    "tags": [
      "Amazon S3",
      "AWS Lambda",
      "S3 Event Notifications",
      "Apache Parquet",
      "AWS Glue",
      "AWS Batch",
      "Amazon EMR"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查利用 AWS Glue 进行数据转换，以及与其他 AWS 服务的集成。",
      "why_correct": "AWS Glue 是一种完全托管的 ETL 服务，非常适合处理数据转换任务。创建 Glue 爬虫可以自动发现 S3 存储桶中的数据，而 Glue ETL 作业则可以方便地进行数据格式转换和存储。通过指定输出步骤，可以轻松地将转换后的数据写入目标存储桶。",
      "why_wrong": "选项 A 需要手动配置和管理 EMR 集群以及编写 Spark 应用程序，开发工作量较大，且管理成本较高。选项 C 使用 AWS Batch 涉及编写 Bash 脚本进行数据转换，相对而言开发工作量较大，也不如 Glue 方便。选项 D 使用 Lambda 函数进行数据转换需要手动编写转换逻辑，而且配置 S3 事件通知和 Lambda 触发器增加了复杂度，不如 Glue 的 ETL 作业直接。另外，Lambda 函数可能在处理大规模数据时，会出现性能瓶颈。"
    },
    "related_terms": [
      "Amazon S3",
      "Apache Parquet",
      "AWS Glue",
      "ETL",
      "AWS Glue Crawler",
      "AWS Glue ETL",
      "Amazon EMR",
      "Apache Spark",
      "EMR File System (EMRFS)",
      "AWS Batch",
      "Bash",
      "AWS Lambda",
      "S3 event notifications"
    ]
  },
  {
    "id": 215,
    "topic": "1",
    "question_en": "A company has 700 TB of backup data stored in network attached storage (NAS) in its data center. This backup data need to be accessible for infrequent regulatory requests and must be retained 7 years. The company has decided to migrate this backup data from its data center to AWS. The migration must be complete within 1 month. The company has 500 Mbps of dedicated bandwidth on its public internet connection available for data transfer. What should a solutions architect do to migrate and store the data at the LOWEST cost?",
    "options_en": {
      "A": "Order AWS Snowball devices to transfer the data. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
      "B": "Deploy a VPN connection between the data center and Amazon VPC. Use the AWS CLI to copy the data from on premises to Amazon S3 Glacier.",
      "C": "Provision a 500 Mbps AWS Direct Connect connection and transfer the data to Amazon S3. Use a lifecycle policy to transition the files to Amazon S3 Glacier Deep Archive.",
      "D": "Use AWS DataSync to transfer the data and deploy a DataSync agent on premises. Use the DataSync task to copy files from the on- premises NAS storage to Amazon S3 Glacier."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其数据中心中拥有 700 TB 的备份数据，存储在网络附加存储 (NAS) 中。此备份数据需要能够访问，以满足不经常出现的法规请求，并且必须保留 7 年。该公司已决定将其备份数据从其数据中心迁移到 AWS。迁移必须在一个月内完成。该公司在其公共互联网连接上有 500 Mbps 的专用带宽可用于数据传输。解决方案架构师应该怎么做，以最低的成本迁移和存储数据？",
    "options_cn": {
      "A": "订购 AWS Snowball 设备来传输数据。使用生命周期策略将文件转换为 Amazon S3 Glacier Deep Archive。",
      "B": "在数据中心和 Amazon VPC 之间部署 VPN 连接。使用 AWS CLI 将数据从本地复制到 Amazon S3 Glacier。",
      "C": "配置 500 Mbps 的 AWS Direct Connect 连接，并将数据传输到 Amazon S3。使用生命周期策略将文件转换为 Amazon S3 Glacier Deep Archive。",
      "D": "使用 AWS DataSync 传输数据，并在本地部署 DataSync 代理。使用 DataSync 任务将文件从本地 NAS 存储复制到 Amazon S3 Glacier。"
    },
    "tags": [
      "AWS Snowball",
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "AWS DataSync",
      "AWS Direct Connect",
      "Amazon VPC",
      "VPN",
      "AWS CLI",
      "S3 Lifecycle Policy",
      "Data Transfer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查大批量数据迁移和低成本存储方案的选择；与 AWS Snowball、AWS DataSync、AWS Direct Connect、S3 存储类和生命周期策略有关。",
      "why_correct": "选择 AWS Snowball 设备是针对大批量数据迁移的常见且经济高效的方案。对于 700 TB 的数据量，通过 500 Mbps 的互联网带宽进行传输需要较长时间，成本也较高。Snowball 设备可以离线运输数据，大大缩短迁移时间，并且其成本通常低于在线传输。使用 S3 生命周期策略将数据转换为 Glacier Deep Archive，可以满足 7 年的长期存储需求，并最大限度地降低存储成本。",
      "why_wrong": "选项 B 方案使用 VPN 和 AWS CLI 上传数据到 Glacier，速度受限于 500 Mbps 的带宽，迁移时间过长，并且手动操作数据上传效率较低。选项 C 方案需要配置 Direct Connect，虽然能提高网络传输速度，但 Direct Connect 的成本高于 Snowball，而且迁移时间同样受限于带宽。选项 D 方案使用 AWS DataSync 迁移到 Glacier，虽然自动化程度高，但 DataSync 部署和配置会产生额外成本，且同样受限于网络带宽，不适用于大批量数据迁移，而且 DataSync 方案并非直接迁移到 Deep Archive，增加了额外的复杂性。"
    },
    "related_terms": [
      "AWS Snowball",
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "AWS DataSync",
      "AWS Direct Connect",
      "VPN",
      "AWS CLI",
      "NAS",
      "DataSync",
      "Amazon VPC",
      "S3 Lifecycle Policy",
      "Data Transfer",
      "Mbps"
    ]
  },
  {
    "id": 216,
    "topic": "1",
    "question_en": "A company has a serverless website with millions of objects in an Amazon S3 bucket. The company uses the S3 bucket as the origin for an Amazon CloudFront distribution. The company did not set encryption on the S3 bucket before the objects were loaded. A solutions architect needs to enable encryption for all existing objects and for all objects that are added to the S3 bucket in the future. Which solution will meet these requirements with the LEAST amount of effort?",
    "options_en": {
      "A": "Create a new S3 bucket. Turn on the default encryption settings for the new S3 bucket. Download all existing objects to temporary local storage. Upload the objects to the new S3 bucket.",
      "B": "Turn on the default encryption settings for the S3 bucket. Use the S3 Inventory feature to create a .csv file that lists the unencrypted objects. Run an S3 Batch Operations job that uses the copy command to encrypt those objects.",
      "C": "Create a new encryption key by using AWS Key Management Service (AWS KMS). Change the settings on the S3 bucket to use server- side encryption with AWS KMS managed encryption keys (SSE-KMS). Turn on versioning for the S3 bucket.",
      "D": "Navigate to Amazon S3 in the AWS Management Console. Browse the S3 bucket’s objects. Sort by the encryption field. Select each unencrypted object. Use the Modify button to apply default encryption settings to every unencrypted object in the S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家公司拥有一个无服务器网站，其 Amazon S3 存储桶中存储了数百万个对象。该公司使用 S3 存储桶作为 Amazon CloudFront 分配的源。在加载对象之前，该公司未在 S3 存储桶上设置加密。一位解决方案架构师需要为所有现有对象以及将来添加到 S3 存储桶中的所有对象启用加密。哪种解决方案将以最少的精力满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 S3 存储桶。为新的 S3 存储桶打开默认加密设置。将所有现有对象下载到临时的本地存储。将对象上传到新的 S3 存储桶。",
      "B": "为 S3 存储桶打开默认加密设置。使用 S3 库存功能创建.csv 文件，列出未加密的对象。运行一个 S3 批量操作作业，该作业使用复制命令对这些对象进行加密。",
      "C": "使用 AWS Key Management Service (AWS KMS) 创建一个新的加密密钥。更改 S3 存储桶上的设置以使用带有 AWS KMS 托管加密密钥 (SSE-KMS) 的服务器端加密。为 S3 存储桶打开版本控制。",
      "D": "在 AWS 管理控制台中导航到 Amazon S3。浏览 S3 存储桶的对象。按加密字段排序。选择每个未加密的对象。使用“修改”按钮将默认加密设置应用于 S3 存储桶中的每个未加密对象。"
    },
    "tags": [
      "Amazon S3",
      "S3",
      "S3 Batch Operations",
      "S3 Inventory",
      "SSE-S3",
      "SSE-KMS",
      "Encryption",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n本题考查 Amazon S3 存储桶的加密配置和现有对象的加密处理。涉及 S3 批量操作（S3 Batch Operations）、S3 库存（S3 Inventory）等服务的应用，以及不同加密方式的对比。",
      "why_correct": "选项 B 提供了最高效的解决方案。首先，启用 S3 存储桶的默认加密（SSE-S3，或SSE-KMS）。然后，使用 S3 Inventory 生成一个未加密对象的清单文件（.csv）。最后，通过 S3 批量操作作业，使用复制操作（Copy Object）来对这些未加密的对象进行加密。这种方法实现了对现有对象的批量处理，且操作流程自动化，满足了最小化工作量的要求。",
      "why_wrong": "选项 A 的方案需要将对象下载到本地，再上传到新的存储桶，效率低下，且容易出错。此外，需要手动管理对象，不符合最小化工作量的要求。\n选项 C 涉及创建一个新的 KMS 加密密钥（SSE-KMS），并启用 S3 版本控制。虽然这种方案提供了额外的安全性和版本管理，但它没有解决现有对象的加密问题。更改存储桶的默认加密设置后，新上传的对象会被加密，但需要额外操作来加密现有对象。\n选项 D 需要手动在 S3 管理控制台中选择和修改每个对象，工作量巨大，不适用于数百万个对象。这种方法是不可扩展的，并且非常耗时，不符合题目的需求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "CloudFront",
      "AWS KMS",
      "SSE-S3",
      "SSE-KMS",
      "S3 Inventory",
      "S3 Batch Operations",
      "Encryption",
      "Copy Object"
    ]
  },
  {
    "id": 217,
    "topic": "1",
    "question_en": "A company runs a global web application on Amazon EC2 instances behind an Application Load Balancer. The application stores data in Amazon Aurora. The company needs to create a disaster recovery solution and can tolerate up to 30 minutes of downtime and potential data loss. The solution does not need to handle the load when the primary infrastructure is healthy. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Deploy the application with the required infrastructure elements in place. Use Amazon Route 53 to configure active-passive failover. Create an Aurora Replica in a second AWS Region.",
      "B": "Host a scaled-down deployment of the application in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora Replica in the second Region.",
      "C": "Replicate the primary infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora database that is restored from the latest snapshot.",
      "D": "Back up data with AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Use Amazon Route 53 to configure active-passive failover. Create an Aurora second primary instance in the second Region."
    },
    "correct_answer": "A",
    "vote_percentage": "70%",
    "question_cn": "一家公司在Application Load Balancer后面的Amazon EC2实例上运行一个全球Web应用程序。该应用程序将数据存储在Amazon Aurora中。该公司需要创建一个灾难恢复解决方案，并且可以容忍长达30分钟的停机时间和潜在的数据丢失。当主要基础设施正常运行时，该解决方案不需要处理负载。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "部署具有所需基础设施元素的应用程序。使用Amazon Route 53配置主动-被动故障转移。在第二个AWS区域中创建一个Aurora副本。",
      "B": "在第二个AWS区域中托管一个缩减规模的应用程序部署。使用Amazon Route 53配置主动-主动故障转移。在第二个区域中创建一个Aurora副本。",
      "C": "在第二个AWS区域中复制主要基础设施。使用Amazon Route 53配置主动-主动故障转移。创建一个从最新快照还原的Aurora数据库。",
      "D": "使用AWS Backup备份数据。使用备份在第二个AWS区域中创建所需的基础设施。使用Amazon Route 53配置主动-被动故障转移。在第二个区域中创建一个Aurora第二个主实例。"
    },
    "tags": [
      "Amazon EC2",
      "Application Load Balancer",
      "Amazon Aurora",
      "Amazon Route 53",
      "Disaster Recovery",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 70%），解析仅供参考。】\n\n考查在满足一定 RTO 和 RPO 的前提下，设计应用程序的灾难恢复方案，并结合 Route 53 的 DNS 故障转移配置。",
      "why_correct": "选项 A 采用主动-被动故障转移模式，这符合“当主要基础设施正常运行时，该解决方案不需要处理负载”的需求。在第二个 AWS 区域中创建 Aurora 副本，可以满足灾难发生时的数据库恢复需求。Route 53 的主动-被动配置保证了在主区域故障时，流量可以自动切换到备用区域。",
      "why_wrong": "选项 B 采用主动-主动模式，这会导致备用区域需要处理负载，与需求不符。选项 C 创建 Aurora 数据库的方式是从快照还原，相比于复制 Aurora 副本，恢复时间更长，可能不满足 30 分钟的 RTO 要求。选项 D 使用 AWS Backup 备份数据，且创建第二个主实例，这会导致数据同步和维护成本过高，同时在恢复时需要重建基础设施，时间较长，也不符合 30 分钟的 RTO 要求，且 Aurora 没有第二个主实例的说法。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon EC2",
      "Amazon Aurora",
      "Amazon Route 53",
      "AWS Backup",
      "RTO",
      "RPO",
      "Active-Passive",
      "Active-Active"
    ]
  },
  {
    "id": 218,
    "topic": "1",
    "question_en": "A company has a web server running on an Amazon EC2 instance in a public subnet with an Elastic IP address. The default security group is assigned to the EC2 instance. The default network ACL has been modified to block all trafic. A solutions architect needs to make the web server accessible from everywhere on port 443. Which combination of steps will accomplish this task? (Choose two.)",
    "options_en": {
      "A": "Create a security group with a rule to allow TCP port 443 from source 0.0.0.0/0.",
      "B": "Create a security group with a rule to allow TCP port 443 to destination 0.0.0.0/0.",
      "C": "Update the network ACL to allow TCP port 443 from source 0.0.0.0/0.",
      "D": "Update the network ACL to allow inbound/outbound TCP port 443 from source 0.0.0.0/0 and to destination 0.0.0.0/0",
      "E": "Update the network ACL to allow inbound TCP port 443 from source 0.0.0.0/0 and outbound TCP port 32768-65535 to destination 0.0.0.0/0."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家公司在公共子网中的 Amazon EC2 实例上运行一个 Web 服务器，该实例具有弹性 IP 地址。默认安全组已分配给 EC2 实例。默认网络 ACL 已被修改以阻止所有流量。解决方案架构师需要使 Web 服务器可以通过端口 443 从任何地方访问。哪两种步骤组合可以完成此任务？（选择两个。）",
    "options_cn": {
      "A": "创建一个安全组，其规则允许来自源 0.0.0.0/0 的 TCP 端口 443。",
      "B": "创建一个安全组，其规则允许到目标 0.0.0.0/0 的 TCP 端口 443。",
      "C": "更新网络 ACL 以允许来自源 0.0.0.0/0 的 TCP 端口 443。",
      "D": "更新网络 ACL 以允许来自源 0.0.0.0/0 和到目标 0.0.0.0/0 的入站/出站 TCP 端口 443。",
      "E": "更新网络 ACL 以允许来自源 0.0.0.0/0 的入站 TCP 端口 443 以及到目标 0.0.0.0/0 的出站 TCP 端口 32768-65535。"
    },
    "tags": [
      "Amazon EC2",
      "Security Group",
      "Network ACL",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n考查 EC2 实例的网络访问控制。涉及 Security Group 和 Network ACL 的配置，以及它们之间的差异。需要理解两者对流量的控制方式，以及如何结合使用以实现所需的访问策略。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：创建一个 Security Group，其规则允许来自源 0.0.0.0/0 的 TCP 端口 443，即可使 Web 服务器通过 HTTPS 协议（端口 443）从任何地方访问。Security Group 是针对 EC2 实例的防火墙，用于控制入站和出站流量。此配置允许来自任何 IP 地址的流量通过 443 端口访问该实例。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，Security Group 规则是针对入站流量和出站流量的。不允许到目标 0.0.0.0/0 的规则无效，因为无法控制出站流量的源地址。\n选项 C 错误，网络 ACL 位于子网级别，用于控制子网内的流量。虽然修改网络 ACL 可以允许入站流量通过 443 端口，但这并非最佳实践，并且忽略了 Security Group 的配置。通常，应该首先配置 Security Group，然后根据需要调整网络 ACL。网络 ACL 对所有子网流量有影响，配置不当容易导致其他问题。\n选项 D 错误，修改 Network ACL 允许入站和出站 TCP 443 端口是不必要的，而且不遵循最小权限原则。网络 ACL 允许入站 TCP 443 端口，允许出站 TCP 443 端口，并不直接解决问题。\n选项 E 错误，虽然网络 ACL 可以允许入站 TCP 443 端口，以及允许出站的 ephemeral ports（通常是 32768-65535），但依然忽略了对 Security Group 的配置，没有针对实例层面的安全设置。同时，出站流量的 ephemeral ports 的范围是动态的，不需要手动指定。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "Network ACL",
      "VPC",
      "TCP",
      "HTTPS",
      "Elastic IP",
      "Security Group"
    ]
  },
  {
    "id": 219,
    "topic": "1",
    "question_en": "A company’s application is having performance issues. The application is stateful and needs to complete in-memory tasks on Amazon EC2 instances. The company used AWS CloudFormation to deploy infrastructure and used the M5 EC2 instance family. As trafic increased, the application performance degraded. Users are reporting delays when the users attempt to access the application. Which solution will resolve these issues in the MOST operationally eficient way?",
    "options_en": {
      "A": "Replace the EC2 instances with T3 EC2 instances that run in an Auto Scaling group. Make the changes by using the AWS Management Console.",
      "B": "Modify the CloudFormation templates to run the EC2 instances in an Auto Scaling group. Increase the desired capacity and the maximum capacity of the Auto Scaling group manually when an increase is necessary.",
      "C": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Use Amazon CloudWatch built-in EC2 memory metrics to track the application performance for future capacity planning.",
      "D": "Modify the CloudFormation templates. Replace the EC2 instances with R5 EC2 instances. Deploy the Amazon CloudWatch agent on the EC2 instances to generate custom application latency metrics for future capacity planning."
    },
    "correct_answer": "D",
    "vote_percentage": "97%",
    "question_cn": "一家公司的应用程序遇到了性能问题。该应用程序是有状态的，需要在 Amazon EC2 实例上完成内存任务。该公司使用 AWS CloudFormation 部署基础设施，并使用了 M5 EC2 实例系列。随着流量的增加，应用程序的性能下降。用户报告在尝试访问应用程序时出现延迟。哪种解决方案将以最具运营效率的方式解决这些问题？",
    "options_cn": {
      "A": "将 EC2 实例替换为在 Auto Scaling 组中运行的 T3 EC2 实例。通过使用 AWS 管理控制台进行更改。",
      "B": "修改 CloudFormation 模板以在 Auto Scaling 组中运行 EC2 实例。当需要增加时，手动增加 Auto Scaling 组的所需容量和最大容量。",
      "C": "修改 CloudFormation 模板。将 EC2 实例替换为 R5 EC2 实例。使用 Amazon CloudWatch 内置的 EC2 内存指标来跟踪应用程序性能，以进行未来的容量规划。",
      "D": "修改 CloudFormation 模板。将 EC2 实例替换为 R5 EC2 实例。在 EC2 实例上部署 Amazon CloudWatch 代理，以生成自定义应用程序延迟指标，用于未来的容量规划。"
    },
    "tags": [
      "EC2",
      "CloudFormation",
      "Auto Scaling",
      "Amazon CloudWatch",
      "EC2 instance types"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 97%），解析仅供参考。】\n\n考查应用程序性能问题诊断与解决，以及如何利用 CloudFormation 和 Amazon CloudWatch 实现运营效率和容量规划。与 EC2 实例类型选择、Auto Scaling 以及监控方案的对比相关。",
      "why_correct": "选项 D 正确。使用 R5 实例可以提供更好的计算性能和内存，从而解决性能瓶颈。通过在 EC2 实例上部署 Amazon CloudWatch 代理，可以收集自定义应用程序延迟指标。这些自定义指标对于识别和解决性能问题至关重要，能够帮助更准确地了解应用程序内部的性能状况，并且支持基于这些自定义指标进行容量规划，从而更有效地应对未来的负载增长，实现运维效率。",
      "why_wrong": "选项 A 错误。T3 实例的性能不如 M5 或 R5 实例，无法有效解决应用程序的性能问题。而且，虽然 Auto Scaling 可以根据负载自动调整实例数量，但更换实例类型需要修改 CloudFormation 模板，手动通过管理控制台更改不符合基础设施即代码（IaC）的最佳实践。选项 B 错误。虽然在 Auto Scaling 组中运行实例有益于弹性伸缩，但手动增加 Auto Scaling 组的容量缺乏自动化，效率低，而且未解决实例性能问题。选项 C 错误。R5 实例能提升性能，但仅使用 EC2 内存指标无法全面了解应用程序的性能瓶颈。应用程序的延迟问题需要更细粒度的指标，例如自定义应用程序延迟指标，以更好地诊断和解决问题。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "CloudFormation",
      "Amazon CloudWatch",
      "Lambda",
      "EBS",
      "CloudWatch",
      "M5",
      "R5",
      "T3",
      "EC2 instance types"
    ]
  },
  {
    "id": 220,
    "topic": "1",
    "question_en": "A solutions architect is designing a new API using Amazon API Gateway that will receive requests from users. The volume of requests is highly variable; several hours can pass without receiving a single request. The data processing will take place asynchronously, but should be completed within a few seconds after a request is made. Which compute service should the solutions architect have the API invoke to deliver the requirements at the lowest cost?",
    "options_en": {
      "A": "An AWS Glue job",
      "B": "An AWS Lambda function",
      "C": "A containerized service hosted in Amazon Elastic Kubernetes Service (Amazon EKS)",
      "D": "A containerized service hosted in Amazon ECS with Amazon EC2"
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一个解决方案架构师正在使用 Amazon API Gateway 设计一个新的 API，该 API 将接收来自用户的请求。请求量变化很大；几个小时内可能没有收到任何请求。数据处理将异步进行，但应在发出请求后的几秒钟内完成。解决方案架构师应该让 API 调用哪个计算服务以最低成本交付需求？",
    "options_cn": {
      "A": "一个 AWS Glue 作业",
      "B": "一个 AWS Lambda 函数",
      "C": "托管在 Amazon Elastic Kubernetes Service (Amazon EKS) 中的容器化服务",
      "D": "托管在 Amazon ECS 与 Amazon EC2 中的容器化服务"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "AWS Lambda",
      "Elastic Kubernetes Service",
      "Amazon EKS",
      "Amazon ECS",
      "EC2",
      "AWS Glue"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n考察 API Gateway 场景下的计算服务选择，以及对成本、异步处理、弹性伸缩的综合考量。与 Lambda、EKS、ECS + EC2、Glue 的特性及适用场景相关。",
      "why_correct": "AWS Lambda 是一种无服务器计算服务，它允许您运行代码而无需预置或管理服务器。对于 API Gateway 后端，Lambda 函数非常适合处理异步请求，且具有自动弹性伸缩能力，可以根据请求量的变化自动调整计算资源。Lambda 采用按需付费模式，这在请求量变化很大的情况下，可以有效降低成本，满足题目的需求。",
      "why_wrong": "选项 A，AWS Glue 是一项 ETL 服务，用于数据转换和处理，启动时间较长，通常用于批处理而不是实时响应，不适合在几秒钟内完成请求。选项 C，托管在 Amazon EKS 中的容器化服务，虽然提供了弹性伸缩能力，但需要管理 Kubernetes 集群，涉及服务器预置和管理成本，在请求量很低时会导致资源闲置，成本较高，且启动时间通常长于 Lambda。选项 D，托管在 Amazon ECS 与 Amazon EC2 中的容器化服务，与 EKS 类似，需要管理 EC2 实例，涉及服务器成本和管理成本，且弹性伸缩不如 Lambda 快速，同样不符合最低成本的要求。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "AWS Lambda",
      "EKS",
      "Amazon EKS",
      "ECS",
      "Amazon ECS",
      "EC2",
      "Glue",
      "AWS Glue"
    ]
  },
  {
    "id": 221,
    "topic": "1",
    "question_en": "A company runs an application on a group of Amazon Linux EC2 instances. For compliance reasons, the company must retain all application log files for 7 years. The log files will be analyzed by a reporting tool that must be able to access all the files concurrently. Which storage solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Amazon Elastic Block Store (Amazon EBS)",
      "B": "Amazon Elastic File System (Amazon EFS)",
      "C": "Amazon EC2 instance store",
      "D": "Amazon S3"
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Amazon Linux EC2 实例组上运行一个应用程序。出于合规性原因，该公司必须将所有应用程序日志文件保留 7 年。这些日志文件将由一个报告工具进行分析，该工具必须能够同时访问所有文件。哪种存储解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "Amazon Elastic Block Store (Amazon EBS)",
      "B": "Amazon Elastic File System (Amazon EFS)",
      "C": "Amazon EC2 实例存储",
      "D": "Amazon S3"
    },
    "tags": [
      "Amazon S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查长期、大规模日志存储的经济高效解决方案。这道题考察了对不同存储方案的理解，以及它们在成本、可访问性、和持久性方面的差异。需要选择满足长期存储需求，且支持并行访问的方案。",
      "why_correct": "Amazon S3 是存储应用程序日志文件的最经济高效的解决方案。S3 提供高持久性，并且设计用于 99.999999999% 的数据耐久性，满足了 7 年的合规性要求。S3 提供了多种存储类别，例如 S3 Standard, S3 Intelligent-Tiering, S3 Glacier，可以根据访问频率和成本需求进行选择。由于报告工具需要同时访问所有文件，S3 的设计使其能够支持高并发的读操作。",
      "why_wrong": "A. Amazon EBS 是一种块存储服务，用于为 EC2 实例提供持久性存储。EBS 适用于需要频繁I/O操作的场景，例如数据库，而非日志文件的长期存储。由于 EBS 的设计，直接用于存储大量的日志文件在成本效益上不具备优势。\nB. Amazon EFS 是一种可扩展的、弹性的文件存储服务。EFS 适用于多个 EC2 实例共享文件系统的场景，并不针对长期归档。EFS 的成本相对 S3 较高，并且其性能优化主要针对文件系统访问，不适合用于大规模日志存储与分析。\nC. Amazon EC2 实例存储提供临时的、基于实例的存储。EC2 实例存储中的数据在实例停止或终止时会丢失，不符合 7 年的合规性要求。因此，EC2 实例存储不适用于长期数据保留。"
    },
    "related_terms": [
      "Amazon EBS",
      "Amazon EFS",
      "Amazon EC2",
      "EC2",
      "EBS",
      "EFS",
      "Amazon S3",
      "S3",
      "S3 Intelligent-Tiering",
      "S3 Glacier",
      "Lambda",
      "S3 Standard"
    ]
  },
  {
    "id": 222,
    "topic": "1",
    "question_en": "A company has hired an external vendor to perform work in the company’s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company’s AWS account. How should a solutions architect grant this access to the vendor?",
    "options_en": {
      "A": "Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.",
      "B": "Create an IAM user in the company’s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.",
      "C": "Create an IAM group in the company’s account. Add the tool’s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.",
      "D": "Create a new identity provider by choosing “AWS account” as the provider type in the IAM console. Supply the vendor’s AWS account ID and user name. Attach the appropriate IAM policies to the new provider for the permissions that the vendor requires."
    },
    "correct_answer": "A",
    "vote_percentage": "87%",
    "question_cn": "一家公司聘请了一家外部供应商在其公司的 AWS 账户中执行工作。该供应商使用托管在其拥有的 AWS 账户中的自动化工具。该供应商无法访问公司 AWS 账户的 IAM。解决方案架构师应如何授予供应商此访问权限？",
    "options_cn": {
      "A": "在公司的账户中创建一个 IAM 角色，以将访问权限委派给供应商的 IAM 角色。将适当的 IAM 策略附加到该角色，以获取供应商所需的权限。",
      "B": "在公司的账户中创建一个 IAM 用户，该用户的密码符合密码复杂性要求。将适当的 IAM 策略附加到该用户，以获取供应商所需的权限。",
      "C": "在公司的账户中创建一个 IAM 组。将供应商账户中的工具的 IAM 用户添加到该组。将适当的 IAM 策略附加到该组，以获取供应商所需的权限。",
      "D": "通过在 IAM 控制台中选择“AWS 账户”作为提供商类型，创建一个新的身份提供商。提供供应商的 AWS 账户 ID 和用户名。将适当的 IAM 策略附加到新的提供商，以获取供应商所需的权限。"
    },
    "tags": [
      "IAM",
      "IAM Role",
      "Cross-Account Access"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 87%），解析仅供参考。】\n\n本题考查跨账户访问权限的授予，与 IAM 角色、IAM 策略和信任策略相关。考察在不共享凭证的情况下，如何安全地授权外部供应商访问客户 AWS 资源。",
      "why_correct": "创建一个 IAM 角色，并将其用于委派访问权限是最佳实践。通过 IAM 角色，公司的账户可以定义一个允许供应商账户访问资源的信任策略。然后，将适当的 IAM 策略附加到该角色，指定供应商所需的具体权限。这样，供应商的 IAM 角色（位于其自己的账户中）可以“承担”公司账户中的角色，从而安全地访问资源。这种方法避免了直接共享 IAM 凭证，并允许细粒度的权限控制。",
      "why_wrong": "选项 B 错误，因为在公司账户中创建 IAM 用户，并将凭证共享给外部供应商，这违反了安全最佳实践。直接共享凭证增加了安全风险，且难以管理。选项 C 错误，因为 IAM 组是管理账户内用户权限的，无法直接用于跨账户访问。将供应商的 IAM 用户添加到组中无法实现跨账户的委托访问。选项 D 错误，因为虽然 IAM 身份提供商可以用于联合身份验证，但它主要用于与外部身份提供商（如 SAML 身份提供商）集成，而不是简单的跨账户访问。此外，该选项提供的流程不正确，无法实现权限委派，且配置复杂性较高，不适合本题场景。"
    },
    "related_terms": [
      "IAM",
      "IAM Role",
      "IAM Policy",
      "IAM User",
      "IAM Group",
      "AWS Account",
      "Cross-Account Access"
    ]
  },
  {
    "id": 223,
    "topic": "1",
    "question_en": "A company has deployed a Java Spring Boot application as a pod that runs on Amazon Elastic Kubernetes Service (Amazon EKS) in private subnets. The application needs to write data to an Amazon DynamoDB table. A solutions architect must ensure that the application can interact with the DynamoDB table without exposing trafic to the internet. Which combination of steps should the solutions architect take to accomplish this goal? (Choose two.)",
    "options_en": {
      "A": "Attach an IAM role that has suficient privileges to the EKS pod.",
      "B": "Attach an IAM user that has suficient privileges to the EKS pod.",
      "C": "Allow outbound connectivity to the DynamoDB table through the private subnets’ network ACLs.",
      "D": "Create a VPC endpoint for DynamoDB",
      "E": "Embed the access keys in the Java Spring Boot code."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "一家公司已将 Java Spring Boot 应用程序部署为在私有子网中的 Amazon Elastic Kubernetes Service (Amazon EKS) 上运行的 pod。该应用程序需要将数据写入 Amazon DynamoDB 表。解决方案架构师必须确保该应用程序可以与 DynamoDB 表交互，而不会将流量暴露到互联网。解决方案架构师应采取哪些步骤组合来实现此目标？（选择两项。）",
    "options_cn": {
      "A": "将具有足够权限的 IAM 角色附加到 EKS pod。",
      "B": "将具有足够权限的 IAM 用户附加到 EKS pod。",
      "C": "允许通过私有子网的网卡 ACL 对 DynamoDB 表进行出站连接。",
      "D": "为 DynamoDB 创建一个 VPC endpoint。",
      "E": "将访问密钥嵌入到 Java Spring Boot 代码中。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon EKS",
      "IAM Role",
      "VPC Endpoint",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n考查 EKS Pod 访问 DynamoDB 的安全、私有连接方案；与 IAM 角色、VPC Endpoint 选型相关。",
      "why_correct": "将具有足够权限的 IAM 角色附加到 EKS pod 是最佳实践。EKS pod 通过 IAM 角色获得访问 DynamoDB 的权限，避免了将凭证嵌入应用程序代码的风险。通过使用 IAM 角色， pod 可以安全地访问 AWS 服务，而无需在代码中硬编码任何长期凭证。",
      "why_wrong": "B 选项错误，在 EKS pod 中使用 IAM 用户是不安全的，因为它需要将用户的访问密钥存储在 pod 中，增加了密钥泄露的风险。C 选项错误，ACL (Network ACL) 是在子网级别进行控制，虽然可以控制出站流量，但无法直接控制 pod 访问 DynamoDB 的权限，且不能保证流量不暴露于公网。D 选项错误，为 DynamoDB 创建 VPC Endpoint 是一个正确的方案，但题目要求选择两个方案，仅仅创建一个 VPC Endpoint 无法实现 pod 访问 DynamoDB 的需求，缺少了必要的授权措施。E 选项错误，将访问密钥嵌入 Java Spring Boot 代码是绝对不推荐的做法，这会导致凭据泄露，严重威胁安全。"
    },
    "related_terms": [
      "Amazon EKS",
      "EKS",
      "DynamoDB",
      "ACL",
      "Pod",
      "IAM Role",
      "IAM User",
      "VPC Endpoint",
      "Java Spring Boot"
    ]
  },
  {
    "id": 224,
    "topic": "1",
    "question_en": "A company recently migrated its web application to AWS by rehosting the application on Amazon EC2 instances in a single AWS Region. The company wants to redesign its application architecture to be highly available and fault tolerant. Trafic must reach all running EC2 instances randomly. Which combination of steps should the company take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon Route 53 failover routing policy.",
      "B": "Create an Amazon Route 53 weighted routing policy.",
      "C": "Create an Amazon Route 53 multivalue answer routing policy.",
      "D": "Launch three EC2 instances: two instances in one Availability Zone and one instance in another Availability Zon",
      "E": "E. Launch four EC2 instances: two instances in one Availability Zone and two instances in another Availability Zone."
    },
    "correct_answer": "CE",
    "vote_percentage": "",
    "question_cn": "一家公司最近通过在单个 AWS 区域中的 Amazon EC2 实例上重新托管应用程序，将其 Web 应用程序迁移到了 AWS。该公司希望重新设计其应用程序架构，使其具有高可用性和容错性。流量必须随机到达所有正在运行的 EC2 实例。该公司应采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建 Amazon Route 53 故障转移路由策略。",
      "B": "创建 Amazon Route 53 加权路由策略。",
      "C": "创建 Amazon Route 53 多值应答路由策略。",
      "D": "启动三个 EC2 实例：两个实例在一个可用区中，一个实例在另一个可用区中。",
      "E": "启动四个 EC2 实例：两个实例在一个可用区中，两个实例在另一个可用区中。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon Route 53",
      "High Availability",
      "Fault Tolerance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 —），解析仅供参考。】\n\n考查如何通过 Route 53 路由策略和 EC2 实例的部署来实现高可用性和容错性，与 Route 53 的不同路由策略、EC2 实例的部署策略相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 CE。理由简述：Amazon Route 53 多值应答路由策略允许您配置 Route 53 从多个资源中返回多个健康记录。在这种情况下，可以配置多个 EC2 实例的 IP 地址作为响应。用户每次访问网站时，DNS 查询都会返回所有健康 EC2 实例的 IP 地址。浏览器或客户端将接收到所有健康实例的地址，并且可以随机地将请求发送到这些实例，从而实现负载均衡和高可用性。这种策略适用于需要将流量路由到多个资源，并且不需特定负载均衡器的情况。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. Amazon Route 53 故障转移路由策略主要用于实现灾难恢复。当主资源出现故障时，Route 53 会将流量路由到备用资源。它不适用于本题要求的所有 EC2 实例接收流量的情况。\nB. Amazon Route 53 加权路由策略允许根据配置的权重将流量分配给不同的资源。虽然可以实现负载均衡，但它不能保证流量是随机地到达所有实例，并且配置的复杂性高于多值应答路由策略。\nD 和 E. 启动多个 EC2 实例（跨多个可用区）是实现高可用性的关键。但是，单独部署 EC2 实例并不能满足负载均衡的要求，还需要结合 Route 53 的路由策略来实现流量分发。单独部署实例并不能自动实现流量在实例间的随机分配，必须结合 Route 53 多值应答路由策略才能满足要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Route 53",
      "EC2",
      "Route 53",
      "Availability Zone",
      "DNS"
    ]
  },
  {
    "id": 225,
    "topic": "1",
    "question_en": "A media company collects and analyzes user activity data on premises. The company wants to migrate this capability to AWS. The user activity data store will continue to grow and will be petabytes in size. The company needs to build a highly available data ingestion solution that facilitates on-demand analytics of existing data and new data with SQL. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Send activity data to an Amazon Kinesis data stream. Configure the stream to deliver the data to an Amazon S3 bucket.",
      "B": "Send activity data to an Amazon Kinesis Data Firehose delivery stream. Configure the stream to deliver the data to an Amazon Redshift cluster.",
      "C": "Place activity data in an Amazon S3 bucket. Configure Amazon S3 to run an AWS Lambda function on the data as the data arrives in the S3 bucket.",
      "D": "Create an ingestion service on Amazon EC2 instances that are spread across multiple Availability Zones. Configure the service to forward data to an Amazon RDS Multi-AZ database."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家媒体公司在其内部部署中收集和分析用户活动数据。该公司希望将此功能迁移到 AWS。用户活动数据存储将持续增长，并将达到 PB 级别。该公司需要构建一个高可用性的数据摄取解决方案，该解决方案能够使用 SQL 对现有数据和新数据进行按需分析。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将活动数据发送到 Amazon Kinesis 数据流。配置该数据流将数据传送到 Amazon S3 存储桶。",
      "B": "将活动数据发送到 Amazon Kinesis Data Firehose 交付流。配置该流将数据传送到 Amazon Redshift 集群。",
      "C": "将活动数据放入 Amazon S3 存储桶。配置 Amazon S3 在数据到达 S3 存储桶时，对数据运行 AWS Lambda 函数。",
      "D": "在分布在多个可用区中的 Amazon EC2 实例上创建一个摄取服务。配置该服务将数据转发到 Amazon RDS 多可用区数据库。"
    },
    "tags": [
      "Amazon Kinesis",
      "Amazon S3",
      "Data ingestion",
      "High availability",
      "SQL analysis"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考查构建高可用性、PB 级别数据摄取方案，并支持 SQL 查询的需求。考察对 Kinesis Data Firehose、Redshift 等服务的理解。",
      "why_correct": "Amazon Kinesis Data Firehose 专为批量数据摄取设计，能够将数据直接交付到 Amazon Redshift 集群。Redshift 是一种数据仓库服务，支持 SQL 查询和 PB 级数据存储。利用 Firehose 和 Redshift 的结合，能够满足高可用性、PB 级数据存储和 SQL 分析的需求，同时运营开销较低。",
      "why_wrong": "A 选项中，Kinesis Data Streams 仅负责数据摄取，需要额外使用 Lambda 函数或其它服务来将数据存储到 S3，并进行数据转换和查询，操作复杂且运营开销较高。C 选项中，直接将数据放入 S3 需要配置 Lambda 函数进行处理，无法直接进行 SQL 查询，并且 Lambda 方案在处理 PB 级别数据时，成本和性能都存在问题。D 选项使用 EC2 和 RDS 构建方案，运营开销大，并且 RDS 并不适合 PB 级别的数据仓库场景，难以满足性能和扩展性需求。"
    },
    "related_terms": [
      "Amazon Kinesis Data Firehose",
      "Amazon Redshift",
      "SQL",
      "Amazon Kinesis Data Streams",
      "Amazon S3",
      "AWS Lambda",
      "Amazon EC2",
      "Amazon RDS"
    ]
  },
  {
    "id": 226,
    "topic": "1",
    "question_en": "A company collects data from thousands of remote devices by using a RESTful web services application that runs on an Amazon EC2 instance. The EC2 instance receives the raw data, transforms the raw data, and stores all the data in an Amazon S3 bucket. The number of remote devices will increase into the millions soon. The company needs a highly scalable solution that minimizes operational overhead. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Glue to process the raw data in Amazon S3.",
      "B": "Use Amazon Route 53 to route trafic to different EC2 instances.",
      "C": "Add more EC2 instances to accommodate the increasing amount of incoming data.",
      "D": "Send the raw data to Amazon Simple Queue Service (Amazon SQS). Use EC2 instances to process the data",
      "E": "Use Amazon API Gateway to send the raw data to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to use the data stream as a source to deliver the data to Amazon S3."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家公司通过运行在 Amazon EC2 实例上的 RESTful Web 服务应用程序，从数千个远程设备收集数据。EC2 实例接收原始数据，转换原始数据，并将所有数据存储在 Amazon S3 存储桶中。远程设备的数量很快将增加到数百万。该公司需要一个高度可扩展的解决方案，以最大限度地减少运营开销。解决方案架构师应采取哪些步骤组合来满足这些要求？（选择两个）",
    "options_cn": {
      "A": "使用 AWS Glue 处理 Amazon S3 中的原始数据。",
      "B": "使用 Amazon Route 53 将流量路由到不同的 EC2 实例。",
      "C": "添加更多 EC2 实例以适应不断增加的传入数据量。",
      "D": "将原始数据发送到 Amazon Simple Queue Service (Amazon SQS)。使用 EC2 实例处理数据。",
      "E": "使用 Amazon API Gateway 将原始数据发送到 Amazon Kinesis 数据流。配置 Amazon Kinesis Data Firehose 以使用该数据流作为 将数据传输到 Amazon S3 的 源。"
    },
    "tags": [
      "EC2",
      "RESTful Web 服务",
      "S3",
      "AWS Glue",
      "Route 53",
      "SQS",
      "API Gateway",
      "Kinesis Data Streams",
      "Kinesis Data Firehose"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n题目考查了高可扩展性的解决方案。结合题目需求和选项，需要一个可扩展的方案来应对快速增长的数据量。此题需要选择两个正确答案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：选项 A 和 E 均符合题目要求。A 使用 AWS Glue 处理 S3 中的原始数据，可以进行数据转换和处理，而 E 使用 API Gateway 和 Kinesis 家族服务构建数据流管道，可以处理大规模数据摄入。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 错误，Route 53 主要用于 DNS 解析，不能解决数据处理问题。C 错误，增加 EC2 实例数量可以提高处理能力，但不够灵活。D 错误，SQS 主要用于消息队列，虽然可以缓冲数据，但并未提供数据转换和存储解决方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "S3",
      "AWS Glue",
      "Route 53",
      "SQS",
      "API Gateway",
      "Kinesis Data Streams",
      "Kinesis Data Firehose",
      "RESTful Web 服务"
    ]
  },
  {
    "id": 227,
    "topic": "1",
    "question_en": "A company needs to retain its AWS CloudTrail logs for 3 years. The company is enforcing CloudTrail across a set of AWS accounts by using AWS Organizations from the parent account. The CloudTrail target S3 bucket is configured with S3 Versioning enabled. An S3 Lifecycle policy is in place to delete current objects after 3 years. After the fourth year of use of the S3 bucket, the S3 bucket metrics show that the number of objects has continued to rise. However, the number of new CloudTrail logs that are delivered to the S3 bucket has remained consistent. Which solution will delete objects that are older than 3 years in the MOST cost-effective manner?",
    "options_en": {
      "A": "Configure the organization’s centralized CloudTrail trail to expire objects after 3 years.",
      "B": "Configure the S3 Lifecycle policy to delete previous versions as well as current versions.",
      "C": "Create an AWS Lambda function to enumerate and delete objects from Amazon S3 that are older than 3 years.",
      "D": "Configure the parent account as the owner of all objects that are delivered to the S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "91%",
    "question_cn": "一家公司需要保留其 AWS CloudTrail 日志 3 年。该公司正在使用父账户的 AWS Organizations 在一组 AWS 账户中强制执行 CloudTrail。CloudTrail 目标 S3 存储桶配置了 S3 版本控制。已设置 S3 生命周期策略，在 3 年后删除当前对象。在使用 S3 存储桶的第四年之后，S3 存储桶指标显示对象的数量持续增加。但是，交付到 S3 存储桶的新 CloudTrail 日志的数量保持一致。哪种解决方案将以最具成本效益的方式删除超过 3 年的对象？",
    "options_cn": {
      "A": "配置该组织的集中 CloudTrail 跟踪，以便在 3 年后过期对象。",
      "B": "配置 S3 生命周期策略以删除以前的版本以及当前版本。",
      "C": "创建一个 AWS Lambda 函数，用于枚举并从 Amazon S3 中删除超过 3 年的对象。",
      "D": "将父账户配置为交付到 S3 存储桶的所有对象的拥有者。"
    },
    "tags": [
      "CloudTrail",
      "S3",
      "Organizations",
      "Lambda",
      "S3 生命周期策略"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 91%），解析仅供参考。】\n\n题目考察了 CloudTrail 日志的存储与管理。需要以成本效益最高的方式删除超过 3 年的旧对象。S3 生命周期策略可以自动管理对象的生命周期。",
      "why_correct": "选项 B 配置 S3 生命周期策略以删除以前的版本以及当前版本。这是最经济有效的方案，因为 S3 生命周期策略可以自动删除超过 3 年的对象。",
      "why_wrong": "A 错误，组织级别的 CloudTrail 设置无法直接配置对象过期时间。C 错误，使用 Lambda 函数虽然可以删除对象，但是增加了运维成本和复杂性。D 错误，设置父账户为对象拥有者，无法自动删除对象。"
    },
    "related_terms": [
      "CloudTrail",
      "S3",
      "Lambda",
      "S3 生命周期策略",
      "Organizations"
    ]
  },
  {
    "id": 228,
    "topic": "1",
    "question_en": "A company has an API that receives real-time data from a fieet of monitoring devices. The API stores this data in an Amazon RDS DB instance for later analysis. The amount of data that the monitoring devices send to the API fiuctuates. During periods of heavy trafic, the API often returns timeout errors. After an inspection of the logs, the company determines that the database is not capable of processing the volume of write trafic that comes from the API. A solutions architect must minimize the number of connections to the database and must ensure that data is not lost during periods of heavy trafic. Which solution will meet these requirements?",
    "options_en": {
      "A": "Increase the size of the DB instance to an instance type that has more available memory.",
      "B": "Modify the DB instance to be a Multi-AZ DB instance. Configure the application to write to all active RDS DB instances.",
      "C": "Modify the API to write incoming data to an Amazon Simple Queue Service (Amazon SQS) queue. Use an AWS Lambda function that Amazon SQS invokes to write data from the queue to the database.",
      "D": "Modify the API to write incoming data to an Amazon Simple Notification Service (Amazon SNS) topic. Use an AWS Lambda function that Amazon SNS invokes to write data from the topic to the database."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个 API，用于接收来自一组监控设备的实时数据。该 API 将这些数据存储在 Amazon RDS 数据库实例中，以供以后分析。监控设备发送到 API 的数据量会波动。在流量高峰期间，API 经常会返回超时错误。在检查日志后，该公司确定数据库无法处理来自 API 的写入流量量。解决方案架构师必须最大限度地减少与数据库的连接数，并必须确保在流量高峰期间数据不会丢失。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将数据库实例的大小增加到具有更多可用内存的实例类型。",
      "B": "修改数据库实例，使其成为多可用区数据库实例。将应用程序配置为写入所有活动的 RDS 数据库实例。",
      "C": "修改 API，将传入数据写入 Amazon Simple Queue Service (Amazon SQS) 队列。使用 AWS Lambda 函数，该函数由 Amazon SQS 调用，用于将数据从队列写入数据库。",
      "D": "修改 API，将传入数据写入 Amazon Simple Notification Service (Amazon SNS) 主题。使用 AWS Lambda 函数，该函数由 Amazon SNS 调用，用于将数据从主题写入数据库。"
    },
    "tags": [
      "RDS",
      "SQS",
      "Lambda",
      "SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何解决数据库写入压力。题目要求减少数据库连接数，并在流量高峰期保证数据不丢失。使用消息队列可以解决此类问题。",
      "why_correct": "选项 C 通过使用 SQS 队列作为缓冲，Lambda 函数从 SQS 中读取数据并写入数据库，可以有效减少数据库连接压力，保证数据不丢失。",
      "why_wrong": "A 错误，增加数据库实例的内存并不能从根本上解决问题。B 错误，使用多可用区数据库实例，并不能解决数据库写入压力问题。D 错误，SNS 主要用于发布通知，无法作为数据缓冲。"
    },
    "related_terms": [
      "RDS",
      "SQS",
      "Lambda",
      "SNS"
    ]
  },
  {
    "id": 229,
    "topic": "1",
    "question_en": "A company manages its own Amazon EC2 instances that run MySQL databases. The company is manually managing replication and scaling as demand increases or decreases. The company needs a new solution that simplifies the process of adding or removing compute capacity to or from its database tier as needed. The solution also must offer improved performance, scaling, and durability with minimal effort from operations. Which solution meets these requirements?",
    "options_en": {
      "A": "Migrate the databases to Amazon Aurora Serverless for Aurora MySQL.",
      "B": "Migrate the databases to Amazon Aurora Serverless for Aurora PostgreSQL.",
      "C": "Combine the databases into one larger MySQL database. Run the larger database on larger EC2 instances.",
      "D": "Create an EC2 Auto Scaling group for the database tier. Migrate the existing databases to the new environment."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司管理运行 MySQL 数据库的 Amazon EC2 实例。当需求增加或减少时，该公司手动管理复制和扩展。该公司需要一个新解决方案，以简化根据需要向其数据库层添加或删除计算容量的过程。该解决方案还必须提供改进的性能、扩展能力和耐用性，同时尽量减少运营工作量。哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon Aurora Serverless for Aurora MySQL。",
      "B": "将数据库迁移到 Amazon Aurora Serverless for Aurora PostgreSQL。",
      "C": "将数据库合并到一个更大的 MySQL 数据库中。在更大的 EC2 实例上运行更大的数据库。",
      "D": "为数据库层创建一个 EC2 Auto Scaling 组。将现有数据库迁移到新环境。"
    },
    "tags": [
      "Aurora Serverless",
      "MySQL",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n题目考察数据库的可扩展性和可用性。Aurora Serverless 可以根据负载自动伸缩，提供更高的性能和更低的运营成本。",
      "why_correct": "选项 A 将数据库迁移到 Aurora Serverless for Aurora MySQL，可以简化数据库层的扩容和缩容，并提高性能、可扩展性和耐用性。",
      "why_wrong": "B 错误，题目是 MySQL 数据库。C 错误，将数据库合并到一个更大的 MySQL 数据库中，无法满足根据需要添加或删除计算容量的需求。D 错误，EC2 Auto Scaling 组无法自动实现数据库的伸缩。"
    },
    "related_terms": [
      "Aurora Serverless",
      "MySQL",
      "EC2",
      "Auto Scaling"
    ]
  },
  {
    "id": 230,
    "topic": "1",
    "question_en": "A company is concerned that two NAT instances in use will no longer be able to support the trafic needed for the company’s application. A solutions architect wants to implement a solution that is highly available, fault tolerant, and automatically scalable. What should the solutions architect recommend?",
    "options_en": {
      "A": "Remove the two NAT instances and replace them with two NAT gateways in the same Availability Zone.",
      "B": "Use Auto Scaling groups with Network Load Balancers for the NAT instances in different Availability Zones.",
      "C": "Remove the two NAT instances and replace them with two NAT gateways in different Availability Zones.",
      "D": "Replace the two NAT instances with Spot Instances in different Availability Zones and deploy a Network Load Balancer."
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一家公司担心正在使用的两个 NAT 实例将无法再支持公司应用程序所需的流量。解决方案架构师希望实施一个具有高可用性、容错能力和自动伸缩性的解决方案。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "删除这两个 NAT 实例，并在同一可用区中用两个 NAT 网关替换它们。",
      "B": "对不同可用区中的 NAT 实例使用带有网络负载均衡器的 Auto Scaling 组。",
      "C": "删除这两个 NAT 实例，并在不同可用区中用两个 NAT 网关替换它们。",
      "D": "用不同可用区中的 Spot 实例替换这两个 NAT 实例，并部署网络负载均衡器。"
    },
    "tags": [
      "NAT 实例",
      "NAT 网关",
      "EC2",
      "Auto Scaling",
      "网络负载均衡器",
      "Spot 实例"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n此题考察 NAT 解决方案的高可用性和自动伸缩性。NAT 网关是 Amazon 提供的托管服务，可以提供高可用性和弹性。",
      "why_correct": "选项 C 用 NAT 网关替换 NAT 实例，并在不同可用区中部署。NAT 网关是高可用服务，可以自动伸缩，并且无需管理。",
      "why_wrong": "A 错误，在同一可用区中部署 NAT 网关，无法实现高可用性。B 错误，NAT 实例不支持使用网络负载均衡器。D 错误，使用 Spot 实例会导致实例的不可靠性。"
    },
    "related_terms": [
      "NAT 实例",
      "NAT 网关",
      "EC2",
      "Auto Scaling",
      "网络负载均衡器",
      "Spot 实例"
    ]
  },
  {
    "id": 231,
    "topic": "1",
    "question_en": "An application runs on an Amazon EC2 instance that has an Elastic IP address in VPC A. The application requires access to a database in VPC",
    "options_en": {
      "B": "Configure a VPC peering connection between VPC A and VPC B.",
      "A": "Create a DB instance security group that allows all trafic from the public IP address of the application server in VPC A.",
      "C": "Make the DB instance publicly accessible. Assign a public IP address to the DB instance.",
      "D": "Launch an EC2 instance with an Elastic IP address into VPC B. Proxy all requests through the new EC2 instance."
    },
    "correct_answer": "B",
    "vote_percentage": "88%",
    "question_cn": "一个应用程序运行在 VPC A 中的一个具有弹性 IP 地址的 Amazon EC2 实例上。该应用程序需要访问 VPC B 中的一个数据库。以下哪项解决方案可以满足此要求？",
    "options_cn": {
      "A": "创建一个数据库实例安全组，允许来自 VPC A 中应用程序服务器的公共 IP 地址的所有流量。",
      "B": "在 VPC A 和 VPC B 之间配置一个 VPC 对等连接。",
      "C": "使数据库实例可公开访问。为数据库实例分配一个公共 IP 地址。",
      "D": "将一个带有弹性 IP 地址的 EC2 实例启动到 VPC B 中。通过新的 EC2 实例代理所有请求。"
    },
    "tags": [
      "VPC",
      "EC2",
      "VPC 对等连接"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 88%），解析仅供参考。】\n\n考察了 VPC 之间互联的方式。题目要求 VPC A 中的应用程序访问 VPC B 中的数据库。",
      "why_correct": "选项 B 在 VPC A 和 VPC B 之间配置一个 VPC 对等连接，使 VPC 之间可以安全地互相通信。",
      "why_wrong": "A 错误，安全组只能控制 VPC 内部的访问，不能实现 VPC 之间的互联。C 错误，数据库实例分配公网 IP 不安全。D 错误，通过新 EC2 实例代理所有请求，增加了额外复杂性。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "VPC 对等连接"
    ]
  },
  {
    "id": 232,
    "topic": "1",
    "question_en": "A company runs demonstration environments for its customers on Amazon EC2 instances. Each environment is isolated in its own VPC. The company’s operations team needs to be notified when RDP or SSH access to an environment has been established.",
    "options_en": {
      "A": "Configure Amazon CloudWatch Application Insights to create AWS Systems Manager OpsItems when RDP or SSH access is detected.",
      "B": "Configure the EC2 instances with an IAM instance profile that has an IAM role with the AmazonSSMManagedInstanceCore policy attached.",
      "C": "Publish VPC fiow logs to Amazon CloudWatch Logs. Create required metric filters. Create an Amazon CloudWatch metric alarm with a notification action for when the alarm is in the ALARM state.",
      "D": "Configure an Amazon EventBridge rule to listen for events of type EC2 Instance State-change Notification. Configure an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the operations team to the topic."
    },
    "correct_answer": "C",
    "vote_percentage": "79%",
    "question_cn": "一家公司在其客户的 Amazon EC2 实例上运行演示环境。每个环境都隔离在其自己的 VPC 中。该公司运营团队需要在检测到 RDP 或 SSH 访问环境时收到通知。",
    "options_cn": {
      "A": "配置 Amazon CloudWatch Application Insights，以便在检测到 RDP 或 SSH 访问时创建 AWS Systems Manager OpsItems。",
      "B": "使用 IAM 实例配置文件配置 EC2 实例，该配置文件具有附加了 AmazonSSMManagedInstanceCore 策略的 IAM 角色。",
      "C": "将 VPC 流日志发布到 Amazon CloudWatch Logs。创建所需的指标筛选器。创建一个 Amazon CloudWatch 指标警报，当警报处于 ALARM 状态时，具有通知操作。",
      "D": "配置一个 Amazon EventBridge 规则来监听 EC2 实例状态更改通知类型的事件。配置一个 Amazon Simple Notification Service (Amazon SNS) 主题作为目标。将运营团队订阅到该主题。"
    },
    "tags": [
      "CloudWatch",
      "EC2",
      "VPC 流日志",
      "SNS",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 79%），解析仅供参考。】\n\n题目考察 EC2 实例的监控和报警。需要监控 RDP 和 SSH 访问行为并收到通知。",
      "why_correct": "选项 C 将 VPC 流日志发布到 CloudWatch Logs，创建指标筛选器和警报，当检测到 RDP 或 SSH 访问时，通过 SNS 发送通知。",
      "why_wrong": "A 错误，CloudWatch Application Insights 无法监控 RDP 或 SSH 访问。B 错误，IAM 实例配置文件无法监控 RDP 或 SSH 访问。D 错误，EventBridge 规则用于监听 EC2 实例状态更改通知类型的事件，但不能监控 RDP 或 SSH 访问。"
    },
    "related_terms": [
      "CloudWatch",
      "EC2",
      "VPC 流日志",
      "SNS",
      "EventBridge"
    ]
  },
  {
    "id": 233,
    "topic": "1",
    "question_en": "A solutions architect has created a new AWS account and must secure AWS account root user access. Which combination of actions will accomplish this? (Choose two.)",
    "options_en": {
      "A": "Ensure the root user uses a strong password.",
      "B": "Enable multi-factor authentication to the root user.",
      "C": "Store root user access keys in an encrypted Amazon S3 bucket.",
      "D": "Add the root user to a group containing administrative permissions",
      "E": "Apply the required permissions to the root user with an inline policy document."
    },
    "correct_answer": "AB",
    "vote_percentage": "79%",
    "question_cn": "一位解决方案架构师创建了一个新的 AWS 账户，并且必须保护 AWS 账户根用户的访问。哪两种操作的组合将实现此目的？（选择两项。）",
    "options_cn": {
      "A": "确保根用户使用强密码。",
      "B": "为根用户启用多因素身份验证。",
      "C": "将根用户访问密钥存储在加密的 Amazon S3 存储桶中。",
      "D": "将根用户添加到包含管理权限的组中。",
      "E": "使用内联策略文档将所需权限应用于根用户。"
    },
    "tags": [
      "IAM",
      "根用户",
      "MFA",
      "访问密钥"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 79%），解析仅供参考。】\n\n考察了 AWS 账户根用户的安全最佳实践。需要保护根用户的访问，这是 AWS 账户安全的重要组成部分。",
      "why_correct": "A，使用强密码是保护根用户的第一步。",
      "why_wrong": "B 错误，启用 MFA 是保护根用户的好方法，但不是必须的。C 错误，访问密钥不应该存储，更不应该存储在 S3 中。D 错误，根用户不应添加到包含管理权限的组中。E 错误，根用户不应应用任何内联策略。"
    },
    "related_terms": [
      "IAM",
      "根用户",
      "MFA",
      "访问密钥"
    ]
  },
  {
    "id": 234,
    "topic": "1",
    "question_en": "A company is building a new web-based customer relationship management application. The application will use several Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS) volumes behind an Application Load Balancer (ALB). The application will also use an Amazon Aurora database. All data for the application must be encrypted at rest and in transit. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Key Management Service (AWS KMS) certificates on the ALB to encrypt data in transit. Use AWS Certificate Manager (ACM) to encrypt the EBS volumes and Aurora database storage at rest.",
      "B": "Use the AWS root account to log in to the AWS Management Console. Upload the company’s encryption certificates. While in the root account, select the option to turn on encryption for all data at rest and in transit for the account.",
      "C": "Use AWS Key Management Service (AWS KMS) to encrypt the EBS volumes and Aurora database storage at rest. Attach an AWS Certificate Manager (ACM) certificate to the ALB to encrypt data in transit.",
      "D": "Use BitLocker to encrypt all data at rest. Import the company’s TLS certificate keys to AWS Key Management Service (AWS KMS) Attach the KMS keys to the ALB to encrypt data in transit."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在构建一个新的基于 Web 的客户关系管理应用程序。该应用程序将使用几个 Amazon EC2 实例，这些实例由 Application Load Balancer (ALB) 后面的 Amazon Elastic Block Store (Amazon EBS) 卷提供支持。该应用程序还将使用 Amazon Aurora 数据库。该应用程序的所有数据都必须在静态和传输过程中加密。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 ALB 上使用 AWS Key Management Service (AWS KMS) 证书来加密传输中的数据。使用 AWS Certificate Manager (ACM) 加密 EBS 卷和 Aurora 数据库存储的静态数据。",
      "B": "使用 AWS 根账户登录 AWS Management Console。上传公司的加密证书。在根账户中，选择打开账户所有静态和传输数据加密的选项。",
      "C": "使用 AWS Key Management Service (AWS KMS) 加密 EBS 卷和 Aurora 数据库存储的静态数据。将 AWS Certificate Manager (ACM) 证书附加到 ALB 以加密传输中的数据。",
      "D": "使用 BitLocker 加密所有静态数据。将公司的 TLS 证书密钥导入 AWS Key Management Service (AWS KMS)。将 KMS 密钥附加到 ALB 以加密传输中的数据。"
    },
    "tags": [
      "EC2",
      "EBS",
      "ALB",
      "Aurora",
      "KMS",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n题目考察数据加密。要求静态和传输中的数据都要加密。",
      "why_correct": "选项 C 正确。使用 KMS 加密 EBS 和 Aurora 数据，使用 ACM 证书加密 ALB 上的传输数据，满足需求。",
      "why_wrong": "A 错误，ACM 无法加密 EBS 和 Aurora。B 错误，不应该使用根账户，而且没有正确的加密方式。D 错误，BitLocker 用于加密 Windows 卷。"
    },
    "related_terms": [
      "EC2",
      "EBS",
      "ALB",
      "Aurora",
      "KMS",
      "ACM"
    ]
  },
  {
    "id": 235,
    "topic": "1",
    "question_en": "A company is moving its on-premises Oracle database to Amazon Aurora PostgreSQL. The database has several applications that write to the same tables. The applications need to be migrated one by one with a month in between each migration. Management has expressed concerns that the database has a high number of reads and writes. The data must be kept in sync across both databases throughout the migration. What should a solutions architect recommend?",
    "options_en": {
      "A": "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a change data capture (CDC) replication task and a table mapping to select all tables.",
      "B": "Use AWS DataSync for the initial migration. Use AWS Database Migration Service (AWS DMS) to create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.",
      "C": "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a memory optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select all tables.",
      "D": "Use the AWS Schema Conversion Tool with AWS Database Migration Service (AWS DMS) using a compute optimized replication instance. Create a full load plus change data capture (CDC) replication task and a table mapping to select the largest tables."
    },
    "correct_answer": "C",
    "vote_percentage": "90%",
    "question_cn": "一家公司正在将其本地 Oracle 数据库迁移到 Amazon Aurora PostgreSQL。该数据库有多个应用程序写入相同的表。这些应用程序需要逐个迁移，每次迁移之间间隔一个月。管理层表示担心数据库的读写次数很高。在整个迁移过程中，数据必须在两个数据库之间保持同步。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "使用 AWS DataSync 进行初始迁移。使用 AWS Database Migration Service (AWS DMS) 创建一个更改数据捕获 (CDC) 复制任务和一个表映射，以选择所有表。",
      "B": "使用 AWS DataSync 进行初始迁移。使用 AWS Database Migration Service (AWS DMS) 创建一个完整加载加上更改数据捕获 (CDC) 复制任务和一个表映射，以选择所有表。",
      "C": "使用 AWS Schema Conversion Tool 和 AWS Database Migration Service (AWS DMS)，使用内存优化复制实例。创建一个完整加载加上更改数据捕获 (CDC) 复制任务和一个表映射，以选择所有表。",
      "D": "使用 AWS Schema Conversion Tool 和 AWS Database Migration Service (AWS DMS)，使用计算优化复制实例。创建一个完整加载加上更改数据捕获 (CDC) 复制任务和一个表映射，以选择最大的表。"
    },
    "tags": [
      "DMS",
      "Aurora PostgreSQL",
      "DataSync",
      "SCT"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 90%），解析仅供参考。】\n\n考察数据库迁移的策略。要求数据在迁移过程中同步。",
      "why_correct": "选项 C 正确。使用 SCT 和 DMS 实现数据库迁移。SCT 用于模式转换，DMS 用于数据迁移和 CDC。",
      "why_wrong": "A 错误，没有使用 SCT。B 错误，没有使用 SCT。D 错误，选择最大的表，无法满足多个应用程序写入相同表的需求。"
    },
    "related_terms": [
      "DMS",
      "Aurora PostgreSQL",
      "DataSync",
      "SCT"
    ]
  },
  {
    "id": 236,
    "topic": "1",
    "question_en": "A company has a three-tier application for image sharing. The application uses an Amazon EC2 instance for the front-end layer, another EC2 instance for the application layer, and a third EC2 instance for a MySQL database. A solutions architect must design a scalable and highly available solution that requires the least amount of change to the application. Which solution meets these requirements?",
    "options_en": {
      "A": "Use Amazon S3 to host the front-end layer. Use AWS Lambda functions for the application layer. Move the database to an Amazon DynamoDB table. Use Amazon S3 to store and serve users’ images.",
      "B": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS DB instance with multiple read replicas to serve users’ images.",
      "C": "Use Amazon S3 to host the front-end layer. Use a fieet of EC2 instances in an Auto Scaling group for the application layer. Move the database to a memory optimized instance type to store and serve users’ images.",
      "D": "Use load-balanced Multi-AZ AWS Elastic Beanstalk environments for the front-end layer and the application layer. Move the database to an Amazon RDS Multi-AZ DB instance. Use Amazon S3 to store and serve users’ images."
    },
    "correct_answer": "D",
    "vote_percentage": "69%",
    "question_cn": "一家公司有一个用于图像共享的三层应用程序。该应用程序使用 Amazon EC2 实例作为前端层，另一个 EC2 实例作为应用程序层，第三个 EC2 实例用于 MySQL 数据库。解决方案架构师必须设计一个可扩展且高度可用的解决方案，该方案要求对应用程序的更改最少。哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 托管前端层。使用 AWS Lambda 函数作为应用程序层。将数据库移至 Amazon DynamoDB 表。使用 Amazon S3 存储和提供用户的图像。",
      "B": "对前端层和应用程序层使用负载均衡的 Multi-AZ AWS Elastic Beanstalk 环境。将数据库移至具有多个只读副本的 Amazon RDS DB 实例，以服务用户的图像。",
      "C": "使用 Amazon S3 托管前端层。使用 Auto Scaling 组中的 EC2 实例组作为应用程序层。将数据库移至内存优化实例类型以存储和提供用户的图像。",
      "D": "对前端层和应用程序层使用负载均衡的 Multi-AZ AWS Elastic Beanstalk 环境。将数据库移至 Amazon RDS Multi-AZ DB 实例。使用 Amazon S3 存储和提供用户的图像。"
    },
    "tags": [
      "EC2",
      "S3",
      "Lambda",
      "RDS",
      "Elastic Beanstalk",
      "Auto Scaling",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 69%），解析仅供参考。】\n\n考察如何设计可扩展且高可用的三层应用程序，并要求对应用程序的更改最少。",
      "why_correct": "选项 D 提供了最佳的解决方案，它最大限度地减少了对应用程序的更改。Elastic Beanstalk 简化了前端层和应用程序层的部署和管理，实现了负载均衡和 Multi-AZ 部署。使用 RDS Multi-AZ DB 实例确保了数据库的高可用性，使用 S3 存储和提供用户图像则满足了可扩展性和可用性的要求。",
      "why_wrong": "选项 A 涉及大规模架构更改，将 EC2 实例替换为 Lambda 和 DynamoDB，可能需要对应用程序进行大量修改，违背了“更改最少”的要求。选项 B 虽然在 Elastic Beanstalk 中使用了 Multi-AZ，但使用只读副本服务用户图像，未提及 S3 存储，且没有利用 RDS Multi-AZ 功能。选项 C 使用 S3 托管前端，使用 Auto Scaling 的 EC2 实例组作为应用程序层，但将数据库迁移到内存优化实例，未提及高可用性，且没有使用 S3 存储用户图片。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "Amazon S3",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon RDS",
      "AWS Elastic Beanstalk",
      "Multi-AZ",
      "Auto Scaling",
      "DB Instance"
    ]
  },
  {
    "id": 237,
    "topic": "1",
    "question_en": "An application running on an Amazon EC2 instance in VPC-A needs to access files in another EC2 instance in VPC-B. Both VPCs are in separate AWS accounts. The network administrator needs to design a solution to configure secure access to EC2 instance in VPC-B from VPC-",
    "options_en": {
      "A": "Set up a VPC peering connection between VPC-A and VPC-B.",
      "B": "Set up VPC gateway endpoints for the EC2 instance running in VPC-B.",
      "C": "Attach a virtual private gateway to VPC-B and set up routing from VPC-A.",
      "D": "Create a private virtual interface (VIF) for the EC2 instance running in VPC-B and add appropriate routes from VPC-A."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "在 VPC-A 中运行的 Amazon EC2 实例需要访问 VPC-B 中另一个 EC2 实例中的文件。这两个 VPC 位于不同的 AWS 账户中。网络管理员需要设计一个解决方案，以配置从 VPC-A 到 VPC-B 中 EC2 实例的安全访问。",
    "options_cn": {
      "A": "在 VPC-A 和 VPC-B 之间建立 VPC 对等连接。",
      "B": "为在 VPC-B 中运行的 EC2 实例设置 VPC 网关端点。",
      "C": "将虚拟私有网关附加到 VPC-B，并设置来自 VPC-A 的路由。",
      "D": "为在 VPC-B 中运行的 EC2 实例创建私有虚拟接口 (VIF)，并从 VPC-A 添加适当的路由。"
    },
    "tags": [
      "VPC",
      "EC2",
      "VPC 对等连接",
      "VPC 端点",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n此题考查跨账户 VPC 互联的方式。题目要求在不同账户的 VPC 之间进行安全访问。",
      "why_correct": "选项 A 正确。在 VPC-A 和 VPC-B 之间建立 VPC 对等连接，可以实现 VPC 之间的互联。",
      "why_wrong": "B 错误，VPC 网关端点用于访问 AWS 服务。C 错误，VPN 增加了复杂性。D 错误，创建私有虚拟接口，需要 AWS Direct Connect。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "VPC 对等连接",
      "VPC 端点",
      "VPN"
    ]
  },
  {
    "id": 238,
    "topic": "1",
    "question_en": "A company wants to experiment with individual AWS accounts for its engineer team. The company wants to be notified as soon as the Amazon EC2 instance usage for a given month exceeds a specific threshold for each account. What should a solutions architect do to meet this requirement MOST cost-effectively?",
    "options_en": {
      "A": "Use Cost Explorer to create a daily report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.",
      "B": "Use Cost Explorer to create a monthly report of costs by service. Filter the report by EC2 instances. Configure Cost Explorer to send an Amazon Simple Email Service (Amazon SES) notification when a threshold is exceeded.",
      "C": "Use AWS Budgets to create a cost budget for each account. Set the period to monthly. Set the scope to EC2 instances. Set an alert threshold for the budget. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded.",
      "D": "Use AWS Cost and Usage Reports to create a report with hourly granularity. Integrate the report data with Amazon Athena. Use Amazon EventBridge to schedule an Athena query. Configure an Amazon Simple Notification Service (Amazon SNS) topic to receive a notification when a threshold is exceeded."
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一家公司希望为其工程师团队试验各个 AWS 账户。该公司希望在给定月份每个账户的 Amazon EC2 实例使用量超过特定阈值时立即收到通知。解决方案架构师应该怎么做才能以最具成本效益的方式满足此要求？",
    "options_cn": {
      "A": "使用 Cost Explorer 创建按服务划分的每日成本报告。按 EC2 实例筛选报告。配置 Cost Explorer 在超过阈值时发送 Amazon Simple Email Service (Amazon SES) 通知。",
      "B": "使用 Cost Explorer 创建按服务划分的每月成本报告。按 EC2 实例筛选报告。配置 Cost Explorer 在超过阈值时发送 Amazon Simple Email Service (Amazon SES) 通知。",
      "C": "使用 AWS Budgets 为每个账户创建成本预算。将周期设置为每月。将范围设置为 EC2 实例。为预算设置警报阈值。配置 Amazon Simple Notification Service (Amazon SNS) 主题以在超过阈值时收到通知。",
      "D": "使用 AWS Cost and Usage Reports 创建具有每小时粒度的报告。将报告数据与 Amazon Athena 集成。使用 Amazon EventBridge 安排 Athena 查询。配置 Amazon Simple Notification Service (Amazon SNS) 主题以在超过阈值时收到通知。"
    },
    "tags": [
      "EC2",
      "Cost Explorer",
      "AWS Budgets",
      "SNS",
      "Athena",
      "EventBridge",
      "SES"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n考查如何使用 AWS Budgets 监控 EC2 实例的成本，并在超过预定阈值时触发通知。",
      "why_correct": "AWS Budgets 允许创建每月预算，并可以针对特定服务（如 EC2）进行范围限定。通过设置警报阈值，当实际成本超过阈值时，Budgets 会触发通知。使用 Amazon SNS 可以方便地接收这些警报通知，满足了题目的成本监控和即时通知需求，且具有成本效益。",
      "why_wrong": "选项 A 和 B 都使用 Cost Explorer 创建成本报告，但 Cost Explorer 主要用于成本分析和可视化，而不是即时警报。虽然 Cost Explorer 可以发送 SES 通知，但其通知功能不如 Budgets 灵活和高效，尤其是在需要按月监控和阈值触发通知的场景。选项 D 使用 Cost and Usage Reports (CUR) 和 Athena，虽然可以进行细粒度的成本分析，但设置过程复杂，且需要额外的费用来存储和查询数据，成本相对较高，且不具备像 Budgets 一样简洁的预警功能。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Budgets",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Cost Explorer",
      "Amazon Simple Email Service (Amazon SES)",
      "AWS Cost and Usage Reports",
      "Amazon Athena",
      "Amazon EventBridge"
    ]
  },
  {
    "id": 239,
    "topic": "1",
    "question_en": "A solutions architect needs to design a new microservice for a company’s application. Clients must be able to call an HTTPS endpoint to reach the microservice. The microservice also must use AWS Identity and Access Management (IAM) to authenticate calls. The solutions architect will write the logic for this microservice by using a single AWS Lambda function that is written in Go 1.x. Which solution will deploy the function in the MOST operationally eficient way?",
    "options_en": {
      "A": "Create an Amazon API Gateway REST API. Configure the method to use the Lambda function. Enable IAM authentication on the API.",
      "B": "Create a Lambda function URL for the function. Specify AWS_IAM as the authentication type.",
      "C": "Create an Amazon CloudFront distribution. Deploy the function to Lambda@Edge. Integrate IAM authentication logic into the Lambda@Edge function.",
      "D": "Create an Amazon CloudFront distribution. Deploy the function to CloudFront Functions. Specify AWS_IAM as the authentication type."
    },
    "correct_answer": "A",
    "vote_percentage": "65%",
    "question_cn": "一位解决方案架构师需要为公司的应用程序设计一个新的微服务。客户端必须能够调用一个 HTTPS 终端节点来访问该微服务。该微服务还必须使用 AWS Identity and Access Management (IAM) 来验证调用。解决方案架构师将使用单个 AWS Lambda 函数编写此微服务的逻辑，该函数用 Go 1.x 编写。哪种解决方案将以最具操作效率的方式部署该函数？",
    "options_cn": {
      "A": "创建一个 Amazon API Gateway REST API。配置该方法以使用 Lambda 函数。在 API 上启用 IAM 身份验证。",
      "B": "为该函数创建一个 Lambda 函数 URL。指定 AWS_IAM 作为身份验证类型。",
      "C": "创建一个 Amazon CloudFront 分发。将该函数部署到 Lambda@Edge。将 IAM 身份验证逻辑集成到 Lambda@Edge 函数中。",
      "D": "创建一个 Amazon CloudFront 分发。将该函数部署到 CloudFront Functions。指定 AWS_IAM 作为身份验证类型。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "IAM",
      "CloudFront",
      "Lambda@Edge",
      "CloudFront Functions"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 65%），解析仅供参考。】\n\n考察 API 的部署方案。需要考虑 IAM 认证和操作效率。",
      "why_correct": "选项 A 正确。使用 API Gateway 可以直接调用 Lambda 函数，并在 API 上启用 IAM 身份验证。",
      "why_wrong": "B 错误，Lambda 函数 URL 并不支持 IAM 认证。C 错误，Lambda@Edge 增加了复杂性。D 错误，CloudFront Functions 无法进行 IAM 身份验证。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "IAM",
      "CloudFront",
      "Lambda@Edge",
      "CloudFront Functions"
    ]
  },
  {
    "id": 240,
    "topic": "1",
    "question_en": "A company previously migrated its data warehouse solution to AWS. The company also has an AWS Direct Connect connection. Corporate ofice users query the data warehouse using a visualization tool. The average size of a query returned by the data warehouse is 50 MB and each webpage sent by the visualization tool is approximately 500 KB. Result sets returned by the data warehouse are not cached. Which solution provides the LOWEST data transfer egress cost for the company?",
    "options_en": {
      "A": "Host the visualization tool on premises and query the data warehouse directly over the internet.",
      "B": "Host the visualization tool in the same AWS Region as the data warehouse. Access it over the internet.",
      "C": "Host the visualization tool on premises and query the data warehouse directly over a Direct Connect connection at a location in the same AWS Region.",
      "D": "Host the visualization tool in the same AWS Region as the data warehouse and access it over a Direct Connect connection at a location in the same Region."
    },
    "correct_answer": "D",
    "vote_percentage": "92%",
    "question_cn": "一家公司先前已将其数据仓库解决方案迁移到 AWS。该公司也有一个 AWS Direct Connect 连接。公司办公室用户使用可视化工具查询数据仓库。数据仓库返回的查询平均大小为 50 MB，可视化工具发送的每个网页大约为 500 KB。数据仓库返回的结果集未被缓存。哪种解决方案为公司提供了最低的数据传输出口成本？",
    "options_cn": {
      "A": "在本地托管可视化工具，并通过互联网直接查询数据仓库。",
      "B": "在与数据仓库相同的 AWS 区域中托管可视化工具。通过互联网访问它。",
      "C": "在本地托管可视化工具，并通过 Direct Connect 连接直接查询数据仓库，该连接位于同一 AWS 区域的某个位置。",
      "D": "在与数据仓库相同的 AWS 区域中托管可视化工具，并通过位于同一区域的 Direct Connect 连接进行访问。"
    },
    "tags": [
      "Data Transfer",
      "Direct Connect",
      "S3",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 92%），解析仅供参考。】\n\n考查在 AWS 环境中优化数据传输成本，涉及 Direct Connect、数据传输费用、数据仓库与可视化工具的部署位置。要求选择最低出口数据传输成本的方案。",
      "why_correct": "选项 D 提供了最低的数据传输出口成本。将可视化工具和数据仓库部署在同一 AWS 区域，并且通过 Direct Connect 连接进行通信，避免了通过互联网的数据传输费用。使用 Direct Connect 降低了数据传输费用，适用于大型数据仓库查询和频繁的数据访问场景。",
      "why_wrong": "选项 A 和 B 都通过互联网进行数据传输，会产生数据传输费用，而且费用相对较高。选项 C 虽然通过 Direct Connect 连接，但可视化工具部署在本地，每次网页请求都需要将数据从 AWS 传回本地，仍然产生数据传输费用，不如选项 D 经济。"
    },
    "related_terms": [
      "AWS Direct Connect",
      "AWS",
      "Data Warehouse"
    ]
  },
  {
    "id": 241,
    "topic": "1",
    "question_en": "An online learning company is migrating to the AWS Cloud. The company maintains its student records in a PostgreSQL database. The company needs a solution in which its data is available and online across multiple AWS Regions at all times. Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Migrate the PostgreSQL database to a PostgreSQL cluster on Amazon EC2 instances.",
      "B": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance with the Multi-AZ feature turned on.",
      "C": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Create a read replica in another Region.",
      "D": "Migrate the PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. Set up DB snapshots to be copied to another Region."
    },
    "correct_answer": "C",
    "vote_percentage": "79%",
    "question_cn": "一家在线学习公司正在迁移到 AWS 云。该公司将其学生记录保存在 PostgreSQL 数据库中。该公司需要一个解决方案，使其数据始终在多个 AWS 区域中在线可用。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将 PostgreSQL 数据库迁移到 Amazon EC2 实例上的 PostgreSQL 集群。",
      "B": "将 PostgreSQL 数据库迁移到启用了 Multi-AZ 功能的 Amazon RDS for PostgreSQL 数据库实例。",
      "C": "将 PostgreSQL 数据库迁移到 Amazon RDS for PostgreSQL 数据库实例。在另一个区域中创建只读副本。",
      "D": "将 PostgreSQL 数据库迁移到 Amazon RDS for PostgreSQL 数据库实例。设置数据库快照，将其复制到另一个区域。"
    },
    "tags": [
      "PostgreSQL",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 79%），解析仅供参考。】\n\n此题考察数据库高可用和跨区域灾备。Amazon RDS for PostgreSQL 的 Multi-AZ 部署提供高可用性，而创建跨区域只读副本则满足了数据在多个区域中在线可用的需求，并提供灾备能力。",
      "why_correct": "选项 C 提供了最高效的解决方案。它利用 RDS for PostgreSQL 的能力，同时实现了高可用和跨区域的只读副本，满足了题目的要求，运营开销最低。",
      "why_wrong": "选项 A 错误，因为在 EC2 上自建 PostgreSQL 集群，运维成本较高。选项 B 错误，Multi-AZ 仅提供高可用，不提供跨区域的数据副本。选项 D 错误，虽然快照可以复制到其他区域，但其恢复过程会产生停机时间，不满足\"始终在线可用\"的要求。"
    },
    "related_terms": [
      "PostgreSQL",
      "Amazon EC2",
      "Amazon RDS for PostgreSQL",
      "Multi-AZ"
    ]
  },
  {
    "id": 242,
    "topic": "1",
    "question_en": "A company hosts its web application on AWS using seven Amazon EC2 instances. The company requires that the IP addresses of all healthy EC2 instances be returned in response to DNS queries. Which policy should be used to meet this requirement?",
    "options_en": {
      "A": "Simple routing policy",
      "B": "Latency routing policy",
      "C": "Multivalue routing policy",
      "D": "Geolocation routing policy"
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一家公司使用七个 Amazon EC2 实例在 AWS 上托管其 Web 应用程序。该公司要求在 DNS 查询的响应中返回所有运行正常的 EC2 实例的 IP 地址。应使用哪种策略来满足此要求？",
    "options_cn": {
      "A": "简单路由策略",
      "B": "延迟路由策略",
      "C": "多值路由策略",
      "D": "地理位置路由策略"
    },
    "tags": [
      "Route 53",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n此题考察 DNS 路由策略。多值路由策略（Multivalue Answer Routing Policy）可以返回多个资源记录，适用于健康检查，从而实现 DNS 级别的负载均衡。",
      "why_correct": "多值路由策略 (Multivalue Answer Routing Policy) 允许 Route 53 返回多个健康且运行正常的 EC2 实例的 IP 地址，以实现负载均衡。",
      "why_wrong": "简单路由策略 (Simple Routing Policy) 只能返回单个资源记录。延迟路由策略 (Latency Routing Policy) 用于根据延迟选择资源。地理位置路由策略 (Geolocation Routing Policy) 用于根据用户地理位置选择资源。"
    },
    "related_terms": [
      "Route 53",
      "EC2",
      "DNS",
      "Simple Routing Policy",
      "Latency Routing Policy",
      "Multivalue Answer Routing Policy",
      "Geolocation Routing Policy"
    ]
  },
  {
    "id": 243,
    "topic": "1",
    "question_en": "A medical research lab produces data that is related to a new study. The lab wants to make the data available with minimum latency to clinics across the country for their on-premises, file-based applications. The data files are stored in an Amazon S3 bucket that has read-only permissions for each clinic. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Deploy an AWS Storage Gateway file gateway as a virtual machine (VM) on premises at each clinic",
      "B": "Migrate the files to each clinic’s on-premises applications by using AWS DataSync for processing.",
      "C": "Deploy an AWS Storage Gateway volume gateway as a virtual machine (VM) on premises at each clinic.",
      "D": "Attach an Amazon Elastic File System (Amazon EFS) file system to each clinic’s on-premises servers."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "一个医学研究实验室产生与一项新研究相关的数据。该实验室希望以最小的延迟将数据提供给全国各地的诊所，以供其本地的、基于文件的应用程序使用。数据文件存储在 Amazon S3 存储桶中，该存储桶为每个诊所提供只读权限。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "在每个诊所的本地部署一个作为虚拟机 (VM) 的 AWS Storage Gateway 文件网关。",
      "B": "使用 AWS DataSync 将文件迁移到每个诊所的本地应用程序进行处理。",
      "C": "在每个诊所的本地部署一个作为虚拟机 (VM) 的 AWS Storage Gateway 卷网关。",
      "D": "将 Amazon Elastic File System (Amazon EFS) 文件系统连接到每个诊所的本地服务器。"
    },
    "tags": [
      "S3",
      "Storage Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n考查如何通过 AWS Storage Gateway 提供低延迟的文件访问，满足诊所对 S3 数据的本地化需求。",
      "why_correct": "AWS Storage Gateway 文件网关可以缓存 S3 存储桶中的数据到本地，实现对文件的低延迟访问。文件网关支持 SMB 和 NFS 协议，方便诊所的本地应用程序使用。通过在每个诊所部署文件网关，可以实现数据本地化，满足低延迟和本地文件访问的要求。",
      "why_wrong": "B 选项 AWS DataSync 用于数据迁移，无法满足持续的数据同步和低延迟访问的需求。C 选项卷网关用于块存储，与基于文件的应用程序不兼容。D 选项 EFS 是一个云原生文件系统，无法直接连接到诊所的本地服务器，且延迟较高，不符合低延迟的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Storage Gateway",
      "File Gateway",
      "AWS DataSync",
      "Volume Gateway",
      "Amazon Elastic File System (Amazon EFS)",
      "SMB",
      "NFS",
      "VM"
    ]
  },
  {
    "id": 244,
    "topic": "1",
    "question_en": "A company is using a content management system that runs on a single Amazon EC2 instance. The EC2 instance contains both the web server and the database software. The company must make its website platform highly available and must enable the website to scale to meet user demand. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Move the database to Amazon RDS, and enable automatic backups. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer in the Availability Zone, and set the two instances as targets.",
      "B": "Migrate the database to an Amazon Aurora instance with a read replica in the same Availability Zone as the existing EC2 instance. Manually launch another EC2 instance in the same Availability Zone. Configure an Application Load Balancer, and set the two EC2 instances as targets.",
      "C": "Move the database to Amazon Aurora with a read replica in another Availability Zone. Create an Amazon Machine Image (AMI) from the EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones.",
      "D": "Move the database to a separate EC2 instance, and schedule backups to Amazon S3. Create an Amazon Machine Image (AMI) from the original EC2 instance. Configure an Application Load Balancer in two Availability Zones. Attach an Auto Scaling group that uses the AMI across two Availability Zones."
    },
    "correct_answer": "C",
    "vote_percentage": "97%",
    "question_cn": "一家公司正在使用一个内容管理系统，该系统运行在单个 Amazon EC2 实例上。EC2 实例包含 Web 服务器和数据库软件。该公司必须使其网站平台具有高可用性，并且必须使网站能够扩展以满足用户需求。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "将数据库移动到 Amazon RDS，并启用自动备份。手动在同一可用区中启动另一个 EC2 实例。在可用区中配置 Application Load Balancer，并将这两个实例设置为目标。",
      "B": "将数据库迁移到 Amazon Aurora 实例，并在与现有 EC2 实例相同的可用区中创建一个只读副本。手动在同一可用区中启动另一个 EC2 实例。配置 Application Load Balancer，并将这两个 EC2 实例设置为目标。",
      "C": "将数据库移动到 Amazon Aurora，并在另一个可用区中创建一个只读副本。从 EC2 实例创建 Amazon Machine Image (AMI)。在两个可用区中配置 Application Load Balancer。附加一个 Auto Scaling 组，该组使用 AMI 跨两个可用区。",
      "D": "将数据库移动到单独的 EC2 实例，并将备份调度到 Amazon S3。从原始 EC2 实例创建 Amazon Machine Image (AMI)。在两个可用区中配置 Application Load Balancer。附加一个 Auto Scaling 组，该组使用 AMI 跨两个可用区。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Aurora",
      "Auto Scaling",
      "Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 97%），解析仅供参考。】\n\n此题考察高可用性、可扩展性。Amazon Aurora 提供了高可用性，同时能通过只读副本和 Auto Scaling 提供可扩展性。将 Aurora 和 ALB、Auto Scaling 结合，即可满足要求。",
      "why_correct": "选项 C 提供了最完整的解决方案。将数据库移动到 Aurora 之后，可以利用 Aurora 的高可用特性，并且通过 ALB 和 Auto Scaling 实现可扩展性。",
      "why_wrong": "选项 A 错误，手动启动 EC2 实例无法自动实现扩展。选项 B 错误，在同一可用区创建只读副本没有实现高可用性。选项 D 错误，单独的 EC2 实例作为数据库性能有限。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "Aurora",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon Machine Image (AMI)"
    ]
  },
  {
    "id": 245,
    "topic": "1",
    "question_en": "A company is launching an application on AWS. The application uses an Application Load Balancer (ALB) to direct trafic to at least two Amazon EC2 instances in a single target group. The instances are in an Auto Scaling group for each environment. The company requires a development environment and a production environment. The production environment will have periods of high trafic. Which solution will configure the development environment MOST cost-effectively?",
    "options_en": {
      "A": "Reconfigure the target group in the development environment to have only one EC2 instance as a target.",
      "B": "Change the ALB balancing algorithm to least outstanding requests.",
      "C": "Reduce the size of the EC2 instances in both environments.",
      "D": "Reduce the maximum number of EC2 instances in the development environment’s Auto Scaling group."
    },
    "correct_answer": "D",
    "vote_percentage": "56%",
    "question_cn": "一家公司正在 AWS 上启动一个应用程序。该应用程序使用 Application Load Balancer (ALB) 将流量定向到单个目标组中至少两个 Amazon EC2 实例。这些实例位于每个环境的 Auto Scaling 组中。该公司需要一个开发环境和一个生产环境。生产环境将有高流量时段。哪个解决方案将以最具成本效益的方式配置开发环境？",
    "options_cn": {
      "A": "将开发环境中的目标组重新配置为仅将一个 EC2 实例作为目标。",
      "B": "将 ALB 负载均衡算法更改为未完成请求最少。",
      "C": "减小两个环境中 EC2 实例的大小。",
      "D": "减少开发环境的 Auto Scaling 组中 EC2 实例的最大数量。"
    },
    "tags": [
      "Application Load Balancer",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 56%），解析仅供参考。】\n\n本题考查了在不同环境下，如何通过 Auto Scaling 和负载均衡的配置来优化开发环境的成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 通过减少开发环境 Auto Scaling 组中 EC2 实例的最大数量，直接降低了开发环境的计算资源规模。这使得开发环境可以在流量较低时，保持较少的运行实例，从而减少成本，同时保留了 Auto Scaling 的弹性。这满足了成本效益的要求，且不影响生产环境的配置。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，因为即使开发环境使用单个 EC2 实例，ALB 仍然需要维护，且实例故障会导致服务中断。选项 B 错误，更改 ALB 的负载均衡算法并不能直接降低开发环境的成本，反而可能影响流量分配和性能。选项 C 错误，虽然减小 EC2 实例大小也能降低成本，但可能影响开发环境的性能，且会影响生产环境的配置，并非最具成本效益的解决方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "Auto Scaling Group",
      "EC2 instance"
    ]
  },
  {
    "id": 246,
    "topic": "1",
    "question_en": "A company runs a web application on Amazon EC2 instances in multiple Availability Zones. The EC2 instances are in private subnets. A solutions architect implements an internet-facing Application Load Balancer (ALB) and specifies the EC2 instances as the target group. However, the internet trafic is not reaching the EC2 instances. How should the solutions architect reconfigure the architecture to resolve this issue?",
    "options_en": {
      "A": "Replace the ALB with a Network Load Balancer. Configure a NAT gateway in a public subnet to allow internet trafic.",
      "B": "Move the EC2 instances to public subnets. Add a rule to the EC2 instances’ security groups to allow outbound trafic to 0.0.0.0/0.",
      "C": "Update the route tables for the EC2 instances’ subnets to send 0.0.0.0/0 trafic through the internet gateway route. Add a rule to the EC2 instances’ security groups to allow outbound trafic to 0.0.0.0/0.",
      "D": "Create public subnets in each Availability Zone. Associate the public subnets with the ALB. Update the route tables for the public subnets with a route to the private subnets."
    },
    "correct_answer": "D",
    "vote_percentage": "83%",
    "question_cn": "一家公司在多个可用区中的 Amazon EC2 实例上运行一个 Web 应用程序。EC2 实例位于私有子网中。解决方案架构师实施了一个面向 Internet 的 Application Load Balancer (ALB) ，并将 EC2 实例指定为目标组。但是，Internet 流量无法到达 EC2 实例。解决方案架构师应如何重新配置架构以解决此问题？",
    "options_cn": {
      "A": "将 ALB 替换为 Network Load Balancer。在公有子网中配置一个 NAT Gateway 以允许 Internet 流量。",
      "B": "将 EC2 实例移至公有子网。向 EC2 实例的安全组添加一条规则，以允许到 0.0.0.0/0 的出站流量。",
      "C": "更新 EC2 实例子网的路由表，以通过 Internet 网关路由发送 0.0.0.0/0 流量。向 EC2 实例的安全组添加一条规则，以允许到 0.0.0.0/0 的出站流量。",
      "D": "在每个可用区中创建公有子网。将公有子网与 ALB 关联。使用指向私有子网的路由更新公有子网的路由表。"
    },
    "tags": [
      "Application Load Balancer",
      "EC2",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 83%），解析仅供参考。】\n\n考查了在多个可用区中使用 Application Load Balancer（ALB）并与私有子网中的 EC2 实例集成时，如何配置架构以允许 Internet 流量访问。",
      "why_correct": "选项 D 提供了正确的配置。ALB 位于公有子网，接收来自 Internet 的流量。公有子网的路由表将流量转发到私有子网中的 EC2 实例。这样，ALB 接收 Internet 流量，并将其路由到私有子网中的 EC2 实例，从而解决了 Internet 流量无法到达 EC2 实例的问题。",
      "why_wrong": "选项 A 引入了 Network Load Balancer (NLB)，这对于处理 HTTP/HTTPS 流量是不必要的，且未解决核心问题。选项 B 将 EC2 实例移动到公有子网是不安全的，并将安全组的出站规则配置为允许所有流量，这违反了最佳实践。选项 C 仅更新了路由表和安全组，但没有解决流量进入私有子网的问题，并且没有考虑到 ALB 的存在。此外，这种配置下，EC2 实例仍然无法直接接收来自 Internet 的流量。"
    },
    "related_terms": [
      "Amazon EC2",
      "Web application",
      "Application Load Balancer (ALB)",
      "EC2 instance",
      "Availability Zone",
      "private subnet",
      "Internet",
      "public subnet",
      "Network Load Balancer (NLB)",
      "NAT Gateway",
      "Security Group",
      "route table",
      "Internet gateway"
    ]
  },
  {
    "id": 247,
    "topic": "1",
    "question_en": "A company has deployed a database in Amazon RDS for MySQL. Due to increased transactions, the database support team is reporting slow reads against the DB instance and recommends adding a read replica. Which combination of actions should a solutions architect take before implementing this change? (Choose two.)",
    "options_en": {
      "A": "Enable binlog replication on the RDS primary node.",
      "B": "Choose a failover priority for the source DB instance.",
      "C": "Allow long-running transactions to complete on the source DB instance.",
      "D": "Create a global table and specify the AWS Regions where the table will be availabl",
      "E": "E. Enable automatic backups on the source instance by setting the backup retention period to a value other than 0."
    },
    "correct_answer": "CE",
    "vote_percentage": "71%",
    "question_cn": "一家公司已在 Amazon RDS for MySQL 中部署了一个数据库。由于事务量增加，数据库支持团队报告针对数据库实例的读取速度变慢，并建议添加一个只读副本。解决方案架构师在实施此更改之前应采取哪些组合操作？（选择两项。）",
    "options_cn": {
      "A": "在 RDS 主节点上启用 binlog 复制。",
      "B": "为 源数据库实例选择故障转移优先级。",
      "C": "允许长时间运行的事务在 源数据库实例上完成。",
      "D": "创建一个全局表，并指定该表将可用的 AWS 区域。",
      "E": "通过将备份保留期设置为大于 0 的值，在 源实例上启用自动备份。"
    },
    "tags": [
      "RDS",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 71%），解析仅供参考。】\n\n考查在 RDS for MySQL 中增加只读副本前的准备工作，包括事务处理和备份策略。",
      "why_correct": "C 选项正确，在创建只读副本前，应确保主数据库上没有长时间运行的事务，以避免复制延迟和数据不一致。E 选项正确，启用自动备份可以确保在需要时，可以从备份中创建新的只读副本，满足高可用和数据恢复的需求。",
      "why_wrong": "A 选项错误，虽然 binlog 复制是 RDS for MySQL 中实现只读副本的基础，但启用 binlog 只是一个前提条件，而不是在创建前需要采取的操作。B 选项错误，故障转移优先级与创建只读副本关系不大，其主要作用是在发生故障时，提升备用实例为主实例。D 选项错误，创建全局表与增加只读副本没有直接关系，全局表主要用于多区域数据共享，与解决读取速度慢的问题无关。"
    },
    "related_terms": [
      "Amazon RDS for MySQL",
      "read replica",
      "binlog replication",
      "failover priority",
      "global table",
      "backup retention"
    ]
  },
  {
    "id": 248,
    "topic": "1",
    "question_en": "A company runs analytics software on Amazon EC2 instances. The software accepts job requests from users to process data that has been uploaded to Amazon S3. Users report that some submitted data is not being processed Amazon CloudWatch reveals that the EC2 instances have a consistent CPU utilization at or near 100%. The company wants to improve system performance and scale the system based on user load. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a copy of the instance. Place all instances behind an Application Load Balancer.",
      "B": "Create an S3 VPC endpoint for Amazon S3. Update the software to reference the endpoint.",
      "C": "Stop the EC2 instances. Modify the instance type to one with a more powerful CPU and more memory. Restart the instances.",
      "D": "Route incoming requests to Amazon Simple Queue Service (Amazon SQS). Configure an EC2 Auto Scaling group based on queue size. Update the software to read from the queue."
    },
    "correct_answer": "D",
    "vote_percentage": "95%",
    "question_cn": "一家公司在 Amazon EC2 实例上运行分析软件。该软件接受用户提交的作业请求，以处理已上传到 Amazon S3 的数据。用户报告说，某些提交的数据未被处理。Amazon CloudWatch 显示 EC2 实例的 CPU 利用率持续稳定在或接近 100%。该公司希望提高系统性能并根据用户负载扩展系统。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建实例的副本。将所有实例置于 Application Load Balancer 之后。",
      "B": "为 Amazon S3 创建一个 S3 VPC endpoint。更新软件以引用该 endpoint。",
      "C": "停止 EC2 实例。将实例类型修改为具有更强大的 CPU 和更多内存的类型。重新启动实例。",
      "D": "将传入请求路由到 Amazon Simple Queue Service (Amazon SQS)。基于队列大小配置一个 EC2 Auto Scaling 组。更新软件以从队列中读取。"
    },
    "tags": [
      "EC2",
      "CloudWatch",
      "Auto Scaling",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 95%），解析仅供参考。】\n\n此题考察解决 CPU 负载高的问题。通过 SQS 和 Auto Scaling 组，可以将请求排队，从而实现异步处理和弹性扩展。",
      "why_correct": "选项 D 正确。将传入请求路由到 SQS，可以实现解耦，并通过 Auto Scaling 组根据队列大小自动扩展 EC2 实例，从而提高系统性能。",
      "why_wrong": "选项 A 错误，负载均衡无法解决 CPU 利用率高的问题。选项 B 错误，S3 VPC endpoint 与 CPU 利用率高的问题无关。选项 C 错误，修改实例类型可能需要停机，也无法解决扩展问题。"
    },
    "related_terms": [
      "EC2",
      "CloudWatch",
      "Auto Scaling",
      "SQS",
      "S3 VPC endpoint"
    ]
  },
  {
    "id": 249,
    "topic": "1",
    "question_en": "A company is implementing a shared storage solution for a media application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?",
    "options_en": {
      "A": "Create an AWS Storage Gateway volume gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
      "B": "Create an AWS Storage Gateway tape gateway. Configure tapes to use Amazon S3. Connect the application server to the tape gateway.",
      "C": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
      "D": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一家公司正在为其托管在 AWS 云中的媒体应用程序实施共享存储解决方案。该公司需要使用 SMB 客户端访问数据的能力。该解决方案必须是完全托管的。哪种 AWS 解决方案符合这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Storage Gateway 卷网关。创建一个使用所需客户端协议的文件共享。将应用程序服务器连接到文件共享。",
      "B": "创建一个 AWS Storage Gateway 磁带网关。配置磁带以使用 Amazon S3。将应用程序服务器连接到磁带网关。",
      "C": "创建一个 Amazon EC2 Windows 实例。在该实例上安装并配置一个 Windows 文件共享角色。将应用程序服务器连接到文件共享。",
      "D": "创建一个 Amazon FSx for Windows File Server 文件系统。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。"
    },
    "tags": [
      "FSx",
      "SMB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n此题考察共享存储解决方案。Amazon FSx for Windows File Server 支持 SMB 协议，并且是完全托管的。",
      "why_correct": "选项 D 正确。FSx for Windows File Server 提供了完全托管的 SMB 文件共享服务，满足了共享存储和 SMB 客户端访问的要求。",
      "why_wrong": "选项 A 错误，Storage Gateway 卷网关不完全托管。选项 B 错误，Storage Gateway 磁带网关不适用此场景。选项 C 错误，EC2 实例上的 Windows 文件共享需要手动管理。"
    },
    "related_terms": [
      "FSx",
      "SMB",
      "Storage Gateway"
    ]
  },
  {
    "id": 250,
    "topic": "1",
    "question_en": "A company’s security team requests that network trafic be captured in VPC Flow Logs. The logs will be frequently accessed for 90 days and then accessed intermittently. What should a solutions architect do to meet these requirements when configuring the logs?",
    "options_en": {
      "A": "Use Amazon CloudWatch as the target. Set the CloudWatch log group with an expiration of 90 days",
      "B": "Use Amazon Kinesis as the target. Configure the Kinesis stream to always retain the logs for 90 days.",
      "C": "Use AWS CloudTrail as the target. Configure CloudTrail to save to an Amazon S3 bucket, and enable S3 Intelligent-Tiering.",
      "D": "Use Amazon S3 as the target. Enable an S3 Lifecycle policy to transition the logs to S3 Standard-Infrequent Access (S3 Standard-IA) after 90 days."
    },
    "correct_answer": "D",
    "vote_percentage": "92%",
    "question_cn": "一家公司的安全团队要求在 VPC 流日志中捕获网络流量。这些日志将被频繁访问 90 天，然后间歇性地访问。 解决方案架构师在配置日志时应采取什么措施来满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch 作为目标。将 CloudWatch 日志组设置为 90 天后过期。",
      "B": "使用 Amazon Kinesis 作为目标。配置 Kinesis 流，始终将日志保留 90 天。",
      "C": "使用 AWS CloudTrail 作为目标。配置 CloudTrail 保存到 Amazon S3 存储桶，并打开 S3 Intelligent-Tiering。",
      "D": "使用 Amazon S3 作为目标。打开 S3 生命周期策略，在 90 天后将日志转换为 S3 Standard-Infrequent Access (S3 Standard-IA)."
    },
    "tags": [
      "CloudWatch",
      "VPC Flow Logs",
      "S3",
      "Lifecycle Policy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 92%），解析仅供参考。】\n\n考察VPC流日志的存储与访问策略，以及S3的生命周期管理和存储类选择。",
      "why_correct": "选项D是最优解。它使用Amazon S3作为目标，满足了存储需求。通过配置S3生命周期策略，可以将90天后的日志转换为S3 Standard-IA，降低存储成本，同时仍能满足间歇性访问的需求。",
      "why_wrong": "选项A错误，CloudWatch Logs主要用于日志监控和分析，不适合长期存储VPC流日志。选项B错误，Kinesis Streams主要用于实时数据流处理，其保留策略无法完全满足90天频繁访问和后续间歇性访问的需求。选项C错误，CloudTrail用于记录AWS API调用，不用于存储VPC流日志。"
    },
    "related_terms": [
      "VPC Flow Logs",
      "Amazon S3",
      "S3 Standard-IA",
      "S3 Lifecycle Policy",
      "Amazon CloudWatch",
      "Amazon Kinesis",
      "AWS CloudTrail",
      "S3 Intelligent-Tiering"
    ]
  },
  {
    "id": 251,
    "topic": "1",
    "question_en": "An Amazon EC2 instance is located in a private subnet in a new VPC. This subnet does not have outbound internet access, but the EC2 instance needs the ability to download monthly security updates from an outside vendor. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an internet gateway, and attach it to the VPC. Configure the private subnet route table to use the internet gateway as the default route.",
      "B": "Create a NAT gateway, and place it in a public subnet. Configure the private subnet route table to use the NAT gateway as the default route.",
      "C": "Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the NAT instance as the default route.",
      "D": "Create an internet gateway, and attach it to the VPC. Create a NAT instance, and place it in the same subnet where the EC2 instance is located. Configure the private subnet route table to use the internet gateway as the default route."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一个 Amazon EC2 实例位于新 VPC 的一个私有子网中。该子网没有出站互联网访问权限，但 EC2 实例需要能够从外部供应商下载每月安全更新。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个互联网网关，并将其附加到 VPC。配置私有子网路由表以使用互联网网关作为默认路由。",
      "B": "创建一个 NAT 网关，并将其放置在公有子网中。配置私有子网路由表以使用 NAT 网关作为默认路由。",
      "C": "创建一个 NAT 实例，并将其放置在 EC2 实例所在的同一子网中。配置私有子网路由表以使用 NAT 实例作为默认路由。",
      "D": "创建一个互联网网关，并将其附加到 VPC。创建一个 NAT 实例，并将其放置在 EC2 实例所在的同一子网中。配置私有子网路由表以使用互联网网关作为默认路由。"
    },
    "tags": [
      "EC2",
      "VPC",
      "NAT Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n此题考察私有子网的互联网访问。为了使私有子网中的 EC2 实例能够访问互联网，需要使用 NAT 网关或 NAT 实例。",
      "why_correct": "选项 B 正确。NAT 网关允许私有子网中的 EC2 实例访问互联网，同时阻止来自互联网的直接访问。",
      "why_wrong": "选项 A 错误，互联网网关用于公有子网，无法使私有子网的 EC2 实例访问互联网。选项 C 错误，NAT 实例需要手动维护。选项 D 错误，NAT 实例需要手动维护。"
    },
    "related_terms": [
      "EC2",
      "VPC",
      "NAT Gateway",
      "Internet Gateway"
    ]
  },
  {
    "id": 252,
    "topic": "1",
    "question_en": "A solutions architect needs to design a system to store client case files. The files are core company assets and are important. The number of files will grow over time. The files must be simultaneously accessible from multiple application servers that run on Amazon EC2 instances. The solution must have built-in redundancy. Which solution meets these requirements?",
    "options_en": {
      "A": "Amazon Elastic File System (Amazon EFS)",
      "B": "Amazon Elastic Block Store (Amazon EBS)",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "AWS Backup"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要设计一个系统来存储客户案例文件。这些文件是公司的核心资产，非常重要。文件数量会随着时间的推移而增加。这些文件必须可以同时从在 Amazon EC2 实例上运行的多个应用程序服务器访问。该解决方案必须具有内置冗余。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "Amazon Elastic File System (Amazon EFS)",
      "B": "Amazon Elastic Block Store (Amazon EBS)",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "AWS Backup"
    },
    "tags": [
      "EFS",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察共享文件系统。Amazon EFS 提供了共享文件系统，支持多个 EC2 实例同时访问。",
      "why_correct": "选项 A 正确。EFS 是一个完全托管的共享文件系统，可以被多个 EC2 实例同时访问，并提供内置冗余。",
      "why_wrong": "选项 B 错误，EBS 是块存储，不支持多实例同时访问。选项 C 错误，S3 Glacier Deep Archive 是归档存储，不适合此类应用。选项 D 错误，AWS Backup 主要用于备份，而不是共享文件存储。"
    },
    "related_terms": [
      "EFS",
      "EC2",
      "EBS",
      "S3 Glacier Deep Archive",
      "AWS Backup"
    ]
  },
  {
    "id": 253,
    "topic": "1",
    "question_en": "A solutions architect has created two IAM policies: Policy1 and Policy2. Both policies are attached to an IAM group. A cloud engineer is added as an IAM user to the IAM group. Which action will the cloud engineer be able to perform?",
    "options_en": {
      "A": "Deleting IAM users",
      "B": "Deleting directories",
      "C": "Deleting Amazon EC2 instances",
      "D": "Deleting logs from Amazon CloudWatch Logs"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师创建了两个 IAM 策略：策略 1 和策略 2。这两个策略都附加到一个 IAM 组。一位云工程师被添加为 IAM 用户到该 IAM 组。这位云工程师将能够执行哪个操作？",
    "options_cn": {
      "A": "删除 IAM 用户",
      "B": "删除目录",
      "C": "删除 Amazon EC2 实例",
      "D": "从 Amazon CloudWatch Logs 中删除日志"
    },
    "tags": [
      "IAM",
      "EC2",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察 IAM 权限。用户被分配到 IAM 组，即继承了该组的策略。",
      "why_correct": "选项 C 正确。如果 IAM 组策略允许执行操作，例如删除 EC2 实例，则该组中的所有用户都可以执行该操作。",
      "why_wrong": "选项 A 错误，IAM 组策略没有关于删除 IAM 用户的权限。选项 B 错误，IAM 组策略没有关于删除目录的权限。选项 D 错误，IAM 组策略没有关于删除 CloudWatch 日志的权限。"
    },
    "related_terms": [
      "IAM",
      "EC2",
      "CloudWatch"
    ]
  },
  {
    "id": 254,
    "topic": "1",
    "question_en": "A company is reviewing a recent migration of a three-tier application to a VPC. The security team discovers that the principle of least privilege is not being applied to Amazon EC2 security group ingress and egress rules between the application tiers. What should a solutions architect do to correct this issue?",
    "options_en": {
      "A": "Create security group rules using the instance ID as the source or destination.",
      "B": "Create security group rules using the security group ID as the source or destination.",
      "C": "Create security group rules using the VPC CIDR blocks as the source or destination.",
      "D": "Create security group rules using the subnet CIDR blocks as the source or destination."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在审查将其三层应用程序迁移到 VPC 的情况。安全团队发现，在应用程序层之间的 Amazon EC2 安全组入站和出站规则中，没有应用最小权限原则。解决方案架构师应该怎么做来纠正这个问题？",
    "options_cn": {
      "A": "使用实例 ID 作为 源 或目标来创建安全组规则。",
      "B": "使用安全组 ID 作为 源 或目标来创建安全组规则。",
      "C": "使用 VPC CIDR 块作为 源 或目标来创建安全组规则。",
      "D": "使用子网 CIDR 块作为 源 或目标来创建安全组规则。"
    },
    "tags": [
      "EC2",
      "安全组"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察安全组最佳实践。安全组规则应该使用最小权限原则，即仅允许必要的访问。使用安全组 ID 作为源和目标，可以实现更细粒度的控制。",
      "why_correct": "选项 B 正确。使用安全组 ID 作为源或目标，可以实现更细粒度的安全策略，有助于实现最小权限原则。",
      "why_wrong": "选项 A 错误，使用实例 ID 无法实现更细粒度的控制，且运维困难。选项 C 错误，使用 VPC CIDR 块过于宽泛，不符合最小权限原则。选项 D 错误，使用子网 CIDR 块也过于宽泛。"
    },
    "related_terms": [
      "EC2",
      "安全组",
      "VPC"
    ]
  },
  {
    "id": 255,
    "topic": "1",
    "question_en": "A company has an ecommerce checkout workfiow that writes an order to a database and calls a service to process the payment. Users are experiencing timeouts during the checkout process. When users resubmit the checkout form, multiple unique orders are created for the same desired transaction. How should a solutions architect refactor this workfiow to prevent the creation of multiple orders?",
    "options_en": {
      "A": "Configure the web application to send an order message to Amazon Kinesis Data Firehose. Set the payment service to retrieve the message from Kinesis Data Firehose and process the order.",
      "B": "Create a rule in AWS CloudTrail to invoke an AWS Lambda function based on the logged application path request. Use Lambda to query the database, call the payment service, and pass in the order information.",
      "C": "Store the order in the database. Send a message that includes the order number to Amazon Simple Notification Service (Amazon SNS). Set the payment service to poll Amazon SNS, retrieve the message, and process the order.",
      "D": "Store the order in the database. Send a message that includes the order number to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the payment service to retrieve the message and process the order. Delete the message from the queue."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个电子商务结账工作流程，该流程将订单写入数据库并调用服务来处理付款。用户在结账过程中遇到超时。当用户重新提交结账表单时，会为同一所需的交易创建多个唯一的订单。解决方案架构师应该如何重构此工作流程以防止创建多个订单？",
    "options_cn": {
      "A": "配置 Web 应用程序将订单消息发送到 Amazon Kinesis Data Firehose。设置付款服务以从 Kinesis Data Firehose 检索消息并处理订单。",
      "B": "在 AWS CloudTrail 中创建一条规则，以基于记录的应用程序路径请求来调用 AWS Lambda 函数。使用 Lambda 查询数据库，调用付款服务，并传入订单信息。",
      "C": "将订单存储在数据库中。发送一条包含订单号的消息到 Amazon Simple Notification Service (Amazon SNS)。设置付款服务以轮询 Amazon SNS，检索消息并处理订单。",
      "D": "将订单存储在数据库中。发送一条包含订单号的消息到 Amazon Simple Queue Service (Amazon SQS) FIFO 队列。设置付款服务以检索消息并处理订单。从队列中删除该消息。"
    },
    "tags": [
      "SQS",
      "FIFO",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察防止重复订单。使用 SQS FIFO 队列可以保证消息的顺序和幂等性，从而防止重复订单。",
      "why_correct": "选项 D 正确。使用 SQS FIFO 队列可以保证消息的顺序和仅处理一次，从而解决重复订单的问题。",
      "why_wrong": "选项 A 错误，Kinesis Data Firehose 不提供消息的顺序保证。选项 B 错误，CloudTrail 用于审计，不适合处理事务。选项 C 错误，SNS 不保证消息的顺序，无法避免重复订单。"
    },
    "related_terms": [
      "SQS",
      "FIFO",
      "Lambda",
      "SNS",
      "Kinesis Data Firehose"
    ]
  },
  {
    "id": 256,
    "topic": "1",
    "question_en": "A solutions architect is implementing a document review application using an Amazon S3 bucket for storage. The solution must prevent accidental deletion of the documents and ensure that all versions of the documents are available. Users must be able to download, modify, and upload documents. Which combination of actions should be taken to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Enable a read-only bucket ACL.",
      "B": "Enable versioning on the bucket.",
      "C": "Attach an IAM policy to the bucket.",
      "D": "Enable MFA Delete on the bucket",
      "E": "Encrypt the bucket using AWS KMS."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在使用 Amazon S3 存储桶实施文档审核应用程序进行存储。该解决方案必须防止文档被意外删除，并确保所有版本的文档都可用。用户必须能够下载、修改和上传文档。应采取哪些操作组合才能满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "启用只读存储桶 ACL。",
      "B": "在存储桶上启用版本控制。",
      "C": "将 IAM 策略附加到存储桶。",
      "D": "在存储桶上启用 MFA 删除。",
      "E": "使用 AWS KMS 加密存储桶。"
    },
    "tags": [
      "Amazon S3",
      "S3 Versioning",
      "IAM",
      "AWS KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n考查 Amazon S3 的数据保护和版本控制；与 S3 版本控制、IAM 权限、MFA 删除和 KMS 加密相关。",
      "why_correct": "在 S3 存储桶上启用版本控制后，每次对对象的更改（包括上传、删除等操作）都会生成一个新的版本。这允许用户检索对象的早期版本，从而防止意外删除，并确保所有版本的文档都可用。版本控制还允许用户上传、下载和修改文档，所有操作都将生成新的版本或覆盖现有的版本，满足题目需求。",
      "why_wrong": "A. 启用只读存储桶 ACL 无法满足用户修改和上传文档的需求，因为 ACL 主要用于控制对存储桶内对象的访问权限，而不是实现版本控制和防止意外删除。C. 将 IAM 策略附加到存储桶，主要用于控制用户或组对 S3 资源的访问权限，但并不能直接防止文档被意外删除或保留所有版本。D. 在存储桶上启用 MFA 删除，可以防止存储桶中的对象被删除，但不能满足题目中需要保留所有文档版本的需求。 E. 使用 AWS KMS 加密存储桶，主要关注数据的安全性和加密，与题目中防止意外删除和保留所有版本的要求不直接相关。 KMS 主要用于加密静态数据，而不是版本控制或防止误删除，与题目需求不匹配。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "IAM",
      "ACL",
      "AWS KMS",
      "MFA",
      "S3 Versioning"
    ]
  },
  {
    "id": 257,
    "topic": "1",
    "question_en": "A company is building a solution that will report Amazon EC2 Auto Scaling events across all the applications in an AWS account. The company needs to use a serverless solution to store the EC2 Auto Scaling status data in Amazon S3. The company then will use the data in Amazon S3 to provide near-real-time updates in a dashboard. The solution must not affect the speed of EC2 instance launches. How should the company move the data to Amazon S3 to meet these requirements?",
    "options_en": {
      "A": "Use an Amazon CloudWatch metric stream to send the EC2 Auto Scaling status data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
      "B": "Launch an Amazon EMR cluster to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3.",
      "C": "Create an Amazon EventBridge rule to invoke an AWS Lambda function on a schedule. Configure the Lambda function to send the EC2 Auto Scaling status data directly to Amazon S3.",
      "D": "Use a bootstrap script during the launch of an EC2 instance to install Amazon Kinesis Agent. Configure Kinesis Agent to collect the EC2 Auto Scaling status data and send the data to Amazon Kinesis Data Firehose. Store the data in Amazon S3."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司正在构建一个解决方案，该方案将报告 AWS 账户中所有应用程序的 Amazon EC2 Auto Scaling 事件。该公司需要使用无服务器解决方案将 EC2 Auto Scaling 状态数据存储在 Amazon S3 中。然后，该公司将使用 Amazon S3 中的数据在仪表板中提供近乎实时的更新。该解决方案不能影响 EC2 实例的启动速度。该公司应如何将数据移动到 Amazon S3 以满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch 指标流将 EC2 Auto Scaling 状态数据发送到 Amazon Kinesis Data Firehose。将数据存储在 Amazon S3 中。",
      "B": "启动 Amazon EMR 集群以收集 EC2 Auto Scaling 状态数据，并将数据发送到 Amazon Kinesis Data Firehose。将数据存储在 Amazon S3 中。",
      "C": "创建 Amazon EventBridge 规则以按计划调用 AWS Lambda 函数。配置 Lambda 函数以将 EC2 Auto Scaling 状态数据直接发送到 Amazon S3。",
      "D": "在启动 EC2 实例期间使用引导脚本安装 Amazon Kinesis Agent。配置 Kinesis Agent 以收集 EC2 Auto Scaling 状态数据，并将数据发送到 Amazon Kinesis Data Firehose。将数据存储在 Amazon S3 中。"
    },
    "tags": [
      "Amazon CloudWatch",
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "EC2 Auto Scaling",
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon EMR",
      "Kinesis Agent"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n本题考查无服务器解决方案中，如何高效且不影响 EC2 实例启动速度地将 EC2 Auto Scaling 事件数据存储到 Amazon S3。涉及 CloudWatch、Kinesis Data Firehose、EventBridge、Lambda、EMR 以及 Kinesis Agent 的选择和对比。",
      "why_correct": "选项 A 使用 CloudWatch 指标流将 EC2 Auto Scaling 事件数据发送到 Kinesis Data Firehose。Kinesis Data Firehose 负责数据的批量处理、转换和可靠地将数据写入 Amazon S3。CloudWatch 指标流能够近乎实时地捕获 Auto Scaling 事件，且这种方案不依赖于 EC2 实例的任何配置，因此不会影响实例的启动时间，完全符合题目要求。此外，Kinesis Data Firehose 具备数据转换和错误处理能力，可以适应不同的数据格式和处理需求。",
      "why_wrong": "选项 B 引入了 Amazon EMR 集群，这增加了解决方案的复杂性，需要额外的基础设施维护成本，不符合无服务器的设计理念，并且会增加延迟。选项 C 使用 EventBridge 触发 Lambda 函数，虽然是无服务器架构，但 Lambda 函数直接写入 S3 效率较低，可能导致并发问题和性能瓶颈。选项 D 在 EC2 实例启动期间使用 Kinesis Agent，这会影响实例的启动速度，与题目“不能影响 EC2 实例的启动速度”的要求相悖。而且，Kinesis Agent 的部署和配置也会增加复杂性，不如使用 CloudWatch 指标流更简洁高效。"
    },
    "related_terms": [
      "Amazon CloudWatch",
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "EC2 Auto Scaling",
      "Amazon EMR",
      "Amazon EventBridge",
      "AWS Lambda",
      "EC2",
      "Kinesis Agent"
    ]
  },
  {
    "id": 258,
    "topic": "1",
    "question_en": "A company has an application that places hundreds of .csv files into an Amazon S3 bucket every hour. The files are 1 GB in size. Each time a file is uploaded, the company needs to convert the file to Apache Parquet format and place the output file into an S3 bucket. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Lambda function to download the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Invoke the Lambda function for each S3 PUT event.",
      "B": "Create an Apache Spark job to read the .csv files, convert the files to Parquet format, and place the output files in an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the Spark job.",
      "C": "Create an AWS Glue table and an AWS Glue crawler for the S3 bucket where the application places the .csv files. Schedule an AWS Lambda function to periodically use Amazon Athena to query the AWS Glue table, convert the query results into Parquet format, and place the output files into an S3 bucket.",
      "D": "Create an AWS Glue extract, transform, and load (ETL) job to convert the .csv files to Parquet format and place the output files into an S3 bucket. Create an AWS Lambda function for each S3 PUT event to invoke the ETL job."
    },
    "correct_answer": "D",
    "vote_percentage": "89%",
    "question_cn": "一家公司有一个应用程序，每小时将数百个 .csv 文件放入 Amazon S3 存储桶中。这些文件的大小为 1 GB。每次上传文件时，公司都需要将文件转换为 Apache Parquet 格式，并将输出文件放入 S3 存储桶中。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数以下载 .csv 文件，将文件转换为 Parquet 格式，并将输出文件放入 S3 存储桶中。为每个 S3 PUT 事件调用 Lambda 函数。",
      "B": "创建一个 Apache Spark 作业以读取 .csv 文件，将文件转换为 Parquet 格式，并将输出文件放入 S3 存储桶中。为每个 S3 PUT 事件创建一个 AWS Lambda 函数来调用 Spark 作业。",
      "C": "为应用程序放置 .csv 文件的 S3 存储桶创建一个 AWS Glue 表和一个 AWS Glue 爬虫。安排一个 AWS Lambda 函数，以定期间隔使用 Amazon Athena 查询 AWS Glue 表，将查询结果转换为 Parquet 格式，并将输出文件放入 S3 存储桶中。",
      "D": "创建一个 AWS Glue 提取、转换和加载 (ETL) 作业，将 .csv 文件转换为 Parquet 格式，并将输出文件放入 S3 存储桶中。为每个 S3 PUT 事件创建一个 AWS Lambda 函数来调用 ETL 作业。"
    },
    "tags": [
      "Amazon S3",
      "AWS Lambda",
      "S3 PUT events",
      "Apache Parquet"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 89%），解析仅供参考。】\n\n考察了在 S3 上处理大规模 CSV 文件转换为 Parquet 格式的解决方案，并需要最小化运营开销。",
      "why_correct": "选项 D 使用了 AWS Glue ETL 作业，该作业专门用于数据转换和加载。通过 Lambda 函数触发 Glue ETL 作业，实现了针对 S3 PUT 事件的近实时处理，满足了文件转换的需求。Glue 提供了托管服务，降低了运维复杂度。",
      "why_wrong": "选项 A 采用 Lambda 函数直接处理 1GB 的文件，可能导致函数超时、内存不足等问题。选项 B 中，为每个 S3 PUT 事件调用 Spark 作业的 Lambda 函数，会增加延迟，且 Spark 部署和维护成本较高。选项 C 中，使用 Athena 查询表并转换数据，涉及额外的步骤，且并非针对单个文件转换，效率较低。"
    },
    "related_terms": [
      "Amazon S3",
      "Apache Parquet",
      "AWS Lambda",
      "AWS Glue",
      "Apache Spark",
      "Amazon Athena",
      "ETL",
      "CSV"
    ]
  },
  {
    "id": 259,
    "topic": "1",
    "question_en": "A company is implementing new data retention policies for all databases that run on Amazon RDS DB instances. The company must retain daily backups for a minimum period of 2 years. The backups must be consistent and restorable. Which solution should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Create a backup vault in AWS Backup to retain RDS backups. Create a new backup plan with a daily schedule and an expiration period of 2 years after creation. Assign the RDS DB instances to the backup plan.",
      "B": "Configure a backup window for the RDS DB instances for daily snapshots. Assign a snapshot retention policy of 2 years to each RDS DB instance. Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule snapshot deletions.",
      "C": "Configure database transaction logs to be automatically backed up to Amazon CloudWatch Logs with an expiration period of 2 years.",
      "D": "Configure an AWS Database Migration Service (AWS DMS) replication task. Deploy a replication instance, and configure a change data capture (CDC) task to stream database changes to Amazon S3 as the target. Configure S3 Lifecycle policies to delete the snapshots after 2 years."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司正在为所有在 Amazon RDS 数据库实例上运行的数据库实施新的数据保留策略。该公司必须将每日备份保留至少 2 年。备份必须一致且可恢复。解决方案架构师应推荐哪种解决方案来满足这些要求？",
    "options_cn": {
      "A": "在 AWS Backup 中创建一个备份库来保留 RDS 备份。创建一个新的备份计划，该计划具有每日时间表和创建后 2 年的到期时间。将 RDS 数据库实例分配给备份计划。",
      "B": "为 RDS 数据库实例配置备份窗口以进行每日快照。为每个 RDS 数据库实例分配 2 年的快照保留策略。使用 Amazon Data Lifecycle Manager (Amazon DLM) 来计划快照删除。",
      "C": "配置数据库事务日志，使其自动备份到 Amazon CloudWatch Logs，并设置 2 年的到期时间。",
      "D": "配置一个 AWS Database Migration Service (AWS DMS) 复制任务。部署一个复制实例，并配置一个变更数据捕获 (CDC) 任务，以将数据库更改流式传输到 Amazon S3 作为目标。配置 S3 生命周期策略以在 2 年后删除快照。"
    },
    "tags": [
      "Amazon RDS",
      "AWS Backup",
      "Backup and Restore"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n考查 RDS 数据库备份的保留策略；与 AWS Backup 服务及其备份计划、生命周期管理相关。",
      "why_correct": "AWS Backup 是一个集中式的备份服务，可以用于保护包括 RDS 在内的多种 AWS 服务的数据。通过创建备份库并设置备份计划，可以轻松满足每日备份、一致性和可恢复性的需求。选择 AWS Backup，能够集中管理所有 RDS 实例的备份策略，并满足题目的数据保留时间要求。",
      "why_wrong": "选项 B 错误，直接为 RDS 数据库实例配置快照保留策略是可以的，但使用 Amazon Data Lifecycle Manager (DLM) 是管理 EBS 快照的工具，无法直接应用于 RDS 快照的删除。选项 C 错误，将数据库事务日志备份到 CloudWatch Logs 并不能提供一致且可恢复的数据库备份，CloudWatch Logs 主要用于日志记录和监控。选项 D 错误，AWS Database Migration Service (DMS) 主要用于数据库迁移，虽然可以捕获变更数据 (CDC) 并流式传输到 S3，但这并不是一个可靠的、一致的数据库备份方案。此外，CDC 捕捉的是数据库的变化，而非全量的备份。使用 S3 生命周期策略删除 S3 中的数据也无法满足数据库的恢复需求。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS",
      "AWS Backup",
      "EBS",
      "CloudWatch Logs",
      "AWS Database Migration Service (AWS DMS)",
      "CDC",
      "Amazon S3",
      "S3",
      "Backup plan",
      "Amazon Data Lifecycle Manager (Amazon DLM)"
    ]
  },
  {
    "id": 260,
    "topic": "1",
    "question_en": "A company’s compliance team needs to move its file shares to AWS. The shares run on a Windows Server SMB file share. A self-managed on- premises Active Directory controls access to the files and folders. The company wants to use Amazon FSx for Windows File Server as part of the solution. The company must ensure that the on-premises Active Directory groups restrict access to the FSx for Windows File Server SMB compliance shares, folders, and files after the move to AWS. The company has created an FSx for Windows File Server file system. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Active Directory Connector to connect to the Active Directory. Map the Active Directory groups to IAM groups to restrict access.",
      "B": "Assign a tag with a Restrict tag key and a Compliance tag value. Map the Active Directory groups to IAM groups to restrict access.",
      "C": "Create an IAM service-linked role that is linked directly to FSx for Windows File Server to restrict access.",
      "D": "Join the file system to the Active Directory to restrict access."
    },
    "correct_answer": "D",
    "vote_percentage": "89%",
    "question_cn": "一家公司的合规团队需要将其文件共享迁移到 AWS。这些共享运行在 Windows Server SMB 文件共享上。一个自管理的本地 Active Directory 控制对文件和文件夹的访问。该公司希望使用 Amazon FSx for Windows File Server 作为解决方案的一部分。该公司必须确保在迁移到 AWS 之后，本地 Active Directory 组限制对 FSx for Windows File Server SMB 合规性共享、文件夹和文件的访问。该公司已经创建了一个 FSx for Windows File Server 文件系统。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Active Directory Connector 以连接到 Active Directory。将 Active Directory 组映射到 IAM 组以限制访问。",
      "B": "分配一个带有 Restrict 标签键和 Compliance 标签值的标签。将 Active Directory 组映射到 IAM 组以限制访问。",
      "C": "创建一个直接链接到 FSx for Windows File Server 的 IAM 服务链接角色以限制访问。",
      "D": "将文件系统加入到 Active Directory 以限制访问。"
    },
    "tags": [
      "FSx for Windows File Server",
      "Active Directory",
      "SMB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 89%），解析仅供参考。】\n\n考查 FSx for Windows File Server 与本地 Active Directory 的集成，以及基于身份验证的访问控制。与 IAM 和标签的使用场景、AD Connector 的功能等相关。",
      "why_correct": "将 FSx for Windows File Server 文件系统加入到现有的本地 Active Directory 域中，是实现 SMB 共享访问控制的关键。通过这种方式，FSx for Windows File Server 可以使用 Active Directory 组策略和权限设置，确保在 AWS 迁移后，继续基于 Active Directory 的身份验证和授权，限制对合规性共享、文件夹和文件的访问，满足题目要求。",
      "why_wrong": "选项 A 错误，虽然 Active Directory Connector 可以连接到 Active Directory，但将 Active Directory 组映射到 IAM 组是一种不合适的访问控制方式。IAM 主要用于控制 AWS 资源访问权限，而不是管理 FSx for Windows File Server 上的文件共享权限。选项 B 错误，标签不用于访问控制，而是用于资源管理和组织。将标签用于访问控制是错误的，因为它没有整合到基于身份验证的 Active Directory 权限管理中。选项 C 错误，IAM 服务链接角色通常用于授权 AWS 服务代表用户执行操作，而不是直接管理 FSx for Windows File Server 上的文件和文件夹的访问权限。这种方式没有利用 Active Directory 进行访问控制，无法满足题目要求。"
    },
    "related_terms": [
      "FSx for Windows File Server",
      "Active Directory",
      "SMB",
      "IAM",
      "Active Directory Connector"
    ]
  },
  {
    "id": 261,
    "topic": "1",
    "question_en": "A company recently announced the deployment of its retail website to a global audience. The website runs on multiple Amazon EC2 instances behind an Elastic Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company wants to provide its customers with different versions of content based on the devices that the customers use to access the website. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Configure Amazon CloudFront to cache multiple versions of the content.",
      "B": "Configure a host header in a Network Load Balancer to forward trafic to different instances.",
      "C": "Configure a Lambda@Edge function to send specific objects to users based on the User-Agent header.",
      "D": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up host-based routing to different EC2 instances",
      "E": "Configure AWS Global Accelerator. Forward requests to a Network Load Balancer (NLB). Configure the NLB to set up path-based routing to different EC2 instances."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "一家公司最近宣布向全球受众部署其零售网站。该网站在 Elastic Load Balancer 之后的多台 Amazon EC2 实例上运行。这些实例在多个可用区中的 Auto Scaling 组中运行。该公司希望根据客户用来访问网站的设备，向其客户提供不同版本的內容。解决方案架构师应采取哪些组合操作来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "配置 Amazon CloudFront 以缓存内容的多个版本。",
      "B": "在 Network Load Balancer 中配置主机标头，以将流量转发到不同的实例。",
      "C": "配置 Lambda@Edge 函数，以根据 User-Agent 标头向用户发送特定对象。",
      "D": "配置 AWS Global Accelerator。将请求转发到 Network Load Balancer (NLB)。配置 NLB 以设置基于主机的路由到不同的 EC2 实例。",
      "E": "配置 AWS Global Accelerator。将请求转发到 Network Load Balancer (NLB)。配置 NLB 以设置基于路径的路由到不同的 EC2 实例。"
    },
    "tags": [
      "Amazon CloudFront",
      "Lambda@Edge",
      "Network Load Balancer",
      "AWS Global Accelerator",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考查内容分发、负载均衡以及根据用户设备提供不同内容。涉及 CloudFront 的配置、Lambda@Edge 的使用、NLB 的作用，以及 Global Accelerator 的选择，并与网站架构、用户设备适配相关。",
      "why_correct": "Amazon CloudFront 可以缓存不同版本的网站内容，以满足根据客户设备提供不同内容的需求。通过配置 CloudFront 的行为，可以根据 User-Agent 标头（即用户设备信息）来缓存并提供不同的内容版本。CloudFront 提供了内容分发网络（CDN）的功能，可以加速全球用户的访问速度，并支持多种缓存策略。",
      "why_wrong": "选项 B 错误，Network Load Balancer (NLB) 主要用于处理 TCP、UDP 流量，虽然可以基于主机标头转发流量，但并不直接支持根据用户设备的不同来分发内容。选项 C 错误，虽然 Lambda@Edge 函数可以根据 User-Agent 标头定制响应，但单独使用 Lambda@Edge 无法构建完整的网站架构，需要与 CloudFront 结合使用。选项 D 和 E 错误，AWS Global Accelerator 用于加速网络流量，并不能直接根据用户设备类型进行内容分发。NLB 可以作为 Global Accelerator 的后端，但 NLB 的基于主机或路径的路由通常用于基于域名或路径进行转发，无法直接满足根据设备类型分发内容的需求。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Lambda@Edge",
      "AWS Global Accelerator",
      "EC2",
      "Auto Scaling",
      "Network Load Balancer (NLB)",
      "User-Agent"
    ]
  },
  {
    "id": 262,
    "topic": "1",
    "question_en": "A company plans to use Amazon ElastiCache for its multi-tier web application. A solutions architect creates a Cache VPC for the ElastiCache cluster and an App VPC for the application’s Amazon EC2 instances. Both VPCs are in the us-east-1 Region. The solutions architect must implement a solution to provide the application’s EC2 instances with access to the ElastiCache cluster. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the ElastiCache cluster’s security group to allow inbound connection from the application’s security group.",
      "B": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trafic through the Transit VPC. Configure an inbound rule for the ElastiCache cluster's security group to allow inbound connection from the application’s security group.",
      "C": "Create a peering connection between the VPCs. Add a route table entry for the peering connection in both VPCs. Configure an inbound rule for the peering connection’s security group to allow inbound connection from the application’s security group.",
      "D": "Create a Transit VPC. Update the VPC route tables in the Cache VPC and the App VPC to route trafic through the Transit VPC. Configure an inbound rule for the Transit VPC’s security group to allow inbound connection from the application’s security group."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划为其多层 Web 应用程序使用 Amazon ElastiCache。一位解决方案架构师为 ElastiCache 集群创建了一个缓存 VPC，并为应用程序的 Amazon EC2 实例创建了一个应用 VPC。这两个 VPC 都在 us-east-1 区域中。解决方案架构师必须实施一个解决方案，为应用程序的 EC2 实例提供对 ElastiCache 集群的访问权限。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "在 VPC 之间创建对等连接。在两个 VPC 中为对等连接添加路由表条目。为 ElastiCache 集群的安全组配置一个入站规则，以允许来自应用程序安全组的入站连接。",
      "B": "创建一个 Transit VPC。更新缓存 VPC 和应用 VPC 中的 VPC 路由表，以通过 Transit VPC 路由流量。为 ElastiCache 集群的安全组配置一个入站规则，以允许来自应用程序安全组的入站连接。",
      "C": "在 VPC 之间创建对等连接。在两个 VPC 中为对等连接添加路由表条目。为对等连接的安全组配置一个入站规则，以允许来自应用程序安全组的入站连接。",
      "D": "创建一个 Transit VPC。更新缓存 VPC 和应用 VPC 中的 VPC 路由表，以通过 Transit VPC 路由流量。为 Transit VPC 的安全组配置一个入站规则，以允许来自应用程序安全组的入站连接。"
    },
    "tags": [
      "Amazon ElastiCache",
      "VPC Peering",
      "Security Group",
      "VPC",
      "Route Tables"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查如何在不同 VPC 之间，以最具成本效益的方式实现 Amazon ElastiCache 的访问。这涉及到 VPC 对等连接、路由表配置以及安全组规则的设置，需要考虑网络连通性和安全性。",
      "why_correct": "选项 A 通过 VPC 对等连接提供最直接、最经济的解决方案。通过在两个 VPC 之间创建对等连接，可以实现 VPC 间的网络互通，允许应用程序 EC2 实例直接访问 ElastiCache 集群。在两个 VPC 的路由表中添加路由条目，确保流量可以在两个 VPC 之间路由。同时，配置 ElastiCache 集群的安全组，允许来自应用程序安全组的入站连接，确保只有授权的流量可以访问 ElastiCache 集群。这种方案简单直接，避免了额外的组件和流量转发成本。",
      "why_wrong": "选项 B 引入了 Transit VPC，增加了网络架构的复杂性，并引入了额外的网络设备，这会增加成本和管理复杂度。 Transit VPC 通常用于集中管理多个 VPC 之间的连接，对于本题这种两个 VPC 之间的简单场景来说，过度设计了。选项 C 错误地将安全组规则配置在 VPC 对等连接上，VPC 对等连接本身没有安全组。安全组必须配置在 ElastiCache 集群上，以控制对集群的访问。选项 D 同样引入了 Transit VPC，增加了成本。并且它错误地将安全组规则配置在 Transit VPC 上，而不是 ElastiCache 集群上，这无法控制哪些流量可以访问 ElastiCache。为了安全，必须在 ElastiCache 集群的安全组中配置允许来自应用程序的安全组的流量。"
    },
    "related_terms": [
      "Amazon ElastiCache",
      "VPC",
      "EC2",
      "VPC Peering",
      "Transit VPC",
      "Security Group",
      "Route Tables"
    ]
  },
  {
    "id": 263,
    "topic": "1",
    "question_en": "A company is building an application that consists of several microservices. The company has decided to use container technologies to deploy its software on AWS. The company needs a solution that minimizes the amount of ongoing effort for maintenance and scaling. The company cannot manage additional infrastructure. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Deploy an Amazon Elastic Container Service (Amazon ECS) cluster.",
      "B": "Deploy the Kubernetes control plane on Amazon EC2 instances that span multiple Availability Zones.",
      "C": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with an Amazon EC2 launch type. Specify a desired task number level of greater than or equal to 2.",
      "D": "Deploy an Amazon Elastic Container Service (Amazon ECS) service with a Fargate launch typ",
      "E": "Specify a desired task number level of greater than or equal to 2. E. Deploy Kubernetes worker nodes on Amazon EC2 instances that span multiple Availability Zones. Create a deployment that specifies two or more replicas for each microservice."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在构建一个由几个微服务组成的应用程序。该公司已决定使用容器技术在 AWS 上部署其软件。该公司需要一个解决方案，以最大限度地减少维护和扩展的持续工作量。该公司无法管理额外的基础设施。解决方案架构师应采取哪两种操作组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "部署一个 Amazon Elastic Container Service (Amazon ECS) 集群。",
      "B": "在跨多个可用区的 Amazon EC2 实例上部署 Kubernetes 控制平面。",
      "C": "使用 Amazon EC2 启动类型部署一个 Amazon Elastic Container Service (Amazon ECS) 服务。指定一个大于或等于 2 的所需任务数量级别。",
      "D": "使用 Fargate 启动类型部署一个 Amazon Elastic Container Service (Amazon ECS) 服务。指定一个大于或等于 2 的所需任务数量级别。",
      "E": "在跨多个可用区的 Amazon EC2 实例上部署 Kubernetes 工作节点。创建一个部署，为每个微服务指定两个或更多副本。"
    },
    "tags": [
      "Amazon ECS",
      "Fargate",
      "Amazon EC2",
      "Kubernetes",
      "Container"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n考察使用容器技术部署微服务应用时，如何选择合适的容器编排服务以及部署方式，以最小化运维工作量和实现弹性伸缩；与 Amazon ECS 和 Kubernetes 服务的对比、不同启动类型的选择有关。",
      "why_correct": "Amazon Elastic Container Service (Amazon ECS) 是一种完全托管的容器编排服务，可以自动管理容器的部署、扩展和管理。选择 Amazon ECS 可以简化容器的部署和管理，减少运维负担。题目要求最小化维护工作量，而 ECS 是完全托管的服务，满足该需求。",
      "why_wrong": "选项 B 涉及 Kubernetes，但需要用户自己管理控制平面，增加了运维工作量，与题目要求不符。选项 C 使用 Amazon EC2 启动类型部署 ECS，需要管理 EC2 实例，增加了运维负担，与题目要求不符。选项 D 使用 Fargate 启动类型部署 ECS，Fargate 是无服务器计算引擎，可以免去对底层基础设施的管理，但是部署 Fargate 需要先部署 ECS 集群，所以需要结合选项 A。选项 E 涉及 Kubernetes，同样需要用户管理工作节点，增加了运维工作量，且需要自行配置副本数量，增加了管理复杂性，与题目要求不符。"
    },
    "related_terms": [
      "Amazon ECS",
      "Kubernetes",
      "Amazon EC2",
      "Fargate",
      "EC2",
      "Docker",
      "Container"
    ]
  },
  {
    "id": 264,
    "topic": "1",
    "question_en": "A company has a web application hosted over 10 Amazon EC2 instances with trafic directed by Amazon Route 53. The company occasionally experiences a timeout error when attempting to browse the application. The networking team finds that some DNS queries return IP addresses of unhealthy instances, resulting in the timeout error. What should a solutions architect implement to overcome these timeout errors?",
    "options_en": {
      "A": "Create a Route 53 simple routing policy record for each EC2 instance. Associate a health check with each record.",
      "B": "Create a Route 53 failover routing policy record for each EC2 instance. Associate a health check with each record.",
      "C": "Create an Amazon CloudFront distribution with EC2 instances as its origin. Associate a health check with the EC2 instances.",
      "D": "Create an Application Load Balancer (ALB) with a health check in front of the EC2 instances. Route to the ALB from Route 53."
    },
    "correct_answer": "D",
    "vote_percentage": "66%",
    "question_cn": "一家公司在其 10 个 Amazon EC2 实例上托管一个 Web 应用程序，流量由 Amazon Route 53 引导。该公司偶尔在尝试浏览该应用程序时遇到超时错误。网络团队发现一些 DNS 查询返回运行状况不佳的实例的 IP 地址，导致超时错误。解决方案架构师应该实施什么来克服这些超时错误？",
    "options_cn": {
      "A": "为每个 EC2 实例创建 Route 53 简单路由策略记录。将运行状况检查与每条记录关联。",
      "B": "为每个 EC2 实例创建 Route 53 故障转移路由策略记录。将运行状况检查与每条记录关联。",
      "C": "创建一个 Amazon CloudFront 分发，将 EC2 实例作为其源。将运行状况检查与 EC2 实例关联。",
      "D": "在 EC2 实例前面创建一个 Application Load Balancer (ALB) 并进行运行状况检查。从 Route 53 路由到 ALB。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon Route 53",
      "Application Load Balancer",
      "ALB",
      "Health Check",
      "DNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 66%），解析仅供参考。】\n\n考察 Route 53 和 EC2 的集成以及负载均衡器的使用，解决应用程序因后端实例故障导致的访问超时问题，与负载均衡、健康检查、DNS 故障转移等概念相关。",
      "why_correct": "选项 D 正确。使用 Application Load Balancer (ALB) 可以作为 EC2 实例的前端，并配置健康检查。Route 53 可以配置为将流量路由到 ALB 的 DNS 记录。当 ALB 检测到后端 EC2 实例不健康时，它会自动将流量路由到健康的实例。这种架构提供了高可用性和故障转移能力，有效解决了因后端实例故障导致的超时问题。同时，ALB 提供了负载均衡功能，可以更好地分配流量，提高应用程序的整体性能。",
      "why_wrong": "选项 A 错误。Route 53 简单路由策略不支持健康检查，即使配置了简单记录，Route 53 也不具备检测后端 EC2 实例健康状态的能力，无法实现故障转移。选项 B 错误。虽然 Route 53 故障转移路由策略支持健康检查，但创建多个指向单个 EC2 实例的 DNS 记录，冗余度不足，且无法有效负载均衡。如果所有记录都指向同一实例，一旦该实例出现问题，用户仍然会遇到超时错误。选项 C 错误。CloudFront 主要用于内容分发，虽然可以作为 EC2 实例的前端，但它并不能进行健康检查和故障转移，无法解决题目中因后端 EC2 实例健康状况不佳导致超时的问题。而且，将 EC2 作为 CloudFront 的源，增加了复杂性，且并非最优解。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "Amazon Route 53",
      "Route 53",
      "Application Load Balancer",
      "ALB",
      "DNS",
      "Amazon CloudFront",
      "CloudFront",
      "Health Check"
    ]
  },
  {
    "id": 265,
    "topic": "1",
    "question_en": "A solutions architect needs to design a highly available application consisting of web, application, and database tiers. HTTPS content delivery should be as close to the edge as possible, with the least delivery time. Which solution meets these requirements and is MOST secure?",
    "options_en": {
      "A": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
      "B": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin.",
      "C": "Configure a public Application Load Balancer (ALB) with multiple redundant Amazon EC2 instances in private subnets. Configure Amazon CloudFront to deliver HTTPS content using the public ALB as the origin.",
      "D": "Configure a public Application Load Balancer with multiple redundant Amazon EC2 instances in public subnets. Configure Amazon CloudFront to deliver HTTPS content using the EC2 instances as the origin."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要设计一个高可用应用程序，该应用程序由 Web、应用程序和数据库层组成。 HTTPS 内容交付应尽可能接近边缘，并具有最短的交付时间。哪种解决方案最符合这些要求，并且最安全？",
    "options_cn": {
      "A": "配置一个公共 Application Load Balancer (ALB)，并在公共子网中使用多个冗余的 Amazon EC2 实例。 配置 Amazon CloudFront 使用公共 ALB 作为源来交付 HTTPS 内容。",
      "B": "配置一个公共 Application Load Balancer，并在私有子网中使用多个冗余的 Amazon EC2 实例。 配置 Amazon CloudFront 使用 EC2 实例作为源来交付 HTTPS 内容。",
      "C": "配置一个公共 Application Load Balancer (ALB)，并在私有子网中使用多个冗余的 Amazon EC2 实例。 配置 Amazon CloudFront 使用公共 ALB 作为源来交付 HTTPS 内容。",
      "D": "配置一个公共 Application Load Balancer，并在公共子网中使用多个冗余的 Amazon EC2 实例。 配置 Amazon CloudFront 使用 EC2 实例作为源来交付 HTTPS 内容。"
    },
    "tags": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "EC2",
      "HTTPS",
      "High Availability",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查结合 Amazon CloudFront 和 Application Load Balancer (ALB) 构建高可用且安全的 Web 应用。与 Web 应用架构设计、内容分发网络（CDN）以及安全最佳实践相关。",
      "why_correct": "选项 C 正确地结合了 CloudFront 和 ALB 以满足需求。将 ALB 部署在私有子网中提高了安全性，因为 ALB 是 CloudFront 的源，间接保护了 EC2 实例。 CloudFront 提供 HTTPS 内容交付，并将内容缓存在边缘位置，缩短了交付时间。 ALB 负责处理流量负载均衡和故障转移，确保了应用程序的高可用性。",
      "why_wrong": "选项 A 和 D 错误地将 ALB 部署在公共子网中，这增加了潜在的安全风险，因为 ALB 暴露在公网上。此外，选项 A 和 D 没有说明安全性是如何实现的，比如 ALB 如何配置 SSL/TLS 证书。选项 B 错误地将 EC2 实例配置为 CloudFront 的源，这意味着 CloudFront 会直接访问 EC2 实例，这违背了安全最佳实践，因为直接暴露 EC2 实例会增加攻击面。使用 ALB 作为源能够实现更好的流量管理和安全性，而 CloudFront 主要是分发内容，并不适合直接作为 Web 服务器暴露。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "ALB",
      "EC2",
      "HTTPS",
      "SSL/TLS",
      "Web application"
    ]
  },
  {
    "id": 266,
    "topic": "1",
    "question_en": "A company has a popular gaming platform running on AWS. The application is sensitive to latency because latency can impact the user experience and introduce unfair advantages to some players. The application is deployed in every AWS Region. It runs on Amazon EC2 instances that are part of Auto Scaling groups configured behind Application Load Balancers (ALBs). A solutions architect needs to implement a mechanism to monitor the health of the application and redirect trafic to healthy endpoints. Which solution meets these requirements?",
    "options_en": {
      "A": "Configure an accelerator in AWS Global Accelerator. Add a listener for the port that the application listens on, and attach it to a Regional endpoint in each Region. Add the ALB as the endpoint.",
      "B": "Create an Amazon CloudFront distribution and specify the ALB as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trafic.",
      "C": "Create an Amazon CloudFront distribution and specify Amazon S3 as the origin server. Configure the cache behavior to use origin cache headers. Use AWS Lambda functions to optimize the trafic.",
      "D": "Configure an Amazon DynamoDB database to serve as the data store for the application. Create a DynamoDB Accelerator (DAX) cluster to act as the in-memory cache for DynamoDB hosting the application data."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行一个受欢迎的游戏平台。该应用程序对延迟很敏感，因为延迟会影响用户体验，并给某些玩家带来不公平的优势。该应用程序部署在每个 AWS 区域中。它运行在作为 Application Load Balancer (ALB) 后面的 Auto Scaling 组一部分的 Amazon EC2 实例上。 解决方案架构师需要实施一种机制来监控应用程序的健康状况，并将流量重定向到健康的端点。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "在 AWS Global Accelerator 中配置一个加速器。为应用程序侦听的端口添加一个侦听器，并将其附加到每个区域中的区域端点。将 ALB 添加为端点。",
      "B": "创建一个 Amazon CloudFront 分发，并将 ALB 指定为源服务器。配置缓存行为以使用源缓存标头。使用 AWS Lambda 函数来优化流量。",
      "C": "创建一个 Amazon CloudFront 分发，并将 Amazon S3 指定为源服务器。配置缓存行为以使用源缓存标头。使用 AWS Lambda 函数来优化流量。",
      "D": "配置一个 Amazon DynamoDB 数据库，作为应用程序的数据存储。创建一个 DynamoDB Accelerator (DAX) 集群，作为 DynamoDB 托管应用程序数据的内存缓存。"
    },
    "tags": [
      "AWS Global Accelerator",
      "Application Load Balancer",
      "Latency"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查如何通过 AWS 服务优化应用程序的全球访问，降低延迟。这与网络层优化、健康检查和流量管理密切相关。",
      "why_correct": "AWS Global Accelerator 针对全球用户优化应用程序的性能。通过将应用程序端点（例如 ALB）添加到 Global Accelerator 中，可以利用 AWS 全球网络将流量引导到最近的健康端点，从而减少延迟。Global Accelerator 还执行健康检查，自动将流量从出现故障的端点重定向到健康的端点，满足了题目中监控应用程序健康状况并重定向流量的要求。",
      "why_wrong": "B 选项使用 Amazon CloudFront 主要用于内容分发和缓存，虽然可以提高某些场景下的性能，但它主要优化静态内容的传输，对于动态应用程序，特别是对延迟敏感的应用程序，Global Accelerator 提供的优化更有效。C 选项与 B 选项类似，将 Amazon S3 作为源服务器不适用于运行在 EC2 实例上的应用程序，且 S3 并非为动态应用程序设计。D 选项描述了 DynamoDB 和 DAX，DAX 优化了 DynamoDB 的读性能，与题目的目标（改善全球应用程序的延迟，并重定向流量）不直接相关，DAX 适用于优化 DynamoDB 访问延迟，而非应用程序整体延迟。"
    },
    "related_terms": [
      "AWS Global Accelerator",
      "Application Load Balancer",
      "ALB",
      "Amazon CloudFront",
      "Amazon S3",
      "EC2",
      "DynamoDB",
      "DAX",
      "Lambda"
    ]
  },
  {
    "id": 267,
    "topic": "1",
    "question_en": "A company has one million users that use its mobile app. The company must analyze the data usage in near-real time. The company also must encrypt the data in near-real time and must store the data in a centralized location in Apache Parquet format for further processing. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data. Invoke an AWS Lambda function to send the data to the Kinesis Data Analytics application.",
      "B": "Create an Amazon Kinesis data stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data. Invoke an AWS Lambda function to send the data to the EMR cluster.",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon EMR cluster to analyze the data.",
      "D": "Create an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Create an Amazon Kinesis Data Analytics application to analyze the data."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一百万使用其移动应用程序的用户。该公司必须近乎实时地分析数据使用情况。该公司还必须近乎实时地加密数据，并以 Apache Parquet 格式将数据存储在集中位置以进行进一步处理。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Kinesis 数据流将数据存储在 Amazon S3 中。创建一个 Amazon Kinesis Data Analytics 应用程序来分析数据。调用一个 AWS Lambda 函数将数据发送到 Kinesis Data Analytics 应用程序。",
      "B": "创建一个 Amazon Kinesis 数据流将数据存储在 Amazon S3 中。创建一个 Amazon EMR 集群来分析数据。调用一个 AWS Lambda 函数将数据发送到 EMR 集群。",
      "C": "创建一个 Amazon Kinesis Data Firehose 交付流将数据存储在 Amazon S3 中。创建一个 Amazon EMR 集群来分析数据。",
      "D": "创建一个 Amazon Kinesis Data Firehose 交付流将数据存储在 Amazon S3 中。创建一个 Amazon Kinesis Data Analytics 应用程序来分析数据。"
    },
    "tags": [
      "Amazon Kinesis",
      "Kinesis Data Firehose",
      "Kinesis Data Analytics",
      "Amazon S3",
      "Apache Parquet",
      "Lambda",
      "Amazon EMR"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察使用 Kinesis 服务进行实时数据处理，以及与其他服务的集成。涉及 Kinesis Data Firehose 的数据存储，Kinesis Data Analytics 的分析能力，以及与 S3 的结合使用，并考虑运营开销。",
      "why_correct": "Kinesis Data Firehose 可以近乎实时地将数据传送到 Amazon S3，并支持数据加密。Kinesis Data Firehose 也支持将数据转换为 Apache Parquet 格式，满足存储需求。Kinesis Data Analytics 可以对 Kinesis Data Firehose 的数据进行近乎实时的分析。此方案无需手动管理 EC2 实例或自定义代码，运营开销最低。",
      "why_wrong": "选项 A 涉及 Lambda 函数向 Kinesis Data Analytics 应用程序发送数据，这增加了额外的复杂性，需要手动编码和维护。选项 B 涉及使用 EMR 集群进行分析，相比 Kinesis Data Analytics 而言，EMR 的配置和管理成本更高，操作复杂度增加。选项 C 虽然使用了 Kinesis Data Firehose 存储数据，但 EMR 集群同样带来了较高的运营成本和复杂性，并且 Kinesis Data Firehose 本身不直接支持数据分析功能，需要借助其他服务进行分析。"
    },
    "related_terms": [
      "Kinesis Data Firehose",
      "Kinesis Data Analytics",
      "Amazon S3",
      "Lambda",
      "Amazon EMR",
      "EC2",
      "Amazon Kinesis",
      "Apache Parquet"
    ]
  },
  {
    "id": 268,
    "topic": "1",
    "question_en": "A gaming company has a web application that displays scores. The application runs on Amazon EC2 instances behind an Application Load Balancer. The application stores data in an Amazon RDS for MySQL database. Users are starting to experience long delays and interruptions that are caused by database read performance. The company wants to improve the user experience while minimizing changes to the application’s architecture. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use Amazon ElastiCache in front of the database.",
      "B": "Use RDS Proxy between the application and the database.",
      "C": "Migrate the application from EC2 instances to AWS Lambda.",
      "D": "Migrate the database from Amazon RDS for MySQL to Amazon DynamoDB."
    },
    "correct_answer": "A",
    "vote_percentage": "51%",
    "question_cn": "一家游戏公司有一个显示分数的 Web 应用程序。该应用程序在 Application Load Balancer 后面的 Amazon EC2 实例上运行。该应用程序将数据存储在 Amazon RDS for MySQL 数据库中。用户开始遇到因数据库读取性能而导致的长延迟和中断。该公司希望改善用户体验，同时最大限度地减少对应用程序架构的更改。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在数据库前面使用 Amazon ElastiCache。",
      "B": "在应用程序和数据库之间使用 RDS Proxy。",
      "C": "将应用程序从 EC2 实例迁移到 AWS Lambda。",
      "D": "将数据库从 Amazon RDS for MySQL 迁移到 Amazon DynamoDB。"
    },
    "tags": [
      "Amazon ElastiCache",
      "RDS for MySQL",
      "EC2",
      "Application Load Balancer",
      "Amazon RDS",
      "Database Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 51%），解析仅供参考。】\n\n考查数据库读取性能优化。本题重点在于选择合适的缓存解决方案以减少数据库负载，提升用户体验。与数据库缓存、数据库选型相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：Amazon ElastiCache 是一种完全托管的内存中缓存服务，可以用来提升 Web 应用程序的性能。在这种情况下，将数据缓存在 ElastiCache 中可以显著减少对 RDS for MySQL 数据库的读取请求，从而降低延迟并改善用户体验。它与 RDS for MySQL 数据库的兼容性良好，且对应用程序架构的改动最小。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB. RDS Proxy 主要用于改善数据库连接管理和提高数据库的弹性，而不是直接优化读取性能。它虽然可以减少应用程序与数据库之间的连接建立时间，但对数据库读取性能的提升有限。C. 将应用程序迁移到 AWS Lambda 会改变应用程序的架构，这违背了题目中“最大限度地减少对应用程序架构的更改”的要求，且 Lambda 更适合事件驱动型和无状态的应用程序。D. 将数据库迁移到 Amazon DynamoDB 涉及对数据库技术的重大更改，这与题目要求不符，并且可能需要大量的时间和精力。虽然 DynamoDB 适用于高性能的读取场景，但需要重新设计数据模型和应用程序访问方式，并非最小改动的方案，且 DynamoDB 并非关系型数据库，与应用程序的数据存储需求不匹配。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon ElastiCache",
      "EC2",
      "Application Load Balancer",
      "Amazon RDS",
      "RDS Proxy",
      "AWS Lambda",
      "Amazon DynamoDB",
      "RDS for MySQL"
    ]
  },
  {
    "id": 269,
    "topic": "1",
    "question_en": "An ecommerce company has noticed performance degradation of its Amazon RDS based web application. The performance degradation is attributed to an increase in the number of read-only SQL queries triggered by business analysts. A solutions architect needs to solve the problem with minimal changes to the existing web application. What should the solutions architect recommend?",
    "options_en": {
      "A": "Export the data to Amazon DynamoDB and have the business analysts run their queries.",
      "B": "Load the data into Amazon ElastiCache and have the business analysts run their queries.",
      "C": "Create a read replica of the primary database and have the business analysts run their queries.",
      "D": "Copy the data into an Amazon Redshift cluster and have the business analysts run their queries."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司注意到其基于 Amazon RDS 的 Web 应用程序的性能下降。这种性能下降归因于业务分析师触发的只读 SQL 查询数量的增加。解决方案架构师需要以最少的更改来解决现有 Web 应用程序的问题。 解决方案架构师应该建议什么？",
    "options_cn": {
      "A": "将数据导出到 Amazon DynamoDB，并让业务分析师运行他们的查询。",
      "B": "将数据加载到 Amazon ElastiCache，并让业务分析师运行他们的查询。",
      "C": "创建主数据库的只读副本，并让业务分析师运行他们的查询。",
      "D": "将数据复制到 Amazon Redshift 集群中，并让业务分析师运行他们的查询。"
    },
    "tags": [
      "Amazon RDS",
      "Read Replicas",
      "Database Performance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查 RDS 数据库性能优化，以及如何应对只读查询负载增加的问题。这与 RDS 的读写分离、数据库的扩展性和性能调优相关。",
      "why_correct": "创建 RDS 数据库的只读副本（Read Replica）是解决只读查询负载增加问题的常见且有效的方案。业务分析师可以在只读副本上运行查询，从而减轻主数据库的负载，提高 Web 应用程序的整体性能，同时对现有应用程序的影响最小。 Read Replica 提供了与主数据库同步数据的能力，确保数据的实时性。",
      "why_wrong": "A. 将数据导出到 DynamoDB 并让业务分析师运行查询，需要进行数据迁移和应用程序代码更改，这与题目要求的『最少的更改』不符，且 DynamoDB 并不适合复杂的 SQL 查询。B. 将数据加载到 ElastiCache 并让业务分析师运行查询，ElastiCache 是一个缓存服务，用于提高读取性能，而非持久化存储数据库。它无法存储整个数据库，不适合运行复杂的业务分析查询，而且数据会因缓存过期而被删除。 D. 将数据复制到 Redshift 集群中，Redshift 是一个数据仓库服务，虽然可以用于分析，但同样需要进行数据迁移，且数据一致性不如 Read Replica，增加了配置复杂性，不符合『最少的更改』的要求。"
    },
    "related_terms": [
      "Amazon RDS",
      "Read Replica",
      "DynamoDB",
      "Amazon ElastiCache",
      "Amazon Redshift",
      "SQL"
    ]
  },
  {
    "id": 270,
    "topic": "1",
    "question_en": "A company is using a centralized AWS account to store log data in various Amazon S3 buckets. A solutions architect needs to ensure that the data is encrypted at rest before the data is uploaded to the S3 buckets. The data also must be encrypted in transit. Which solution meets these requirements?",
    "options_en": {
      "A": "Use client-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
      "B": "Use server-side encryption to encrypt the data that is being uploaded to the S3 buckets.",
      "C": "Create bucket policies that require the use of server-side encryption with S3 managed encryption keys (SSE-S3) for S3 uploads.",
      "D": "Enable the security option to encrypt the S3 buckets through the use of a default AWS Key Management Service (AWS KMS) key."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在使用一个集中的 AWS 账户，将日志数据存储在各种 Amazon S3 存储桶中。一位解决方案架构师需要确保在将数据上传到 S3 存储桶之前，数据在静态时被加密。数据也必须在传输过程中被加密。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "使用客户端加密来加密上传到 S3 存储桶的数据。",
      "B": "使用服务器端加密来加密上传到 S3 存储桶的数据。",
      "C": "创建存储桶策略，该策略要求使用服务器端加密，并使用 S3 托管加密密钥 (SSE-S3) 进行 S3 上传。",
      "D": "通过使用默认 AWS Key Management Service (AWS KMS) 密钥，打开加密 S3 存储桶的安全选项。"
    },
    "tags": [
      "Amazon S3",
      "S3 Encryption",
      "Client-Side Encryption",
      "Server-Side Encryption",
      "AWS KMS",
      "SSE-S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n考查 S3 数据的加密方案，包括静态数据加密和传输过程中加密。与客户端加密、服务器端加密、AWS KMS 密钥和 S3 存储桶策略的选型相关。",
      "why_correct": "客户端加密是在数据上传到 S3 之前，在客户端进行加密。这种方法满足了“静态时加密”的要求，因为数据在写入 S3 之前就已经被加密。此外，传输过程中加密是由 HTTPS 协议自动处理的，可以满足需求。",
      "why_wrong": "B 选项使用服务器端加密，虽然满足了静态数据加密的需求，但它主要是在 S3 服务器端进行加密。这无法保证数据在客户端上传到 S3 的过程中就被加密。C 选项使用存储桶策略并结合 SSE-S3，也能实现静态加密，但无法解决数据在客户端上传之前就被加密的需求，而且存储桶策略需要手动配置。D 选项开启加密 S3 存储桶的安全选项，虽然提供了静态加密，但其无法保证数据在传输过程中加密。选项 C 和 D 均不满足题目的所有要求，即需要在静态和传输过程中都加密。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS KMS",
      "SSE-S3",
      "HTTPS",
      "Client-Side Encryption",
      "Server-Side Encryption"
    ]
  },
  {
    "id": 271,
    "topic": "1",
    "question_en": "A solutions architect observes that a nightly batch processing job is automatically scaled up for 1 hour before the desired Amazon EC2 capacity is reached. The peak capacity is the ‘same every night and the batch jobs always start at 1 AM. The solutions architect needs to find a cost-effective solution that will allow for the desired EC2 capacity to be reached quickly and allow the Auto Scaling group to scale down after the batch jobs are complete. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Increase the minimum capacity for the Auto Scaling group.",
      "B": "Increase the maximum capacity for the Auto Scaling group.",
      "C": "Configure scheduled scaling to scale up to the desired compute level.",
      "D": "Change the scaling policy to add more EC2 instances during each scaling operation."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师观察到，一个夜间批处理作业在达到所需的 Amazon EC2 容量之前，会自动扩展 1 小时。峰值容量“每晚相同”，批处理作业总是从凌晨 1 点开始。解决方案架构师需要找到一个经济高效的解决方案，该方案将允许快速达到所需的 EC2 容量，并在批处理作业完成后允许 Auto Scaling 组缩小。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "增加 Auto Scaling 组的最小容量。",
      "B": "增加 Auto Scaling 组的最大容量。",
      "C": "配置预定的缩放以扩展到所需的计算级别。",
      "D": "更改缩放策略以在每次缩放操作期间添加更多 EC2 实例。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "Scheduled Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查 EC2 Auto Scaling 的弹性伸缩策略和预定义的缩放操作。与 Auto Scaling 组的配置、缩放策略、以及资源优化相关。",
      "why_correct": "配置预定的缩放（Scheduled Scaling）是解决问题的最佳方法。由于批处理作业在固定时间（凌晨 1 点）开始，且峰值容量“每晚相同”，可以使用 Scheduled Scaling 在作业开始前预先将 Auto Scaling 组扩展到所需的计算级别，确保快速达到 EC2 容量。作业完成后，Auto Scaling 组可以自动缩小，实现资源利用率的优化。",
      "why_wrong": "A. 增加 Auto Scaling 组的最小容量并不能解决快速达到所需容量的问题。最小容量仅定义了 Auto Scaling 组保持的 EC2 实例的下限，不会影响扩展速度。\nB. 增加 Auto Scaling 组的最大容量并不能加快达到所需容量的速度，它只定义了 Auto Scaling 组可以扩展到的上限。它本身也不能解决快速达到所需容量的问题。\nD. 更改缩放策略以在每次缩放操作期间添加更多 EC2 实例，虽然可以一定程度上提高扩展速度，但这种方法依赖于触发条件，无法保证在批处理作业开始前达到所需的容量，并且可能导致扩展时间不确定。此外，这种策略通常与动态扩展策略（如基于 CloudWatch 指标）结合使用，不适用于这种已知固定时间的场景。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "CloudWatch",
      "Scheduled Scaling"
    ]
  },
  {
    "id": 272,
    "topic": "1",
    "question_en": "A company serves a dynamic website from a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB). The website needs to support multiple languages to serve customers around the world. The website’s architecture is running in the us-west-1 Region and is exhibiting high request latency for users that are located in other parts of the world. The website needs to serve requests quickly and eficiently regardless of a user’s location. However, the company does not want to recreate the existing architecture across multiple Regions. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Replace the existing architecture with a website that is served from an Amazon S3 bucket. Configure an Amazon CloudFront distribution with the S3 bucket as the origin. Set the cache behavior settings to cache based on the Accept-Language request header.",
      "B": "Configure an Amazon CloudFront distribution with the ALB as the origin. Set the cache behavior settings to cache based on the Accept- Language request header.",
      "C": "Create an Amazon API Gateway API that is integrated with the ALB. Configure the API to use the HTTP integration type. Set up an API Gateway stage to enable the API cache based on the Accept-Language request header.",
      "D": "Launch an EC2 instance in each additional Region and configure NGINX to act as a cache server for that Region. Put all the EC2 instances and the ALB behind an Amazon Route 53 record set with a geolocation routing policy."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司通过 Application Load Balancer (ALB) 后的 Amazon EC2 实例集群提供动态网站。该网站需要支持多种语言来服务世界各地的客户。该网站的架构运行在 us-west-1 区域，并且对于位于世界其他地方的用户表现出高请求延迟。该网站需要快速有效地服务请求，而不管用户的位置如何。但是，该公司不想在多个区域中重新创建现有架构。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将现有架构替换为从 Amazon S3 存储桶提供的网站。配置一个 Amazon CloudFront 分发，将 S3 存储桶作为源。将缓存行为设置配置为基于 Accept-Language 请求标头进行缓存。",
      "B": "配置一个 Amazon CloudFront 分发，将 ALB 作为源。将缓存行为设置配置为基于 Accept-Language 请求标头进行缓存。",
      "C": "创建一个 Amazon API Gateway API，该 API 与 ALB 集成。将 API 配置为使用 HTTP 集成类型。设置一个 API Gateway 阶段，以基于 Accept-Language 请求标头启用 API 缓存。",
      "D": "在每个附加区域启动一个 EC2 实例，并配置 NGINX 以充当该区域的缓存服务器。将所有 EC2 实例和 ALB 放在一个 Amazon Route 53 记录集中，并使用地理位置路由策略。"
    },
    "tags": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "ALB",
      "Content Delivery Network",
      "CDN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查通过 CloudFront 优化网站性能，并根据用户语言提供个性化内容。涉及 CloudFront 配置、ALB 集成以及缓存策略。也间接考察了对全球化用户访问的优化和对现有架构的最小改动。",
      "why_correct": "选项 B 提供了最佳解决方案，它使用 Amazon CloudFront 分发，将 Application Load Balancer (ALB) 作为源。 通过配置 CloudFront 的缓存行为，基于 `Accept-Language` 请求头来缓存内容，可以根据用户的语言偏好，将不同语言版本的网站内容缓存在 CloudFront 边缘节点。这显著减少了用户请求的延迟，提高了网站的性能，而无需改变现有的EC2 实例和ALB 架构。",
      "why_wrong": "选项 A 不适用，因为它建议将整个网站迁移到 Amazon S3 存储桶，这与题目中“不想在多个区域中重新创建现有架构”的要求相悖。虽然 S3 静态网站托管和 CloudFront 结合可以优化性能，但此处网站是动态的，需要 ALB 和 EC2 实例提供后端处理。 选项 C 使用 API Gateway 并不合适，API Gateway 主要用于构建和管理 API，此处使用 ALB 已经能够满足需求，引入 API Gateway 会增加复杂性和额外的开销，并且API Gateway 的缓存功能也不如 CloudFront 强大。 选项 D 涉及在多个区域部署 EC2 实例和 NGINX 缓存服务器，并使用 Route 53 进行地理位置路由，虽然可以一定程度优化性能，但需要手动管理多个区域的实例，增加了运维复杂度和成本，并且不如 CloudFront 的全球边缘节点网络便捷。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "ALB",
      "Amazon S3",
      "EC2",
      "Amazon API Gateway",
      "Route 53",
      "NGINX",
      "Accept-Language"
    ]
  },
  {
    "id": 273,
    "topic": "1",
    "question_en": "A rapidly growing ecommerce company is running its workloads in a single AWS Region. A solutions architect must create a disaster recovery (DR) strategy that includes a different AWS Region. The company wants its database to be up to date in the DR Region with the least possible latency. The remaining infrastructure in the DR Region needs to run at reduced capacity and must be able to scale up if necessary. Which solution will meet these requirements with the LOWEST recovery time objective (RTO)?",
    "options_en": {
      "A": "Use an Amazon Aurora global database with a pilot light deployment.",
      "B": "Use an Amazon Aurora global database with a warm standby deployment.",
      "C": "Use an Amazon RDS Multi-AZ DB instance with a pilot light deployment.",
      "D": "Use an Amazon RDS Multi-AZ DB instance with a warm standby deployment."
    },
    "correct_answer": "B",
    "vote_percentage": "98%",
    "question_cn": "一家快速发展的电子商务公司正在单个 AWS 区域中运行其工作负载。一位解决方案架构师必须创建一个灾难恢复 (DR) 策略，该策略包括另一个 AWS 区域。该公司希望其数据库在 DR 区域中保持最新状态，且延迟尽可能低。DR 区域中的其余基础设施需要在缩减的容量下运行，并且必须能够根据需要进行扩展。哪种解决方案将以最低的恢复时间目标 (RTO) 满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Aurora 全局数据库和试点灯部署。",
      "B": "使用 Amazon Aurora 全局数据库和暖备用部署。",
      "C": "使用 Amazon RDS 多可用区数据库实例和试点灯部署。",
      "D": "使用 Amazon RDS 多可用区数据库实例和暖备用部署。"
    },
    "tags": [
      "Amazon Aurora",
      "Amazon RDS",
      "Disaster Recovery",
      "RTO"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 98%），解析仅供参考。】\n\n本题考查灾难恢复（DR）策略设计，特别是针对数据库的 DR 方案选择，以及 RTO 优化。需要考虑数据库的同步方案、备用环境的部署方式，并结合业务对 RTO 和成本的要求。",
      "why_correct": "Amazon Aurora 全局数据库结合暖备用部署能够以较低的 RTO 满足要求。Aurora 全局数据库支持跨区域的数据库同步，保证 DR 区域数据库的实时性；暖备用部署模式下，DR 区域的基础设施在缩减容量下运行，需要时可以快速扩展。这种组合平衡了 RTO、成本和可扩展性，能以较低延迟保持 DR 区域数据库的最新状态，并提供快速的故障切换能力。",
      "why_wrong": "选项 A 和 C 的试点灯部署模式的 RTO 较高。在试点灯模式下，DR 区域中的基础设施仅维持核心组件，需要手动或脚本干预才能启动和配置完整的基础设施，耗时较长，无法满足低 RTO 的需求。选项 C 使用 Amazon RDS 多可用区实例，虽然提供了高可用性，但其主要针对单个区域内的故障，无法直接满足跨区域 DR 的需求。选项 D 使用 RDS 多可用区实例和暖备用部署，RDS 实例本身的跨区域同步能力不如 Aurora 全局数据库，难以实现低延迟的 DR 需求。此外，多可用区实例的部署和维护成本也相对较高。"
    },
    "related_terms": [
      "Amazon Aurora",
      "Amazon RDS",
      "Multi-AZ",
      "RTO",
      "Disaster Recovery",
      "Global Database",
      "Warm Standby",
      "Pilot Light"
    ]
  },
  {
    "id": 274,
    "topic": "1",
    "question_en": "A company runs an application on Amazon EC2 instances. The company needs to implement a disaster recovery (DR) solution for the application. The DR solution needs to have a recovery time objective (RTO) of less than 4 hours. The DR solution also needs to use the fewest possible AWS resources during normal operations. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS Lambda and custom scripts.",
      "B": "Create Amazon Machine Images (AMIs) to back up the EC2 instances. Copy the AMIs to a secondary AWS Region. Automate infrastructure deployment in the secondary Region by using AWS CloudFormation.",
      "C": "Launch EC2 instances in a secondary AWS Region. Keep the EC2 instances in the secondary Region active at all times.",
      "D": "Launch EC2 instances in a secondary Availability Zone. Keep the EC2 instances in the secondary Availability Zone active at all times."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Amazon EC2 实例上运行一个应用程序。该公司需要为该应用程序实施灾难恢复 (DR) 解决方案。 DR 解决方案需要小于 4 小时的恢复时间目标 (RTO)。 DR 解决方案还需要在正常运行期间使用最少的 AWS 资源。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "创建 Amazon Machine Images (AMI) 以备份 EC2 实例。将 AMI 复制到辅助 AWS 区域。使用 AWS Lambda 和自定义脚本在辅助区域中自动部署基础设施。",
      "B": "创建 Amazon Machine Images (AMI) 以备份 EC2 实例。将 AMI 复制到辅助 AWS 区域。使用 AWS CloudFormation 在辅助区域中自动部署基础设施。",
      "C": "在辅助 AWS 区域中启动 EC2 实例。始终保持辅助区域中的 EC2 实例处于活动状态。",
      "D": "在辅助可用区中启动 EC2 实例。始终保持辅助可用区中的 EC2 实例处于活动状态。"
    },
    "tags": [
      "Amazon EC2",
      "Disaster Recovery",
      "RTO",
      "AWS Lambda",
      "AMI",
      "AWS CloudFormation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查灾难恢复（DR）解决方案的 RTO 和资源利用率，以及如何使用 AMI 和 CloudFormation 实现高效的 DR 方案。",
      "why_correct": "选项 B 提供了最有效率的灾难恢复解决方案。它使用 AMI 备份 EC2 实例，确保数据一致性。CloudFormation 自动化在辅助区域的部署，缩短了 RTO，且仅在需要时才创建和运行基础设施，最大程度地减少了正常运行期间的资源消耗。",
      "why_wrong": "选项 A 使用 Lambda 和自定义脚本部署基础设施，相比 CloudFormation，其管理和维护成本更高，更容易出错。选项 C 保持辅助区域的 EC2 实例始终活动，会产生持续的计算和存储费用，不符合最小资源消耗的要求。选项 D 仅在辅助可用区中启动实例，不满足跨区域灾备的需求，无法实现真正的灾难恢复。"
    },
    "related_terms": [
      "Amazon EC2",
      "DR",
      "RTO",
      "Amazon Machine Images (AMI)",
      "AWS Lambda",
      "AWS CloudFormation",
      "AWS"
    ]
  },
  {
    "id": 275,
    "topic": "1",
    "question_en": "A company runs an internal browser-based application. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales up to 20 instances during work hours, but scales down to 2 instances overnight. Staff are complaining that the application is very slow when the day begins, although it runs well by mid-morning. How should the scaling be changed to address the staff complaints and keep costs to a minimum?",
    "options_en": {
      "A": "Implement a scheduled action that sets the desired capacity to 20 shortly before the ofice opens.",
      "B": "Implement a step scaling action triggered at a lower CPU threshold, and decrease the cooldown period.",
      "C": "Implement a target tracking action triggered at a lower CPU threshold, and decrease the cooldown period.",
      "D": "Implement a scheduled action that sets the minimum and maximum capacity to 20 shortly before the ofice opens."
    },
    "correct_answer": "C",
    "vote_percentage": "66%",
    "question_cn": "一家公司运行一个内部基于浏览器的应用程序。该应用程序在Application Load Balancer后面的 Amazon EC2 实例上运行。这些实例在跨多个可用区的 Amazon EC2 Auto Scaling 组中运行。Auto Scaling 组在工作时间内扩展到 20 个实例，但在夜间缩减到 2 个实例。员工抱怨说，虽然应用程序在上午中期运行良好，但在一天开始时非常慢。应该如何更改扩展以解决员工的抱怨并最大限度地降低成本？",
    "options_cn": {
      "A": "实施一个预定操作，在办公室开始前不久将期望容量设置为 20。",
      "B": "实施一个在较低 CPU 阈值触发的步进扩展操作，并减少冷却时间。",
      "C": "实施一个在较低 CPU 阈值触发的目标跟踪操作，并减少冷却时间。",
      "D": "实施一个预定操作，在办公室开始前不久将最小和最大容量设置为 20。"
    },
    "tags": [
      "Amazon EC2 Auto Scaling",
      "Auto Scaling",
      "Application Load Balancer",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 66%），解析仅供参考。】\n\n考查了利用 Auto Scaling 的目标跟踪策略，解决应用程序启动时的性能问题，并在控制成本的同时优化弹性伸缩。",
      "why_correct": "目标跟踪策略可以根据 CPU 使用率等指标自动调整 Auto Scaling 组的容量，以维持目标值。在较低 CPU 阈值触发目标跟踪策略，可以在应用程序负载增加时快速扩展 EC2 实例的数量。减少冷却时间可以使扩展策略对负载变化做出更快的反应，提高应用程序的响应速度。",
      "why_wrong": "选项 A，预定操作仅根据时间进行容量调整，无法动态响应应用程序负载的变化，无法解决启动时的问题。选项 B，步进扩展策略响应速度较慢，且在负载初期容易出现容量不足的问题，也不能很好地解决启动慢的问题。选项 D，预定操作调整最小和最大容量虽然可以确保有足够的实例，但无法自动根据负载变化进行调整，仍然无法解决启动时的问题，且增加了成本。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon EC2",
      "EC2 Auto Scaling",
      "CPU",
      "Auto Scaling"
    ]
  },
  {
    "id": 276,
    "topic": "1",
    "question_en": "A company has a multi-tier application deployed on several Amazon EC2 instances in an Auto Scaling group. An Amazon RDS for Oracle instance is the application’ s data layer that uses Oracle-specific PL/SQL functions. Trafic to the application has been steadily increasing. This is causing the EC2 instances to become overloaded and the RDS instance to run out of storage. The Auto Scaling group does not have any scaling metrics and defines the minimum healthy instance count only. The company predicts that trafic will continue to increase at a steady but unpredictable rate before leveling off. What should a solutions architect do to ensure the system can automatically scale for the increased trafic? (Choose two.)",
    "options_en": {
      "A": "Configure storage Auto Scaling on the RDS for Oracle instance.",
      "B": "Migrate the database to Amazon Aurora to use Auto Scaling storage.",
      "C": "Configure an alarm on the RDS for Oracle instance for low free storage space.",
      "D": "Configure the Auto Scaling group to use the average CPU as the scaling metric",
      "E": "Configure the Auto Scaling group to use the average free memory as the scaling metric."
    },
    "correct_answer": "AD",
    "vote_percentage": "88%",
    "question_cn": "一家公司在其自动伸缩组中的多个 Amazon EC2 实例上部署了一个多层应用程序。一个 Amazon RDS for Oracle 实例是该应用程序的数据层，它使用 Oracle 特有的 PL/SQL 函数。应用程序的流量一直在稳步增加。这导致 EC2 实例过载，并且 RDS 实例的存储空间不足。自动伸缩组没有任何伸缩指标，仅定义了最小的健康实例数。该公司预测，在达到稳定水平之前，流量将继续以稳定但不可预测的速度增长。 解决方案架构师应该怎么做才能确保系统可以自动扩展以应对增加的流量？（选择两个。）",
    "options_cn": {
      "A": "在 RDS for Oracle 实例上配置存储自动伸缩。",
      "B": "将数据库迁移到 Amazon Aurora 以使用自动伸缩存储。",
      "C": "在 RDS for Oracle 实例上配置一个关于可用存储空间低的告警。",
      "D": "配置自动伸缩组以使用平均 CPU 作为伸缩指标。",
      "E": "配置自动伸缩组以使用平均可用内存作为伸缩指标。"
    },
    "tags": [
      "Amazon RDS",
      "RDS for Oracle",
      "Auto Scaling",
      "Storage Auto Scaling",
      "Amazon EC2",
      "Amazon Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 88%），解析仅供参考。】\n\n本题考察了 RDS for Oracle 数据库的存储管理和 EC2 实例的自动伸缩策略的配置，要求选择合适的方案来应对流量增长和资源不足的问题。",
      "why_correct": "A. 在 RDS for Oracle 实例上配置存储自动伸缩，可以动态增加存储容量以应对存储空间不足的问题，符合题目中 RDS 实例存储空间不足的现状。D. 配置自动伸缩组以使用平均 CPU 作为伸缩指标，可以根据 EC2 实例的 CPU 负载来自动扩展实例数量，从而应对流量增长导致的实例过载，这与题目中 EC2 实例过载的情况相符。",
      "why_wrong": "B. 将数据库迁移到 Amazon Aurora 涉及数据库迁移工作，且需要重写 PL/SQL 函数，操作复杂，不符合快速解决问题的需求。C. 在 RDS for Oracle 实例上配置一个关于可用存储空间低的告警，只能提供告警通知，无法自动解决存储空间不足的问题。E. 配置自动伸缩组以使用平均可用内存作为伸缩指标，仅根据内存使用情况进行伸缩，可能无法准确反映负载情况，CPU 负载更适合作为伸缩指标。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS for Oracle",
      "PL/SQL",
      "Amazon Aurora",
      "Auto Scaling Group",
      "CPU",
      "Memory"
    ]
  },
  {
    "id": 277,
    "topic": "1",
    "question_en": "A company provides an online service for posting video content and transcoding it for use by any mobile platform. The application architecture uses Amazon Elastic File System (Amazon EFS) Standard to collect and store the videos so that multiple Amazon EC2 Linux instances can access the video content for processing. As the popularity of the service has grown over time, the storage costs have become too expensive. Which storage solution is MOST cost-effective?",
    "options_en": {
      "A": "Use AWS Storage Gateway for files to store and process the video content.",
      "B": "Use AWS Storage Gateway for volumes to store and process the video content.",
      "C": "Use Amazon EFS for storing the video content. Once processing is complete, transfer the files to Amazon Elastic Block Store (Amazon EBS).",
      "D": "Use Amazon S3 for storing the video content. Move the files temporarily over to an Amazon Elastic Block Store (Amazon EBS) volume attached to the server for processing."
    },
    "correct_answer": "D",
    "vote_percentage": "78%",
    "question_cn": "一家公司提供在线服务，用于发布视频内容并转码，以便在任何移动平台上使用。应用程序架构使用 Amazon Elastic File System (Amazon EFS) Standard 来收集和存储视频，以便多个 Amazon EC2 Linux 实例可以访问视频内容进行处理。随着服务越来越受欢迎，存储成本变得过于昂贵。哪种存储解决方案最具成本效益？",
    "options_cn": {
      "A": "使用 AWS Storage Gateway for files 来存储和处理视频内容。",
      "B": "使用 AWS Storage Gateway for volumes 来存储和处理视频内容。",
      "C": "使用 Amazon EFS 存储视频内容。处理完成后，将文件传输到 Amazon Elastic Block Store (Amazon EBS)。",
      "D": "使用 Amazon S3 存储视频内容。将文件临时移动到连接到服务器的 Amazon Elastic Block Store (Amazon EBS) 卷上进行处理。"
    },
    "tags": [
      "AWS Storage Gateway",
      "Amazon EFS",
      "Amazon S3",
      "Amazon EBS",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 78%），解析仅供参考。】\n\n考查了在视频处理场景下，如何选择最具成本效益的存储解决方案，并权衡存储成本、访问速度和处理效率。",
      "why_correct": "选项 D 提供了最具成本效益的解决方案。使用 Amazon S3 作为主要存储，利用其低成本和高扩展性。处理视频时，将文件临时移动到 Amazon EBS 卷上，确保了处理性能。这种组合在存储成本和处理性能之间取得了良好的平衡。",
      "why_wrong": "选项 A 和 B 使用 AWS Storage Gateway。Storage Gateway 适合于连接本地环境和云存储，并不直接优化云端存储成本。选项 C 使用 Amazon EFS 存储视频，虽然易于访问，但成本高于 S3。将处理后的文件转移到 EBS 无法解决 EFS 存储成本高昂的问题，并且增加了额外的文件传输步骤，降低了效率。"
    },
    "related_terms": [
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon EC2",
      "AWS Storage Gateway",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon S3"
    ]
  },
  {
    "id": 278,
    "topic": "1",
    "question_en": "A company wants to create an application to store employee data in a hierarchical structured relationship. The company needs a minimum- latency response to high-trafic queries for the employee data and must protect any sensitive data. The company also needs to receive monthly email messages if any financial information is present in the employee data. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use Amazon Redshift to store the employee data in hierarchies. Unload the data to Amazon S3 every month.",
      "B": "Use Amazon DynamoDB to store the employee data in hierarchies. Export the data to Amazon S3 every month.",
      "C": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly events to AWS Lambda.",
      "D": "Use Amazon Athena to analyze the employee data in Amazon S3. Integrate Athena with Amazon QuickSight to publish analysis dashboards and share the dashboards with users",
      "E": "Configure Amazon Macie for the AWS account. Integrate Macie with Amazon EventBridge to send monthly notifications through an Amazon Simple Notification Service (Amazon SNS) subscription."
    },
    "correct_answer": "BE",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望创建一个应用程序，以分层结构关系存储员工数据。该公司需要对员工数据的高流量查询提供最低延迟的响应，并且必须保护任何敏感数据。该公司还需要在员工数据中存在任何财务信息时，每月收到电子邮件消息。一个解决方案架构师应该采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 Amazon Redshift 以分层结构存储员工数据。每月将数据卸载到 Amazon S3。",
      "B": "使用 Amazon DynamoDB 以分层结构存储员工数据。每月将数据导出到 Amazon S3。",
      "C": "为 AWS 账户配置 Amazon Macie。将 Macie 与 Amazon EventBridge 集成，以便每月向 AWS Lambda 发送事件。",
      "D": "使用 Amazon Athena 分析 Amazon S3 中的员工数据。将 Athena 与 Amazon QuickSight 集成，以发布分析仪表板并将仪表板与用户共享。",
      "E": "为 AWS 账户配置 Amazon Macie。将 Macie 与 Amazon EventBridge 集成，通过 Amazon Simple Notification Service (Amazon SNS) 订阅发送每月通知。"
    },
    "tags": [
      "Amazon Macie",
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon S3",
      "Amazon Redshift",
      "Amazon Athena",
      "Amazon QuickSight",
      "Amazon SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 100%），解析仅供参考。】\n\n考察使用 DynamoDB 存储分层数据，利用 Macie 进行敏感数据检测和通知，并结合 S3 存储和 EventBridge 实现数据处理和告警。",
      "why_correct": "选项 B 提供了使用 DynamoDB 存储分层结构数据的方案，DynamoDB 适合高流量查询并提供低延迟响应。选项 E 使用 Macie 检测敏感数据，并通过 EventBridge 和 SNS 定期发送通知，满足了保护敏感数据和月度邮件通知的需求。",
      "why_wrong": "选项 A 使用 Redshift 存储分层数据，Redshift 并非为高频查询设计，不适合低延迟要求。选项 C 仅配置 Macie 并集成 EventBridge 到 Lambda，没有存储员工数据和处理财务信息告警的方案。选项 D 侧重于分析，并没有提供存储方案，也无法满足每月发送通知的需求。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon S3",
      "Amazon Macie",
      "Amazon EventBridge",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Redshift",
      "AWS Lambda",
      "Amazon Athena",
      "Amazon QuickSight"
    ]
  },
  {
    "id": 279,
    "topic": "1",
    "question_en": "A company has an application that is backed by an Amazon DynamoDB table. The company’s compliance requirements specify that database backups must be taken every month, must be available for 6 months, and must be retained for 7 years. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an AWS Backup plan to back up the DynamoDB table on the first day of each month. Specify a lifecycle policy that transitions the backup to cold storage after 6 months. Set the retention period for each backup to 7 years.",
      "B": "Create a DynamoDB on-demand backup of the DynamoDB table on the first day of each month. Transition the backup to Amazon S3 Glacier Flexible Retrieval after 6 months. Create an S3 Lifecycle policy to delete backups that are older than 7 years.",
      "C": "Use the AWS SDK to develop a script that creates an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the script on the first day of each month. Create a second script that will run on the second day of each month to transition DynamoDB backups that are older than 6 months to cold storage and to delete backups that are older than 7 years.",
      "D": "Use the AWS CLI to create an on-demand backup of the DynamoDB table. Set up an Amazon EventBridge rule that runs the command on the first day of each month with a cron expression. Specify in the command to transition the backups to cold storage after 6 months and to delete the backups after 7 years."
    },
    "correct_answer": "A",
    "vote_percentage": "86%",
    "question_cn": "一家公司的应用程序由 Amazon DynamoDB 表提供支持。 公司的合规性要求规定，必须每月进行数据库备份，备份必须保留 6 个月，并且必须保留 7 年。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Backup 计划，在每月的第 1 天备份 DynamoDB 表。 指定一个生命周期策略，该策略在 6 个月后将备份转移到冷存储。 将每个备份的保留期设置为 7 年。",
      "B": "在每月的第 1 天创建 DynamoDB 按需备份。 在 6 个月后将备份转移到 Amazon S3 Glacier Flexible Retrieval。 创建一个 S3 生命周期策略以删除超过 7 年的备份。",
      "C": "使用 AWS SDK 开发一个脚本，该脚本创建 DynamoDB 表的按需备份。 设置一个 Amazon EventBridge 规则，该规则在每月的第 1 天运行该脚本。 创建第二个脚本，该脚本将在每月的第 2 天运行，以将超过 6 个月的 DynamoDB 备份转移到冷存储，并删除超过 7 年的备份。",
      "D": "使用 AWS CLI 创建 DynamoDB 表的按需备份。 设置一个 Amazon EventBridge 规则，该规则使用 cron 表达式在每月的第 1 天运行该命令。 在命令中指定在 6 个月后将备份转移到冷存储，并在 7 年后删除备份。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB Backup",
      "Amazon S3 Glacier Flexible Retrieval",
      "S3 Lifecycle Policy",
      "AWS Backup",
      "Amazon EventBridge",
      "AWS CLI",
      "AWS SDK"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 86%），解析仅供参考。】\n\n考察 DynamoDB 的备份策略配置，包括备份频率、存储位置和保留期限。以及理解 AWS Backup 服务的功能。",
      "why_correct": "选项 A 使用 AWS Backup 计划来备份 DynamoDB 表，满足每月备份的需求。通过生命周期策略，将备份转移到冷存储并设置 7 年的保留期，满足了合规性要求中的存储期限和保留期限。AWS Backup 服务简化了备份的管理，包括设置备份频率、保留策略和生命周期管理。",
      "why_wrong": "选项 B 使用 DynamoDB 按需备份，但其备份数据存储在 DynamoDB 中，而不是冷存储。选项 C 和 D 均需要开发脚本或手动编写 CLI 命令，这增加了复杂性，并且在备份管理、保留策略等方面不如 AWS Backup 灵活。此外，选项 D 中无法直接在 CLI 命令中指定冷存储和删除操作，需要额外的流程处理。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "AWS Backup",
      "Amazon S3 Glacier Flexible Retrieval",
      "Amazon S3",
      "AWS SDK",
      "Amazon EventBridge",
      "AWS CLI",
      "cron"
    ]
  },
  {
    "id": 280,
    "topic": "1",
    "question_en": "A company is using Amazon CloudFront with its website. The company has enabled logging on the CloudFront distribution, and logs are saved in one of the company’s Amazon S3 buckets. The company needs to perform advanced analyses on the logs and build visualizations. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
      "B": "Use standard SQL queries in Amazon Athena to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight.",
      "C": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with AWS Glue.",
      "D": "Use standard SQL queries in Amazon DynamoDB to analyze the CloudFront logs in the S3 bucket. Visualize the results with Amazon QuickSight."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司在其网站上使用 Amazon CloudFront。该公司已在 CloudFront 分发上启用了日志记录，并将日志保存在公司的 Amazon S3 存储桶之一中。该公司需要对日志执行高级分析并构建可视化。解决方案架构师应如何满足这些要求？",
    "options_cn": {
      "A": "在 Amazon Athena 中使用标准 SQL 查询来分析 S3 存储桶中的 CloudFront 日志。 使用 AWS Glue 可视化结果。",
      "B": "在 Amazon Athena 中使用标准 SQL 查询来分析 S3 存储桶中的 CloudFront 日志。 使用 Amazon QuickSight 可视化结果。",
      "C": "在 Amazon DynamoDB 中使用标准 SQL 查询来分析 S3 存储桶中的 CloudFront 日志。使用 AWS Glue 可视化结果。",
      "D": "在 Amazon DynamoDB 中使用标准 SQL 查询来分析 S3 存储桶中的 CloudFront 日志。 使用 Amazon QuickSight 可视化结果。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "Amazon DynamoDB",
      "Amazon QuickSight",
      "CloudFront logs",
      "Data Analysis",
      "Data Visualization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考察使用 Amazon Athena 和 Amazon QuickSight 分析 CloudFront 日志。题目要求对 CloudFront 日志进行高级分析和可视化。",
      "why_correct": "选项 B 提供了正确的解决方案。Amazon Athena 能够直接查询存储在 Amazon S3 中的 CloudFront 日志，而 Amazon QuickSight 能够方便地将 Athena 的查询结果可视化，满足了题目对高级分析和构建可视化的需求。",
      "why_wrong": "选项 A 和 C 错误的原因在于，AWS Glue 并不直接提供可视化功能，虽然它可以处理数据，但无法满足题目中对可视化的要求。选项 C 和 D 错误的原因在于，Amazon DynamoDB 不是为分析 CloudFront 日志而设计的存储服务，CloudFront 日志保存在 S3 中，需要使用 Athena 查询 S3 中的数据。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "Amazon QuickSight",
      "Amazon DynamoDB",
      "SQL"
    ]
  },
  {
    "id": 281,
    "topic": "1",
    "question_en": "A company runs a fieet of web servers using an Amazon RDS for PostgreSQL DB instance. After a routine compliance check, the company sets a standard that requires a recovery point objective (RPO) of less than 1 second for all its production databases. Which solution meets these requirements?",
    "options_en": {
      "A": "Enable a Multi-AZ deployment for the DB instance.",
      "B": "Enable auto scaling for the DB instance in one Availability Zone.",
      "C": "Configure the DB instance in one Availability Zone, and create multiple read replicas in a separate Availability Zone.",
      "D": "Configure the DB instance in one Availability Zone, and configure AWS Database Migration Service (AWS DMS) change data capture (CDC) tasks."
    },
    "correct_answer": "A",
    "vote_percentage": "91%",
    "question_cn": "一家公司使用 Amazon RDS for PostgreSQL 数据库实例运行一组 Web 服务器。在例行合规性检查之后，该公司制定了一项标准，要求其所有生产数据库的恢复点目标 (RPO) 小于 1 秒。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "为数据库实例启用多可用区部署。",
      "B": "在一个可用区中为数据库实例启用自动缩放。",
      "C": "在一个可用区中配置数据库实例，并在单独的可用区中创建多个只读副本。",
      "D": "在一个可用区中配置数据库实例，并配置 AWS Database Migration Service (AWS DMS) 变更数据捕获 (CDC) 任务。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "RPO",
      "AWS Database Migration Service (AWS DMS)",
      "CDC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 91%），解析仅供参考。】\n\n考查了 RDS for PostgreSQL 数据库实例的 RPO 要求，即如何配置以满足小于 1 秒的恢复点目标。",
      "why_correct": "启用 RDS 多可用区部署可以提供高可用性和故障转移能力。当主数据库实例发生故障时，RDS 会自动故障转移到备用实例，这减少了数据丢失的风险，从而满足了 RPO 小于 1 秒的要求。",
      "why_wrong": "选项 B 自动缩放与 RPO 无直接关系，自动缩放关注的是数据库的性能和容量。选项 C 创建只读副本提高了读取性能，但不能保证在主数据库故障时 RPO 小于 1 秒。选项 D 的 AWS Database Migration Service (AWS DMS) 变更数据捕获 (CDC) 主要用于数据迁移和复制，不直接提供满足 RPO 需求的解决方案，且CDC的延迟通常大于1秒。"
    },
    "related_terms": [
      "Amazon RDS for PostgreSQL",
      "RPO",
      "Multi-AZ deployment",
      "Auto Scaling",
      "Read Replicas",
      "AWS Database Migration Service (AWS DMS)",
      "CDC"
    ]
  },
  {
    "id": 282,
    "topic": "1",
    "question_en": "A company runs a web application that is deployed on Amazon EC2 instances in the private subnet of a VPC. An Application Load Balancer (ALB) that extends across the public subnets directs web trafic to the EC2 instances. The company wants to implement new security measures to restrict inbound trafic from the ALB to the EC2 instances while preventing access from any other source inside or outside the private subnet of the EC2 instances. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure a route in a route table to direct trafic from the internet to the private IP addresses of the EC2 instances.",
      "B": "Configure the security group for the EC2 instances to only allow trafic that comes from the security group for the ALB.",
      "C": "Move the EC2 instances into the public subnet. Give the EC2 instances a set of Elastic IP addresses.",
      "D": "Configure the security group for the ALB to allow any TCP trafic on any port."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司运行一个部署在 VPC 私有子网中的 Amazon EC2 实例上的 Web 应用程序。一个跨越公有子网的 Application Load Balancer (ALB) 将 Web 流量定向到 EC2 实例。该公司希望实施新的安全措施，以限制来自 ALB 到 EC2 实例的入站流量，同时阻止来自 EC2 实例私有子网内部或外部任何其他 源 的访问。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在路由表中配置一个路由，将来自互联网的流量定向到 EC2 实例的私有 IP 地址。",
      "B": "配置 EC2 实例的安全组，只允许来自 ALB 安全组的流量。",
      "C": "将 EC2 实例移至公有子网。为 EC2 实例分配一组弹性 IP 地址。",
      "D": "配置 ALB 的安全组以允许任何端口上的任何 TCP 流量。"
    },
    "tags": [
      "Amazon EC2",
      "Application Load Balancer",
      "Security Group",
      "VPC",
      "Elastic IP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查了如何使用安全组限制对 EC2 实例的访问，以满足安全需求。",
      "why_correct": "选项 B 是最佳解决方案。通过配置 EC2 实例的安全组，只允许来自 Application Load Balancer (ALB) 安全组的流量，可以精确地控制入站流量，只允许 ALB 的流量访问 EC2 实例。这满足了题目中限制访问来源的需求，同时阻止了来自私有子网内部或外部的任何其他源的访问。",
      "why_wrong": "选项 A 错误，因为在路由表中配置路由，将互联网流量定向到 EC2 实例的私有 IP 地址，会违反私有子网的设计初衷，使其暴露在公网，并无法阻止来自非 ALB 的流量。选项 C 错误，将 EC2 实例移至公有子网，并分配弹性 IP 地址，会使实例暴露在公网，这与限制外部访问的需求相悖。选项 D 错误，配置 ALB 的安全组以允许任何端口上的任何 TCP 流量，无法限制 ALB 的访问，并且不能阻止来自 EC2 实例私有子网内部或外部的访问，无法满足题目的安全要求。"
    },
    "related_terms": [
      "VPC",
      "Amazon EC2",
      "Web application",
      "Application Load Balancer (ALB)",
      "EC2 instance",
      "private subnet",
      "public subnet",
      "security group",
      "Elastic IP address",
      "TCP"
    ]
  },
  {
    "id": 283,
    "topic": "1",
    "question_en": "A research company runs experiments that are powered by a simulation application and a visualization application. The simulation application runs on Linux and outputs intermediate data to an NFS share every 5 minutes. The visualization application is a Windows desktop application that displays the simulation output and requires an SMB file system. The company maintains two synchronized file systems. This strategy is causing data duplication and ineficient resource usage. The company needs to migrate the applications to AWS without making code changes to either application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate both applications to AWS Lambda. Create an Amazon S3 bucket to exchange data between the applications.",
      "B": "Migrate both applications to Amazon Elastic Container Service (Amazon ECS). Configure Amazon FSx File Gateway for storage.",
      "C": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon Simple Queue Service (Amazon SQS) to exchange data between the applications.",
      "D": "Migrate the simulation application to Linux Amazon EC2 instances. Migrate the visualization application to Windows EC2 instances. Configure Amazon FSx for NetApp ONTAP for storage."
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一家研究公司运行由模拟应用程序和可视化应用程序驱动的实验。模拟应用程序在 Linux 上运行，每 5 分钟将中间数据输出到 NFS 共享。可视化应用程序是一个 Windows 桌面应用程序，用于显示模拟输出，并需要一个 SMB 文件系统。该公司维护两个同步的文件系统。这种策略导致数据重复和资源使用效率低下。该公司需要将应用程序迁移到 AWS，而无需对任何一个应用程序进行代码更改。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将两个应用程序迁移到 AWS Lambda。创建一个 Amazon S3 存储桶以在应用程序之间交换数据。",
      "B": "将两个应用程序迁移到 Amazon Elastic Container Service (Amazon ECS)。配置 Amazon FSx File Gateway 用于存储。",
      "C": "将模拟应用程序迁移到 Linux Amazon EC2 实例。将可视化应用程序迁移到 Windows EC2 实例。配置 Amazon Simple Queue Service (Amazon SQS) 以在应用程序之间交换数据。",
      "D": "将模拟应用程序迁移到 Linux Amazon EC2 实例。将可视化应用程序迁移到 Windows EC2 实例。配置 Amazon FSx for NetApp ONTAP 用于存储。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon FSx",
      "FSx for NetApp ONTAP",
      "NFS",
      "SMB",
      "AWS Lambda",
      "Amazon S3",
      "Amazon ECS",
      "Amazon SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n考查在 AWS 上迁移具有 NFS 和 SMB 需求的应用程序。该题考察了在不更改代码的情况下，如何通过选择合适的存储和计算服务，实现应用程序的迁移，并解决数据同步和资源利用效率低下的问题，与存储服务的选择、计算服务的选型相关。",
      "why_correct": "选项 D 满足了所有需求。将模拟应用程序迁移到 Linux Amazon EC2 实例，并使用 NFS 协议与 FSx for NetApp ONTAP 文件系统交互。将可视化应用程序迁移到 Windows EC2 实例，并使用 SMB 协议与 FSx for NetApp ONTAP 文件系统交互。 FSx for NetApp ONTAP 支持 NFS 和 SMB 协议，这确保了应用程序可以不作任何代码更改地进行迁移，并满足了题目的要求。",
      "why_wrong": "选项 A 错误，因为 Lambda 函数是无服务器计算服务，它不适合需要长时间运行的模拟应用程序。Lambda 函数也无法直接与 NFS 和 SMB 文件系统交互。选项 B 错误，因为 FSx File Gateway 用于将本地存储连接到 AWS，而不是支持 NFS 和 SMB 协议之间的交互，且没有说明应用程序运行在哪种计算服务上。选项 C 错误，因为它使用 Amazon SQS 作为应用程序之间交换数据的机制。虽然 SQS 是一种消息队列服务，但它不适合用于传输中间数据（尤其是文件），并且也没有说明使用哪种文件系统支持 NFS 和 SMB 协议的需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon FSx",
      "FSx for NetApp ONTAP",
      "NFS",
      "SMB",
      "AWS Lambda",
      "Amazon S3",
      "Amazon ECS",
      "Amazon SQS",
      "Windows EC2",
      "Linux EC2",
      "FSx File Gateway"
    ]
  },
  {
    "id": 284,
    "topic": "1",
    "question_en": "As part of budget planning, management wants a report of AWS billed items listed by user. The data will be used to create department budgets. A solutions architect needs to determine the most eficient way to obtain this report information. Which solution meets these requirements?",
    "options_en": {
      "A": "Run a query with Amazon Athena to generate the report.",
      "B": "Create a report in Cost Explorer and download the report.",
      "C": "Access the bill details from the billing dashboard and download the bill.",
      "D": "Modify a cost budget in AWS Budgets to alert with Amazon Simple Email Service (Amazon SES)."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "作为预算规划的一部分，管理层希望获得一份按用户列出的 AWS 计费项目的报告。这些数据将用于创建部门预算。一位解决方案架构师需要确定获取此报告信息的最有效方法。哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 Amazon Athena 运行查询以生成报告。",
      "B": "在 Cost Explorer 中创建报告并下载该报告。",
      "C": "从账单仪表板访问账单详细信息并下载账单。",
      "D": "在 AWS Budgets 中修改一个成本预算，以通过 Amazon Simple Email Service (Amazon SES) 发出警报。"
    },
    "tags": [
      "AWS Cost Explorer",
      "AWS Budgets",
      "Amazon Athena",
      "Amazon SES"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何获取按用户划分的 AWS 计费项目报告，用于预算规划；与 AWS Cost Explorer、AWS Budgets、Amazon Athena 和 Amazon SES 的功能对比相关。",
      "why_correct": "在 AWS Cost Explorer 中，可以创建自定义报告，并根据组织结构（如用户）进行筛选和分组。这些报告可以下载为 CSV 或其他格式，满足按用户列出计费项目的需求，并用于创建部门预算。",
      "why_wrong": {
        "A": "Amazon Athena 主要用于对 Amazon S3 中的数据进行交互式查询，不直接用于生成按用户划分的计费报告。虽然可以查询成本和使用情况报告，但需要更复杂的数据处理，效率不如 Cost Explorer 直接。生成报告不如 Cost Explorer 方便。",
        "C": "账单仪表板提供账单的概览，账单详细信息则包含更细致的成本数据，但通常不提供按用户分组的预定义报告功能。下载整个账单文件需要手动处理，提取用户相关信息，效率不如 Cost Explorer。",
        "D": "AWS Budgets 用于设置预算和监控成本，并能通过 Amazon SES 发送警报。它主要用于成本监控和超支提醒，而非生成按用户划分的详细计费报告，无法满足生成报告的需求。"
      }
    },
    "related_terms": [
      "Amazon Athena",
      "Amazon SES",
      "AWS Cost Explorer",
      "AWS Budgets"
    ]
  },
  {
    "id": 285,
    "topic": "1",
    "question_en": "A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Host a dynamic contact form page in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to any third-party email provider.",
      "B": "Create an Amazon API Gateway endpoint with an AWS Lambda backend that makes a call to Amazon Simple Email Service (Amazon SES).",
      "C": "Convert the static webpage to dynamic by deploying Amazon Lightsail. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.",
      "D": "Create a t2.micro Amazon EC2 instance. Deploy a LAMP (Linux, Apache, MySQL, PHP/Perl/Python) stack to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司使用 Amazon S3 托管其静态网站。该公司希望在其网页中添加一个联系表单。联系表单将具有动态服务器端组件，供用户输入他们的姓名、电子邮件地址、电话号码和用户消息。该公司预计每月访问次数少于 100 次。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "在 Amazon Elastic Container Service (Amazon ECS) 中托管动态联系表单页面。 设置 Amazon Simple Email Service (Amazon SES) 以连接到任何第三方电子邮件提供商。",
      "B": "创建一个 Amazon API Gateway 端点，该端点具有一个调用 Amazon Simple Email Service (Amazon SES) 的 AWS Lambda 后端。",
      "C": "通过部署 Amazon Lightsail 将静态网页转换为动态网页。 使用客户端脚本构建联系表单。 将该表单与 Amazon WorkMail 集成。",
      "D": "创建一个 t2.micro Amazon EC2 实例。 部署一个 LAMP（Linux、Apache、MySQL、PHP/Perl/Python）堆栈来托管网页。 使用客户端脚本构建联系表单。 将该表单与 Amazon WorkMail 集成。"
    },
    "tags": [
      "Amazon S3",
      "API Gateway",
      "Lambda",
      "Amazon SES"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查了在 Amazon S3 托管静态网站场景下，如何经济高效地实现动态联系表单功能。涉及 API Gateway、Lambda、SES 等服务的选型，以及与成本、性能、扩展性相关的考量。",
      "why_correct": "选项 B 提供了最经济高效的解决方案。通过 Amazon API Gateway 暴露一个 HTTP 端点，该端点触发一个 Lambda 函数。Lambda 函数负责处理联系表单提交的数据，并使用 Amazon SES 发送电子邮件。这种方案仅在表单提交时才产生费用，符合每月访问量少于 100 次的场景，成本较低。API Gateway 提供了可靠的扩展性，能够应对未来潜在的访问量增长。",
      "why_wrong": "选项 A 在 Amazon ECS 中部署动态联系表单页面，ECS 运行容器的成本相对较高，不适合低访问量的场景。此外，使用 SES 连接到第三方邮件提供商，虽然功能可行，但增加了额外的配置和管理复杂性。选项 C 使用 Amazon Lightsail 托管，将静态网页转换为动态网页，虽然 Lightsail 提供了简化部署的方案，但相比无服务器方案，其成本效率较低。将表单与 Amazon WorkMail 集成的方案增加了不必要的额外成本。选项 D 使用 EC2 实例部署 LAMP 堆栈，其维护成本远高于无服务器方案，需要手动管理服务器、安装配置软件，且 EC2 的固定运行成本不适合低访问量的场景，扩展性也较差。"
    },
    "related_terms": [
      "Amazon S3",
      "API Gateway",
      "Lambda",
      "Amazon SES",
      "Amazon ECS",
      "Lightsail",
      "EC2",
      "LAMP",
      "WorkMail"
    ]
  },
  {
    "id": 286,
    "topic": "1",
    "question_en": "A company has a static website that is hosted on Amazon CloudFront in front of Amazon S3. The static website uses a database backend. The company notices that the website does not refiect updates that have been made in the website’s Git repository. The company checks the continuous integration and continuous delivery (CI/CD) pipeline between the Git repository and Amazon S3. The company verifies that the webhooks are configured properly and that the CI/CD pipeline is sending messages that indicate successful deployments. A solutions architect needs to implement a solution that displays the updates on the website. Which solution will meet these requirements?",
    "options_en": {
      "A": "Add an Application Load Balancer.",
      "B": "Add Amazon ElastiCache for Redis or Memcached to the database layer of the web application.",
      "C": "Invalidate the CloudFront cache.",
      "D": "Use AWS Certificate Manager (ACM) to validate the website’s SSL certificate."
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一家公司在其 Amazon S3 前面托管了一个静态网站，该网站位于 Amazon CloudFront 上。该静态网站使用数据库后端。该公司注意到，网站并未反映在网站的 Git 存储库中进行的更新。该公司检查了 Git 存储库和 Amazon S3 之间的持续集成和持续交付 (CI/CD) 管道。该公司验证了 Webhook 配置正确，并且 CI/CD 管道正在发送指示部署成功的消息。一位解决方案架构师需要实施一个解决方案，以在网站上显示更新。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "添加一个 Application Load Balancer。",
      "B": "在 Web 应用程序的数据库层添加 Amazon ElastiCache for Redis 或 Memcached。",
      "C": "使 CloudFront 缓存失效。",
      "D": "使用 AWS Certificate Manager (ACM) 验证网站的 SSL 证书。"
    },
    "tags": [
      "S3",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n考查 CloudFront 缓存失效机制，以及 CI/CD 流程中的静态网站更新问题排查。",
      "why_correct": "CloudFront 作为 CDN 缓存了 S3 静态网站的内容。网站内容未更新的原因可能是 CloudFront 缓存了旧的内容。通过使 CloudFront 缓存失效，可以强制 CloudFront 从 S3 获取最新的网站内容，从而解决网站未更新的问题。",
      "why_wrong": "A. Application Load Balancer 用于负载均衡应用程序流量，与静态网站更新问题无关。B. ElastiCache 主要用于缓存数据库查询结果，优化数据库性能，与静态网站内容更新问题无关。D. AWS Certificate Manager 用于管理 SSL 证书，与静态网站内容更新问题无关，SSL 证书通常不会导致内容不更新的情况。"
    },
    "related_terms": [
      "Amazon S3",
      "CloudFront",
      "CI/CD",
      "Application Load Balancer",
      "Amazon ElastiCache for Redis",
      "Memcached",
      "AWS Certificate Manager (ACM)",
      "CDN",
      "SSL"
    ]
  },
  {
    "id": 287,
    "topic": "1",
    "question_en": "A company wants to migrate a Windows-based application from on premises to the AWS Cloud. The application has three tiers: an application tier, a business tier, and a database tier with Microsoft SQL Server. The company wants to use specific features of SQL Server such as native backups and Data Quality Services. The company also needs to share files for processing between the tiers. How should a solutions architect design the architecture to meet these requirements?",
    "options_en": {
      "A": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx File Gateway for file sharing between the tiers.",
      "B": "Host all three tiers on Amazon EC2 instances. Use Amazon FSx for Windows File Server for file sharing between the tiers.",
      "C": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use Amazon Elastic File System (Amazon EFS) for file sharing between the tiers.",
      "D": "Host the application tier and the business tier on Amazon EC2 instances. Host the database tier on Amazon RDS. Use a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume for file sharing between the tiers."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司希望将基于 Windows 的应用程序从本地迁移到 AWS 云。该应用程序有三层：应用层、业务层和具有 Microsoft SQL Server 的数据库层。该公司希望使用 SQL Server 的特定功能，例如原生备份和数据质量服务。该公司还需要在各层之间共享文件以进行处理。解决方案架构师应该如何设计架构以满足这些要求？",
    "options_cn": {
      "A": "将所有三层托管在 Amazon EC2 实例上。使用 Amazon FSx File Gateway 在各层之间共享文件。",
      "B": "将所有三层托管在 Amazon EC2 实例上。使用 Amazon FSx for Windows File Server 在各层之间共享文件。",
      "C": "将应用层和业务层托管在 Amazon EC2 实例上。将数据库层托管在 Amazon RDS 上。使用 Amazon Elastic File System (Amazon EFS) 在各层之间共享文件。",
      "D": "将应用层和业务层托管在 Amazon EC2 实例上。将数据库层托管在 Amazon RDS 上。使用 Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) 卷在各层之间共享文件。"
    },
    "tags": [
      "EC2",
      "FSx",
      "RDS",
      "Windows",
      "SQL Server"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n此题考察 Windows 应用程序迁移到 AWS 的架构设计，特别是文件共享和数据库的选择。FSx for Windows File Server 提供了 Windows 文件共享功能，与 SQL Server 的兼容性好；RDS 不支持 SQL Server 的原生备份和数据质量服务。",
      "why_correct": "FSx for Windows File Server 专为 Windows 环境设计，与 SQL Server 兼容，提供了文件共享服务。在 Windows 环境下，此服务性能和功能都更胜一筹。",
      "why_wrong": "使用 FSx File Gateway 无法实现文件共享，也不是专门针对 Windows 的文件服务。使用 EFS 无法在 Windows 环境下运行，性能也无法满足需求。使用 EBS 卷无法提供文件共享功能。"
    },
    "related_terms": [
      "EC2",
      "FSx",
      "RDS",
      "Windows",
      "SQL Server",
      "EFS",
      "EBS",
      "FSx File Gateway",
      "Provisioned IOPS SSD (io2)"
    ]
  },
  {
    "id": 288,
    "topic": "1",
    "question_en": "A company is migrating a Linux-based web server group to AWS. The web servers must access files in a shared file store for some content. The company must not make any changes to the application. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an Amazon S3 Standard bucket with access to the web servers.",
      "B": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin.",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on all web servers.",
      "D": "Configure a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume to all web servers."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将基于 Linux 的 Web 服务器组迁移到 AWS。Web 服务器必须访问共享文件存储中的文件以获取一些内容。公司不得对应用程序进行任何更改。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon S3 标准存储桶，并允许 Web 服务器访问。",
      "B": "配置一个 Amazon CloudFront 分发，将 Amazon S3 存储桶作为源。",
      "C": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统。在所有 Web 服务器上挂载 EFS 文件系统。",
      "D": "配置一个通用型 SSD (gp3) Amazon Elastic Block Store (Amazon EBS) 卷。将 EBS 卷挂载到所有 Web 服务器。"
    },
    "tags": [
      "EC2",
      "EFS",
      "S3",
      "Linux"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察如何在不更改应用程序的情况下，为 Linux Web 服务器组提供共享文件存储。重点在于选择合适的 AWS 存储服务。",
      "why_correct": "Amazon Elastic File System (Amazon EFS) 专为共享文件存储设计，非常适合在多个 EC2 实例之间共享文件。EFS 可以通过 NFS 协议挂载到 Linux 服务器，满足题目中“不得对应用程序进行任何更改”的要求。这种方式提供了可扩展性和高可用性。",
      "why_wrong": "选项 A 错误，因为 Amazon S3 是一种对象存储服务，不适合作为共享文件存储，无法通过文件系统的方式访问。选项 B 错误，CloudFront 是 CDN 服务，不能直接提供共享文件存储。选项 D 错误，虽然 Amazon Elastic Block Store (Amazon EBS) 可以用于存储，但它是一块块存储，不是为多个服务器共享文件而设计的，且会增加管理复杂性，也不满足共享文件系统的需求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon Elastic File System (Amazon EFS)",
      "Linux",
      "Web server",
      "Amazon Elastic Block Store (Amazon EBS)",
      "NFS",
      "EC2",
      "SSD",
      "gp3"
    ]
  },
  {
    "id": 289,
    "topic": "1",
    "question_en": "A company has an AWS Lambda function that needs read access to an Amazon S3 bucket that is located in the same AWS account. Which solution will meet these requirements in the MOST secure manner?",
    "options_en": {
      "A": "Apply an S3 bucket policy that grants read access to the S3 bucket.",
      "B": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to the S3 bucket.",
      "C": "Embed an access key and a secret key in the Lambda function’s code to grant the required IAM permissions for read access to the S3 bucket.",
      "D": "Apply an IAM role to the Lambda function. Apply an IAM policy to the role to grant read access to all S3 buckets in the account."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个 AWS Lambda 函数，需要读取位于同一 AWS 账户中的 Amazon S3 存储桶。哪种解决方案将以最安全的方式满足这些要求？",
    "options_cn": {
      "A": "应用一个 S3 存储桶策略，授予对 S3 存储桶的读取访问权限。",
      "B": "将一个 IAM 角色应用于 Lambda 函数。将一个 IAM 策略应用于该角色以授予对 S3 存储桶的读取访问权限。",
      "C": "将访问密钥和秘密密钥嵌入到 Lambda 函数的代码中，以授予对 S3 存储桶的读取访问所需的 IAM 权限。",
      "D": "将一个 IAM 角色应用于 Lambda 函数。将一个 IAM 策略应用于该角色，以授予对该账户中所有 S3 存储桶的读取访问权限。"
    },
    "tags": [
      "Lambda",
      "S3",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何使用 IAM 角色和策略，安全地授予 Lambda 函数访问 S3 存储桶的权限。",
      "why_correct": "选项 B 是最佳实践，它通过 IAM 角色关联 IAM 策略，赋予 Lambda 函数访问 S3 存储桶的权限。这种方法遵循最小权限原则，仅授予 Lambda 函数所需的最小权限，从而提高了安全性。这种方式易于管理和审计。",
      "why_wrong": "选项 A 仅使用 S3 存储桶策略，这无法授予 Lambda 函数的执行权限，必须通过 IAM 角色进行授权。选项 C 将访问密钥和秘密密钥嵌入代码中，这是一种不安全的做法，会导致密钥泄露的风险，不符合安全最佳实践。选项 D 虽然使用了 IAM 角色，但授予了对所有 S3 存储桶的读取访问权限，违反了最小权限原则，权限范围过大，不安全，应该限制访问到特定的 S3 存储桶。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon S3",
      "IAM role",
      "IAM policy",
      "S3 bucket",
      "access key",
      "secret key"
    ]
  },
  {
    "id": 290,
    "topic": "1",
    "question_en": "A company hosts a web application on multiple Amazon EC2 instances. The EC2 instances are in an Auto Scaling group that scales in response to user demand. The company wants to optimize cost savings without making a long-term commitment. Which EC2 instance purchasing option should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Dedicated Instances only",
      "B": "On-Demand Instances only",
      "C": "A mix of On-Demand Instances and Spot Instances",
      "D": "A mix of On-Demand Instances and Reserved Instances"
    },
    "correct_answer": "C",
    "vote_percentage": "90%",
    "question_cn": "一家公司在多个 Amazon EC2 实例上托管一个 Web 应用程序。EC2 实例位于一个 Auto Scaling 组中，该组会根据用户需求进行扩展。该公司希望优化成本节约，而无需做出长期承诺。解决方案架构师应该推荐哪种 EC2 实例购买选项来满足这些要求？",
    "options_cn": {
      "A": "仅限 Dedicated Instances",
      "B": "仅限 On-Demand Instances",
      "C": "On-Demand Instances 和 Spot Instances 的混合",
      "D": "On-Demand Instances 和 Reserved Instances 的混合"
    },
    "tags": [
      "EC2",
      "On-Demand",
      "Spot",
      "Reserved",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 90%），解析仅供参考。】\n\n考查在弹性伸缩的环境中，如何通过选择合适的 Amazon EC2 实例购买选项，来实现成本优化。",
      "why_correct": "On-Demand Instances 提供了基本的可用性保障，适合处理基本工作负载。Spot Instances 提供了大幅折扣，可以利用闲置的 EC2 容量，从而实现成本节约。将两者结合使用，可以充分利用 Spot Instances 的低成本优势，同时确保工作负载的持续可用性。",
      "why_wrong": "Dedicated Instances 成本较高，不适合追求成本优化的场景。On-Demand Instances 价格相对较高，虽然提供了灵活性，但无法满足成本节约的需求。Reserved Instances 虽可提供折扣，但需要长期承诺，与题干中“无需做出长期承诺”的要求相悖。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 Instances",
      "Auto Scaling",
      "On-Demand Instances",
      "Spot Instances",
      "Dedicated Instances",
      "Reserved Instances"
    ]
  },
  {
    "id": 291,
    "topic": "1",
    "question_en": "A media company uses Amazon CloudFront for its publicly available streaming video content. The company wants to secure the video content that is hosted in Amazon S3 by controlling who has access. Some of the company’s users are using a custom HTTP client that does not support cookies. Some of the company’s users are unable to change the hardcoded URLs that they are using for access. Which services or methods will meet these requirements with the LEAST impact to the users? (Choose two.)",
    "options_en": {
      "A": "Signed cookies",
      "B": "Signed URLs",
      "C": "AWS AppSync",
      "D": "JSON Web Token (JWT)",
      "E": "AWS Secrets Manager"
    },
    "correct_answer": "AB",
    "vote_percentage": "85%",
    "question_cn": "一家媒体公司使用 Amazon CloudFront 提供公开流媒体视频内容。该公司希望通过控制访问权限来保护托管在 Amazon S3 中的视频内容。该公司的一些用户正在使用不支持 cookies 的自定义 HTTP 客户端。该公司的一些用户无法更改他们用于访问的硬编码 URL。哪些服务或方法将以对用户影响最小的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "签名 cookies",
      "B": "签名 URLs",
      "C": "AWS AppSync",
      "D": "JSON Web Token (JWT)",
      "E": "AWS Secrets Manager"
    },
    "tags": [
      "CloudFront",
      "S3",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 85%），解析仅供参考。】\n\n考查 CloudFront 保护 S3 视频内容访问权限的方案，重点关注用户客户端限制及 URL 不可更改的情况。",
      "why_correct": "签名 cookies 和签名 URLs 都是 CloudFront 提供的用于控制访问权限的机制。签名 URLs 通过在 URL 中嵌入签名信息来限制访问，适用于用户无法修改 URL 的情况。签名 cookies 允许创建允许访问多个文件的 cookies，更灵活，适用于用户不支持 cookies 的情况。",
      "why_wrong": "AWS AppSync 用于构建应用程序的 API，与保护 S3 上的视频内容无关。JSON Web Token (JWT) 需要客户端支持和处理 Token，不符合题目中用户无法更改 URL 和不支持 cookies 的要求。AWS Secrets Manager 用于安全地管理凭证，与直接控制视频内容的访问权限无关。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "cookies",
      "HTTP",
      "URL",
      "AWS AppSync",
      "JSON Web Token (JWT)",
      "AWS Secrets Manager",
      "signed URLs",
      "signed cookies"
    ]
  },
  {
    "id": 292,
    "topic": "1",
    "question_en": "A company is preparing a new data platform that will ingest real-time streaming data from multiple sources. The company needs to transform the data before writing the data to Amazon S3. The company needs the ability to use SQL to query the transformed data. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use Amazon Kinesis Data Streams to stream the data. Use Amazon Kinesis Data Analytics to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
      "B": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use AWS Glue to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
      "C": "Use AWS Database Migration Service (AWS DMS) to ingest the data. Use Amazon EMR to transform the data and to write the data to Amazon S3. Use Amazon Athena to query the transformed data from Amazon S3.",
      "D": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to stream the data. Use Amazon Kinesis Data Analytics to transform the data and to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3",
      "E": "Use Amazon Kinesis Data Streams to stream the data. Use AWS Glue to transform the data. Use Amazon Kinesis Data Firehose to write the data to Amazon S3. Use the Amazon RDS query editor to query the transformed data from Amazon S3."
    },
    "correct_answer": "AB",
    "vote_percentage": "91%",
    "question_cn": "一家公司正在准备一个新的数据平台，该平台将从多个 源 获取实时流数据。该公司需要在将数据写入 Amazon S3 之前转换数据。该公司需要能够使用 SQL 查询转换后的数据。哪些解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Streams 传输数据流。使用 Amazon Kinesis Data Analytics 转换数据。使用 Amazon Kinesis Data Firehose 将数据写入 Amazon S3。使用 Amazon Athena 从 Amazon S3 查询转换后的数据。",
      "B": "使用 Amazon Managed Streaming for Apache Kafka (Amazon MSK) 传输数据流。使用 AWS Glue 转换数据并将数据写入 Amazon S3。使用 Amazon Athena 从 Amazon S3 查询转换后的数据。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 获取数据。使用 Amazon EMR 转换数据并将数据写入 Amazon S3。使用 Amazon Athena 从 Amazon S3 查询转换后的数据。",
      "D": "使用 Amazon Managed Streaming for Apache Kafka (Amazon MSK) 传输数据流。使用 Amazon Kinesis Data Analytics 转换数据并将数据写入 Amazon S3。使用 Amazon RDS 查询编辑器从 Amazon S3 查询转换后的数据。",
      "E": "使用 Amazon Kinesis Data Streams 传输数据流。使用 AWS Glue 转换数据。使用 Amazon Kinesis Data Firehose 将数据写入 Amazon S3。使用 Amazon RDS 查询编辑器从 Amazon S3 查询转换后的数据。"
    },
    "tags": [
      "Kinesis",
      "S3",
      "Athena",
      "Glue",
      "MSK"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 91%），解析仅供参考。】\n\n此题考察实时数据流处理的架构，需要同时使用流式数据摄入、数据转换、存储和查询。使用 Kinesis 和 Glue 组合，或者使用 MSK 和 Glue 组合，可以满足要求。",
      "why_correct": "使用 Kinesis Data Streams 摄入数据，Kinesis Data Analytics 进行转换，Kinesis Data Firehose 写入 S3，Athena 查询数据。这种组合提供了完整的流数据处理流水线。使用 MSK 搭配 Glue 和 Athena 同样可行，但需要用户管理 MSK 集群。",
      "why_wrong": "AWS DMS 不用于处理实时数据。使用 RDS 查询 S3 中的数据是不正确的。使用 EMR 进行转换并不能提供实时性。"
    },
    "related_terms": [
      "Kinesis",
      "S3",
      "Athena",
      "Glue",
      "MSK",
      "Kinesis Data Streams",
      "Kinesis Data Analytics",
      "Kinesis Data Firehose",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon EMR",
      "Amazon RDS",
      "Apache Kafka"
    ]
  },
  {
    "id": 293,
    "topic": "1",
    "question_en": "A company has an on-premises volume backup solution that has reached its end of life. The company wants to use AWS as part of a new backup solution and wants to maintain local access to all the data while it is backed up on AWS. The company wants to ensure that the data backed up on AWS is automatically and securely transferred. Which solution meets these requirements?",
    "options_en": {
      "A": "Use AWS Snowball to migrate data out of the on-premises solution to Amazon S3. Configure on-premises systems to mount the Snowball S3 endpoint to provide local access to the data.",
      "B": "Use AWS Snowball Edge to migrate data out of the on-premises solution to Amazon S3. Use the Snowball Edge file interface to provide on-premises systems with local access to the data.",
      "C": "Use AWS Storage Gateway and configure a cached volume gateway. Run the Storage Gateway software appliance on premises and configure a percentage of data to cache locally. Mount the gateway storage volumes to provide local access to the data.",
      "D": "Use AWS Storage Gateway and configure a stored volume gateway. Run the Storage Gateway software appliance on premises and map the gateway storage volumes to on-premises storage. Mount the gateway storage volumes to provide local access to the data."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司现有的本地卷备份解决方案已达到生命周期终点。该公司希望使用 AWS 作为新备份解决方案的一部分，并希望在 AWS 上备份数据的同时保持对所有数据的本地访问。该公司希望确保在 AWS 上备份的数据能够自动且安全地传输。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 AWS Snowball 将数据从本地解决方案迁移到 Amazon S3。配置本地系统以挂载 Snowball S3 端点，以提供对数据的本地访问。",
      "B": "使用 AWS Snowball Edge 将数据从本地解决方案迁移到 Amazon S3。使用 Snowball Edge 文件接口为本地系统提供对数据的本地访问。",
      "C": "使用 AWS Storage Gateway 并配置缓存卷网关。在本地运行 Storage Gateway 软件设备，并配置一部分数据在本地缓存。挂载网关存储卷以提供对数据的本地访问。",
      "D": "使用 AWS Storage Gateway 并配置存储卷网关。在本地运行 Storage Gateway 软件设备，并将网关存储卷映射到本地存储。挂载网关存储卷以提供对数据的本地访问。"
    },
    "tags": [
      "Storage Gateway",
      "S3",
      "Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察本地数据备份到 AWS，并保持本地访问。Storage Gateway 提供的存储卷网关和缓存卷网关都符合要求，但只有存储卷网关支持本地访问。",
      "why_correct": "存储卷网关提供了 iSCSI 接口，允许本地服务器挂载网关存储卷，从而实现对 AWS 上备份数据的本地访问。同时备份数据也会自动备份到 S3。",
      "why_wrong": "Snowball 用于离线数据传输，不提供持续的备份能力，也不能本地访问。缓存卷网关会将缓存数据保存在本地，备份数据保存在 S3，不支持本地访问。"
    },
    "related_terms": [
      "Storage Gateway",
      "S3",
      "Snowball"
    ]
  },
  {
    "id": 294,
    "topic": "1",
    "question_en": "An application that is hosted on Amazon EC2 instances needs to access an Amazon S3 bucket. Trafic must not traverse the internet. How should a solutions architect configure access to meet these requirements?",
    "options_en": {
      "A": "Create a private hosted zone by using Amazon Route 53.",
      "B": "Set up a gateway VPC endpoint for Amazon S3 in the VPC.",
      "C": "Configure the EC2 instances to use a NAT gateway to access the S3 bucket.",
      "D": "Establish an AWS Site-to-Site VPN connection between the VPC and the S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "托管在 Amazon EC2 实例上的应用程序需要访问一个 Amazon S3 存储桶。流量不能穿过互联网。解决方案架构师应该如何配置访问以满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Route 53 创建一个私有托管区域。",
      "B": "在 VPC 中为 Amazon S3 设置一个网关 VPC endpoint。",
      "C": "配置 EC2 实例使用 NAT Gateway 访问 S3 存储桶。",
      "D": "在 VPC 和 S3 存储桶之间建立一个 AWS Site-to-Site VPN 连接。"
    },
    "tags": [
      "EC2",
      "VPC",
      "S3",
      "Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 EC2 实例访问 S3 存储桶，且流量不能穿过 Internet。VPC endpoint 是最佳解决方案。",
      "why_correct": "VPC endpoint 允许 EC2 实例通过 VPC 内部的网络访问 S3，而无需经过 Internet 或 NAT Gateway，保证了网络安全。",
      "why_wrong": "Route 53 不提供网络连接。NAT Gateway 提供 Internet 出口，不符合流量不能穿过 Internet 的要求。Site-to-Site VPN 也不满足需求，因为它需要经过 Internet。"
    },
    "related_terms": [
      "EC2",
      "VPC",
      "S3",
      "Route 53",
      "NAT Gateway",
      "VPN",
      "Endpoint"
    ]
  },
  {
    "id": 295,
    "topic": "1",
    "question_en": "An ecommerce company stores terabytes of customer data in the AWS Cloud. The data contains personally identifiable information (PII). The company wants to use the data in three applications. Only one of the applications needs to process the PII. The PII must be removed before the other two applications process the data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the data in an Amazon DynamoDB table. Create a proxy application layer to intercept and process the data that each application requests.",
      "B": "Store the data in an Amazon S3 bucket. Process and transform the data by using S3 Object Lambda before returning the data to the requesting application.",
      "C": "Process the data and store the transformed data in three separate Amazon S3 buckets so that each application has its own custom dataset. Point each application to its respective S3 bucket.",
      "D": "Process the data and store the transformed data in three separate Amazon DynamoDB tables so that each application has its own custom dataset. Point each application to its respective DynamoDB table."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家电子商务公司将数TB的客户数据存储在 AWS Cloud 中。数据包含个人身份信息 (PII)。该公司希望在三个应用程序中使用这些数据。只有一个应用程序需要处理 PII。在另外两个应用程序处理数据之前，必须删除 PII。哪种解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将数据存储在 Amazon DynamoDB 表中。创建一个代理应用程序层来拦截和处理每个应用程序请求的数据。",
      "B": "将数据存储在 Amazon S3 存储桶中。在将数据返回给请求应用程序之前，使用 S3 Object Lambda 处理和转换数据。",
      "C": "处理数据并将转换后的数据存储在三个单独的 Amazon S3 存储桶中，以便每个应用程序都有自己的自定义数据集。将每个应用程序指向其各自的 S3 存储桶。",
      "D": "处理数据并将转换后的数据存储在三个单独的 Amazon DynamoDB 表中，以便每个应用程序都有自己的自定义数据集。将每个应用程序指向其各自的 DynamoDB 表。"
    },
    "tags": [
      "S3",
      "PII",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n此题考察 PII 数据处理。需要对数据进行脱敏，并减少运维成本。S3 Object Lambda 是最佳选择。",
      "why_correct": "S3 Object Lambda 允许在数据返回给应用程序之前进行转换，从而实现 PII 的删除或匿名化。这种方式不需要额外的代理层或多个存储桶，运维成本最低。",
      "why_wrong": "将数据存储在 DynamoDB 不利于存储大数据。使用代理应用程序层增加了复杂性。为每个应用程序创建单独的 S3 存储桶会增加存储和管理成本，且数据一致性难以保证。将数据存储在 DynamoDB 会增加复杂性。"
    },
    "related_terms": [
      "S3",
      "PII",
      "DynamoDB",
      "Object Lambda"
    ]
  },
  {
    "id": 296,
    "topic": "1",
    "question_en": "A development team has launched a new application that is hosted on Amazon EC2 instances inside a development VPC. A solutions architect needs to create a new VPC in the same account. The new VPC will be peered with the development VPC. The VPC CIDR block for the development VPC is 192.168.0.0/24. The solutions architect needs to create a CIDR block for the new VPC. The CIDR block must be valid for a VPC peering connection to the development VPC. What is the SMALLEST CIDR block that meets these requirements?",
    "options_en": {
      "A": "10.0.1.0/32",
      "B": "192.168.0.0/24",
      "C": "192.168.1.0/32",
      "D": "10.0.1.0/24"
    },
    "correct_answer": "D",
    "vote_percentage": "98%",
    "question_cn": "一个开发团队启动了一个新的应用程序，该应用程序托管在开发 VPC 中的 Amazon EC2 实例上。 一位解决方案架构师需要在同一账户中创建一个新的 VPC。 新的 VPC 将与开发 VPC 对等互连。 开发 VPC 的 VPC CIDR 块是 192.168.0.0/24。 解决方案架构师需要为新的 VPC 创建一个 CIDR 块。 CIDR 块必须对与开发 VPC 的 VPC 对等连接有效。 满足这些要求的最小 CIDR 块是什么？",
    "options_cn": {
      "A": "10.0.1.0/32",
      "B": "192.168.0.0/24",
      "C": "192.168.1.0/32",
      "D": "10.0.1.0/24"
    },
    "tags": [
      "VPC",
      "CIDR",
      "Networking"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 98%），解析仅供参考。】\n\n考查 VPC 对等连接的 CIDR 块规划，以及 CIDR 块的有效性和最小化。",
      "why_correct": "选项 D (10.0.1.0/24) 是满足要求的最小 CIDR 块。它与开发 VPC 的 CIDR 块 (192.168.0.0/24) 不重叠，从而允许建立 VPC 对等连接。 /24 的子网掩码可以提供足够的 IP 地址供应用程序使用。",
      "why_wrong": "选项 A (10.0.1.0/32) 和选项 C (192.168.1.0/32) 都使用了 /32 的子网掩码，这意味着它们只能包含一个 IP 地址，这对于应用程序来说通常不够用。选项 B (192.168.0.0/24) 与开发 VPC 的 CIDR 块重叠，不能建立对等连接，因为 CIDR 块需要不重叠。"
    },
    "related_terms": [
      "VPC",
      "CIDR",
      "VPC peering",
      "EC2"
    ]
  },
  {
    "id": 297,
    "topic": "1",
    "question_en": "A company deploys an application on five Amazon EC2 instances. An Application Load Balancer (ALB) distributes trafic to the instances by using a target group. The average CPU usage on each of the instances is below 10% most of the time, with occasional surges to 65%. A solutions architect needs to implement a solution to automate the scalability of the application. The solution must optimize the cost of the architecture and must ensure that the application has enough CPU resources when surges occur. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon CloudWatch alarm that enters the ALARM state when the CPUUtilization metric is less than 20%. Create an AWS Lambda function that the CloudWatch alarm invokes to terminate one of the EC2 instances in the ALB target group.",
      "B": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set a target tracking scaling policy that is based on the ASGAverageCPUUtilization metric. Set the minimum instances to 2, the desired capacity to 3, the maximum instances to 6, and the target value to 50%. Add the EC2 instances to the Auto Scaling group.",
      "C": "Create an EC2 Auto Scaling group. Select the existing ALB as the load balancer and the existing target group as the target group. Set the minimum instances to 2, the desired capacity to 3, and the maximum instances to 6. Add the EC2 instances to the Auto Scaling group.",
      "D": "Create two Amazon CloudWatch alarms. Configure the first CloudWatch alarm to enter the ALARM state when the average CPUUtilization metric is below 20%. Configure the second CloudWatch alarm to enter the ALARM state when the average CPUUtilization matric is above 50%. Configure the alarms to publish to an Amazon Simple Notification Service (Amazon SNS) topic to send an email message. After receiving the message, log in to decrease or increase the number of EC2 instances that are running."
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一家公司在五个 Amazon EC2 实例上部署了一个应用程序。一个 Application Load Balancer (ALB) 通过使用目标组将流量分配到这些实例。大多数时候，每个实例的平均 CPU 使用率低于 10%，偶尔会激增到 65%。一位解决方案架构师需要实施一个解决方案来自动化应用程序的可伸缩性。该解决方案必须优化架构的成本，并确保应用程序在激增时拥有足够的 CPU 资源。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon CloudWatch 警报，当 CPUUtilization 指标小于 20% 时进入 ALARM 状态。创建一个 AWS Lambda 函数， CloudWatch 警报会调用该函数来终止 ALB 目标组中的一个 EC2 实例。",
      "B": "创建一个 EC2 Auto Scaling 组。选择现有的 ALB 作为负载均衡器，并选择现有的目标组作为目标组。设置一个基于 ASGAverageCPUUtilization 指标的目标跟踪扩展策略。将最小实例数设置为 2，期望容量设置为 3，最大实例数设置为 6，目标值设置为 50%。将 EC2 实例添加到 Auto Scaling 组。",
      "C": "创建一个 EC2 Auto Scaling 组。选择现有的 ALB 作为负载均衡器，并选择现有的目标组作为目标组。将最小实例数设置为 2，期望容量设置为 3，最大实例数设置为 6。将 EC2 实例添加到 Auto Scaling 组。",
      "D": "创建两个 Amazon CloudWatch 警报。将第一个 CloudWatch 警报配置为在平均 CPUUtilization 指标低于 20% 时进入 ALARM 状态。将第二个 CloudWatch 警报配置为在平均 CPUUtilization 指标高于 50% 时进入 ALARM 状态。配置警报以发布到 Amazon Simple Notification Service (Amazon SNS) 主题以发送电子邮件消息。在收到消息后，登录以减少或增加正在运行的 EC2 实例的数量。"
    },
    "tags": [
      "EC2",
      "ALB",
      "Auto Scaling",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n考查利用 EC2 Auto Scaling 实现应用程序的自动伸缩，并结合 Application Load Balancer (ALB) 和 CPU 利用率指标进行优化。",
      "why_correct": "选项 B 提供了最经济高效的解决方案。通过创建 EC2 Auto Scaling 组 (ASG)，并配置基于 ASGAverageCPUUtilization 指标的目标跟踪扩展策略，可以根据 CPU 使用率自动调整 EC2 实例的数量。设置最小实例数、期望容量和最大实例数，以及目标值，确保了应用程序在不同负载下的可用性和成本效益。",
      "why_wrong": "选项 A 错误，因为终止 EC2 实例会导致服务中断，这与题目要求不符。而且，CPU 利用率低于 20% 时终止实例也无法解决峰值负载问题。选项 C 错误，虽然创建了 ASG，但没有配置伸缩策略，无法根据 CPU 使用率进行自动伸缩。选项 D 错误，虽然 CloudWatch 警报可以监控 CPU 使用率，但需要手动增加或减少实例，无法实现自动伸缩，并且增加了运维成本。"
    },
    "related_terms": [
      "Amazon EC2",
      "Application Load Balancer (ALB)",
      "CPUUtilization",
      "EC2 Auto Scaling",
      "ASGAverageCPUUtilization",
      "Amazon CloudWatch",
      "AWS Lambda",
      "Amazon Simple Notification Service (Amazon SNS)"
    ]
  },
  {
    "id": 298,
    "topic": "1",
    "question_en": "A company is running a critical business application on Amazon EC2 instances behind an Application Load Balancer. The EC2 instances run in an Auto Scaling group and access an Amazon RDS DB instance. The design did not pass an operational review because the EC2 instances and the DB instance are all located in a single Availability Zone. A solutions architect must update the design to use a second Availability Zone. Which solution will make the application highly available?",
    "options_en": {
      "A": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.",
      "B": "Provision two subnets that extend across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance with connections to each network.",
      "C": "Provision a subnet in each Availability Zone. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment.",
      "D": "Provision a subnet that extends across both Availability Zones. Configure the Auto Scaling group to distribute the EC2 instances across both Availability Zones. Configure the DB instance for Multi-AZ deployment."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在Application Load Balancer之后使用Amazon EC2实例运行关键业务应用程序。EC2实例在Auto Scaling组中运行并访问Amazon RDS DB实例。该设计未通过运营审查，因为EC2实例和DB实例都位于单个可用区中。一个解决方案架构师必须更新设计以使用第二个可用区。哪个解决方案将使应用程序具有高可用性？",
    "options_cn": {
      "A": "在每个可用区中预置一个子网。配置Auto Scaling组以跨两个可用区分配EC2实例。配置DB实例与每个网络建立连接。",
      "B": "预置两个跨越两个可用区的子网。配置Auto Scaling组以跨两个可用区分配EC2实例。配置DB实例与每个网络建立连接。",
      "C": "在每个可用区中预置一个子网。配置Auto Scaling组以跨两个可用区分配EC2实例。为DB实例配置Multi-AZ部署。",
      "D": "预置一个跨越两个可用区的子网。配置Auto Scaling组以跨两个可用区分配EC2实例。为DB实例配置Multi-AZ部署。"
    },
    "tags": [
      "EC2",
      "ALB",
      "RDS",
      "High Availability",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察如何在跨可用区部署应用程序时实现高可用性，包括EC2实例的Auto Scaling和RDS数据库的配置。",
      "why_correct": "选项 C 提供了实现高可用性的最佳组合。它将EC2实例部署在两个可用区，并通过Auto Scaling组实现实例的负载均衡。同时，它为 RDS 数据库配置了 Multi-AZ 部署，确保数据库在不同可用区的冗余，从而提高整体应用程序的可用性。",
      "why_wrong": "选项 A 错误在于没有为 RDS 数据库配置 Multi-AZ 部署，数据库单点故障会影响整体应用的可用性。选项 B 错误在于创建了每个可用区都连接到数据库的单独子网，而并非最佳实践。选项 D 错误在于只有一个跨可用区的子网，这不符合在不同可用区部署的原则。同时，虽然配置了 Multi-AZ 部署，但单子网的设计不能充分利用可用区之间的隔离性，从而影响高可用性。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "DB instance",
      "Multi-AZ deployment",
      "Availability Zone",
      "Subnet"
    ]
  },
  {
    "id": 299,
    "topic": "1",
    "question_en": "A research laboratory needs to process approximately 8 TB of data. The laboratory requires sub-millisecond latencies and a minimum throughput of 6 GBps for the storage subsystem. Hundreds of Amazon EC2 instances that run Amazon Linux will distribute and process the data. Which solution will meet the performance requirements?",
    "options_en": {
      "A": "Create an Amazon FSx for NetApp ONTAP file system. Sat each volume’ tiering policy to ALL. Import the raw data into the file system. Mount the fila system on the EC2 instances.",
      "B": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent SSD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.",
      "C": "Create an Amazon S3 bucket to store the raw data. Create an Amazon FSx for Lustre file system that uses persistent HDD storage. Select the option to import data from and export data to Amazon S3. Mount the file system on the EC2 instances.",
      "D": "Create an Amazon FSx for NetApp ONTAP file system. Set each volume’s tiering policy to NONE. Import the raw data into the file system. Mount the file system on the EC2 instances."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个研究实验室需要处理大约 8 TB 的数据。该实验室需要亚毫秒级的延迟和存储子系统的最低 6 GBps 吞吐量。数百个运行 Amazon Linux 的 Amazon EC2 实例将分发和处理数据。哪种解决方案将满足性能要求？",
    "options_cn": {
      "A": "创建一个 Amazon FSx for NetApp ONTAP 文件系统。将每个卷的分层策略设置为 ALL。将原始数据导入文件系统。在 EC2 实例上挂载文件系统。",
      "B": "创建一个 Amazon S3 存储桶来存储原始数据。创建一个使用持久性 SSD 存储的 Amazon FSx for Lustre 文件系统。选择从 Amazon S3 导入数据和将数据导出到 Amazon S3 的选项。在 EC2 实例上挂载文件系统。",
      "C": "创建一个 Amazon S3 存储桶来存储原始数据。创建一个使用持久性 HDD 存储的 Amazon FSx for Lustre 文件系统。选择从 Amazon S3 导入数据和将数据导出到 Amazon S3 的选项。在 EC2 实例上挂载文件系统。",
      "D": "创建一个 Amazon FSx for NetApp ONTAP 文件系统。将每个卷的分层策略设置为 NONE。将原始数据导入文件系统。在 EC2 实例上挂载文件系统。"
    },
    "tags": [
      "FSx for Lustre",
      "S3",
      "EC2",
      "Performance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何为需要低延迟和高吞吐量的 EC2 实例选择合适的存储解决方案。",
      "why_correct": "Amazon FSx for Lustre 专为高性能计算工作负载设计，能够提供亚毫秒级延迟和高吞吐量。结合持久性 SSD 存储，可以满足题目的性能要求。通过 S3 导入/导出数据，方便与 S3 存储桶中的原始数据进行交互。",
      "why_wrong": "选项 A 和 D 使用 Amazon FSx for NetApp ONTAP，虽然也提供文件系统服务，但 NetApp ONTAP 主要针对通用文件共享和数据管理，不以低延迟和高吞吐量为主要优化目标，无法满足题目要求。选项 C 使用持久性 HDD 存储的 FSx for Lustre，虽然 FSx for Lustre 满足高性能需求，但 HDD 无法提供亚毫秒级的延迟，不能满足题目要求。"
    },
    "related_terms": [
      "Amazon FSx for NetApp ONTAP",
      "Amazon EC2",
      "Amazon Linux",
      "Amazon FSx for Lustre",
      "Amazon S3",
      "SSD",
      "HDD"
    ]
  },
  {
    "id": 300,
    "topic": "1",
    "question_en": "A company needs to migrate a legacy application from an on-premises data center to the AWS Cloud because of hardware capacity constraints. The application runs 24 hours a day, 7 days a week. The application’s database storage continues to grow over time. What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Migrate the application layer to Amazon EC2 Spot Instances. Migrate the data storage layer to Amazon S3.",
      "B": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon RDS On-Demand Instances.",
      "C": "Migrate the application layer to Amazon EC2 Reserved Instances. Migrate the data storage layer to Amazon Aurora Reserved Instances.",
      "D": "Migrate the application layer to Amazon EC2 On-Demand Instances. Migrate the data storage layer to Amazon RDS Reserved Instances."
    },
    "correct_answer": "C",
    "vote_percentage": "86%",
    "question_cn": "由于硬件容量限制，一家公司需要将其旧版应用程序从本地数据中心迁移到 AWS 云。该应用程序每周 7 天、每天 24 小时运行。应用程序的数据库存储空间随着时间的推移不断增长。 解决方案架构师应该怎么做才能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将应用程序层迁移到 Amazon EC2 Spot 实例。将数据存储层迁移到 Amazon S3。",
      "B": "将应用程序层迁移到 Amazon EC2 Reserved 实例。将数据存储层迁移到 Amazon RDS On-Demand 实例。",
      "C": "将应用程序层迁移到 Amazon EC2 Reserved 实例。将数据存储层迁移到 Amazon Aurora Reserved 实例。",
      "D": "将应用程序层迁移到 Amazon EC2 On-Demand 实例。将数据存储层迁移到 Amazon RDS Reserved 实例。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Cost Optimization",
      "Reserved Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 86%），解析仅供参考。】\n\n此题考察如何以最具成本效益的方式进行迁移。对于持续运行的应用程序，Reserved Instances 提供最大的成本节约。",
      "why_correct": "将应用程序层迁移到 Reserved Instances 可以节省成本。将数据层迁移到 Aurora Reserved Instances 也可以节省成本，Aurora 提供了高性能和高可用性。",
      "why_wrong": "将应用程序层迁移到 Spot 实例，会增加应用程序的中断风险。将数据层迁移到 On-Demand 实例无法最大化成本节约。将数据层迁移到 RDS On-Demand 实例无法最大化成本节约。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "Aurora",
      "Reserved Instances",
      "Spot Instances",
      "On-Demand Instances"
    ]
  },
  {
    "id": 301,
    "topic": "1",
    "question_en": "A university research laboratory needs to migrate 30 TB of data from an on-premises Windows file server to Amazon FSx for Windows File Server. The laboratory has a 1 Gbps network link that many other departments in the university share. The laboratory wants to implement a data migration service that will maximize the performance of the data transfer. However, the laboratory needs to be able to control the amount of bandwidth that the service uses to minimize the impact on other departments. The data migration must take place within the next 5 days. Which AWS solution will meet these requirements?",
    "options_en": {
      "A": "AWS Snowcone",
      "B": "Amazon FSx File Gateway",
      "C": "AWS DataSync",
      "D": "AWS Transfer Family"
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一所大学研究实验室需要将 30 TB 的数据从本地 Windows 文件服务器迁移到 Amazon FSx for Windows File Server。该实验室拥有一个 1 Gbps 的网络链接，该链接由大学的许多其他部门共享。实验室希望实施一个数据迁移服务，以最大限度地提高数据传输的性能。然而，实验室需要能够控制该服务使用的带宽量，以最大限度地减少对其他部门的影响。数据迁移必须在接下来的 5 天内完成。以下哪个 AWS 解决方案将满足这些要求？",
    "options_cn": {
      "A": "AWS Snowcone",
      "B": "Amazon FSx File Gateway",
      "C": "AWS DataSync",
      "D": "AWS Transfer Family"
    },
    "tags": [
      "DataSync",
      "FSx for Windows File Server",
      "Snowcone",
      "Transfer Family"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n该题考查数据迁移方案的选择。 AWS DataSync 是一种在线数据传输服务，可以安全地在本地存储和 AWS 存储服务之间以及 AWS 存储服务之间传输数据。DataSync 可以控制带宽使用，满足题干对带宽控制的要求，并且可以满足 5 天内完成的需求。",
      "why_correct": "C 选项 DataSync 提供了带宽控制功能，可以满足题目需求。",
      "why_wrong": "A 选项 AWS Snowcone 适用于大量数据的物理迁移，不适合控制带宽；B 选项 Amazon FSx File Gateway 是一个文件网关服务，用于连接本地环境和 FSx，不涉及数据迁移；D 选项 AWS Transfer Family 用于通过 SFTP、FTPS 和 FTP 协议传输文件，不是为大量数据迁移设计的。"
    },
    "related_terms": [
      "AWS DataSync",
      "FSx for Windows File Server",
      "Snowcone",
      "Transfer Family",
      "Windows",
      "1 Gbps"
    ]
  },
  {
    "id": 302,
    "topic": "1",
    "question_en": "A company wants to create a mobile app that allows users to stream slow-motion video clips on their mobile devices. Currently, the app captures video clips and uploads the video clips in raw format into an Amazon S3 bucket. The app retrieves these video clips directly from the S3 bucket. However, the videos are large in their raw format. Users are experiencing issues with buffering and playback on mobile devices. The company wants to implement solutions to maximize the performance and scalability of the app while minimizing operational overhead. Which combination of solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Deploy Amazon CloudFront for content delivery and caching.",
      "B": "Use AWS DataSync to replicate the video files across AW'S Regions in other S3 buckets.",
      "C": "Use Amazon Elastic Transcoder to convert the video files to more appropriate formats.",
      "D": "Deploy an Auto Sealing group of Amazon EC2 instances in Local Zones for content delivery and caching",
      "E": "Deploy an Auto Scaling group of Amazon EC2 instances to convert the video files to more appropriate formats."
    },
    "correct_answer": "AC",
    "vote_percentage": "57%",
    "question_cn": "一家公司希望创建一个移动应用程序，允许用户在其移动设备上流式传输慢动作视频片段。目前，该应用程序捕获视频片段并将原始格式的视频片段上传到 Amazon S3 存储桶。该应用程序直接从 S3 存储桶检索这些视频片段。但是，视频以原始格式存储时非常大。用户在使用移动设备进行缓冲和播放时遇到问题。该公司希望实施解决方案，以最大限度地提高应用程序的性能和可扩展性，同时最大限度地减少运营开销。哪两种解决方案的组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "部署 Amazon CloudFront 进行内容交付和缓存。",
      "B": "使用 AWS DataSync 将视频文件复制到其他 AWS 区域的 S3 存储桶。",
      "C": "使用 Amazon Elastic Transcoder 将视频文件转换为更合适的格式。",
      "D": "在本地区域部署 Amazon EC2 实例的 Auto Scaling 组，用于内容交付和缓存。",
      "E": "部署 Amazon EC2 实例的 Auto Scaling 组，将视频文件转换为更合适的格式。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "Elastic Transcoder",
      "EC2",
      "Auto Scaling",
      "DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 57%），解析仅供参考。】\n\n本题考查视频流媒体解决方案。 结合 CloudFront 和 Amazon S3 可以实现高效的内容分发。CloudFront 可以缓存视频内容，减少源站的负载，从而提高性能；同时，可以优化用户的观看体验。使用 Amazon Elastic Transcoder 将视频转码成更适合流式传输的格式可以提升性能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AC。理由简述：A 选项 CloudFront 能够加速内容分发，实现内容缓存，从而提高性能；C 选项可以使用 Amazon Elastic Transcoder 将视频文件转换为更合适的格式，提升性能。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项 DataSync 主要用于数据同步和迁移，与流媒体应用场景关系不大；D 选项 部署 EC2 实例进行内容分发和缓存，运维成本高，不如 CloudFront；E 选项部署 EC2 实例将视频文件转换为更合适的格式，需要自己管理实例，成本和复杂度较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "S3",
      "EC2",
      "Auto Scaling",
      "AWS DataSync",
      "Amazon Elastic Transcoder"
    ]
  },
  {
    "id": 303,
    "topic": "1",
    "question_en": "A company is launching a new application deployed on an Amazon Elastic Container Service (Amazon ECS) cluster and is using the Fargate launch type for ECS tasks. The company is monitoring CPU and memory usage because it is expecting high trafic to the application upon its launch. However, the company wants to reduce costs when utilization decreases. What should a solutions architect recommend?",
    "options_en": {
      "A": "Use Amazon EC2 Auto Scaling to scale at certain periods based on previous trafic patterns.",
      "B": "Use an AWS Lambda function to scale Amazon ECS based on metric breaches that trigger an Amazon CloudWatch alarm.",
      "C": "Use Amazon EC2 Auto Scaling with simple scaling policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm.",
      "D": "Use AWS Application Auto Scaling with target tracking policies to scale when ECS metric breaches trigger an Amazon CloudWatch alarm."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在推出部署在 Amazon Elastic Container Service (Amazon ECS) 集群上的新应用程序，并且正在为 ECS 任务使用 Fargate 启动类型。该公司正在监控 CPU 和内存使用情况，因为它预计该应用程序在启动后会有高流量。但是，当利用率下降时，该公司希望降低成本。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "使用 Amazon EC2 Auto Scaling 根据之前的流量模式在特定时间段进行扩展。",
      "B": "使用 AWS Lambda 函数根据触发 Amazon CloudWatch 警报的指标突破来扩展 Amazon ECS。",
      "C": "使用 Amazon EC2 Auto Scaling 和简单扩展策略，以便在 ECS 指标突破触发 Amazon CloudWatch 警报时进行扩展。",
      "D": "使用 AWS Application Auto Scaling 和目标跟踪策略，以便在 ECS 指标突破触发 Amazon CloudWatch 警报时进行扩展。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "Auto Scaling",
      "CloudWatch",
      "Application Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考察 ECS 弹性伸缩策略的选择。 使用 Application Auto Scaling 可以根据 ECS 的指标自动伸缩任务数量。目标跟踪策略可以基于 CPU、内存等指标进行伸缩，满足动态调整的需求。",
      "why_correct": "D 选项使用 Application Auto Scaling 搭配目标跟踪策略，可以根据 ECS 指标实现自动弹性伸缩。",
      "why_wrong": "A 选项使用 EC2 Auto Scaling 不适用于 Fargate；B 选项使用 Lambda 函数扩展 ECS，实现复杂且运维成本高；C 选项使用 EC2 Auto Scaling 不适用于 Fargate。"
    },
    "related_terms": [
      "Amazon ECS",
      "Fargate",
      "EC2",
      "Auto Scaling",
      "CloudWatch",
      "Application Auto Scaling"
    ]
  },
  {
    "id": 304,
    "topic": "1",
    "question_en": "A company recently created a disaster recovery site in a different AWS Region. The company needs to transfer large amounts of data back and forth between NFS file systems in the two Regions on a periodic basis. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS DataSync.",
      "B": "Use AWS Snowball devices.",
      "C": "Set up an SFTP server on Amazon EC2.",
      "D": "Use AWS Database Migration Service (AWS DMS)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司最近在另一个 AWS 区域创建了一个灾难恢复站点。该公司需要定期间隔地在两个区域的 NFS 文件系统之间来回传输大量数据。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync。",
      "B": "使用 AWS Snowball 设备。",
      "C": "在 Amazon EC2 上设置 SFTP 服务器。",
      "D": "使用 AWS Database Migration Service (AWS DMS)。"
    },
    "tags": [
      "DataSync",
      "Snowball",
      "EC2",
      "SFTP",
      "DMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查数据同步方案。AWS DataSync 专门用于在不同存储位置之间同步数据，尤其适用于跨区域数据同步。其自动化、托管的特性可以减少运营开销。",
      "why_correct": "A 选项 DataSync 专门用于数据同步，能满足题目的需求。",
      "why_wrong": "B 选项 AWS Snowball 适用于大量数据的离线传输；C 选项 在 EC2 上设置 SFTP 服务器需要自己维护，运维成本高；D 选项 AWS DMS 主要用于数据库迁移，不适合通用数据同步。"
    },
    "related_terms": [
      "AWS DataSync",
      "Snowball",
      "EC2",
      "SFTP",
      "DMS"
    ]
  },
  {
    "id": 305,
    "topic": "1",
    "question_en": "A company is designing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use SMB clients to access data. The solution must be fully managed. Which AWS solution meets these requirements?",
    "options_en": {
      "A": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.",
      "B": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
      "C": "Create an Amazon FSx for Windows File Server file system. Attach the file system to the origin server. Connect the application server to the file system.",
      "D": "Create an Amazon S3 bucket. Assign an IAM role to the application to grant access to the S3 bucket. Mount the S3 bucket to the application server."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为托管在 AWS 云中的游戏应用程序设计共享存储解决方案。该公司需要能够使用 SMB 客户端访问数据。该解决方案必须是完全托管的。哪种 AWS 解决方案满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS DataSync 任务，将数据共享为可挂载的文件系统。将文件系统挂载到应用程序服务器。",
      "B": "创建一个 Amazon EC2 Windows 实例。在该实例上安装并配置 Windows 文件共享角色。将应用程序服务器连接到文件共享。",
      "C": "创建一个 Amazon FSx for Windows File Server 文件系统。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。",
      "D": "创建一个 Amazon S3 存储桶。为应用程序分配一个 IAM 角色以授予对 S3 存储桶的访问权限。将 S3 存储桶挂载到应用程序服务器。"
    },
    "tags": [
      "FSx for Windows File Server",
      "DataSync",
      "EC2",
      "S3",
      "SMB",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察共享存储方案的选择。Amazon FSx for Windows File Server 提供完全托管的 Windows 文件服务器，支持 SMB 协议。可以直接作为应用程序的共享存储使用。",
      "why_correct": "C 选项 FSx for Windows File Server 提供 SMB 支持，完全托管，满足所有需求。",
      "why_wrong": "A 选项 DataSync 无法提供可挂载的文件系统；B 选项 EC2 需要自己管理，不是完全托管，且配置复杂；D 选项 S3 无法直接挂载到应用程序服务器。"
    },
    "related_terms": [
      "Amazon FSx for Windows File Server",
      "AWS DataSync",
      "EC2",
      "S3",
      "SMB",
      "IAM"
    ]
  },
  {
    "id": 306,
    "topic": "1",
    "question_en": "A company wants to run an in-memory database for a latency-sensitive application that runs on Amazon EC2 instances. The application processes more than 100,000 transactions each minute and requires high network throughput. A solutions architect needs to provide a cost- effective network design that minimizes data transfer charges. Which solution meets these requirements?",
    "options_en": {
      "A": "Launch all EC2 instances in the same Availability Zone within the same AWS Region. Specify a placement group with cluster strategy when launching EC2 instances.",
      "B": "Launch all EC2 instances in different Availability Zones within the same AWS Region. Specify a placement group with partition strategy when launching EC2 instances.",
      "C": "Deploy an Auto Scaling group to launch EC2 instances in different Availability Zones based on a network utilization target.",
      "D": "Deploy an Auto Scaling group with a step scaling policy to launch EC2 instances in different Availability Zones."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望为在 Amazon EC2 实例上运行的对延迟敏感的应用程序运行内存数据库。该应用程序每分钟处理超过 100,000 笔交易，并且需要高网络吞吐量。 解决方案架构师需要提供一个经济高效的网络设计，以最大限度地减少数据传输费用。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "在同一 AWS 区域内的同一可用区中启动所有 EC2 实例。在启动 EC2 实例时，指定具有集群策略的放置组。",
      "B": "在同一 AWS 区域内的不同可用区中启动所有 EC2 实例。在启动 EC2 实例时，指定具有分区策略的放置组。",
      "C": "部署一个 Auto Scaling 组，以根据网络利用率目标在不同的可用区中启动 EC2 实例。",
      "D": "部署一个具有步进扩展策略的 Auto Scaling 组，以在不同的可用区中启动 EC2 实例。"
    },
    "tags": [
      "EC2",
      "Placement Group",
      "Availability Zone",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查数据库实例的网络性能优化。在同一可用区启动 EC2 实例，并使用具有集群策略的放置组可以最大程度地减少延迟，并提高网络吞吐量。",
      "why_correct": "A 选项 在同一可用区启动 EC2 实例，并使用具有集群策略的放置组，可以最大程度减少延迟。",
      "why_wrong": "B 选项 在不同可用区部署实例会增加延迟；C 和 D 选项 使用 Auto Scaling 会导致实例分布在多个可用区，无法保证最低延迟。"
    },
    "related_terms": [
      "EC2",
      "Availability Zone",
      "Auto Scaling",
      "Placement Group"
    ]
  },
  {
    "id": 307,
    "topic": "1",
    "question_en": "A company that primarily runs its application servers on premises has decided to migrate to AWS. The company wants to minimize its need to scale its Internet Small Computer Systems Interface (iSCSI) storage on premises. The company wants only its recently accessed data to remain stored locally. Which AWS solution should the company use to meet these requirements?",
    "options_en": {
      "A": "Amazon S3 File Gateway",
      "B": "AWS Storage Gateway Tape Gateway",
      "C": "AWS Storage Gateway Volume Gateway stored volumes",
      "D": "AWS Storage Gateway Volume Gateway cached volumes"
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家主要在其本地运行应用程序服务器的公司已决定迁移到 AWS。该公司希望尽量减少其对本地 Internet Small Computer Systems Interface (iSCSI) 存储进行扩展的需求。该公司希望只有最近访问的数据保留在本地存储。该公司应该使用哪个 AWS 解决方案来满足这些要求？",
    "options_cn": {
      "A": "Amazon S3 File Gateway",
      "B": "AWS Storage Gateway Tape Gateway",
      "C": "AWS Storage Gateway Volume Gateway stored volumes",
      "D": "AWS Storage Gateway Volume Gateway cached volumes"
    },
    "tags": [
      "S3 File Gateway",
      "Storage Gateway",
      "Tape Gateway",
      "Volume Gateway",
      "iSCSI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查 AWS Storage Gateway 中 Volume Gateway 的不同存储模式，以及如何使用缓存存储来减少本地存储需求。",
      "why_correct": "选项 D，AWS Storage Gateway Volume Gateway cached volumes，是满足需求的最佳方案。它允许将最近访问的数据缓存在本地，而将不经常访问的数据存储在 Amazon S3 中，从而减少了对本地 iSCSI 存储的需求。缓存卷能够提供对数据的低延迟访问，同时降低本地存储的成本。",
      "why_wrong": "选项 A，Amazon S3 File Gateway，主要用于将本地文件共享连接到 Amazon S3，不直接与 iSCSI 存储交互，且无法实现数据本地缓存。选项 B，AWS Storage Gateway Tape Gateway，用于创建和管理虚拟磁带，与题目中对最近访问数据的需求不符。选项 C，AWS Storage Gateway Volume Gateway stored volumes，将所有数据存储在本地，与减少本地存储的需求相悖。"
    },
    "related_terms": [
      "AWS Storage Gateway",
      "Amazon S3",
      "iSCSI",
      "File Gateway",
      "Tape Gateway",
      "Volume Gateway",
      "cached volumes",
      "stored volumes"
    ]
  },
  {
    "id": 308,
    "topic": "1",
    "question_en": "A company has multiple AWS accounts that use consolidated billing. The company runs several active high performance Amazon RDS for Oracle On-Demand DB instances for 90 days. The company’s finance team has access to AWS Trusted Advisor in the consolidated billing account and all other AWS accounts. The finance team needs to use the appropriate AWS account to access the Trusted Advisor check recommendations for RDS. The finance team must review the appropriate Trusted Advisor check to reduce RDS costs. Which combination of steps should the finance team take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use the Trusted Advisor recommendations from the account where the RDS instances are running.",
      "B": "Use the Trusted Advisor recommendations from the consolidated billing account to see all RDS instance checks at the same time.",
      "C": "Review the Trusted Advisor check for Amazon RDS Reserved Instance Optimization.",
      "D": "Review the Trusted Advisor check for Amazon RDS Idle DB Instances",
      "E": "Review the Trusted Advisor check for Amazon Redshift Reserved Node Optimization."
    },
    "correct_answer": "BD",
    "vote_percentage": "61%",
    "question_cn": "一家公司有多个使用合并账单的 AWS 账户。该公司运行多个活跃的高性能 Amazon RDS for Oracle 按需 DB 实例 90 天。该公司的财务团队可以在合并账单账户和所有其他 AWS 账户中访问 AWS Trusted Advisor。财务团队需要使用适当的 AWS 账户来访问 Trusted Advisor 检查 RDS 的建议。财务团队必须查看适当的 Trusted Advisor 检查以减少 RDS 成本。财务团队应采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用运行 RDS 实例的账户中的 Trusted Advisor 建议。",
      "B": "使用合并账单账户中的 Trusted Advisor 建议，同时查看所有 RDS 实例检查。",
      "C": "查看 Amazon RDS 保留实例优化 的 Trusted Advisor 检查。",
      "D": "查看 Amazon RDS 空闲数据库实例 的 Trusted Advisor 检查。",
      "E": "查看 Amazon Redshift 保留节点优化 的 Trusted Advisor 检查。"
    },
    "tags": [
      "Trusted Advisor",
      "RDS",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 61%），解析仅供参考。】\n\n考查 AWS Trusted Advisor 的使用场景，以及在合并账单环境下，财务团队如何通过 Trusted Advisor 优化 RDS 成本。",
      "why_correct": "B：合并账单账户可以访问所有成员账户的 Trusted Advisor 检查结果，包括 RDS 实例检查，满足了财务团队在合并账单账户中查看所有 RDS 相关检查的需求。D：查看空闲数据库实例检查可以帮助财务团队识别并关闭未使用的 RDS 实例，从而减少成本，符合题目中“减少 RDS 成本”的要求。",
      "why_wrong": "A：虽然可以在运行 RDS 实例的账户中使用 Trusted Advisor，但财务团队需要查看所有账户的检查结果，而不仅仅是单个账户的。C：题目要求是针对 RDS 成本优化，而 Amazon RDS 保留实例优化是针对保留实例的，与按需实例不直接相关。E：Amazon Redshift 保留节点优化与题目中 RDS 实例不符，Redshift 是另一个数据库服务，不满足题目要求。"
    },
    "related_terms": [
      "AWS Trusted Advisor",
      "Amazon RDS",
      "RDS",
      "Oracle",
      "DB instance",
      "Consolidated Billing",
      "Amazon RDS Reserved Instance Optimization",
      "Amazon RDS Idle Database Instance",
      "Amazon Redshift Reserved Node Optimization"
    ]
  },
  {
    "id": 309,
    "topic": "1",
    "question_en": "A solutions architect needs to optimize storage costs. The solutions architect must identify any Amazon S3 buckets that are no longer being accessed or are rarely accessed. Which solution will accomplish this goal with the LEAST operational overhead?",
    "options_en": {
      "A": "Analyze bucket access patterns by using the S3 Storage Lens dashboard for advanced activity metrics.",
      "B": "Analyze bucket access patterns by using the S3 dashboard in the AWS Management Console.",
      "C": "Turn on the Amazon CloudWatch BucketSizeBytes metric for buckets. Analyze bucket access patterns by using the metrics data with Amazon Athena.",
      "D": "Turn on AWS CloudTrail for S3 object monitoring. Analyze bucket access patterns by using CloudTrail logs that are integrated with Amazon CloudWatch Logs."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一个解决方案架构师需要优化存储成本。该解决方案架构师必须识别不再被访问或很少被访问的 Amazon S3 存储桶。哪个解决方案将以最少的运营开销完成此目标？",
    "options_cn": {
      "A": "使用 S3 Storage Lens 仪表板分析存储桶访问模式以获取高级活动指标。",
      "B": "使用 AWS 管理控制台中 S3 仪表板分析存储桶访问模式。",
      "C": "为存储桶打开 Amazon CloudWatch BucketSizeBytes 指标。使用与 Amazon Athena 的指标数据分析存储桶访问模式。",
      "D": "为 S3 对象监控打开 AWS CloudTrail。使用与 Amazon CloudWatch Logs 集成的 CloudTrail 日志分析存储桶访问模式。"
    },
    "tags": [
      "S3",
      "Storage Lens",
      "CloudWatch",
      "CloudTrail",
      "Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n考查如何通过监控和分析 S3 存储桶的访问模式来优化存储成本，并选择运营开销最小的方案。",
      "why_correct": "S3 Storage Lens 提供了全面的存储使用情况和活动指标，包括访问模式分析，能够帮助识别很少被访问或未被访问的对象。Storage Lens 的仪表板直观易用，且管理开销较低，符合题目要求的以最少运营开销完成此目标。",
      "why_wrong": "选项 B 提供的 S3 仪表板通常只提供基本的存储容量信息，缺乏高级的访问模式分析能力，无法有效识别不常访问的对象。选项 C 依赖于 CloudWatch BucketSizeBytes 指标和 Athena 分析，虽然可以分析存储桶大小，但无法直接分析访问频率。选项 D 使用 CloudTrail 记录对象访问，虽然可以提供访问日志，但配置和分析 CloudTrail 日志的运营开销相对较高，且不如 Storage Lens 提供的指标更直接、更易于使用。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Storage Lens",
      "AWS Management Console",
      "Amazon CloudWatch",
      "BucketSizeBytes",
      "Amazon Athena",
      "AWS CloudTrail",
      "CloudWatch Logs"
    ]
  },
  {
    "id": 310,
    "topic": "1",
    "question_en": "A company sells datasets to customers who do research in artificial intelligence and machine learning (AI/ML). The datasets are large, formatted files that are stored in an Amazon S3 bucket in the us-east-1 Region. The company hosts a web application that the customers use to purchase access to a given dataset. The web application is deployed on multiple Amazon EC2 instances behind an Application Load Balancer. After a purchase is made, customers receive an S3 signed URL that allows access to the files. The customers are distributed across North America and Europe. The company wants to reduce the cost that is associated with data transfers and wants to maintain or improve performance. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Configure S3 Transfer Acceleration on the existing S3 bucket. Direct customer requests to the S3 Transfer Acceleration endpoint. Continue to use S3 signed URLs for access control.",
      "B": "Deploy an Amazon CloudFront distribution with the existing S3 bucket as the origin. Direct customer requests to the CloudFront URL. Switch to CloudFront signed URLs for access control.",
      "C": "Set up a second S3 bucket in the eu-central-1 Region with S3 Cross-Region Replication between the buckets. Direct customer requests to the closest Region. Continue to use S3 signed URLs for access control.",
      "D": "Modify the web application to enable streaming of the datasets to end users. Configure the web application to read the data from the existing S3 bucket. Implement access control directly in the application."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司向从事人工智能和机器学习 (AI/ML) 研究的客户销售数据集。数据集是大型格式化文件，存储在 us-east-1 区域的 Amazon S3 存储桶中。该公司托管一个 Web 应用程序，供客户用来购买对给定数据集的访问权限。Web 应用程序部署在 Application Load Balancer 后面的多个 Amazon EC2 实例上。购买后，客户会收到一个 S3 签名 URL，允许访问这些文件。客户分布在北美和欧洲。该公司希望降低与数据传输相关的成本，并希望保持或提高性能。解决方案架构师应如何做才能满足这些要求？",
    "options_cn": {
      "A": "在现有的 S3 存储桶上配置 S3 Transfer Acceleration。将客户请求定向到 S3 Transfer Acceleration 终端节点。继续使用 S3 签名 URL 进行访问控制。",
      "B": "部署一个 Amazon CloudFront 分发，将现有的 S3 存储桶作为源。将客户请求定向到 CloudFront URL。切换到 CloudFront 签名 URL 进行访问控制。",
      "C": "在 eu-central-1 区域设置第二个 S3 存储桶，并在存储桶之间设置 S3 跨区域复制。将客户请求定向到最近的区域。继续使用 S3 签名 URL 进行访问控制。",
      "D": "修改 Web 应用程序，以允许将数据集流式传输到最终用户。配置 Web 应用程序以从现有的 S3 存储桶读取数据。直接在应用程序中实施访问控制。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "Transfer Acceleration",
      "S3 Transfer Acceleration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察内容分发策略。使用 CloudFront 可以加速内容分发，并支持签名 URL 实现访问控制，可以降低成本并提升性能。",
      "why_correct": "B 选项 部署 CloudFront 分发，将 S3 存储桶作为源，使用 CloudFront 签名 URL 进行访问控制，可以降低成本并提升性能。",
      "why_wrong": "A 选项 使用 S3 Transfer Acceleration 虽然可以提升性能，但无法解决跨区域传输成本；C 选项 跨区域复制增加成本，且没有利用 CDN 的优势；D 选项 修改 Web 应用程序，将内容流式传输会增加复杂度和维护成本。"
    },
    "related_terms": [
      "S3",
      "CloudFront",
      "S3 Transfer Acceleration",
      "Transfer Acceleration"
    ]
  },
  {
    "id": 311,
    "topic": "1",
    "question_en": "A company is using AWS to design a web application that will process insurance quotes. Users will request quotes from the application. Quotes must be separated by quote type, must be responded to within 24 hours, and must not get lost. The solution must maximize operational eficiency and must minimize maintenance. Which solution meets these requirements?",
    "options_en": {
      "A": "Create multiple Amazon Kinesis data streams based on the quote type. Configure the web application to send messages to the proper data stream. Configure each backend group of application servers to use the Kinesis Client Library (KCL) to pool messages from its own data stream.",
      "B": "Create an AWS Lambda function and an Amazon Simple Notification Service (Amazon SNS) topic for each quote type. Subscribe the Lambda function to its associated SNS topic. Configure the application to publish requests for quotes to the appropriate SNS topic.",
      "C": "Create a single Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure SNS message filtering to publish messages to the proper SQS queue based on the quote type. Configure each backend application server to use its own SQS queue.",
      "D": "Create multiple Amazon Kinesis Data Firehose delivery streams based on the quote type to deliver data streams to an Amazon OpenSearch Service cluster. Configure the application to send messages to the proper delivery stream. Configure each backend group of application servers to search for the messages from OpenSearch Service and process them accordingly."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 AWS 设计一个将处理保险报价的 Web 应用程序。用户将从应用程序请求报价。报价必须按报价类型分隔，必须在 24 小时内响应，并且不得丢失。该解决方案必须最大限度地提高运营效率，并最大限度地减少维护。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "根据报价类型创建多个 Amazon Kinesis 数据流。配置 Web 应用程序以将消息发送到适当的数据流。配置每个后端应用程序服务器组，以使用 Kinesis Client Library (KCL) 从其自己的数据流中提取消息。",
      "B": "为每种报价类型创建一个 AWS Lambda 函数和一个 Amazon Simple Notification Service (Amazon SNS) 主题。将 Lambda 函数订阅到其关联的 SNS 主题。配置应用程序以将报价请求发布到相应的 SNS 主题。",
      "C": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题。将 Amazon Simple Queue Service (Amazon SQS) 队列订阅到 SNS 主题。配置 SNS 消息筛选，以根据报价类型将消息发布到适当的 SQS 队列。配置每个后端应用程序服务器以使用其自己的 SQS 队列。",
      "D": "根据报价类型创建多个 Amazon Kinesis Data Firehose 交付流，以将数据流传递到 Amazon OpenSearch Service 集群。配置应用程序以将消息发送到适当的交付流。配置每个后端应用程序服务器组以从 OpenSearch Service 搜索消息并相应地处理它们。"
    },
    "tags": [
      "Kinesis",
      "SNS",
      "SQS",
      "Lambda",
      "OpenSearch Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS 服务构建可靠、可扩展的异步消息处理系统，满足报价处理的需求，包括消息筛选、延迟处理和高可用性。",
      "why_correct": "选项 C 提供了最合适的解决方案。它使用 SNS 作为消息发布/订阅服务，允许应用程序将报价请求发布到 SNS 主题。SQS 队列订阅 SNS 主题，并通过 SNS 消息筛选功能，根据报价类型将消息路由到不同的 SQS 队列。后端应用程序服务器从各自的 SQS 队列中读取消息进行处理。这种设计满足了报价类型分隔、24 小时内响应、消息不丢失的要求，并且最大限度地提高了运营效率和减少了维护成本。",
      "why_wrong": "选项 A 使用 Kinesis 数据流和 KCL，虽然可以实现消息处理，但 Kinesis 主要用于流数据处理，不适合这种异步请求/响应场景，且维护成本较高。选项 B 使用 Lambda 函数订阅 SNS 主题，可能导致过多的 Lambda 函数实例和复杂性，并且难以控制消息的顺序和并发性。选项 D 使用 Kinesis Data Firehose 和 OpenSearch Service，适用于日志分析和搜索，不适用于异步消息队列，且不满足 24 小时内处理的需求，维护成本也较高。"
    },
    "related_terms": [
      "Amazon SNS",
      "Amazon SQS",
      "Kinesis Data Streams",
      "Kinesis Client Library (KCL)",
      "AWS Lambda",
      "Kinesis Data Firehose",
      "Amazon OpenSearch Service"
    ]
  },
  {
    "id": 312,
    "topic": "1",
    "question_en": "A company has an application that runs on several Amazon EC2 instances. Each EC2 instance has multiple Amazon Elastic Block Store (Amazon EBS) data volumes attached to it. The application’s EC2 instance configuration and data need to be backed up nightly. The application also needs to be recoverable in a different AWS Region. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Write an AWS Lambda function that schedules nightly snapshots of the application’s EBS volumes and copies the snapshots to a different Region.",
      "B": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EC2 instances as resources.",
      "C": "Create a backup plan by using AWS Backup to perform nightly backups. Copy the backups to another Region. Add the application’s EBS volumes as resources.",
      "D": "Write an AWS Lambda function that schedules nightly snapshots of the application's EBS volumes and copies the snapshots to a different Availability Zone."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家公司有一个在多个 Amazon EC2 实例上运行的应用程序。每个 EC2 实例都附带了多个 Amazon Elastic Block Store (Amazon EBS) 数据卷。该应用程序的 EC2 实例配置和数据需要每天晚上进行备份。该应用程序也需要在不同的 AWS 区域中可恢复。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "编写一个 AWS Lambda 函数，该函数计划每天晚上拍摄应用程序的 EBS 卷快照，并将快照复制到不同的区域。",
      "B": "通过使用 AWS Backup 创建一个备份计划来执行夜间备份。将备份复制到另一个区域。将应用程序的 EC2 实例添加为资源。",
      "C": "通过使用 AWS Backup 创建一个备份计划来执行夜间备份。将备份复制到另一个区域。将应用程序的 EBS 卷添加为资源。",
      "D": "编写一个 AWS Lambda 函数，该函数计划每天晚上拍摄应用程序的 EBS 卷快照，并将快照复制到不同的可用区。"
    },
    "tags": [
      "EBS",
      "EC2",
      "AWS Backup",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n考查 AWS Backup 服务的使用，包括备份计划的创建、资源的选择以及跨区域备份。评估哪种方案能最有效率地满足 EC2 实例及其 EBS 卷的备份和跨区域恢复需求。",
      "why_correct": "AWS Backup 是一个集中式的备份服务，可以简化和自动化备份过程。选项 B 通过创建一个备份计划，并将应用程序的 EC2 实例作为资源，可以自动备份 EC2 实例及其关联的 EBS 卷。备份计划还支持跨区域复制，满足了跨区域可恢复的需求。这种方案具有最高的运营效率，因为它减少了手动编写和维护 Lambda 函数的工作量。",
      "why_wrong": "选项 A 和 D 使用 Lambda 函数手动管理 EBS 快照的创建和复制，增加了复杂性和维护成本。选项 D 只将快照复制到不同的可用区，不满足题目要求的跨区域可恢复性。选项 C 虽然使用了 AWS Backup，但是将 EBS 卷单独作为资源，无法完整备份 EC2 实例的配置信息，也无法保证应用程序在不同区域的可恢复性，因此不具备最高的运营效率。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS Lambda",
      "AWS Backup",
      "EBS volume",
      "EC2 instance",
      "snapshot",
      "AWS region",
      "Availability Zone"
    ]
  },
  {
    "id": 313,
    "topic": "1",
    "question_en": "A company is building a mobile app on AWS. The company wants to expand its reach to millions of users. The company needs to build a platform so that authorized users can watch the company’s content on their mobile devices. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Publish content to a public Amazon S3 bucket. Use AWS Key Management Service (AWS KMS) keys to stream content.",
      "B": "Set up IPsec VPN between the mobile app and the AWS environment to stream content.",
      "C": "Use Amazon CloudFront. Provide signed URLs to stream content.",
      "D": "Set up AWS Client VPN between the mobile app and the AWS environment to stream content."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上构建一个移动应用程序。该公司希望将其业务扩展到数百万用户。该公司需要构建一个平台，以便授权用户可以在他们的移动设备上观看公司的内容。 解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "将内容发布到公共 Amazon S3 存储桶。 使用 AWS Key Management Service (AWS KMS) 密钥来流式传输内容。",
      "B": "在移动应用程序和 AWS 环境之间设置 IPsec VPN 来流式传输内容。",
      "C": "使用 Amazon CloudFront。 提供已签名的 URL 来流式传输内容。",
      "D": "在移动应用程序和 AWS 环境之间设置 AWS Client VPN 来流式传输内容。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "CDN",
      "Signed URL",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察内容分发方案的选择。使用 CloudFront 结合 Signed URL 可以安全高效地向大量用户提供内容。",
      "why_correct": "C 选项 使用 CloudFront 和 Signed URL 可以安全地向大量用户提供内容。",
      "why_wrong": "A 选项 将内容发布到公共 S3 存储桶不安全；B 选项 VPN 不适合大规模内容分发；D 选项 AWS Client VPN 也不适合大规模内容分发。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "CDN",
      "VPN",
      "Signed URL"
    ]
  },
  {
    "id": 314,
    "topic": "1",
    "question_en": "A company has an on-premises MySQL database used by the global sales team with infrequent access patterns. The sales team requires the database to have minimal downtime. A database administrator wants to migrate this database to AWS without selecting a particular instance type in anticipation of more users in the future. Which service should a solutions architect recommend?",
    "options_en": {
      "A": "Amazon Aurora MySQL",
      "B": "Amazon Aurora Serverless for MySQL",
      "C": "Amazon Redshift Spectrum",
      "D": "Amazon RDS for MySQL"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个内部部署的 MySQL 数据库，供全球销售团队使用，访问模式不频繁。销售团队要求数据库停机时间最短。数据库管理员希望在不选择特定实例类型的情况下将此数据库迁移到 AWS，以应对未来的更多用户。解决方案架构师应该推荐哪种服务？",
    "options_cn": {
      "A": "Amazon Aurora MySQL",
      "B": "Amazon Aurora Serverless for MySQL",
      "C": "Amazon Redshift Spectrum",
      "D": "Amazon RDS for MySQL"
    },
    "tags": [
      "Aurora Serverless",
      "MySQL",
      "RDS",
      "Aurora",
      "Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查数据库迁移方案。Aurora Serverless for MySQL 可以自动调整计算资源，无需指定实例类型，满足数据库管理员的需求。",
      "why_correct": "B 选项 Aurora Serverless for MySQL 可以自动调整计算资源，无需指定实例类型，满足题干的需求。",
      "why_wrong": "A 选项 Aurora MySQL 需要选择实例类型；C 选项 Redshift Spectrum 用于查询数据仓库中的数据，不适用于数据库迁移；D 选项 RDS for MySQL 需要选择实例类型。"
    },
    "related_terms": [
      "Amazon Aurora MySQL",
      "Amazon Redshift",
      "MySQL",
      "Amazon Aurora Serverless for MySQL",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "id": 315,
    "topic": "1",
    "question_en": "A company experienced a breach that affected several applications in its on-premises data center. The attacker took advantage of vulnerabilities in the custom applications that were running on the servers. The company is now migrating its applications to run on Amazon EC2 instances. The company wants to implement a solution that actively scans for vulnerabilities on the EC2 instances and sends a report that details the findings. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy AWS Shield to scan the EC2 instances for vulnerabilities. Create an AWS Lambda function to log any findings to AWS CloudTrail.",
      "B": "Deploy Amazon Macie and AWS Lambda functions to scan the EC2 instances for vulnerabilities. Log any findings to AWS CloudTrail.",
      "C": "Turn on Amazon GuardDuty. Deploy the GuardDuty agents to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings.",
      "D": "Turn on Amazon Inspector. Deploy the Amazon Inspector agent to the EC2 instances. Configure an AWS Lambda function to automate the generation and distribution of reports that detail the findings."
    },
    "correct_answer": "D",
    "vote_percentage": "97%",
    "question_cn": "一家公司遭遇了一次影响其本地数据中心中几个应用程序的入侵。攻击者利用了在服务器上运行的自定义应用程序中的漏洞。该公司现在正在迁移其应用程序以在 Amazon EC2 实例上运行。该公司希望实施一个主动扫描 EC2 实例中漏洞并发送详细结果报告的解决方案。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "部署 AWS Shield 以扫描 EC2 实例中的漏洞。创建 AWS Lambda 函数以将任何发现结果记录到 AWS CloudTrail。",
      "B": "部署 Amazon Macie 和 AWS Lambda 函数以扫描 EC2 实例中的漏洞。将任何发现结果记录到 AWS CloudTrail。",
      "C": "打开 Amazon GuardDuty。将 GuardDuty 代理部署到 EC2 实例。配置 AWS Lambda 函数以自动化生成和分发详细说明发现结果的报告。",
      "D": "打开 Amazon Inspector。将 Amazon Inspector 代理部署到 EC2 实例。配置 AWS Lambda 函数以自动化生成和分发详细说明发现结果的报告。"
    },
    "tags": [
      "Inspector",
      "GuardDuty",
      "Macie",
      "Shield",
      "EC2",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 97%），解析仅供参考。】\n\n考查在 EC2 实例上部署漏洞扫描解决方案，并生成详细报告。",
      "why_correct": "Amazon Inspector 专门用于自动化安全评估，可以扫描 EC2 实例中的漏洞。部署 Inspector 代理到 EC2 实例后，Inspector 可以检测应用程序中的漏洞。通过配置 Lambda 函数，可以自动化生成和分发详细的报告，满足题目的需求。",
      "why_wrong": "AWS Shield 主要用于抵御 DDoS 攻击，而非漏洞扫描，因此选项 A 错误。Amazon Macie 专注于数据安全和合规性，检测敏感数据泄露风险，不直接执行漏洞扫描，因此选项 B 错误。Amazon GuardDuty 用于威胁检测，它可以检测潜在的恶意活动，但它不直接提供详细的漏洞扫描报告，且 GuardDuty 代理部署到 EC2 实例是错误的配置，因此选项 C 错误。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Shield",
      "AWS Lambda",
      "AWS CloudTrail",
      "Amazon Macie",
      "Amazon GuardDuty",
      "Amazon Inspector",
      "EC2 instance"
    ]
  },
  {
    "id": 316,
    "topic": "1",
    "question_en": "A company uses an Amazon EC2 instance to run a script to poll for and process messages in an Amazon Simple Queue Service (Amazon SQS) queue. The company wants to reduce operational costs while maintaining its ability to process a growing number of messages that are added to the queue. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Increase the size of the EC2 instance to process messages faster.",
      "B": "Use Amazon EventBridge to turn off the EC2 instance when the instance is underutilized.",
      "C": "Migrate the script on the EC2 instance to an AWS Lambda function with the appropriate runtime.",
      "D": "Use AWS Systems Manager Run Command to run the script on demand."
    },
    "correct_answer": "C",
    "vote_percentage": "90%",
    "question_cn": "一家公司使用 Amazon EC2 实例来运行脚本，以轮询和处理 Amazon Simple Queue Service (Amazon SQS) 队列中的消息。该公司希望降低运营成本，同时保持其处理添加到队列中的越来越多消息的能力。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "增加 EC2 实例的大小以更快地处理消息。",
      "B": "使用 Amazon EventBridge 在实例未被充分利用时关闭 EC2 实例。",
      "C": "将 EC2 实例上的脚本迁移到具有适当运行时的 AWS Lambda 函数。",
      "D": "使用 AWS Systems Manager Run Command 按需运行脚本。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon SQS",
      "Cost Optimization",
      "Performance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 90%），解析仅供参考。】\n\n考查如何通过优化计算资源来降低 SQS 消息处理的成本，同时保持可扩展性。",
      "why_correct": "将 EC2 实例上的脚本迁移到 AWS Lambda 函数是降低成本并提高可扩展性的有效方法。 Lambda 是一种无服务器计算服务，仅在代码运行时才会收费，这意味着在消息量较低时可以降低成本。 Lambda 还能自动扩展以适应消息量的变化，从而满足了处理不断增长的消息量的需求。",
      "why_wrong": "选项 A 增加 EC2 实例的大小并不能直接降低成本，反而可能增加成本。选项 B 使用 EventBridge 关闭 EC2 实例虽然可能降低成本，但需要复杂的配置且可能会导致消息处理延迟。 选项 D 使用 Systems Manager Run Command 虽然可以用于运行脚本，但需要依赖 EC2 实例，无法达到无服务器架构的按需付费和自动扩展的目的，从而无法有效降低运营成本。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Simple Queue Service (Amazon SQS)",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Systems Manager Run Command"
    ]
  },
  {
    "id": 317,
    "topic": "1",
    "question_en": "A company uses a legacy application to produce data in CSV format. The legacy application stores the output data in Amazon S3. The company is deploying a new commercial off-the-shelf (COTS) application that can perform complex SQL queries to analyze data that is stored in Amazon Redshift and Amazon S3 only. However, the COTS application cannot process the .csv files that the legacy application produces. The company cannot update the legacy application to produce data in another format. The company needs to implement a solution so that the COTS application can use the data that the legacy application produces. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Glue extract, transform, and load (ETL) job that runs on a schedule. Configure the ETL job to process the .csv files and store the processed data in Amazon Redshift.",
      "B": "Develop a Python script that runs on Amazon EC2 instances to convert the .csv files to .sql files. Invoke the Python script on a cron schedule to store the output files in Amazon S3.",
      "C": "Create an AWS Lambda function and an Amazon DynamoDB table. Use an S3 event to invoke the Lambda function. Configure the Lambda function to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in the DynamoDB table.",
      "D": "Use Amazon EventBridge to launch an Amazon EMR cluster on a weekly schedule. Configure the EMR cluster to perform an extract, transform, and load (ETL) job to process the .csv files and store the processed data in an Amazon Redshift table."
    },
    "correct_answer": "A",
    "vote_percentage": "97%",
    "question_cn": "一家公司使用旧版应用程序生成 CSV 格式的数据。旧版应用程序将输出数据存储在 Amazon S3 中。该公司正在部署一个新的商业现成 (COTS) 应用程序，该应用程序可以执行复杂的 SQL 查询来分析存储在 Amazon Redshift 和 Amazon S3 中的数据。但是，COTS 应用程序无法处理旧版应用程序生成的 .csv 文件。该公司无法更新旧版应用程序以生成另一种格式的数据。该公司需要实施一个解决方案，以便 COTS 应用程序可以使用旧版应用程序生成的数据。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个在计划上运行的 AWS Glue 提取、转换和加载 (ETL) 作业。配置 ETL 作业以处理 .csv 文件并将处理后的数据存储在 Amazon Redshift 中。",
      "B": "开发一个在 Amazon EC2 实例上运行的 Python 脚本，以将 .csv 文件转换为 .sql 文件。在 cron 计划上调用 Python 脚本，以将输出文件存储在 Amazon S3 中。",
      "C": "创建一个 AWS Lambda 函数和一个 Amazon DynamoDB 表。使用 S3 事件调用 Lambda 函数。配置 Lambda 函数以执行提取、转换和加载 (ETL) 作业来处理 .csv 文件并将处理后的数据存储在 DynamoDB 表中。",
      "D": "使用 Amazon EventBridge 每周启动一个 Amazon EMR 集群。配置 EMR 集群以执行提取、转换和加载 (ETL) 作业来处理 .csv 文件，并将处理后的数据存储在 Amazon Redshift 表中。"
    },
    "tags": [
      "AWS Glue",
      "Amazon Redshift",
      "ETL",
      "CSV"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 97%），解析仅供参考。】\n\n考察使用 AWS Glue 进行 ETL 任务，将 CSV 数据转换为 Redshift 兼容格式。与数据转换、存储方案的选择和运营开销的评估相关。",
      "why_correct": "AWS Glue 是一项完全托管的 ETL 服务，可以自动发现数据模式、转换数据并加载到 Redshift 中。针对本题，Glue 可以配置为处理 CSV 文件，转换数据，并将其加载到 Redshift 表中，从而满足 COTS 应用程序的需求。并且，使用计划任务，可以根据需求自动运行 ETL 作业，实现自动化处理，降低运营开销。",
      "why_wrong": "B 选项的方案需要手动开发和维护 Python 脚本，这增加了运营开销。此外，将 CSV 转换为 SQL 文件并不是直接满足 COTS 应用程序需求的最佳方法，COTS 程序可能需要特定格式的数据而不是 SQL 文件。C 选项使用 Lambda 和 DynamoDB，DynamoDB 并非设计用于存储大量结构化数据，存储成本高昂，且不适合用于 COTS 应用程序的复杂 SQL 查询。D 选项使用 Amazon EMR，虽然 EMR 可以处理 ETL 任务，但按需启动集群增加了开销，而且相较于 AWS Glue，管理成本更高，使用也更复杂，不满足最少运营开销的要求。"
    },
    "related_terms": [
      "AWS Glue",
      "ETL",
      "CSV",
      "Amazon Redshift",
      "Amazon S3",
      "Amazon EC2",
      "cron",
      "SQL",
      "AWS Lambda",
      "Amazon DynamoDB",
      "S3",
      "Amazon EventBridge",
      "Amazon EMR",
      "COTS",
      "Python"
    ]
  },
  {
    "id": 318,
    "topic": "1",
    "question_en": "A company recently migrated its entire IT environment to the AWS Cloud. The company discovers that users are provisioning oversized Amazon EC2 instances and modifying security group rules without using the appropriate change control process. A solutions architect must devise a strategy to track and audit these inventory and configuration changes. Which actions should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Enable AWS CloudTrail and use it for auditing.",
      "B": "Use data lifecycle policies for the Amazon EC2 instances.",
      "C": "Enable AWS Trusted Advisor and reference the security dashboard.",
      "D": "Enable AWS Config and create rules for auditing and compliance purposes",
      "E": "Restore previous resource configurations with an AWS CloudFormation template."
    },
    "correct_answer": "AD",
    "vote_percentage": "94%",
    "question_cn": "一家公司最近将其整个 IT 环境迁移到了 AWS 云。该公司发现用户正在预置超大 Amazon EC2 实例，并且在没有使用适当的变更控制流程的情况下修改安全组规则。解决方案架构师必须设计一种策略来跟踪和审计这些清单和配置更改。解决方案架构师应该采取哪些行动来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "启用 AWS CloudTrail 并将其用于审计。",
      "B": "为 Amazon EC2 实例使用数据生命周期策略。",
      "C": "启用 AWS Trusted Advisor 并参考安全仪表板。",
      "D": "启用 AWS Config 并创建用于审计和合规性的规则。",
      "E": "使用 AWS CloudFormation 模板还原先前的资源配置。"
    },
    "tags": [
      "AWS CloudTrail",
      "AWS Config",
      "Amazon EC2",
      "Security Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 94%），解析仅供参考。】\n\n考查对 AWS CloudTrail 和 AWS Config 的理解和应用，以及如何通过审计和合规性规则来跟踪和管理 EC2 实例配置更改，包括安全组规则。与审计、合规性、变更管理相关。",
      "why_correct": "启用 AWS CloudTrail 能够记录 AWS 账户中发生的 API 调用，包括对 EC2 实例和安全组的修改。CloudTrail 捕获的用户、时间、IP 地址、操作等信息，能够提供对配置更改的全面审计追踪，满足审计需求。通过 CloudTrail 日志，可以追踪谁、何时、如何对 EC2 实例或安全组进行了更改。",
      "why_wrong": {
        "B": "为 Amazon EC2 实例使用数据生命周期策略主要用于管理数据的存储和归档，而非跟踪配置更改或审计行为，不符合审计需求。",
        "C": "AWS Trusted Advisor 提供关于安全、性能、成本优化和容错性的建议，并不能直接跟踪和审计配置更改。虽然安全仪表板可以提供安全方面的建议，但其功能与审计配置更改的目标不符。",
        "D": "启用 AWS Config 并创建用于审计和合规性的规则可以用于跟踪配置更改，监控 EC2 实例的状态，以及检查是否符合预定义的规则。 这虽然能够解决问题，但题目要求选择两个选项，A 选项也需要。虽然 AWS Config 可以进行合规性审计，但仅依靠 Config，无法记录是谁进行了更改，即无法直接追溯用户的操作行为。",
        "E": "使用 AWS CloudFormation 模板还原先前的资源配置，这是一种配置管理策略，主要用于快速回滚到之前的配置，而不是用于跟踪配置更改或进行审计。CloudFormation 提供了基础设施即代码的功能，但它本身不记录更改的历史记录，无法满足审计的要求。"
      }
    },
    "related_terms": [
      "AWS CloudTrail",
      "Amazon EC2",
      "AWS Trusted Advisor",
      "AWS Config",
      "AWS CloudFormation",
      "EC2",
      "API",
      "Security Group"
    ]
  },
  {
    "id": 319,
    "topic": "1",
    "question_en": "A company has hundreds of Amazon EC2 Linux-based instances in the AWS Cloud. Systems administrators have used shared SSH keys to manage the instances. After a recent audit, the company’s security team is mandating the removal of all shared keys. A solutions architect must design a solution that provides secure access to the EC2 instances. Which solution will meet this requirement with the LEAST amount of administrative overhead?",
    "options_en": {
      "A": "Use AWS Systems Manager Session Manager to connect to the EC2 instances.",
      "B": "Use AWS Security Token Service (AWS STS) to generate one-time SSH keys on demand.",
      "C": "Allow shared SSH access to a set of bastion instances. Configure all other instances to allow only SSH access from the bastion instances.",
      "D": "Use an Amazon Cognito custom authorizer to authenticate users. Invoke an AWS Lambda function to generate a temporary SSH key."
    },
    "correct_answer": "A",
    "vote_percentage": "83%",
    "question_cn": "一家公司在 AWS 云中拥有数百个基于 Linux 的 Amazon EC2 实例。系统管理员使用共享 SSH 密钥来管理这些实例。在最近的一次审计之后，公司的安全团队强制要求删除所有共享密钥。解决方案架构师必须设计一个解决方案，为 EC2 实例提供安全访问。哪个解决方案将以最少的管理开销满足此要求？",
    "options_cn": {
      "A": "使用 AWS Systems Manager Session Manager 连接到 EC2 实例。",
      "B": "使用 AWS Security Token Service (AWS STS) 按需生成一次性 SSH 密钥。",
      "C": "允许共享 SSH 访问一组堡垒实例。将所有其他实例配置为仅允许从堡垒实例进行 SSH 访问。",
      "D": "使用 Amazon Cognito 自定义授权程序对用户进行身份验证。调用 AWS Lambda 函数以生成临时 SSH 密钥。"
    },
    "tags": [
      "EC2",
      "SSH",
      "AWS STS",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 83%），解析仅供参考。】\n\n考查如何通过最小的管理开销，为 EC2 实例提供安全的访问，并解决共享 SSH 密钥的安全隐患。",
      "why_correct": "AWS Systems Manager Session Manager 提供了一种安全、可审计的远程访问 EC2 实例的方式，无需使用 SSH 密钥。它通过 AWS 基础设施进行连接，简化了管理，并减少了密钥管理需求。Session Manager 还支持集中式日志记录和审计，满足了安全团队的要求。",
      "why_wrong": "选项 B 涉及 STS，但用于生成一次性 SSH 密钥在实现和管理上比 Session Manager 更复杂，并且仍然需要管理密钥。选项 C 涉及堡垒机，增加了基础设施复杂性和管理开销，并且并非最安全的方案。选项 D 涉及 Cognito 和 Lambda 函数，方案更为复杂，引入了额外的管理负担，并且相比 Session Manager 不提供更简洁的解决方案。"
    },
    "related_terms": [
      "Amazon EC2",
      "SSH",
      "AWS Systems Manager Session Manager",
      "AWS Security Token Service (AWS STS)",
      "AWS Lambda",
      "Amazon Cognito"
    ]
  },
  {
    "id": 320,
    "topic": "1",
    "question_en": "A company is using a fieet of Amazon EC2 instances to ingest data from on-premises data sources. The data is in JSON format and ingestion rates can be as high as 1 MB/s. When an EC2 instance is rebooted, the data in-fiight is lost. The company’s data science team wants to query ingested data in near-real time. Which solution provides near-real-time data querying that is scalable with minimal data loss?",
    "options_en": {
      "A": "Publish data to Amazon Kinesis Data Streams, Use Kinesis Data Analytics to query the data.",
      "B": "Publish data to Amazon Kinesis Data Firehose with Amazon Redshift as the destination. Use Amazon Redshift to query the data.",
      "C": "Store ingested data in an EC2 instance store. Publish data to Amazon Kinesis Data Firehose with Amazon S3 as the destination. Use Amazon Athena to query the data.",
      "D": "Store ingested data in an Amazon Elastic Block Store (Amazon EBS) volume. Publish data to Amazon ElastiCache for Redis. Subscribe to the Redis channel to query the data."
    },
    "correct_answer": "A",
    "vote_percentage": "70%",
    "question_cn": "一家公司正在使用一组 Amazon EC2 实例从本地数据源摄取数据。数据为 JSON 格式，摄取速率可能高达 1 MB/s。当 EC2 实例重新启动时，正在传输的数据将会丢失。该公司的Data Science团队希望近乎实时地查询摄取的数据。哪种解决方案提供了近乎实时的数据查询，具有可扩展性且数据丢失最少？",
    "options_cn": {
      "A": "将数据发布到 Amazon Kinesis Data Streams，使用 Kinesis Data Analytics 查询数据。",
      "B": "将数据发布到 Amazon Kinesis Data Firehose，并将 Amazon Redshift 作为目标。使用 Amazon Redshift 查询数据。",
      "C": "将摄取的数据存储在 EC2 实例存储中。将数据发布到 Amazon Kinesis Data Firehose，并将 Amazon S3 作为目标。使用 Amazon Athena 查询数据。",
      "D": "将摄取的数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。将数据发布到 Amazon ElastiCache for Redis。订阅 Redis 频道以查询数据。"
    },
    "tags": [
      "Amazon Kinesis Data Streams",
      "Kinesis Data Analytics",
      "Amazon EC2",
      "Data ingestion",
      "Real-time data processing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 70%），解析仅供参考。】\n\n考查近乎实时的流式数据处理方案选择，涉及 Amazon Kinesis Data Streams、Kinesis Data Analytics、数据持久化和查询服务选型。与数据摄取、可扩展性、数据丢失最小化等要求相关。",
      "why_correct": "选项 A 提供了最佳的近乎实时数据查询方案。EC2 实例将 JSON 数据发布到 Amazon Kinesis Data Streams，Kinesis Data Streams 提供了高吞吐量的数据摄取和持久化能力。然后，Kinesis Data Analytics 可以直接对 Kinesis Data Streams 中的数据进行近乎实时的处理和查询。这种方案具有可扩展性，并最大程度地减少了数据丢失的风险，因为数据首先被持久化在 Kinesis Data Streams 中。",
      "why_wrong": "选项 B 使用 Kinesis Data Firehose 和 Amazon Redshift 作为目标，虽然 Redshift 提供了强大的数据分析能力，但它通常用于数据仓库，写入延迟较高，不适合近乎实时的查询需求。选项 C 将数据存储在 EC2 实例存储中，实例存储是临时存储，在实例重启时数据会丢失，无法满足“数据丢失最少”的要求。选项 D 使用 ElastiCache for Redis 存储数据。Redis 虽然查询速度快，但它主要用于缓存，不适合持久化大量数据，且订阅 Redis 频道进行数据查询的方式也不够灵活，更适合少量数据的实时处理，且与本题的实时查询需求匹配度不高。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Kinesis Data Streams",
      "Kinesis Data Analytics",
      "Amazon Kinesis Data Firehose",
      "Amazon Redshift",
      "Amazon S3",
      "Amazon Athena",
      "Amazon ElastiCache for Redis",
      "JSON",
      "Amazon Elastic Block Store (Amazon EBS)"
    ]
  },
  {
    "id": 321,
    "topic": "1",
    "question_en": "What should a solutions architect do to ensure that all objects uploaded to an Amazon S3 bucket are encrypted?",
    "options_en": {
      "A": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set.",
      "B": "Update the bucket policy to deny if the PutObject does not have an s3:x-amz-acl header set to private.",
      "C": "Update the bucket policy to deny if the PutObject does not have an aws:SecureTransport header set to true.",
      "D": "Update the bucket policy to deny if the PutObject does not have an x-amz-server-side-encryption header set."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "解决方案架构师应采取什么措施来确保上传到 Amazon S3 存储桶的所有对象都被加密？",
    "options_cn": {
      "A": "更新存储桶策略，如果 PutObject 没有设置 s3:x-amz-acl 标头，则拒绝操作。",
      "B": "更新存储桶策略，如果 PutObject 没有将 s3:x-amz-acl 标头设置为 private，则拒绝操作。",
      "C": "更新存储桶策略，如果 PutObject 没有将 aws:SecureTransport 标头设置为 true，则拒绝操作。",
      "D": "更新存储桶策略，如果 PutObject 没有设置 x-amz-server-side-encryption 标头，则拒绝操作。"
    },
    "tags": [
      "Amazon S3",
      "S3",
      "S3 Bucket Policy",
      "Server-Side Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查如何通过 S3 存储桶策略强制对象加密。这与 S3 存储桶策略的配置、Server-Side Encryption 的工作原理相关。",
      "why_correct": "选项 D 正确。通过更新存储桶策略，如果 PutObject 请求没有设置 `x-amz-server-side-encryption` 标头，则拒绝操作，从而确保所有上传到 S3 的对象都被加密。该标头指定了服务器端加密的方式，如果未设置该标头，则表示对象没有被加密，此策略有效地强制了所有上传都必须使用 Server-Side Encryption。",
      "why_wrong": "选项 A 错误。`s3:x-amz-acl` 标头控制对象的访问控制列表（ACL），而非加密。选项 B 错误。设置 `s3:x-amz-acl` 为 `private` 只能控制对象的访问权限，无法确保加密。选项 C 错误。`aws:SecureTransport` 标头检查的是是否使用了安全传输，比如 HTTPS，这与对象是否被加密无关，并且在绝大多数情况下，使用 S3 都是使用 HTTPS 传输数据，故此选项与题目需求无关。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "HTTPS",
      "ACL",
      "S3 Bucket Policy",
      "PutObject",
      "x-amz-server-side-encryption",
      "s3:x-amz-acl",
      "private",
      "aws:SecureTransport"
    ]
  },
  {
    "id": 322,
    "topic": "1",
    "question_en": "A solutions architect is designing a multi-tier application for a company. The application's users upload images from a mobile device. The application generates a thumbnail of each image and returns a message to the user to confirm that the image was uploaded successfully. The thumbnail generation can take up to 60 seconds, but the company wants to provide a faster response time to its users to notify them that the original image was received. The solutions architect must design the application to asynchronously dispatch requests to the different application tiers. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Write a custom AWS Lambda function to generate the thumbnail and alert the user. Use the image upload process as an event source to invoke the Lambda function.",
      "B": "Create an AWS Step Functions workfiow. Configure Step Functions to handle the orchestration between the application tiers and alert the user when thumbnail generation is complete.",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) message queue. As images are uploaded, place a message on the SQS queue for thumbnail generation. Alert the user through an application message that the image was received.",
      "D": "Create Amazon Simple Notification Service (Amazon SNS) notification topics and subscriptions. Use one subscription with the application to generate the thumbnail after the image upload is complete. Use a second subscription to message the user's mobile app by way of a push notification after thumbnail generation is complete."
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一位解决方案架构师正在为一家公司设计一个多层应用程序。应用程序的用户从移动设备上传图像。应用程序为每张图像生成缩略图，并向用户返回一条消息，以确认图像已成功上传。生成缩略图可能需要长达 60 秒的时间，但公司希望为用户提供更快的响应时间，以通知他们已收到原始图像。解决方案架构师必须设计应用程序以异步地将请求分派给不同的应用程序层。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "编写一个自定义 AWS Lambda 函数来生成缩略图并提醒用户。使用图像上传过程作为事件 源 来调用 Lambda 函数。",
      "B": "创建一个 AWS Step Functions 工作流程。配置 Step Functions 来处理应用程序层之间的编排，并在缩略图生成完成后提醒用户。",
      "C": "创建一个 Amazon Simple Queue Service (Amazon SQS) 消息队列。当上传图像时，将一条消息放置在 SQS 队列上以进行缩略图生成。通过应用程序消息提醒用户图像已收到。",
      "D": "创建 Amazon Simple Notification Service (Amazon SNS) 通知主题和订阅。使用一个订阅与应用程序一起在图像上传完成后生成缩略图。使用第二个订阅通过推送通知向用户的移动应用程序发送消息，在缩略图生成完成后。"
    },
    "tags": [
      "Amazon SQS",
      "Lambda",
      "Amazon SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n考查异步处理、解耦设计。与 Amazon SQS、Lambda、SNS 服务选型、消息队列、事件驱动架构相关。",
      "why_correct": "选项 C 提供了异步处理上传图像和生成缩略图的解决方案。当用户上传图像时，应用程序将消息放入 SQS 队列中。应用程序立即响应用户，通知图像已收到。一个单独的消费者，比如 Lambda 函数或 EC2 实例上的应用程序，会从 SQS 队列中读取消息，生成缩略图，从而实现异步处理，满足题目中对更快的用户响应时间的要求。",
      "why_wrong": "选项 A 的问题在于，使用 Lambda 函数直接生成缩略图且作为事件源。虽然 Lambda 可以处理图像生成，但没有实现异步处理，直接生成缩略图会阻塞主线程，无法达到题目的目标——更快的用户响应。选项 B 使用 Step Functions 编排工作流，可以实现复杂任务的编排，但对于简单的异步任务来说，Step Functions 的开销相对较大，并且不直接解决更快的用户响应问题。选项 D 使用 SNS 主题，这更适合广播通知，而非处理异步任务。虽然 SNS 可以用于在图像上传完成后通知用户，但它并没有提供一种机制来处理耗时的缩略图生成任务，无法实现异步处理。同时，维护两个 SNS 订阅，增加了复杂性，且无法满足快速响应的需求。"
    },
    "related_terms": [
      "Amazon SQS",
      "Lambda",
      "EC2",
      "SNS",
      "Step Functions"
    ]
  },
  {
    "id": 323,
    "topic": "1",
    "question_en": "A company’s facility has badge readers at every entrance throughout the building. When badges are scanned, the readers send a message over HTTPS to indicate who attempted to access that particular entrance. A solutions architect must design a system to process these messages from the sensors. The solution must be highly available, and the results must be made available for the company’s security team to analyze. Which system architecture should the solutions architect recommend?",
    "options_en": {
      "A": "Launch an Amazon EC2 instance to serve as the HTTPS endpoint and to process the messages. Configure the EC2 instance to save the results to an Amazon S3 bucket.",
      "B": "Create an HTTPS endpoint in Amazon API Gateway. Configure the API Gateway endpoint to invoke an AWS Lambda function to process the messages and save the results to an Amazon DynamoDB table.",
      "C": "Use Amazon Route 53 to direct incoming sensor messages to an AWS Lambda function. Configure the Lambda function to process the messages and save the results to an Amazon DynamoDB table.",
      "D": "Create a gateway VPC endpoint for Amazon S3. Configure a Site-to-Site VPN connection from the facility network to the VPC so that sensor data can be written directly to an S3 bucket by way of the VPC endpoint."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司的设施在整个建筑物的每个入口处都设有读卡器。扫描徽章时，读卡器通过 HTTPS 发送一条消息，以指示谁试图访问该特定入口。解决方案架构师必须设计一个系统来处理来自传感器的这些消息。该解决方案必须具有高可用性，并且必须使结果可供公司的安全团队进行分析。解决方案架构师应该推荐哪种系统架构？",
    "options_cn": {
      "A": "启动一个 Amazon EC2 实例作为 HTTPS 端点并处理消息。配置 EC2 实例将结果保存到 Amazon S3 存储桶。",
      "B": "在 Amazon API Gateway 中创建 HTTPS 端点。配置 API Gateway 端点以调用 AWS Lambda 函数来处理消息并将结果保存到 Amazon DynamoDB 表中。",
      "C": "使用 Amazon Route 53 将传入的传感器消息定向到 AWS Lambda 函数。配置 Lambda 函数以处理消息并将结果保存到 Amazon DynamoDB 表中。",
      "D": "创建 Amazon S3 的网关 VPC endpoint。配置从设施网络到 VPC 的站点到站点 VPN 连接，以便可以通过 VPC endpoint 将传感器数据直接写入 S3 存储桶。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "High Availability",
      "Amazon EC2",
      "Amazon S3",
      "Route 53",
      "VPC Endpoint",
      "Site-to-Site VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查了构建高可用、可扩展的消息处理系统，重点关注 API Gateway、Lambda、DynamoDB 等服务的组合应用，并考察了对高可用性、安全性和数据存储的理解。也涉及了与 EC2、S3、Route 53、VPC Endpoint 和 Site-to-Site VPN 等其他 AWS 服务的对比。",
      "why_correct": "选项 B 提供了最适合的解决方案。它使用 API Gateway 作为 HTTPS 端点，API Gateway 本身具有高可用性和可扩展性。API Gateway 调用 Lambda 函数来处理消息，Lambda 函数可以快速响应和横向扩展以应对负载。Lambda 函数将结果存储到 DynamoDB 表中，DynamoDB 也提供了高可用性、可扩展性和数据持久性。API Gateway 提供了 HTTPS 端点，满足了传感器通过 HTTPS 发送消息的要求。这种架构实现了高可用性，并且 DynamoDB 中的数据可供安全团队进行分析。",
      "why_wrong": "选项 A 的错误在于使用 EC2 实例作为 HTTPS 端点。EC2 实例的部署、维护和扩展需要额外的管理工作，且其本身不如 API Gateway 具备的高可用性。虽然它将结果存储在 S3 中，满足了存储需求，但与 API Gateway + Lambda 的方案相比，其整体架构的可用性、可扩展性和开发效率都较低。选项 C 使用 Route 53 直接将消息定向到 Lambda。然而，Route 53 并不直接支持作为 HTTPS 端点接收数据，这与题目中“通过 HTTPS 发送一条消息”的要求相矛盾。此外，虽然 Lambda 和 DynamoDB 提供了可扩展性，但直接将数据发送到 Lambda 需要额外的安全措施，并且不如 API Gateway 提供的 HTTPS 身份验证和授权功能。选项 D 使用 S3 的网关 VPC endpoint 和 Site-to-Site VPN。此方案虽然满足了数据传输到 S3 的需求，但没有处理传入 HTTPS 消息的机制，且不直接提供计算资源处理消息，无法满足题目中“处理来自传感器的这些消息”的要求，同时也不便于安全团队进行分析。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "HTTPS",
      "API Gateway",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Route 53",
      "Site-to-Site VPN",
      "VPC Endpoint"
    ]
  },
  {
    "id": 324,
    "topic": "1",
    "question_en": "A company wants to implement a disaster recovery plan for its primary on-premises file storage volume. The file storage volume is mounted from an Internet Small Computer Systems Interface (iSCSI) device on a local storage server. The file storage volume holds hundreds of terabytes (TB) of data. The company wants to ensure that end users retain immediate access to all file types from the on-premises systems without experiencing latency. Which solution will meet these requirements with the LEAST amount of change to the company's existing infrastructure?",
    "options_en": {
      "A": "Provision an Amazon S3 File Gateway as a virtual machine (VM) that is hosted on premises. Set the local cache to 10 TB. Modify existing applications to access the files through the NFS protocol. To recover from a disaster, provision an Amazon EC2 instance and mount the S3 bucket that contains the files.",
      "B": "Provision an AWS Storage Gateway tape gateway. Use a data backup solution to back up all existing data to a virtual tape library. Configure the data backup solution to run nightly after the initial backup is complete. To recover from a disaster, provision an Amazon EC2 instance and restore the data to an Amazon Elastic Block Store (Amazon EBS) volume from the volumes in the virtual tape library.",
      "C": "Provision an AWS Storage Gateway Volume Gateway cached volume. Set the local cache to 10 TB. Mount the Volume Gateway cached volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance.",
      "D": "Provision an AWS Storage Gateway Volume Gateway stored volume with the same amount of disk space as the existing file storage volume. Mount the Volume Gateway stored volume to the existing file server by using iSCSI, and copy all files to the storage volume. Configure scheduled snapshots of the storage volume. To recover from a disaster, restore a snapshot to an Amazon Elastic Block Store (Amazon EBS) volume and attach the EBS volume to an Amazon EC2 instance."
    },
    "correct_answer": "D",
    "vote_percentage": "77%",
    "question_cn": "一家公司希望为其主要的本地文件存储卷实施灾难恢复计划。文件存储卷是从本地存储服务器上的 Internet Small Computer Systems Interface (iSCSI) 设备挂载的。文件存储卷包含数百 TB 的数据。该公司希望确保最终用户保留对来自本地系统的所有文件类型的即时访问权限，而不会遇到延迟。哪种解决方案以对公司现有基础设施的更改最少的方式满足这些要求？",
    "options_cn": {
      "A": "将 Amazon S3 File Gateway 作为虚拟机 (VM) 预置，该虚拟机托管在本地。将本地缓存设置为 10 TB。修改现有应用程序以通过 NFS 协议访问文件。为了从灾难中恢复，预置 Amazon EC2 实例并挂载包含文件的 S3 存储桶。",
      "B": "预置 AWS Storage Gateway 磁带网关。使用数据备份解决方案将所有现有数据备份到虚拟磁带库。配置数据备份解决方案以在初始备份完成后每晚运行。为了从灾难中恢复，预置 Amazon EC2 实例并将数据从虚拟磁带库中的卷恢复到 Amazon Elastic Block Store (Amazon EBS) 卷。",
      "C": "预置 AWS Storage Gateway Volume Gateway 缓存卷。将本地缓存设置为 10 TB。通过 iSCSI 将 Volume Gateway 缓存卷挂载到现有文件服务器，并将所有文件复制到存储卷。配置存储卷的计划快照。为了从灾难中恢复，将快照还原到 Amazon Elastic Block Store (Amazon EBS) 卷，并将 EBS 卷附加到 Amazon EC2 实例。",
      "D": "预置 AWS Storage Gateway Volume Gateway 存储卷，其磁盘空间与现有文件存储卷相同。通过 iSCSI 将 Volume Gateway 存储卷挂载到现有文件服务器，并将所有文件复制到存储卷。配置存储卷的计划快照。为了从灾难中恢复，将快照还原到 Amazon Elastic Block Store (Amazon EBS) 卷，并将 EBS 卷附加到 Amazon EC2 实例。"
    },
    "tags": [
      "AWS Storage Gateway",
      "Volume Gateway",
      "Amazon EBS",
      "Disaster Recovery",
      "iSCSI",
      "Caching"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 77%），解析仅供参考。】\n\n考察使用 AWS Storage Gateway Volume Gateway 实现本地文件存储卷的灾难恢复，并确保快速访问数据。",
      "why_correct": "选项 D 使用 Storage Gateway Volume Gateway 存储卷，它直接在 AWS 中存储数据，适用于大型数据集。通过 iSCSI 挂载到本地服务器，与现有基础设施兼容，且通过快照实现灾难恢复，无需更改现有文件访问协议，从而满足了低延迟和最小化变更的要求。",
      "why_wrong": "选项 A 使用 S3 File Gateway，需要将应用程序修改为使用 NFS 协议，增加了变更。选项 B 使用 Tape Gateway，更适合于归档，不符合即时访问的需求，且恢复过程较慢。选项 C 使用 Volume Gateway 缓存卷，虽然也是通过 Volume Gateway，但缓存卷的设计旨在提供对数据的本地访问加速，而不是针对主要存储。在灾难恢复场景下，其缓存机制无法保证快速恢复所有数据，且缓存大小限制也可能影响数据完整性，不满足题目对所有文件类型的即时访问要求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon S3 File Gateway",
      "Internet Small Computer Systems Interface (iSCSI)",
      "NFS",
      "Amazon EC2",
      "AWS Storage Gateway",
      "AWS Storage Gateway Tape Gateway",
      "Virtual Tape Library",
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS Storage Gateway Volume Gateway",
      "Volume Gateway",
      "Volume Gateway Cached Volume",
      "Volume Gateway Stored Volume"
    ]
  },
  {
    "id": 325,
    "topic": "1",
    "question_en": "A company is hosting a web application from an Amazon S3 bucket. The application uses Amazon Cognito as an identity provider to authenticate users and return a JSON Web Token (JWT) that provides access to protected resources that are stored in another S3 bucket. Upon deployment of the application, users report errors and are unable to access the protected content. A solutions architect must resolve this issue by providing proper permissions so that users can access the protected content. Which solution meets these requirements?",
    "options_en": {
      "A": "Update the Amazon Cognito identity pool to assume the proper IAM role for access to the protected content.",
      "B": "Update the S3 ACL to allow the application to access the protected content.",
      "C": "Redeploy the application to Amazon S3 to prevent eventually consistent reads in the S3 bucket from affecting the ability of users to access the protected content.",
      "D": "Update the Amazon Cognito pool to use custom attribute mappings within the identity pool and grant users the proper permissions to access the protected content."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司正在从一个 Amazon S3 存储桶托管一个 Web 应用程序。该应用程序使用 Amazon Cognito 作为身份提供商来验证用户身份，并返回一个 JSON Web Token (JWT)，该令牌提供对存储在另一个 S3 存储桶中的受保护资源的访问权限。部署应用程序后，用户报告错误并且无法访问受保护的内容。 解决方案架构师必须通过提供适当的权限来解决此问题，以便用户可以访问受保护的内容。 哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "更新 Amazon Cognito 身份池，以承担适当的 IAM 角色来访问受保护的内容。",
      "B": "更新 S3 ACL 以允许应用程序访问受保护的内容。",
      "C": "将应用程序重新部署到 Amazon S3，以防止 S3 存储桶中最终一致性读取影响用户访问受保护内容的能力。",
      "D": "更新 Amazon Cognito 池，以在身份池中使用自定义属性映射，并授予用户访问受保护内容的适当权限。"
    },
    "tags": [
      "Amazon Cognito",
      "IAM",
      "Amazon S3",
      "JWT"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n考查用户通过 Cognito 获取 JWT，访问 S3 保护资源权限的配置。涉及 Cognito 身份池、IAM 角色授权、S3 访问控制等。需要理解这些服务之间的协作关系，以及如何配置权限。",
      "why_correct": "更新 Amazon Cognito 身份池，以承担适当的 IAM 角色来访问受保护的内容是最佳解决方案。用户通过 Cognito 验证身份后获得 JWT，JWT 包含用户身份信息。通过配置身份池，让其承担一个 IAM 角色，该角色具有访问第二个 S3 存储桶的权限。这样，用户使用 JWT 访问资源时，就会使用此 IAM 角色的权限，从而能够访问受保护的内容。这种方式实现了最小权限原则，安全且高效。",
      "why_wrong": "B 选项：更新 S3 ACL 以允许应用程序访问受保护的内容是错误的。ACL（访问控制列表）是针对单个 S3 对象的低级访问控制，并不适合处理用户身份验证和授权。这种方法过于繁琐，难以管理，并且不符合最佳实践，特别是当用户数量众多时，无法精细控制每个用户的访问权限。C 选项：将应用程序重新部署到 Amazon S3 与解决用户无法访问受保护内容的问题无关。S3 最终一致性读取是指在数据更新后，可能需要一些时间才能在所有副本中完成同步，这与用户访问权限问题无关。D 选项：更新 Amazon Cognito 池，在身份池中使用自定义属性映射并授予用户访问受保护内容的适当权限，是一种不完整的解决方案。自定义属性映射主要用于将用户属性从身份提供商传递到 Cognito 身份池，并没有直接提供访问 S3 资源的权限。虽然可以在身份池中配置自定义属性，但这并不能替代 IAM 角色授权。授予访问受保护内容的权限必须通过 IAM 角色实现，而非依赖自定义属性。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "Amazon Cognito",
      "JWT",
      "IAM",
      "ACL",
      "Amazon Cognito Identity Pools"
    ]
  },
  {
    "id": 326,
    "topic": "1",
    "question_en": "An image hosting company uploads its large assets to Amazon S3 Standard buckets. The company uses multipart upload in parallel by using S3 APIs and overwrites if the same object is uploaded again. For the first 30 days after upload, the objects will be accessed frequently. The objects will be used less frequently after 30 days, but the access patterns for each object will be inconsistent. The company must optimize its S3 storage costs while maintaining high availability and resiliency of stored assets. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Move assets to S3 Intelligent-Tiering after 30 days.",
      "B": "Configure an S3 Lifecycle policy to clean up incomplete multipart uploads.",
      "C": "Configure an S3 Lifecycle policy to clean up expired object delete markers.",
      "D": "Move assets to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days",
      "E": "Move assets to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days."
    },
    "correct_answer": "AB",
    "vote_percentage": "60%",
    "question_cn": "一家图片托管公司将其大型资产上传到 Amazon S3 Standard 存储桶。该公司使用 S3 API 并行执行 multipart uploads，如果再次上传相同的对象，则会覆盖。上传后的前 30 天，这些对象将被频繁访问。30 天后，这些对象的使用频率会降低，但每个对象的访问模式将是不一致的。该公司必须优化其 S3 存储成本，同时保持存储资产的高可用性和弹性。解决方案架构师应推荐哪两种操作组合来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "30 天后将资产移至 S3 Intelligent-Tiering。",
      "B": "配置 S3 生命周期策略以清理未完成的 multipart uploads。",
      "C": "配置 S3 生命周期策略以清理过期的对象删除标记。",
      "D": "30 天后将资产移至 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "E": "30 天后将资产移至 S3 One Zone-Infrequent Access (S3 One Zone-IA)。"
    },
    "tags": [
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3 Standard-IA",
      "S3 One Zone-IA",
      "S3 Lifecycle"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 60%），解析仅供参考。】\n\n考查 S3 存储类的选择和 S3 生命周期策略的配置，以优化存储成本。与不同 S3 存储类的特性、使用场景，以及 S3 生命周期策略在不同场景下的应用相关。",
      "why_correct": "Amazon S3 Intelligent-Tiering 存储类可以根据访问模式自动将对象转移到更经济的存储层。在对象上传后的前 30 天频繁访问，之后访问频率降低且访问模式不一致的情况下，使用 Intelligent-Tiering 可以自动优化存储成本，同时保持高可用性和弹性。该存储类会根据访问模式在频繁访问、不频繁访问和存档访问层之间移动对象，无需手动干预，符合题目需求。",
      "why_wrong": "选项 B 描述的是清理未完成的 multipart uploads 的生命周期策略，与存储成本优化无关，主要用于清理不完整上传的文件，以释放存储空间，不符合题目场景；选项 C 描述的是清理过期的对象删除标记，这与存储成本优化无关，主要用于处理版本控制场景下的对象删除标记，不符合题目需求；选项 D 描述的是将对象移动到 S3 Standard-IA，虽然成本比 Standard 低，但这种存储类是针对访问频率较低的对象设计的，并且需要手动配置。频繁访问和不一致的访问模式会导致检索成本增加，且不如 Intelligent-Tiering 自动化；选项 E 描述的是将对象移动到 S3 One Zone-IA，这种存储类适用于可再生数据的场景，数据只存储在一个可用区内，尽管成本最低，但可用性不如 Standard、Standard-IA 和 Intelligent-Tiering，且不适用于访问模式不一致的情况，同时它也没有 Intelligent-Tiering 的自动化功能。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3 Standard-IA",
      "S3 Standard",
      "S3 One Zone-IA",
      "S3 Lifecycle",
      "multipart uploads"
    ]
  },
  {
    "id": 327,
    "topic": "1",
    "question_en": "A solutions architect must secure a VPC network that hosts Amazon EC2 instances. The EC2 instances contain highly sensitive data and run in a private subnet. According to company policy, the EC2 instances that run in the VPC can access only approved third-party software repositories on the internet for software product updates that use the third party’s URL. Other internet trafic must be blocked. Which solution meets these requirements?",
    "options_en": {
      "A": "Update the route table for the private subnet to route the outbound trafic to an AWS Network Firewall firewall. Configure domain list rule groups.",
      "B": "Set up an AWS WAF web ACL. Create a custom set of rules that filter trafic requests based on source and destination IP address range sets.",
      "C": "Implement strict inbound security group rules. Configure an outbound rule that allows trafic only to the authorized software repositories on the internet by specifying the URLs.",
      "D": "Configure an Application Load Balancer (ALB) in front of the EC2 instances. Direct all outbound trafic to the ALB. Use a URL-based rule listener in the ALB’s target group for outbound access to the internet."
    },
    "correct_answer": "A",
    "vote_percentage": "89%",
    "question_cn": "一位解决方案架构师必须保护托管 Amazon EC2 实例的 VPC 网络。EC2 实例包含高度敏感的数据，并在私有子网中运行。根据公司策略，在 VPC 中运行的 EC2 实例只能访问互联网上经过批准的第三方软件存储库，以获取使用第三方 URL 的软件产品更新。其他互联网流量必须被阻止。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "更新私有子网的路由表，将出站流量路由到 AWS Network Firewall 防火墙。配置域名列表规则组。",
      "B": "设置 AWS WAF Web ACL。创建自定义规则集，根据 源和目标 IP 地址范围集过滤流量请求。",
      "C": "实施严格的入站安全组规则。配置一个出站规则，仅允许流量通过指定 URL 访问互联网上的授权软件存储库。",
      "D": "在 EC2 实例前面配置 Application Load Balancer (ALB)。将所有出站流量定向到 ALB。在 ALB 的目标组中使用基于 URL 的规则监听器以访问互联网。"
    },
    "tags": [
      "VPC",
      "EC2",
      "Network Firewall",
      "Route Table",
      "Security Group",
      "Application Load Balancer (ALB)",
      "AWS WAF"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 89%），解析仅供参考。】\n\n考查保护托管 EC2 实例的 VPC 网络，以及控制 EC2 实例访问互联网流量。与 VPC 设计、网络安全策略、AWS Network Firewall、安全组、和 Application Load Balancer (ALB) 的选型相关。",
      "why_correct": "选项 A 提供了最合适的解决方案。通过配置私有子网的路由表，将所有出站流量导向 AWS Network Firewall。Network Firewall 允许您创建域名列表规则组，从而控制 EC2 实例对互联网上指定第三方软件存储库的访问，并阻止其他所有互联网流量。这种方案既满足了安全需求，又实现了对互联网访问的精细控制。",
      "why_wrong": "选项 B 错误，因为 AWS WAF 主要用于保护 Web 应用程序，而非 VPC 内的出站流量控制。使用 IP 地址范围过滤不适用于基于 URL 的访问控制。选项 C 错误，因为安全组规则虽然可以限制流量，但它无法执行基于 URL 的控制，且仅有出站规则不能满足题干要求。选项 D 错误，因为 ALB 主要用于负载均衡和流量分发，不具备 URL 级别的出站流量控制能力，无法满足仅允许访问特定第三方 URL 的要求。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "Internet",
      "AWS WAF",
      "URL",
      "AWS Network Firewall",
      "Route Table",
      "Security Group",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 328,
    "topic": "1",
    "question_en": "A company is hosting a three-tier ecommerce application in the AWS Cloud. The company hosts the website on Amazon S3 and integrates the website with an API that handles sales requests. The company hosts the API on three Amazon EC2 instances behind an Application Load Balancer (ALB). The API consists of static and dynamic front-end content along with backend workers that process sales requests asynchronously. The company is expecting a significant and sudden increase in the number of sales requests during events for the launch of new products. What should a solutions architect recommend to ensure that all the requests are processed successfully?",
    "options_en": {
      "A": "Add an Amazon CloudFront distribution for the dynamic content. Increase the number of EC2 instances to handle the increase in trafic.",
      "B": "Add an Amazon CloudFront distribution for the static content. Place the EC2 instances in an Auto Scaling group to launch new instances based on network trafic.",
      "C": "Add an Amazon CloudFront distribution for the dynamic content. Add an Amazon ElastiCache instance in front of the ALB to reduce trafic for the API to handle.",
      "D": "Add an Amazon CloudFront distribution for the static content. Add an Amazon Simple Queue Service (Amazon SQS) queue to receive requests from the website for later processing by the EC2 instances."
    },
    "correct_answer": "D",
    "vote_percentage": "69%",
    "question_cn": "一家公司在 AWS 云中托管一个三层电商应用程序。该公司将网站托管在 Amazon S3 上，并将网站与处理销售请求的 API 集成。该公司将 API 托管在 Application Load Balancer (ALB) 后面的三个 Amazon EC2 实例上。API 由静态和动态前端内容以及异步处理销售请求的后端工作程序组成。该公司预计在新产品发布活动期间，销售请求数量将出现显着且突然的增长。解决方案架构师应该推荐什么来确保所有请求都被成功处理？",
    "options_cn": {
      "A": "为动态内容添加 Amazon CloudFront 分发。增加 EC2 实例的数量以处理增加的流量。",
      "B": "为静态内容添加 Amazon CloudFront 分发。将 EC2 实例放入 Auto Scaling 组中，以根据网络流量启动新实例。",
      "C": "为动态内容添加 Amazon CloudFront 分发。在 ALB 前面添加 Amazon ElastiCache 实例，以减少 API 处理的流量。",
      "D": "为静态内容添加 Amazon CloudFront 分发。添加 Amazon Simple Queue Service (Amazon SQS) 队列以接收来自网站的请求，供 EC2 实例稍后处理。"
    },
    "tags": [
      "Amazon CloudFront",
      "Amazon S3",
      "EC2",
      "Application Load Balancer (ALB)",
      "Auto Scaling",
      "Amazon SQS",
      "Amazon ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 69%），解析仅供参考。】\n\n本题考查在应对突增流量时，如何优化三层 Web 应用程序的架构。这涉及到了内容分发、负载均衡、异步处理以及弹性伸缩等多个方面的知识，需要综合考虑。特别是对静态资源分发和异步任务处理的选型。",
      "why_correct": "选项 D 正确。通过为静态内容添加 Amazon CloudFront 分发，可以加速静态内容的访问，减轻 EC2 实例的负载。然后，引入 Amazon SQS 队列来接收来自网站的请求，使得EC2 实例可以异步处理这些请求，从而有效地应对流量的突增。这种方案能够实现流量削峰填谷，提高系统的可用性和稳定性，并且避免了对 EC2 实例进行频繁扩容，从而降低了成本。",
      "why_wrong": "选项 A 错误。为动态内容添加 CloudFront 分发通常没有直接的性能提升，因为动态内容需要后端处理，而 CloudFront 主要是加速静态内容。增加 EC2 实例的数量可以处理增加的流量，但无法解决突增流量的问题，并且需要手动干预，不够灵活。选项 B 错误。虽然使用 CloudFront 加速静态内容是正确的，并且使用 Auto Scaling 组可以根据流量自动扩展 EC2 实例，但 Auto Scaling 组只能应对有状态的负载，无法处理突增流量。选项 C 错误。为动态内容添加 CloudFront 分发，以及在 ALB 前面添加 Amazon ElastiCache 实例，可以减少 API 处理的流量，但这无法有效地应对流量突增，并且 ElastiCache 主要用于缓存数据，优化读性能，对解决突发请求的性能瓶颈作用有限。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "EC2",
      "Auto Scaling",
      "Amazon SQS",
      "Amazon ElastiCache",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 329,
    "topic": "1",
    "question_en": "A security audit reveals that Amazon EC2 instances are not being patched regularly. A solutions architect needs to provide a solution that will run regular security scans across a large fieet of EC2 instances. The solution should also patch the EC2 instances on a regular schedule and provide a report of each instance’s patch status. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up Amazon Macie to scan the EC2 instances for software vulnerabilities. Set up a cron job on each EC2 instance to patch the instance on a regular schedule.",
      "B": "Turn on Amazon GuardDuty in the account. Configure GuardDuty to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Session Manager to patch the EC2 instances on a regular schedule.",
      "C": "Set up Amazon Detective to scan the EC2 instances for software vulnerabilities. Set up an Amazon EventBridge scheduled rule to patch the EC2 instances on a regular schedule.",
      "D": "Turn on Amazon Inspector in the account. Configure Amazon Inspector to scan the EC2 instances for software vulnerabilities. Set up AWS Systems Manager Patch Manager to patch the EC2 instances on a regular schedule."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "安全审计显示，Amazon EC2 实例未定期进行补丁更新。 解决方案架构师需要提供一个解决方案，该方案将对大量的 EC2 实例定期运行安全扫描。 该解决方案还应定期对 EC2 实例进行补丁更新，并提供每个实例的补丁状态报告。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置 Amazon Macie 以扫描 EC2 实例中的软件漏洞。 在每个 EC2 实例上设置一个 cron 作业，以定期间隔对实例进行补丁更新。",
      "B": "在账户中打开 Amazon GuardDuty。 配置 GuardDuty 以扫描 EC2 实例中的软件漏洞。 设置 AWS Systems Manager Session Manager 以定期间隔对 EC2 实例进行补丁更新。",
      "C": "设置 Amazon Detective 以扫描 EC2 实例中的软件漏洞。 设置 Amazon EventBridge 定时规则，以定期间隔对 EC2 实例进行补丁更新。",
      "D": "在账户中打开 Amazon Inspector。 配置 Amazon Inspector 以扫描 EC2 实例中的软件漏洞。 设置 AWS Systems Manager Patch Manager 以定期间隔对 EC2 实例进行补丁更新。"
    },
    "tags": [
      "Amazon Inspector",
      "Amazon EC2",
      "AWS Systems Manager Patch Manager",
      "Vulnerability scanning",
      "Patching"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查 EC2 实例的安全扫描和补丁管理解决方案，需要考虑漏洞扫描、补丁应用及状态报告。涉及与 Amazon Inspector、AWS Systems Manager 等服务相关的知识。",
      "why_correct": "选项 D 提供了满足需求的最佳解决方案。Amazon Inspector 能够对 EC2 实例进行漏洞扫描，从而识别安全隐患。AWS Systems Manager Patch Manager 能够实现对 EC2 实例的定期补丁更新，并提供补丁状态报告。 这种组合方案能够有效实现自动化安全扫描和补丁管理，符合题干要求，并且具备完善的管理和报告功能。",
      "why_wrong": "选项 A 存在问题。Amazon Macie 主要用于数据安全，并不能直接扫描 EC2 实例中的软件漏洞，无法满足题目需求。选项 B 也存在问题。虽然 GuardDuty 可以检测潜在的安全问题，但其侧重点在于威胁检测，而并非专门的漏洞扫描。并且 Session Manager 用于交互式连接，并不是自动化补丁的最佳选择。 选项 C 使用 Amazon Detective，其主要用于调查安全问题，而非漏洞扫描，无法满足定期安全扫描的需求。 EventBridge 定时规则可以触发操作，但其不能直接进行补丁更新操作。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Inspector",
      "Amazon Macie",
      "Amazon GuardDuty",
      "Amazon EventBridge",
      "AWS Systems Manager Patch Manager",
      "AWS Systems Manager Session Manager",
      "Amazon Detective",
      "Vulnerability scanning",
      "Patching"
    ]
  },
  {
    "id": 330,
    "topic": "1",
    "question_en": "A company is planning to store data on Amazon RDS DB instances. The company must encrypt the data at rest. What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Create a key in AWS Key Management Service (AWS KMS). Enable encryption for the DB instances.",
      "B": "Create an encryption key. Store the key in AWS Secrets Manager. Use the key to encrypt the DB instances.",
      "C": "Generate a certificate in AWS Certificate Manager (ACM). Enable SSL/TLS on the DB instances by using the certificate.",
      "D": "Generate a certificate in AWS Identity and Access Management (IAM). Enable SSL/TLS on the DB instances by using the certificate."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划将数据存储在 Amazon RDS 数据库实例上。该公司必须对静态数据进行加密。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "在 AWS Key Management Service (AWS KMS) 中创建一个密钥。为数据库实例启用加密。",
      "B": "创建一个加密密钥。将密钥存储在 AWS Secrets Manager 中。使用该密钥对数据库实例进行加密。",
      "C": "在 AWS Certificate Manager (ACM) 中生成一个证书。通过使用该证书在数据库实例上启用 SSL/TLS。",
      "D": "在 AWS Identity and Access Management (IAM) 中生成一个证书。通过使用该证书在数据库实例上启用 SSL/TLS。"
    },
    "tags": [
      "Amazon RDS",
      "Database Encryption",
      "SSL/TLS",
      "AWS Certificate Manager (ACM)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在 RDS 数据库实例上对静态数据进行加密的解决方案。需要选择能够满足加密要求的 AWS 服务。",
      "why_correct": "选项 A 提供了正确的解决方案。通过使用 AWS KMS 创建密钥，并为 RDS 数据库实例启用加密，可以满足静态数据加密的要求。RDS 自动使用 KMS 密钥对存储在磁盘上的数据进行加密。",
      "why_wrong": "选项 B 错误，虽然可以使用 Secrets Manager 存储密钥，但 RDS 自身不直接使用 Secrets Manager 管理的密钥进行加密。选项 C 和 D 错误，ACM 和 IAM 提供的证书主要用于安全连接，而不是对静态数据进行加密。SSL/TLS 加密的是传输中的数据，而非静态数据。"
    },
    "related_terms": [
      "Amazon RDS",
      "AWS KMS",
      "AWS Secrets Manager",
      "AWS Certificate Manager (ACM)",
      "AWS Identity and Access Management (IAM)",
      "SSL/TLS"
    ]
  },
  {
    "id": 331,
    "topic": "1",
    "question_en": "A company must migrate 20 TB of data from a data center to the AWS Cloud within 30 days. The company’s network bandwidth is limited to 15 Mbps and cannot exceed 70% utilization. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use AWS Snowball.",
      "B": "Use AWS DataSync.",
      "C": "Use a secure VPN connection.",
      "D": "Use Amazon S3 Transfer Acceleration."
    },
    "correct_answer": "A",
    "vote_percentage": "91%",
    "question_cn": "一家公司必须在 30 天内将 20 TB 的数据从数据中心迁移到 AWS 云。该公司的网络带宽限制为 15 Mbps，且利用率不得超过 70%。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Snowball。",
      "B": "使用 AWS DataSync。",
      "C": "使用安全的 VPN 连接。",
      "D": "使用 Amazon S3 Transfer Acceleration。"
    },
    "tags": [
      "AWS Snowball",
      "Amazon S3",
      "Data Center",
      "Network Bandwidth"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 91%），解析仅供参考。】\n\n这道题考察了大规模数据迁移的解决方案选择。关键在于考点：网络带宽限制、数据量大、迁移时间要求。AWS Snowball 是一个物理数据传输设备，适用于大规模数据迁移，可以绕过网络带宽限制，加快迁移速度。DataSync适用于持续的数据同步和在线数据传输。",
      "why_correct": "AWS Snowball 可以满足题目的需求，它是一个物理设备，可以运输大量数据，绕过网络带宽限制，满足 30 天内迁移 20TB 数据的需求。",
      "why_wrong": "选项 B (AWS DataSync) 适用于在线数据传输和同步，无法满足网络带宽限制，而且可能无法在 30 天内完成 20TB 的数据传输。选项 C (安全的 VPN 连接) 也会受到网络带宽限制，无法满足传输时间要求。选项 D (Amazon S3 Transfer Acceleration) 可以加速 S3 的上传，但仍然受限于带宽限制，且不是数据迁移的主要方式。"
    },
    "related_terms": [
      "AWS Snowball",
      "Amazon S3",
      "DataSync",
      "VPN",
      "Transfer Acceleration",
      "Network Bandwidth",
      "Data Center"
    ]
  },
  {
    "id": 332,
    "topic": "1",
    "question_en": "A company needs to provide its employees with secure access to confidential and sensitive files. The company wants to ensure that the files can be accessed only by authorized users. The files must be downloaded securely to the employees’ devices. The files are stored in an on-premises Windows file server. However, due to an increase in remote usage, the file server is running out of capacity. . Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the file server to an Amazon EC2 instance in a public subnet. Configure the security group to limit inbound trafic to the employees’ IP addresses.",
      "B": "Migrate the files to an Amazon FSx for Windows File Server file system. Integrate the Amazon FSx file system with the on-premises Active Directory. Configure AWS Client VPN.",
      "C": "Migrate the files to Amazon S3, and create a private VPC endpoint. Create a signed URL to allow download.",
      "D": "Migrate the files to Amazon S3, and create a public VPC endpoint. Allow employees to sign on with AWS IAM Identity Center (AWS Single Sign-On)."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司需要为其员工提供对机密和敏感文件的安全访问。该公司希望确保只有授权用户才能访问这些文件。这些文件必须安全地下载到员工的设备。这些文件存储在本地 Windows 文件服务器中。然而，由于远程使用量的增加，文件服务器的容量即将耗尽。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将文件服务器迁移到公有子网中的 Amazon EC2 实例。配置安全组以限制流入员工 IP 地址的流量。",
      "B": "将文件迁移到 Amazon FSx for Windows File Server 文件系统。将 Amazon FSx 文件系统与本地 Active Directory 集成。配置 AWS 客户端 VPN。",
      "C": "将文件迁移到 Amazon S3，并创建私有 VPC endpoint。创建签名 URL 以允许下载。",
      "D": "将文件迁移到 Amazon S3，并创建公有 VPC endpoint。允许员工使用 AWS IAM Identity Center (AWS Single Sign-On) 登录。"
    },
    "tags": [
      "Amazon FSx for Windows File Server",
      "Amazon S3",
      "Amazon EC2",
      "VPC endpoint",
      "AWS Client VPN",
      "Active Directory"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n这道题考察了如何安全地存储和访问文件。关键在于考点：安全访问、远程访问、容量限制。FSx for Windows File Server 提供了 Windows 文件共享服务，可以与本地 AD 集成，并结合 VPN 提供安全访问。Amazon S3 需要配合签名 URL 和 VPC endpoint 实现安全访问。",
      "why_correct": "选项 B (将文件迁移到 Amazon FSx for Windows File Server 文件系统，并与本地 Active Directory 集成，配置 AWS 客户端 VPN) 满足所有需求：提供了 Windows 文件共享，与本地 AD 集成提供了用户认证和授权，客户端 VPN 提供了安全访问。",
      "why_wrong": "选项 A (将文件服务器迁移到公有子网中的 Amazon EC2 实例) 需要手动配置安全组，增加了管理复杂性，并且没有提供文件共享功能。选项 C (将文件迁移到 Amazon S3, 并创建私有 VPC endpoint) 使用签名 URL 实现访问，虽然安全，但文件共享功能实现复杂。选项 D (将文件迁移到 Amazon S3，并创建公有 VPC endpoint) 暴露了访问点，不安全。"
    },
    "related_terms": [
      "Amazon FSx for Windows File Server",
      "Amazon EC2",
      "Amazon S3",
      "Active Directory",
      "IAM",
      "VPC",
      "VPC endpoint",
      "AWS Client VPN",
      "Security Group"
    ]
  },
  {
    "id": 333,
    "topic": "1",
    "question_en": "A company’s application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. On the first day of every month at midnight, the application becomes much slower when the month-end financial calculation batch runs. This causes the CPU utilization of the EC2 instances to immediately peak to 100%, which disrupts the application. What should a solutions architect recommend to ensure the application is able to handle the workload and avoid downtime?",
    "options_en": {
      "A": "Configure an Amazon CloudFront distribution in front of the ALB.",
      "B": "Configure an EC2 Auto Scaling simple scaling policy based on CPU utilization.",
      "C": "Configure an EC2 Auto Scaling scheduled scaling policy based on the monthly schedule.",
      "D": "Configure Amazon ElastiCache to remove some of the workload from the EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的应用程序运行在 Application Load Balancer (ALB) 后面的 Amazon EC2 实例上。这些实例在跨多个可用区的 Amazon EC2 Auto Scaling 组中运行。在每个月的第一天午夜，当月末财务计算批处理运行时，应用程序会变得非常慢。这导致 EC2 实例的 CPU 利用率立即达到 100%，从而中断了应用程序。解决方案架构师应该建议什么来确保应用程序能够处理工作负载并避免停机？",
    "options_cn": {
      "A": "在 ALB 前配置 Amazon CloudFront 分发。",
      "B": "基于 CPU 利用率配置 EC2 Auto Scaling 简单伸缩策略。",
      "C": "基于每月的时间表配置 EC2 Auto Scaling 计划伸缩策略。",
      "D": "配置 Amazon ElastiCache 以从 EC2 实例中移除部分工作负载。"
    },
    "tags": [
      "Application Load Balancer (ALB)",
      "Amazon EC2 Auto Scaling",
      "Amazon CloudFront",
      "Amazon ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n这道题考察了如何应对应用程序的性能瓶颈。关键在于考点：CPU 利用率高、批处理作业、计划性扩展。使用 Auto Scaling 的计划扩展策略可以根据时间表调整容量，从而应对批处理作业带来的负载高峰。",
      "why_correct": "选项 C (基于每月的时间表配置 EC2 Auto Scaling 计划伸缩策略) 可以根据预定义的计划自动增加实例数量，以应对月初的批处理负载，并减少停机时间。",
      "why_wrong": "选项 A (在 ALB 前配置 Amazon CloudFront 分发) 可能会提高内容分发速度，但无法解决实例 CPU 达到 100% 的问题。选项 B (基于 CPU 利用率配置 EC2 Auto Scaling 简单伸缩策略) 在初始阶段可能因为响应延迟，来不及增加实例，导致应用程序中断。选项 D (配置 Amazon ElastiCache 以从 EC2 实例中移除部分工作负载) 无法应对 CPU 利用率达到 100% 的问题，更多是用于缓存，减轻数据库负载。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "CloudFront",
      "ElastiCache",
      "Availability Zone",
      "Application Load Balancer (ALB)",
      "CPU Utilization"
    ]
  },
  {
    "id": 334,
    "topic": "1",
    "question_en": "A company wants to give a customer the ability to use on-premises Microsoft Active Directory to download files that are stored in Amazon S3. The customer’s application uses an SFTP client to download the files. Which solution will meet these requirements with the LEAST operational overhead and no changes to the customer’s application?",
    "options_en": {
      "A": "Set up AWS Transfer Family with SFTP for Amazon S3. Configure integrated Active Directory authentication.",
      "B": "Set up AWS Database Migration Service (AWS DMS) to synchronize the on-premises client with Amazon S3. Configure integrated Active Directory authentication.",
      "C": "Set up AWS DataSync to synchronize between the on-premises location and the S3 location by using AWS IAM Identity Center (AWS Single Sign-On).",
      "D": "Set up a Windows Amazon EC2 instance with SFTP to connect the on-premises client with Amazon S3. Integrate AWS Identity and Access Management (IAM)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望让客户能够使用本地 Microsoft Active Directory 下载存储在 Amazon S3 中的文件。 客户的应用程序使用 SFTP 客户端下载文件。 哪种解决方案将以最低的运营开销满足这些要求，并且不对客户的应用程序进行任何更改？",
    "options_cn": {
      "A": "为 Amazon S3 设置 AWS Transfer Family 与 SFTP。 配置集成的 Active Directory 身份验证。",
      "B": "设置 AWS Database Migration Service (AWS DMS) 以将本地客户端与 Amazon S3 同步。 配置集成的 Active Directory 身份验证。",
      "C": "设置 AWS DataSync 以使用 AWS IAM Identity Center (AWS 单点登录) 在本地位置和 S3 位置之间进行同步。",
      "D": "设置一个带 SFTP 的 Windows Amazon EC2 实例，以将本地客户端与 Amazon S3 连接。 集成 AWS Identity and Access Management (IAM)。"
    },
    "tags": [
      "AWS Transfer Family",
      "Amazon S3",
      "AWS DMS",
      "AWS DataSync",
      "Active Directory",
      "IAM Identity Center (AWS 单点登录)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查通过 SFTP 协议访问 Amazon S3 上的文件，并使用 Active Directory 进行身份验证的解决方案。重点考察了运营开销和最小化应用程序更改的需求。",
      "why_correct": "AWS Transfer Family with SFTP 服务允许客户通过 SFTP 协议安全地访问 S3 存储桶。它支持与 Active Directory 集成，从而实现用户身份验证。这种解决方案无需更改客户应用程序，并且具有较低的运营开销，因为它是一个完全托管的服务。",
      "why_wrong": "选项 B 中，AWS Database Migration Service (AWS DMS) 主要用于数据库迁移，不适合直接支持 SFTP 文件下载。 选项 C 中，AWS DataSync 用于数据同步，不支持 SFTP 协议，且使用 IAM Identity Center (AWS SSO) 无法满足使用 Active Directory 进行身份验证的需求。 选项 D 中，设置 Windows EC2 实例需要手动管理，涉及更高的运营开销，并且仍然需要配置 SFTP 服务器以支持身份验证，与题目中低运营开销的要求不符，而且需要客户程序进行修改才能连接到 EC2 实例上的 SFTP 服务器。"
    },
    "related_terms": [
      "Amazon S3",
      "SFTP",
      "AWS Transfer Family",
      "Active Directory",
      "AWS Database Migration Service (AWS DMS)",
      "AWS DataSync",
      "AWS IAM Identity Center (AWS SSO)",
      "Amazon EC2",
      "AWS Identity and Access Management (IAM)"
    ]
  },
  {
    "id": 335,
    "topic": "1",
    "question_en": "A company is experiencing sudden increases in demand. The company needs to provision large Amazon EC2 instances from an Amazon Machine Image (AMI). The instances will run in an Auto Scaling group. The company needs a solution that provides minimum initialization latency to meet the demand. Which solution meets these requirements?",
    "options_en": {
      "A": "Use the aws ec2 register-image command to create an AMI from a snapshot. Use AWS Step Functions to replace the AMI in the Auto Scaling group.",
      "B": "Enable Amazon Elastic Block Store (Amazon EBS) fast snapshot restore on a snapshot. Provision an AMI by using the snapshot. Replace the AMI in the Auto Scaling group with the new AMI.",
      "C": "Enable AMI creation and define lifecycle rules in Amazon Data Lifecycle Manager (Amazon DLM). Create an AWS Lambda function that modifies the AMI in the Auto Scaling group.",
      "D": "Use Amazon EventBridge to invoke AWS Backup lifecycle policies that provision AMIs. Configure Auto Scaling group capacity limits as an event source in EventBridge."
    },
    "correct_answer": "B",
    "vote_percentage": "91%",
    "question_cn": "一家公司正在经历需求的突然增长。该公司需要从一个 Amazon Machine Image (AMI) 中配置大型 Amazon EC2 实例。这些实例将在一个 Auto Scaling 组中运行。该公司需要一个能提供最短初始化延迟以满足需求的解决方案。哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "使用 aws ec2 register-image 命令从快照创建 AMI。使用 AWS Step Functions 替换 Auto Scaling 组中的 AMI。",
      "B": "在快照上启用 Amazon Elastic Block Store (Amazon EBS) 快速快照恢复。使用该快照配置一个 AMI。用新的 AMI 替换 Auto Scaling 组中的 AMI。",
      "C": "启用 AMI 创建并在 Amazon Data Lifecycle Manager (Amazon DLM) 中定义生命周期规则。创建一个 AWS Lambda 函数来修改 Auto Scaling 组中的 AMI。",
      "D": "使用 Amazon EventBridge 调用 AWS Backup 生命周期策略来配置 AMI。将 Auto Scaling 组容量限制配置为 EventBridge 中的事件源。"
    },
    "tags": [
      "Amazon Machine Image (AMI)",
      "Amazon EC2 Auto Scaling",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon Data Lifecycle Manager (Amazon DLM)",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 91%），解析仅供参考。】\n\n考查在 EC2 Auto Scaling 环境中，应对需求突增时，如何通过缩短 AMI 初始化时间来提高实例可用性。重点关注快速启动实例的方案。",
      "why_correct": "Amazon EBS 快速快照恢复 (Fast Snapshot Restore, FSR) 可以在指定的可用区预先将快照数据加载到后台，从而显著减少从该快照创建卷的初始化时间。通过 FSR 加速快照，可以快速创建 AMI，并替换 Auto Scaling 组中的 AMI，从而满足快速启动实例的需求。",
      "why_wrong": "选项 A 涉及从快照创建 AMI，但没有利用 FSR 技术加速启动，因此初始化时间较长。选项 C 使用 Amazon DLM 创建 AMI，并使用 Lambda 函数修改 Auto Scaling 组，流程较复杂，且 AMI 创建和生命周期管理本身也会增加延迟。选项 D 使用了 EventBridge 和 AWS Backup，与快速启动实例的需求不直接相关，而且配置和启动流程较为复杂，初始化延迟长，不符合题意。"
    },
    "related_terms": [
      "Amazon Machine Image (AMI)",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Fast Snapshot Restore (FSR)",
      "AWS Step Functions",
      "Amazon Data Lifecycle Manager (Amazon DLM)",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Backup"
    ]
  },
  {
    "id": 336,
    "topic": "1",
    "question_en": "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company’s IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options_en": {
      "A": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.",
      "B": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.",
      "C": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.",
      "D": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管了一个多层 Web 应用程序，该应用程序使用 Amazon Aurora MySQL 数据库集群进行存储。应用程序层托管在 Amazon EC2 实例上。该公司 IT 安全准则规定数据库凭证必须加密并每 14 天轮换一次。解决方案架构师应该怎么做才能以最少的运营工作量来满足此要求？",
    "options_cn": {
      "A": "创建一个新的 AWS Key Management Service (AWS KMS) 加密密钥。使用 AWS Secrets Manager 创建一个新密钥，该密钥使用 KMS 密钥和适当的凭证。将该密钥与 Aurora 数据库集群关联。配置 14 天的自定义轮换周期。",
      "B": "在 AWS Systems Manager Parameter Store 中创建两个参数：一个用于用户名，作为字符串参数，另一个使用 SecureString 类型作为密码。为密码参数选择 AWS Key Management Service (AWS KMS) 加密，并在应用程序层中加载这些参数。实现一个 AWS Lambda 函数，每 14 天轮换一次密码。",
      "C": "将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon Elastic File System (Amazon EFS) 文件系统中。在应用程序层的所有 EC2 实例中挂载 EFS 文件系统。限制对文件系统的文件的访问，以便应用程序可以读取该文件，并且只有超级用户才能修改该文件。实现一个 AWS Lambda 函数，每 14 天轮换一次 Aurora 中的密钥，并将新凭证写入该文件。",
      "D": "将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon S3 存储桶中，应用程序使用该存储桶来加载凭证。定期将文件下载到应用程序，以确保使用正确的凭证。实现一个 AWS Lambda 函数，每 14 天轮换一次 Aurora 凭证，并将这些凭证上传到 S3 存储桶中的文件。"
    },
    "tags": [
      "AWS Secrets Manager",
      "AWS Key Management Service (AWS KMS)",
      "Amazon Aurora MySQL",
      "AWS Systems Manager Parameter Store",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon S3",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n这道题考察了如何安全地管理数据库凭证。关键在于考点：凭证加密、定期轮换、最小化运营工作量。AWS Secrets Manager 提供了密钥管理和轮换功能，能满足需求。",
      "why_correct": "选项 A (创建 AWS Secrets Manager 密钥，该密钥使用 KMS 密钥和适当的凭证，并与 Aurora 数据库集群关联。配置 14 天的自定义轮换周期。)  Secrets Manager 提供了加密、轮换和存储凭证的功能，结合 KMS 提供了安全性和自动化的轮换机制，最符合最少运营工作量的要求。",
      "why_wrong": "选项 B (在 AWS Systems Manager Parameter Store 中创建参数) 无法自动轮换密钥，需要手动实现，且不如 Secrets Manager 方便。选项 C (将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon Elastic File System (Amazon EFS) 文件系统中) 增加了复杂性，EFS 的管理也增加了运营开销。选项 D (将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon S3 存储桶中) 增加了复杂性，需要手动管理 S3 存储桶，并且需要应用程序定期下载凭证。"
    },
    "related_terms": [
      "AWS Secrets Manager",
      "AWS Key Management Service (AWS KMS)",
      "Amazon Aurora MySQL",
      "AWS Systems Manager Parameter Store",
      "Amazon S3",
      "AWS Lambda",
      "Amazon Elastic File System (Amazon EFS)"
    ]
  },
  {
    "id": 337,
    "topic": "1",
    "question_en": "A company has deployed a web application on AWS. The company hosts the backend database on Amazon RDS for MySQL with a primary DB instance and five read replicas to support scaling needs. The read replicas must lag no more than 1 second behind the primary DB instance. The database routinely runs scheduled stored procedures. As trafic on the website increases, the replicas experience additional lag during periods of peak load. A solutions architect must reduce the replication lag as much as possible. The solutions architect must minimize changes to the application code and must minimize ongoing operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the database to Amazon Aurora MySQL. Replace the read replicas with Aurora Replicas, and configure Aurora Auto Scaling. Replace the stored procedures with Aurora MySQL native functions.",
      "B": "Deploy an Amazon ElastiCache for Redis cluster in front of the database. Modify the application to check the cache before the application queries the database. Replace the stored procedures with AWS Lambda functions.",
      "C": "Migrate the database to a MySQL database that runs on Amazon EC2 instances. Choose large, compute optimized EC2 instances for all replica nodes. Maintain the stored procedures on the EC2 instances.",
      "D": "Migrate the database to Amazon DynamoDB. Provision a large number of read capacity units (RCUs) to support the required throughput, and configure on-demand capacity scaling. Replace the stored procedures with DynamoDB streams."
    },
    "correct_answer": "A",
    "vote_percentage": "86%",
    "question_cn": "一家公司已在 AWS 上部署了一个 Web 应用程序。该公司将后端数据库托管在 Amazon RDS for MySQL 上，其中包含一个主数据库实例和五个只读副本以支持扩展需求。只读副本的延迟不得超过主数据库实例 1 秒。数据库会定期运行计划的存储过程。随着网站流量的增加，副本在高峰负载期间会遇到额外的延迟。一位解决方案架构师必须尽可能减少复制延迟。该解决方案架构师必须尽量减少对应用程序代码的更改，并尽量减少持续的运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon Aurora MySQL。用 Aurora 副本替换只读副本，并配置 Aurora 自动缩放。使用 Aurora MySQL 原生函数替换存储过程。",
      "B": "在数据库前面部署一个 Amazon ElastiCache for Redis 集群。修改应用程序以在应用程序查询数据库之前检查缓存。使用 AWS Lambda 函数替换存储过程。",
      "C": "将数据库迁移到在 Amazon EC2 实例上运行的 MySQL 数据库。为所有副本节点选择大型、计算优化的 EC2 实例。在 EC2 实例上维护存储过程。",
      "D": "将数据库迁移到 Amazon DynamoDB。配置大量读取容量单元 (RCU) 以支持所需的吞吐量，并配置按需容量扩展。使用 DynamoDB 流替换存储过程。"
    },
    "tags": [
      "Amazon Aurora MySQL",
      "Amazon RDS for MySQL",
      "Amazon ElastiCache for Redis",
      "Amazon DynamoDB",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 86%），解析仅供参考。】\n\n这道题考察了数据库复制延迟优化。关键在于考点：复制延迟、读写分离、最小化代码改动。Amazon Aurora MySQL 具有更好的复制性能，且易于扩展.",
      "why_correct": "选项 A (将数据库迁移到 Amazon Aurora MySQL。用 Aurora 副本替换只读副本，并配置 Aurora 自动缩放。使用 Aurora MySQL 原生函数替换存储过程。) Aurora MySQL 提供了更快的复制性能和扩展能力，并且可以通过 Aurora 副本满足只读副本的需求，减少复制延迟。",
      "why_wrong": "选项 B (在数据库前面部署一个 Amazon ElastiCache for Redis 集群) 主要用于缓存，无法解决副本延迟问题。选项 C (将数据库迁移到在 Amazon EC2 实例上运行的 MySQL 数据库) 需要手动管理数据库，复制性能不如 Aurora。选项 D (将数据库迁移到 Amazon DynamoDB) 改变了数据库类型，需要大量代码改动，与题目要求不符。"
    },
    "related_terms": [
      "Amazon Aurora MySQL",
      "Amazon ElastiCache for Redis",
      "Amazon DynamoDB",
      "AWS Lambda",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "id": 338,
    "topic": "1",
    "question_en": "A solutions architect must create a disaster recovery (DR) plan for a high-volume software as a service (SaaS) platform. All data for the platform is stored in an Amazon Aurora MySQL DB cluster. The DR plan must replicate data to a secondary AWS Region. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use MySQL binary log replication to an Aurora cluster in the secondary Region. Provision one DB instance for the Aurora cluster in the secondary Region.",
      "B": "Set up an Aurora global database for the DB cluster. When setup is complete, remove the DB instance from the secondary Region.",
      "C": "Use AWS Database Migration Service (AWS DMS) to continuously replicate data to an Aurora cluster in the secondary Region. Remove the DB instance from the secondary Region.",
      "D": "Set up an Aurora global database for the DB cluster. Specify a minimum of one DB instance in the secondary Region."
    },
    "correct_answer": "B",
    "vote_percentage": "56%",
    "question_cn": "一位解决方案架构师必须为高容量软件即服务 (SaaS) 平台创建一个灾难恢复 (DR) 计划。该平台的所有数据都存储在 Amazon Aurora MySQL 数据库集群中。 DR 计划必须将数据复制到辅助 AWS 区域。哪种解决方案能够以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 MySQL 二进制日志复制到辅助区域中的 Aurora 集群。为辅助区域中的 Aurora 集群预置一个数据库实例。",
      "B": "为数据库集群设置 Aurora 全局数据库。设置完成后，从辅助区域中删除数据库实例。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 持续将数据复制到辅助区域中的 Aurora 集群。 从辅助区域中删除数据库实例。",
      "D": "为数据库集群设置 Aurora 全局数据库。指定辅助区域中至少一个数据库实例。"
    },
    "tags": [
      "Amazon Aurora MySQL",
      "MySQL",
      "AWS DMS",
      "Aurora Global Database"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 56%），解析仅供参考。】\n\n考察 Aurora MySQL 的灾难恢复方案，以及成本效益的考量。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：Aurora Global Database 专为跨区域灾难恢复设计。它提供快速、可靠的跨区域数据复制，而且在数据复制到辅助区域后，如果选择不保留数据库实例，可以进一步优化成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 MySQL 二进制日志复制，配置复杂，维护成本高。选项 C 使用 AWS DMS，虽然可以实现数据复制，但持续使用 DMS 的成本较高，不符合题目的最具成本效益的要求。选项 D 虽然使用 Aurora Global Database，但要求在辅助区域保留至少一个数据库实例，增加了成本，不符合题意。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Aurora MySQL",
      "Disaster Recovery (DR)",
      "Aurora Global Database",
      "AWS Database Migration Service (AWS DMS)"
    ]
  },
  {
    "id": 339,
    "topic": "1",
    "question_en": "A company has a custom application with embedded credentials that retrieves information from an Amazon RDS MySQL DB instance. Management says the application must be made more secure with the least amount of programming effort. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use AWS Key Management Service (AWS KMS) to create keys. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation.",
      "B": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Create an AWS Lambda function that rotates the credentials in Secret Manager.",
      "C": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Secrets Manager.",
      "D": "Create credentials on the RDS for MySQL database for the application user and store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule for the application user in the RDS for MySQL database using Parameter Store."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个自定义应用程序，其中嵌入了凭证，用于从 Amazon RDS MySQL 数据库实例中检索信息。 管理层表示，必须以最少的编程工作量使应用程序更加安全。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Key Management Service (AWS KMS) 创建密钥。 将应用程序配置为从 AWS KMS 加载数据库凭证。 启用自动密钥轮换。",
      "B": "为应用程序用户在 RDS for MySQL 数据库上创建凭证，并将凭证存储在 AWS Secrets Manager 中。 将应用程序配置为从 Secrets Manager 加载数据库凭证。 创建一个 AWS Lambda 函数，该函数在 Secret Manager 中轮换凭证。",
      "C": "为应用程序用户在 RDS for MySQL 数据库上创建凭证，并将凭证存储在 AWS Secrets Manager 中。 将应用程序配置为从 Secrets Manager 加载数据库凭证。 使用 Secrets Manager 为应用程序用户在 RDS for MySQL 数据库中设置凭证轮换计划。",
      "D": "为应用程序用户在 RDS for MySQL 数据库上创建凭证，并将凭证存储在 AWS Systems Manager Parameter Store 中。 将应用程序配置为从 Parameter Store 加载数据库凭证。 使用 Parameter Store 为应用程序用户在 RDS for MySQL 数据库中设置凭证轮换计划。"
    },
    "tags": [
      "Amazon RDS MySQL",
      "AWS Key Management Service (AWS KMS)",
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS Secrets Manager 安全存储和轮换数据库凭证，以减少应用程序的编程工作量。",
      "why_correct": "选项 C 提供了最直接且最少编程工作的解决方案。它使用 Secrets Manager 安全地存储数据库凭证，并利用 Secrets Manager 内置的轮换功能，自动化凭证更新，无需编写额外的代码来处理轮换逻辑。这满足了题目中“以最少的编程工作量”的要求。",
      "why_wrong": "选项 A 使用了 AWS KMS，但 KMS 主要用于加密数据，而不是安全存储和轮换数据库凭证。它虽然可以用于保护密钥，但没有提供直接的数据库凭证管理和轮换功能，需要更多的编程工作量。选项 B 涉及创建 Lambda 函数来轮换凭证，这需要编写和维护额外的代码，不符合“最少的编程工作量”的要求。选项 D 使用了 Parameter Store，Parameter Store 无法像 Secrets Manager 一样提供内置的数据库凭证轮换功能，无法满足题目要求，需要更多的自定义实现。"
    },
    "related_terms": [
      "AWS Key Management Service (AWS KMS)",
      "Amazon RDS MySQL",
      "AWS Secrets Manager",
      "AWS Lambda",
      "AWS Systems Manager Parameter Store"
    ]
  },
  {
    "id": 340,
    "topic": "1",
    "question_en": "A media company hosts its website on AWS. The website application’s architecture includes a fieet of Amazon EC2 instances behind an Application Load Balancer (ALB) and a database that is hosted on Amazon Aurora. The company’s cybersecurity team reports that the application is vulnerable to SQL injection. How should the company resolve this issue?",
    "options_en": {
      "A": "Use AWS WAF in front of the ALB. Associate the appropriate web ACLs with AWS WAF.",
      "B": "Create an ALB listener rule to reply to SQL injections with a fixed response.",
      "C": "Subscribe to AWS Shield Advanced to block all SQL injection attempts automatically.",
      "D": "Set up Amazon Inspector to block all SQL injection attempts automatically."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家媒体公司在其 AWS 上托管其网站。该网站应用程序的架构包括位于 Application Load Balancer (ALB) 后的 Amazon EC2 实例集群以及托管在 Amazon Aurora 上的数据库。该公司网络安全团队报告称，该应用程序容易受到 SQL 注入的攻击。该公司应如何解决此问题？",
    "options_cn": {
      "A": "在 ALB 前使用 AWS WAF。将适当的 Web ACL 与 AWS WAF 关联。",
      "B": "创建 ALB 监听器规则，以固定响应回复 SQL 注入。",
      "C": "订阅 AWS Shield Advanced 以自动阻止所有 SQL 注入尝试。",
      "D": "设置 Amazon Inspector 以自动阻止所有 SQL 注入尝试。"
    },
    "tags": [
      "AWS WAF",
      "Application Load Balancer (ALB)",
      "SQL 注入",
      "AWS Shield Advanced",
      "Amazon Inspector"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n这道题考察了如何应对应用程序的 SQL 注入攻击。关键在于考点：Web 应用程序保护、安全性。AWS WAF 和 AWS Shield 提供了 Web 应用程序保护，可以防御 SQL 注入攻击.",
      "why_correct": "选项 C (订阅 AWS Shield Advanced 以自动阻止所有 SQL 注入尝试。)  AWS Shield Advanced 提供了针对各种攻击的保护，包括 SQL 注入，可以提供自动化的保护。",
      "why_wrong": "选项 A (在 ALB 前使用 AWS WAF) 只能进行手动配置，不能自动防御SQL注入。选项 B (创建 ALB 监听器规则，以固定响应回复 SQL 注入) 不能完全防御 SQL 注入攻击，并且需要维护。选项 D (设置 Amazon Inspector 以自动阻止所有 SQL 注入尝试)  Inspector 更多用于安全评估，而不是直接防御攻击。"
    },
    "related_terms": [
      "AWS WAF",
      "AWS Shield Advanced",
      "Amazon Inspector",
      "Application Load Balancer (ALB)",
      "SQL Injection"
    ]
  },
  {
    "id": 341,
    "topic": "1",
    "question_en": "A company has an Amazon S3 data lake that is governed by AWS Lake Formation. The company wants to create a visualization in Amazon QuickSight by joining the data in the data lake with operational data that is stored in an Amazon Aurora MySQL database. The company wants to enforce column-level authorization so that the company’s marketing team can access only a subset of columns in the database. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon EMR to ingest the data directly from the database to the QuickSight SPICE engine. Include only the required columns.",
      "B": "Use AWS Glue Studio to ingest the data from the database to the S3 data lake. Attach an IAM policy to the QuickSight users to enforce column-level access control. Use Amazon S3 as the data source in QuickSight.",
      "C": "Use AWS Glue Elastic Views to create a materialized view for the database in Amazon S3. Create an S3 bucket policy to enforce column- level access control for the QuickSight users. Use Amazon S3 as the data source in QuickSight.",
      "D": "Use a Lake Formation blueprint to ingest the data from the database to the S3 data lake. Use Lake Formation to enforce column-level access control for the QuickSight users. Use Amazon Athena as the data source in QuickSight."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个由 AWS Lake Formation 管理的 Amazon S3 数据湖。该公司希望通过将数据湖中的数据与存储在 Amazon Aurora MySQL 数据库中的运营数据连接起来，在 Amazon QuickSight 中创建可视化。该公司希望实施列级授权，以便公司的营销团队只能访问数据库中的列子集。哪种解决方案能以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EMR 将数据直接从数据库摄取到 QuickSight SPICE 引擎。仅包含所需的列。",
      "B": "使用 AWS Glue Studio 将数据从数据库摄取到 S3 数据湖。为 QuickSight 用户附加一个 IAM 策略，以强制执行列级访问控制。使用 Amazon S3 作为 QuickSight 中的数据源。",
      "C": "使用 AWS Glue Elastic Views 为 Amazon S3 中的数据库创建物化视图。创建 S3 存储桶策略以强制执行 QuickSight 用户的列级访问控制。使用 Amazon S3 作为 QuickSight 中的数据源。",
      "D": "使用 Lake Formation 蓝图将数据从数据库摄取到 S3 数据湖。使用 Lake Formation 为 QuickSight 用户强制执行列级访问控制。使用 Amazon Athena 作为 QuickSight 中的数据源。"
    },
    "tags": [
      "Amazon QuickSight",
      "AWS Lake Formation",
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n这道题考察了 QuickSight 的数据源以及列级权限。关键在于考点：数据湖、列级授权、最小化运营开销。AWS Glue Elastic Views 可以创建物化视图，通过 S3 实现列级授权.",
      "why_correct": "选项 C (使用 AWS Glue Elastic Views 为 Amazon S3 中的数据库创建物化视图。创建 S3 存储桶策略以强制执行 QuickSight 用户的列级访问控制。使用 Amazon S3 作为 QuickSight 中的数据源。) 使用 Glue Elastic Views 创建物化视图，配合 S3 存储桶策略可以实现列级授权，且 QuickSight 支持 S3 作为数据源。",
      "why_wrong": "选项 A (使用 Amazon EMR 将数据直接从数据库摄取到 QuickSight SPICE 引擎。) EMR 方案无法实现列级别访问控制。选项 B (使用 AWS Glue Studio 将数据从数据库摄取到 S3 数据湖。为 QuickSight 用户附加一个 IAM 策略，以强制执行列级访问控制) 无法实现列级别访问控制。选项 D (使用 Lake Formation 蓝图将数据从数据库摄取到 S3 数据湖。使用 Lake Formation 为 QuickSight 用户强制执行列级访问控制。使用 Amazon Athena 作为 QuickSight 中的数据源。) 需要使用 Athena，不满足列级权限。"
    },
    "related_terms": [
      "Amazon QuickSight",
      "AWS Lake Formation",
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "IAM",
      "QuickSight SPICE"
    ]
  },
  {
    "id": 342,
    "topic": "1",
    "question_en": "A transaction processing company has weekly scripted batch jobs that run on Amazon EC2 instances. The EC2 instances are in an Auto Scaling group. The number of transactions can vary, but the baseline CPU utilization that is noted on each run is at least 60%. The company needs to provision the capacity 30 minutes before the jobs run. Currently, engineers complete this task by manually modifying the Auto Scaling group parameters. The company does not have the resources to analyze the required capacity trends for the Auto Scaling group counts. The company needs an automated way to modify the Auto Scaling group’s desired capacity. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a dynamic scaling policy for the Auto Scaling group. Configure the policy to scale based on the CPU utilization metric. Set the target value for the metric to 60%.",
      "B": "Create a scheduled scaling policy for the Auto Scaling group. Set the appropriate desired capacity, minimum capacity, and maximum capacity. Set the recurrence to weekly. Set the start time to 30 minutes before the batch jobs run.",
      "C": "Create a predictive scaling policy for the Auto Scaling group. Configure the policy to scale based on forecast. Set the scaling metric to CPU utilization. Set the target value for the metric to 60%. In the policy, set the instances to pre-launch 30 minutes before the jobs run.",
      "D": "Create an Amazon EventBridge event to invoke an AWS Lambda function when the CPU utilization metric value for the Auto Scaling group reaches 60%. Configure the Lambda function to increase the Auto Scaling group’s desired capacity and maximum capacity by 20%."
    },
    "correct_answer": "C",
    "vote_percentage": "63%",
    "question_cn": "一家交易处理公司拥有每周在 Amazon EC2 实例上运行的脚本批量作业。 EC2 实例位于 Auto Scaling 组中。 交易数量可能有所不同，但在每次运行中注意到的基线 CPU 利用率至少为 60%。 该公司需要在作业运行前 30 分钟预置容量。 目前，工程师通过手动修改 Auto Scaling 组的参数来完成此任务。 该公司没有资源来分析 Auto Scaling 组计数所需的容量趋势。 该公司需要一种自动化的方式来修改 Auto Scaling 组的所需容量。 哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "为 Auto Scaling 组创建一个动态扩展策略。 将该策略配置为基于 CPU 利用率指标进行扩展。 将该指标的目标值设置为 60%。",
      "B": "为 Auto Scaling 组创建一个计划扩展策略。 设置适当的所需容量、最小容量和最大容量。 将重复设置为每周。 将开始时间设置为批量作业运行前 30 分钟。",
      "C": "为 Auto Scaling 组创建一个预测扩展策略。 将该策略配置为基于预测进行扩展。 将扩展指标设置为 CPU 利用率。 将该指标的目标值设置为 60%。 在策略中，将实例设置为在作业运行前 30 分钟预启动。",
      "D": "创建一个 Amazon EventBridge 事件，以便在 Auto Scaling 组的 CPU 利用率指标值达到 60% 时调用一个 AWS Lambda 函数。 配置 Lambda 函数，将 Auto Scaling 组的所需容量和最大容量增加 20%。"
    },
    "tags": [
      "Amazon EC2 Auto Scaling",
      "Amazon EventBridge",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 63%），解析仅供参考。】\n\n这道题考察了如何根据时间预配置 EC2 容量。关键在于考点：预置容量、批量作业、自动化。预置策略能够满足提前扩容的需求。预测伸缩策略基于预测分析，自动化程度高，可以自动修改 Auto Scaling 组的所需容量，以应对负载。",
      "why_correct": "选项 C (为 Auto Scaling 组创建一个预测扩展策略。 将该策略配置为基于预测进行扩展。 将扩展指标设置为 CPU 利用率。 将该指标的目标值设置为 60%。 在策略中，将实例设置为在作业运行前 30 分钟预启动。) 预测伸缩策略能够根据指标预测，并在作业运行前 30 分钟预启动实例，满足题目的需求。",
      "why_wrong": "选项 A (为 Auto Scaling 组创建一个动态扩展策略。 将该策略配置为基于 CPU 利用率指标进行扩展。 将该指标的目标值设置为 60%。)  动态扩展策略不能提前预置容量。选项 B (为 Auto Scaling 组创建一个计划扩展策略)  需要手动配置，没有预先启动实例的操作。选项 D (创建一个 Amazon EventBridge 事件，以便在 Auto Scaling 组的 CPU 利用率指标值达到 60% 时调用一个 AWS Lambda 函数。 配置 Lambda 函数，将 Auto Scaling 组的所需容量和最大容量增加 20%。) 需要手动配置，增加了运营开销。"
    },
    "related_terms": [
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon EC2 Auto Scaling",
      "CPU Utilization",
      "Scaling Policy"
    ]
  },
  {
    "id": 343,
    "topic": "1",
    "question_en": "A solutions architect is designing a company’s disaster recovery (DR) architecture. The company has a MySQL database that runs on an Amazon EC2 instance in a private subnet with scheduled backup. The DR design needs to include multiple AWS Regions. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Migrate the MySQL database to multiple EC2 instances. Configure a standby EC2 instance in the DR Region. Turn on replication.",
      "B": "Migrate the MySQL database to Amazon RDS. Use a Multi-AZ deployment. Turn on read replication for the primary DB instance in the different Availability Zones.",
      "C": "Migrate the MySQL database to an Amazon Aurora global database. Host the primary DB cluster in the primary Region. Host the secondary DB cluster in the DR Region.",
      "D": "Store the scheduled backup of the MySQL database in an Amazon S3 bucket that is configured for S3 Cross-Region Replication (CRR). Use the data backup to restore the database in the DR Region."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在设计一家公司的灾难恢复（DR）架构。该公司有一个 MySQL 数据库，该数据库在私有子网中的 Amazon EC2 实例上运行，并有计划的备份。DR 设计需要在多个 AWS 区域中进行。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "将 MySQL 数据库迁移到多个 EC2 实例。在 DR 区域中配置一个备用 EC2 实例。打开复制功能。",
      "B": "将 MySQL 数据库迁移到 Amazon RDS。使用多可用区部署。打开在不同可用区中主数据库实例的读取复制。",
      "C": "将 MySQL 数据库迁移到 Amazon Aurora 全局数据库。在主区域中托管主数据库集群。在 DR 区域中托管辅助数据库集群。",
      "D": "将 MySQL 数据库的计划备份存储在配置为 S3 跨区域复制 (CRR) 的 Amazon S3 存储桶中。使用数据备份在 DR 区域中恢复数据库。"
    },
    "tags": [
      "MySQL",
      "Amazon RDS",
      "Amazon Aurora",
      "Amazon S3",
      "Cross-Region Replication"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察跨区域灾难恢复场景下，MySQL 数据库的解决方案架构设计，以及运营开销的考量。",
      "why_correct": "Amazon Aurora Global Database 专为跨区域灾难恢复设计，提供了最低的运营开销。它允许在主区域托管主数据库集群，并在 DR 区域中托管辅助数据库集群，实现了跨区域的数据库复制，保证了数据的持续同步与快速恢复能力。",
      "why_wrong": "A 选项需要手动管理 EC2 实例的复制和数据库同步，增加了运营开销。B 选项的 Multi-AZ 部署仅限于单个区域内的可用性提升，不满足跨区域 DR 的需求。D 选项依赖于备份和恢复，恢复时间较长，不符合快速灾难恢复的需求，且不如 Aurora Global Database 自动化程度高。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "DR",
      "AWS",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon Aurora Global Database",
      "S3",
      "CRR",
      "Amazon S3"
    ]
  },
  {
    "id": 344,
    "topic": "1",
    "question_en": "A company has a Java application that uses Amazon Simple Queue Service (Amazon SQS) to parse messages. The application cannot parse messages that are larger than 256 KB in size. The company wants to implement a solution to give the application the ability to parse messages as large as 50 MB. Which solution will meet these requirements with the FEWEST changes to the code?",
    "options_en": {
      "A": "Use the Amazon SQS Extended Client Library for Java to host messages that are larger than 256 KB in Amazon S3.",
      "B": "Use Amazon EventBridge to post large messages from the application instead of Amazon SQS.",
      "C": "Change the limit in Amazon SQS to handle messages that are larger than 256 KB.",
      "D": "Store messages that are larger than 256 KB in Amazon Elastic File System (Amazon EFS). Configure Amazon SQS to reference this location in the messages."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个使用 Amazon Simple Queue Service (Amazon SQS) 解析消息的 Java 应用程序。该应用程序无法解析大于 256 KB 的消息。该公司希望实施一个解决方案，使应用程序能够解析大到 50 MB 的消息。哪种解决方案以对代码的更改最少的方式满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon SQS 扩展客户端库 for Java 将大于 256 KB 的消息托管在 Amazon S3 中。",
      "B": "使用 Amazon EventBridge 从应用程序发布大消息，而不是 Amazon SQS。",
      "C": "更改 Amazon SQS 中的限制以处理大于 256 KB 的消息。",
      "D": "将大于 256 KB 的消息存储在 Amazon Elastic File System (Amazon EFS) 中。配置 Amazon SQS 在消息中引用此位置。"
    },
    "tags": [
      "Amazon SQS",
      "Amazon S3",
      "Amazon Elastic File System (Amazon EFS)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n这道题考察了如何处理大型消息。关键在于考点：大型消息、代码改动最小化。扩展客户端库是最佳选择。",
      "why_correct": "选项 A (使用 Amazon SQS 扩展客户端库 for Java 将大于 256 KB 的消息托管在 Amazon S3 中。)  SQS 扩展客户端库能帮助处理大于 256KB 的消息，并且对代码改动最小。",
      "why_wrong": "选项 B (使用 Amazon EventBridge 从应用程序发布大消息，而不是 Amazon SQS。) 改变了消息队列的架构，需要大量的代码修改，不满足需求。选项 C (更改 Amazon SQS 中的限制以处理大于 256 KB 的消息。) 无法改变 SQS 的限制。选项 D (将大于 256 KB 的消息存储在 Amazon Elastic File System (Amazon EFS) 中。配置 Amazon SQS 在消息中引用此位置。)  方案复杂，而且需要修改代码，不符合要求。"
    },
    "related_terms": [
      "Amazon SQS",
      "Amazon S3",
      "Amazon Elastic File System (Amazon EFS)",
      "Extended Client Library"
    ]
  },
  {
    "id": 345,
    "topic": "1",
    "question_en": "A company wants to restrict access to the content of one of its main web applications and to protect the content by using authorization techniques available on AWS. The company wants to implement a serverless architecture and an authentication solution for fewer than 100 users. The solution needs to integrate with the main web application and serve web content globally. The solution must also scale as the company's user base grows while providing the lowest login latency possible. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use Amazon Cognito for authentication. Use Lambda@Edge for authorization. Use Amazon CloudFront to serve the web application globally.",
      "B": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.",
      "C": "Use Amazon Cognito for authentication. Use AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.",
      "D": "Use AWS Directory Service for Microsoft Active Directory for authentication. Use Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望限制对其主要 Web 应用程序内容的访问，并通过使用 AWS 上可用的授权技术来保护内容。该公司希望实施无服务器架构和针对少于 100 个用户的身份验证解决方案。该解决方案需要与主要的 Web 应用程序集成并在全球范围内提供 Web 内容。该解决方案还必须随着公司用户群的增长而扩展，同时提供尽可能低的登录延迟。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Cognito 进行身份验证。使用 Lambda@Edge 进行授权。使用 Amazon CloudFront 在全球范围内提供 Web 应用程序。",
      "B": "使用 AWS Directory Service for Microsoft Active Directory 进行身份验证。使用 AWS Lambda 进行授权。使用 Application Load Balancer 在全球范围内提供 Web 应用程序。",
      "C": "使用 Amazon Cognito 进行身份验证。使用 AWS Lambda 进行授权。使用 Amazon S3 Transfer Acceleration 在全球范围内提供 Web 应用程序。",
      "D": "使用 AWS Directory Service for Microsoft Active Directory 进行身份验证。使用 Lambda@Edge 进行授权。使用 AWS Elastic Beanstalk 在全球范围内提供 Web 应用程序。"
    },
    "tags": [
      "Amazon Cognito",
      "Lambda@Edge",
      "Amazon CloudFront",
      "AWS Directory Service for Microsoft Active Directory",
      "AWS Lambda",
      "AWS Elastic Beanstalk"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n这道题考察了无服务器架构的身份验证和授权方案。关键在于考点：无服务器、全球分发、扩展性、低延迟、成本效益。Amazon Cognito 和 CloudFront 结合 Lambda@Edge 是一个很好的选择。",
      "why_correct": "选项 A (使用 Amazon Cognito 进行身份验证。使用 Lambda@Edge 进行授权。使用 Amazon CloudFront 在全球范围内提供 Web 应用程序。)  Amazon Cognito 提供用户身份验证，Lambda@Edge 进行授权，Amazon CloudFront 提供全球内容分发，可以满足需求，成本效益高。",
      "why_wrong": "选项 B (使用 AWS Directory Service for Microsoft Active Directory 进行身份验证。使用 AWS Lambda 进行授权。使用 Application Load Balancer 在全球范围内提供 Web 应用程序。) 需要 Active Directory 和 ALB，增加了成本和复杂性。选项 C (使用 Amazon Cognito 进行身份验证。使用 AWS Lambda 进行授权。使用 Amazon S3 Transfer Acceleration 在全球范围内提供 Web 应用程序。)  Amazon S3 Transfer Acceleration 不适合全球范围的内容分发。选项 D (使用 AWS Directory Service for Microsoft Active Directory 进行身份验证。使用 Lambda@Edge 进行授权。使用 AWS Elastic Beanstalk 在全球范围内提供 Web 应用程序。) 需要 Active Directory 和 Elastic Beanstalk，成本较高，不满足无服务器的要求。"
    },
    "related_terms": [
      "Amazon Cognito",
      "Lambda@Edge",
      "Amazon CloudFront",
      "AWS Lambda",
      "AWS Elastic Beanstalk",
      "Serverless",
      "AWS Directory Service for Microsoft Active Directory"
    ]
  },
  {
    "id": 346,
    "topic": "1",
    "question_en": "A company has an aging network-attached storage (NAS) array in its data center. The NAS array presents SMB shares and NFS shares to client workstations. The company does not want to purchase a new NAS array. The company also does not want to incur the cost of renewing the NAS array’s support contract. Some of the data is accessed frequently, but much of the data is inactive. A solutions architect needs to implement a solution that migrates the data to Amazon S3, uses S3 Lifecycle policies, and maintains the same look and feel for the client workstations. The solutions architect has identified AWS Storage Gateway as part of the solution. Which type of storage gateway should the solutions architect provision to meet these requirements?",
    "options_en": {
      "A": "Volume Gateway",
      "B": "Tape Gateway",
      "C": "Amazon FSx File Gateway",
      "D": "Amazon S3 File Gateway"
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其数据中心拥有一个老化的网络附加存储 (NAS) 阵列。该 NAS 阵列向客户端工作站提供 SMB 共享和 NFS 共享。该公司不想购买新的 NAS 阵列。该公司也不想承担续订 NAS 阵列支持合同的费用。一些数据被频繁访问，但很多数据是不活动的。解决方案架构师需要实施一个解决方案，该方案将数据迁移到 Amazon S3，使用 S3 生命周期策略，并保持客户端工作站相同的外观和感觉。解决方案架构师已确定 AWS Storage Gateway 是解决方案的一部分。解决方案架构师应该配置哪种类型的存储网关来满足这些要求？",
    "options_cn": {
      "A": "卷网关",
      "B": "磁带网关",
      "C": "Amazon FSx 文件网关",
      "D": "Amazon S3 文件网关"
    },
    "tags": [
      "AWS Storage Gateway",
      "Amazon S3",
      "Amazon FSx",
      "FSx File Gateway",
      "SMB",
      "NFS",
      "S3 Lifecycle policies"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查通过 AWS Storage Gateway 将本地 NAS 数据迁移到 S3 并保持文件访问方式的方案选择。",
      "why_correct": "Amazon S3 File Gateway 允许客户端工作站通过 SMB 和 NFS 协议访问存储在 Amazon S3 中的数据。它缓存经常访问的数据，并将不活动的数据移至 S3，实现按需访问。这满足了题目中保持客户端外观和感觉、迁移数据到 S3 以及支持 SMB/NFS 共享的需求。",
      "why_wrong": "卷网关 (Volume Gateway) 提供基于块的存储卷，不直接支持 SMB 和 NFS 共享，不适用本场景。磁带网关 (Tape Gateway) 用于将数据备份到虚拟磁带库 (VTL)，通常用于数据归档，不满足频繁访问需求。Amazon FSx File Gateway 提供对 FSx for Windows File Server 和 FSx for Lustre 的访问，而不是直接访问 S3，且 FSx 需要额外的部署和管理，与题目中避免购买新 NAS 的需求相悖。"
    },
    "related_terms": [
      "Amazon S3",
      "NAS",
      "SMB",
      "NFS",
      "AWS Storage Gateway",
      "S3 lifecycle policy",
      "Volume Gateway",
      "Tape Gateway",
      "Amazon FSx File Gateway",
      "FSx for Windows File Server",
      "FSx for Lustre"
    ]
  },
  {
    "id": 347,
    "topic": "1",
    "question_en": "A company has an application that is running on Amazon EC2 instances. A solutions architect has standardized the company on a particular instance family and various instance sizes based on the current needs of the company. The company wants to maximize cost savings for the application over the next 3 years. The company needs to be able to change the instance family and sizes in the next 6 months based on application popularity and usage. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Compute Savings Plan",
      "B": "EC2 Instance Savings Plan",
      "C": "Zonal Reserved Instances",
      "D": "Standard Reserved Instances"
    },
    "correct_answer": "A",
    "vote_percentage": "79%",
    "question_cn": "一家公司在其 Amazon EC2 实例上运行一个应用程序。解决方案架构师已根据公司当前的需求，将公司标准化为特定的实例系列和各种实例大小。该公司希望在未来 3 年内最大限度地节省应用程序的成本。该公司需要在未来 6 个月内根据应用程序的受欢迎程度和使用情况更改实例系列和大小。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "计算节省计划",
      "B": "EC2 实例节省计划",
      "C": "区域预留实例",
      "D": "标准预留实例"
    },
    "tags": [
      "EC2",
      "Reserved Instances",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 79%），解析仅供参考。】\n\n考查 EC2 成本优化策略；与 Reserved Instances 的类型、计费模式、灵活性相关。",
      "why_correct": "标准预留实例（Standard Reserved Instances）提供了最大的灵活性。它们允许在实例系列、操作系统和租赁类型等属性上进行修改，而无需交换实例。这满足了公司在未来 6 个月内更改实例系列和大小的需求，同时在三年内提供成本节约。标准预留实例是长期承诺的，因此提供比按需实例更低的费率，这符合题干中“最大限度地节省成本”的要求。",
      "why_wrong": "计算节省计划（Compute Savings Plans）和 EC2 实例节省计划（EC2 Instance Savings Plans）虽然也能提供成本节约，但其灵活性不如标准预留实例。计算节省计划适用于各种计算使用情况，但不能像预留实例那样在实例属性（如实例系列）之间进行灵活的修改。EC2 实例节省计划则绑定到特定的实例系列和大小，不满足未来 6 个月内更改需求。区域预留实例（Regional Reserved Instances）提供了在同一区域内的实例上进行灵活性的能力，但是不能跨实例系列变更，并且不像标准预留实例那样灵活，并且对于未来 6 个月内的变更不友好。"
    },
    "related_terms": [
      "EC2",
      "Compute Savings Plans",
      "EC2 Instance Savings Plans",
      "Reserved Instances",
      "Standard Reserved Instances",
      "Regional Reserved Instances"
    ]
  },
  {
    "id": 348,
    "topic": "1",
    "question_en": "A company collects data from a large number of participants who use wearable devices. The company stores the data in an Amazon DynamoDB table and uses applications to analyze the data. The data workload is constant and predictable. The company wants to stay at or below its forecasted budget for DynamoDB. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use provisioned mode and DynamoDB Standard-Infrequent Access (DynamoDB Standard-IA). Reserve capacity for the forecasted workload.",
      "B": "Use provisioned mode. Specify the read capacity units (RCUs) and write capacity units (WCUs).",
      "C": "Use on-demand mode. Set the read capacity units (RCUs) and write capacity units (WCUs) high enough to accommodate changes in the workload.",
      "D": "Use on-demand mode. Specify the read capacity units (RCUs) and write capacity units (WCUs) with reserved capacity."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家公司从使用可穿戴设备的众多参与者那里收集数据。该公司将数据存储在 Amazon DynamoDB 表中，并使用应用程序分析数据。数据工作负载是恒定的且可预测的。该公司希望保持或低于其 DynamoDB 的预测预算。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用预置模式和 DynamoDB 标准-不频繁访问（DynamoDB Standard-IA）。为预测的工作负载预留容量。",
      "B": "使用预置模式。指定读取容量单元 (RCU) 和写入容量单元 (WCU)。",
      "C": "使用按需模式。将读取容量单元 (RCU) 和写入容量单元 (WCU) 设置得足够高，以适应工作负载的变化。",
      "D": "使用按需模式。使用预留容量指定读取容量单元 (RCU) 和写入容量单元 (WCU)。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB Standard-IA",
      "DynamoDB Provisioned Mode",
      "DynamoDB On-Demand Mode",
      "RCU",
      "WCU",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n考查 DynamoDB 的存储模式、容量模式和存储类选择；与成本优化、数据访问模式相关。",
      "why_correct": "选项 A 选择了预置模式（Provisioned Mode）和 DynamoDB 标准-不频繁访问（DynamoDB Standard-IA）存储类，并为预测的工作负载预留容量。由于数据工作负载是恒定的且可预测的，预留容量可以确保成本效益。DynamoDB Standard-IA 存储类针对不频繁访问的数据进行了优化，进一步降低了存储成本。",
      "why_wrong": "选项 B 选择了预置模式，但未考虑存储类的选择，这意味着没有针对不频繁访问的数据进行优化。选项 C 选择了按需模式（On-Demand Mode），按需模式虽然无需预先配置容量，但成本通常高于预置模式，尤其是在工作负载可预测的情况下。另外，将 RCU 和 WCU 设置得足够高来适应变化，意味着为工作负载的峰值付费，这增加了成本，且与题目的预算约束相悖。选项 D 描述了一种错误的组合，按需模式下无法使用预留容量。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "RCU",
      "DynamoDB Standard-IA",
      "DynamoDB Provisioned Mode",
      "DynamoDB On-Demand Mode",
      "WCU"
    ]
  },
  {
    "id": 349,
    "topic": "1",
    "question_en": "A company stores confidential data in an Amazon Aurora PostgreSQL database in the ap-southeast-3 Region. The database is encrypted with an AWS Key Management Service (AWS KMS) customer managed key. The company was recently acquired and must securely share a backup of the database with the acquiring company’s AWS account in ap-southeast-3. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a database snapshot. Copy the snapshot to a new unencrypted snapshot. Share the new snapshot with the acquiring company’s AWS account.",
      "B": "Create a database snapshot. Add the acquiring company’s AWS account to the KMS key policy. Share the snapshot with the acquiring company’s AWS account.",
      "C": "Create a database snapshot that uses a different AWS managed KMS key. Add the acquiring company’s AWS account to the KMS key alias. Share the snapshot with the acquiring company's AWS account.",
      "D": "Create a database snapshot. Download the database snapshot. Upload the database snapshot to an Amazon S3 bucket. Update the S3 bucket policy to allow access from the acquiring company’s AWS account."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 ap-southeast-3 区域的 Amazon Aurora PostgreSQL 数据库中存储机密数据。该数据库使用 AWS Key Management Service (AWS KMS) 客户托管密钥进行加密。该公司最近被收购，必须安全地与收购公司在 ap-southeast-3 区域的 AWS 账户共享数据库的备份。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个数据库快照。将快照复制到一个新的未加密快照。与收购公司的 AWS 账户共享新快照。",
      "B": "创建一个数据库快照。将收购公司的 AWS 账户添加到 KMS 密钥策略中。与收购公司的 AWS 账户共享快照。",
      "C": "创建一个使用不同 AWS 托管 KMS 密钥的数据库快照。将收购公司的 AWS 账户添加到 KMS 密钥别名中。与收购公司的 AWS 账户共享快照。",
      "D": "创建一个数据库快照。下载数据库快照。将数据库快照上传到 Amazon S3 存储桶。更新 S3 存储桶策略以允许从收购公司的 AWS 账户进行访问。"
    },
    "tags": [
      "Amazon Aurora",
      "PostgreSQL",
      "AWS KMS",
      "Database Snapshot",
      "Cross-Account Access"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何在跨账户之间安全地共享使用客户托管 KMS 密钥加密的 Aurora 数据库备份，涉及 Aurora 数据库快照、KMS 密钥策略以及跨账户访问授权。重点在于理解如何授予另一个 AWS 账户访问 KMS 密钥的权限。",
      "why_correct": "选项 B 是正确答案，因为它直接使用了 KMS 密钥策略来授权收购公司的 AWS 账户访问加密数据库快照所需的 KMS 密钥。 通过将收购公司的 AWS 账户添加到 KMS 密钥策略中，收购公司就能使用 KMS 密钥解密快照，从而访问数据库备份。创建数据库快照之后，可以直接共享快照，无需任何额外的解密或重新加密操作，最简洁且安全地满足了要求。",
      "why_wrong": "选项 A 错误，因为创建未加密快照会导致数据的加密保护失效，违反了题目中对机密数据安全性的要求。选项 C 错误，因为创建一个使用不同 KMS 密钥的快照，无法满足通过原始 KMS 密钥解密的需求，并且将账户添加到 KMS 密钥别名无法赋予访问权限。 选项 D 错误，虽然可以通过 S3 存储桶分享快照，但此操作增加了数据泄露的风险，需要先解密再上传，流程复杂，并且忽略了直接利用 KMS 密钥策略进行授权的更安全的方案。此外，下载和上传数据涉及到数据传输，比直接共享快照的效率更低。"
    },
    "related_terms": [
      "Amazon Aurora",
      "PostgreSQL",
      "AWS KMS",
      "Amazon S3",
      "Database Snapshot",
      "AWS Account",
      "KMS Key Policy"
    ]
  },
  {
    "id": 350,
    "topic": "1",
    "question_en": "A company uses a 100 GB Amazon RDS for Microsoft SQL Server Single-AZ DB instance in the us-east-1 Region to store customer transactions. The company needs high availability and automatic recovery for the DB instance. The company must also run reports on the RDS database several times a year. The report process causes transactions to take longer than usual to post to the customers’ accounts. The company needs a solution that will improve the performance of the report process. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Modify the DB instance from a Single-AZ DB instance to a Multi-AZ deployment.",
      "B": "Take a snapshot of the current DB instance. Restore the snapshot to a new RDS deployment in another Availability Zone.",
      "C": "Create a read replica of the DB instance in a different Availability Zone. Point all requests for reports to the read replica.",
      "D": "Migrate the database to RDS Custom",
      "E": "Use RDS Proxy to limit reporting requests to the maintenance window."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用位于 us-east-1 区域的 100 GB Amazon RDS for Microsoft SQL Server 单可用区数据库实例来存储客户交易。该公司需要高可用性和数据库实例的自动恢复。该公司还必须每年多次在 RDS 数据库上运行报告。报告流程导致交易花费比平时更长的时间才能发布到客户的帐户。该公司需要一个能够提高报告流程性能的解决方案。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将数据库实例从单可用区数据库实例修改为多可用区部署。",
      "B": "拍摄当前数据库实例的快照。将快照还原到另一个可用区中的新 RDS 部署。",
      "C": "在不同的可用区中创建数据库实例的只读副本。将所有报告请求指向只读副本。",
      "D": "将数据库迁移到 RDS Custom。",
      "E": "使用 RDS Proxy 将报告请求限制在维护时段内。"
    },
    "tags": [
      "Amazon RDS",
      "Multi-AZ",
      "Read Replica",
      "RDS Proxy",
      "RDS Custom"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考查 RDS 数据库的高可用性和性能优化方案，特别是针对单可用区数据库的改进以及如何分离读写负载。与 Multi-AZ 部署、Read Replica、RDS Proxy 和 RDS Custom 的选型与对比相关。",
      "why_correct": "将单可用区数据库实例修改为多可用区部署（Multi-AZ）能够提高数据库的可用性。Multi-AZ 部署在主数据库实例发生故障时，自动将故障转移到备用数据库实例，从而减少停机时间。虽然 Multi-AZ 部署主要关注高可用性，但其内在的备用实例也可以一定程度地减轻主实例的负载压力。",
      "why_wrong": "选项 B 错误，因为在另一个可用区中还原快照创建了一个新的数据库实例，虽然可以用于报告，但无法解决主数据库实例的可用性问题，也增加了维护和同步的复杂性。选项 C 错误，虽然 Read Replica 可以将报告请求分流，提高主数据库性能，但如果只是单个只读副本，故障转移不如 Multi-AZ 部署，并且也增加了数据库管理复杂性。选项 D 错误，RDS Custom 允许更精细的数据库配置和管理，但它本身并不能直接提高可用性，也不能解决报告流程导致交易延迟的问题，反而会增加复杂性。选项 E 错误，使用 RDS Proxy 将报告请求限制在维护时段内，无法提升报告流程的性能，反而可能因为维护时段限制了报告的运行，也无法提高数据库可用性。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "Read Replica",
      "RDS Proxy",
      "RDS for Microsoft SQL Server",
      "RDS Custom"
    ]
  },
  {
    "id": 351,
    "topic": "1",
    "question_en": "A company is moving its data management application to AWS. The company wants to transition to an event-driven architecture. The architecture needs to be more distributed and to use serverless concepts while performing the different aspects of the workfiow. The company also wants to minimize operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Build out the workfiow in AWS Glue. Use AWS Glue to invoke AWS Lambda functions to process the workfiow steps.",
      "B": "Build out the workfiow in AWS Step Functions. Deploy the application on Amazon EC2 instances. Use Step Functions to invoke the workfiow steps on the EC2 instances.",
      "C": "Build out the workfiow in Amazon EventBridge. Use EventBridge to invoke AWS Lambda functions on a schedule to process the workfiow steps.",
      "D": "Build out the workfiow in AWS Step Functions. Use Step Functions to create a state machine. Use the state machine to invoke AWS Lambda functions to process the workfiow steps."
    },
    "correct_answer": "D",
    "vote_percentage": "88%",
    "question_cn": "一家公司正在将其数据管理应用程序迁移到 AWS。该公司希望过渡到事件驱动架构。该架构需要更分散，并使用无服务器概念来执行工作流的不同方面。该公司还希望最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Glue 中构建工作流。使用 AWS Glue 调用 AWS Lambda 函数来处理工作流步骤。",
      "B": "在 AWS Step Functions 中构建工作流。将应用程序部署在 Amazon EC2 实例上。使用 Step Functions 在 EC2 实例上调用工作流步骤。",
      "C": "在 Amazon EventBridge 中构建工作流。使用 EventBridge 定期间隔调用 AWS Lambda 函数来处理工作流步骤。",
      "D": "在 AWS Step Functions 中构建工作流。使用 Step Functions 创建状态机。使用状态机调用 AWS Lambda 函数来处理工作流步骤。"
    },
    "tags": [
      "AWS Step Functions",
      "AWS Lambda",
      "Serverless"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 88%），解析仅供参考。】\n\n本题考查事件驱动架构在 AWS 上实现。需要选择一个能够支持事件驱动、无服务器，并减少运营开销的方案。与 AWS Step Functions、AWS Lambda、EventBridge、以及不同计算服务（EC2、Glue）的选型相关。",
      "why_correct": "选项 D 最佳，因为它结合了 Step Functions 和 Lambda。Step Functions 是一种完全托管的无服务器编排服务，可以协调多个 Lambda 函数。 使用状态机将 Lambda 函数链接在一起，实现工作流的编排和执行。 这种方案是事件驱动的，并且是无服务器的，因此无需管理底层基础设施，符合题目的要求，同时也最大限度地减少了运营开销。",
      "why_wrong": "选项 A 错误，虽然 AWS Glue 可以用于构建 ETL（Extract, Transform, Load）工作流，并且可以调用 Lambda 函数，但它主要用于数据处理和转换，而非通用的事件驱动架构。选项 B 错误，将应用程序部署在 EC2 实例上与无服务器的目标相悖，会增加运营开销，EC2 实例的维护和管理也增加了复杂性。选项 C 错误，虽然 EventBridge 支持事件驱动，并且可以调用 Lambda 函数，但使用定期间隔来调用 Lambda 函数并非理想的事件驱动方式，不如直接通过事件触发更有效，并且这种方式不一定是最佳的、最节约成本的方式。"
    },
    "related_terms": [
      "AWS Step Functions",
      "AWS Lambda",
      "Serverless",
      "Amazon EC2",
      "AWS Glue",
      "Amazon EventBridge"
    ]
  },
  {
    "id": 352,
    "topic": "1",
    "question_en": "A company is designing the network for an online multi-player game. The game uses the UDP networking protocol and will be deployed in eight AWS Regions. The network architecture needs to minimize latency and packet loss to give end users a high-quality gaming experience. Which solution will meet these requirements?",
    "options_en": {
      "A": "Setup a transit gateway in each Region. Create inter-Region peering attachments between each transit gateway.",
      "B": "Set up AWS Global Accelerator with UDP listeners and endpoint groups in each Region.",
      "C": "Set up Amazon CloudFront with UDP turned on. Configure an origin in each Region.",
      "D": "Set up a VPC peering mesh between each Region. Turn on UDP for each VPC."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为其在线多人游戏设计网络。该游戏使用 UDP 网络协议，并将部署在八个 AWS 区域中。网络架构需要最大限度地减少延迟和丢包，以给最终用户提供高质量的游戏体验。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在每个区域设置一个 Transit Gateway。在每个 Transit Gateway 之间创建区域互连附件。",
      "B": "设置 AWS Global Accelerator，并在每个区域中使用 UDP 监听器和终端节点组。",
      "C": "设置 Amazon CloudFront，并打开 UDP。在每个区域配置一个源。",
      "D": "在每个区域之间设置 VPC 对等互联网状结构。为每个 VPC 打开 UDP。"
    },
    "tags": [
      "AWS Global Accelerator",
      "UDP",
      "Latency",
      "Packet Loss"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查如何在跨多个 AWS 区域部署的在线游戏中，最大限度地减少延迟和丢包。这与网络优化、全球加速服务以及 UDP 协议的使用有关。",
      "why_correct": "AWS Global Accelerator 是一个网络服务，可以提高应用程序的性能，尤其是在需要通过互联网访问的应用程序。它通过使用 AWS 的全球网络基础设施来优化流量路由，从而减少延迟和丢包。对于使用 UDP 的游戏场景，Global Accelerator 支持 UDP 监听器和终端节点组，这允许游戏服务器接收和处理来自全球用户的 UDP 流量，从而满足题目中对低延迟和低丢包的需求。",
      "why_wrong": "A 选项，使用 Transit Gateway 和区域互连附件虽然可以实现 VPC 之间的连接，但主要关注的是 VPC 之间的网络连接，而不能直接优化客户端到服务器的流量路径，无法有效减少延迟和丢包，不如 Global Accelerator 针对全球用户的优化效果好。C 选项，CloudFront 是一种内容分发网络（CDN），主要用于加速静态和动态内容的交付，虽然 CloudFront 可以支持 UDP，但它不专注于游戏服务器的流量优化，并且可能无法提供最佳的 UDP 游戏体验，因为它主要针对 HTTP(S) 流量优化。D 选项，VPC 对等互连虽然能实现 VPC 之间的连接，但类似于选项 A，无法提供对 UDP 流量的全球加速和优化，不能解决跨区域的低延迟和丢包问题，这种架构的延迟主要取决于网络路径，而没有像 Global Accelerator 这样的优化机制。"
    },
    "related_terms": [
      "AWS Global Accelerator",
      "UDP",
      "Transit Gateway",
      "VPC",
      "Amazon CloudFront",
      "Latency",
      "Packet Loss"
    ]
  },
  {
    "id": 353,
    "topic": "1",
    "question_en": "A company hosts a three-tier web application on Amazon EC2 instances in a single Availability Zone. The web application uses a self- managed MySQL database that is hosted on an EC2 instance to store data in an Amazon Elastic Block Store (Amazon EBS) volume. The MySQL database currently uses a 1 TB Provisioned IOPS SSD (io2) EBS volume. The company expects trafic of 1,000 IOPS for both reads and writes at peak trafic. The company wants to minimize any disruptions, stabilize performance, and reduce costs while retaining the capacity for double the IOPS. The company wants to move the database tier to a fully managed solution that is highly available and fault tolerant. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with an io2 Block Express EBS volume.",
      "B": "Use a Multi-AZ deployment of an Amazon RDS for MySQL DB instance with a General Purpose SSD (gp2) EBS volume.",
      "C": "Use Amazon S3 Intelligent-Tiering access tiers.",
      "D": "Use two large EC2 instances to host the database in active-passive mode."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司在单个可用区内的 Amazon EC2 实例上托管一个三层 Web 应用程序。 该 Web 应用程序使用托管在 EC2 实例上的自管理 MySQL 数据库将数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。 MySQL 数据库当前使用 1 TB Provisioned IOPS SSD (io2) EBS 卷。该公司预计在峰值流量时，读取和写入的 IOPS 均为 1,000。该公司希望最大限度地减少任何中断、稳定性能并降低成本，同时保留双倍 IOPS 的容量。该公司希望将数据库层移动到完全托管的、具有高可用性和容错能力的解决方案。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon RDS for MySQL 数据库实例的多可用区部署，该实例带有 io2 Block Express EBS 卷。",
      "B": "使用 Amazon RDS for MySQL 数据库实例的多可用区部署，该实例带有 General Purpose SSD (gp2) EBS 卷。",
      "C": "使用 Amazon S3 Intelligent-Tiering 访问层。",
      "D": "使用两个大型 EC2 实例以主动-被动模式托管数据库。"
    },
    "tags": [
      "Amazon RDS",
      "MySQL",
      "Amazon EBS",
      "Multi-AZ Deployment",
      "General Purpose SSD (gp2)",
      "Provisioned IOPS SSD (io2)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n考查 Amazon RDS for MySQL 的选型，以及多可用区部署对可用性、容错能力和成本的影响。与 EBS 存储类型（特别是 io2 和 gp2）的性能和成本对比相关。",
      "why_correct": "Amazon RDS for MySQL 多可用区部署（Multi-AZ deployment）提供了高可用性和容错能力，满足了题目对数据库层高可用性的要求。使用 General Purpose SSD (gp2) EBS 卷可以满足 1,000 IOPS 的需求，且成本低于 io2 卷。由于 RDS 自动管理数据库，简化了管理，并降低了维护成本。",
      "why_wrong": "A. io2 Block Express EBS 卷成本较高，不符合题目的“降低成本”要求。虽然提供高性能，但并非最具成本效益的方案。C. Amazon S3 Intelligent-Tiering 是一种对象存储服务，不适用于数据库层。将数据库迁移到 S3 不可行，且无法满足性能和一致性的要求。D. 使用两个 EC2 实例以主动-被动模式托管数据库虽然提供高可用性，但需要手动管理，增加了管理成本和复杂性。并且，主动-被动模式的资源利用率不高，并且无法像 RDS 一样自动处理故障转移，整体成本效益不如 RDS。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "Amazon EBS",
      "RDS",
      "io2",
      "gp2",
      "Amazon S3",
      "EC2",
      "Multi-AZ Deployment",
      "Provisioned IOPS SSD (io2)",
      "General Purpose SSD (gp2)",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "id": 354,
    "topic": "1",
    "question_en": "A company hosts a serverless application on AWS. The application uses Amazon API Gateway, AWS Lambda, and an Amazon RDS for PostgreSQL database. The company notices an increase in application errors that result from database connection timeouts during times of peak trafic or unpredictable trafic. The company needs a solution that reduces the application failures with the least amount of change to the code. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Reduce the Lambda concurrency rate.",
      "B": "Enable RDS Proxy on the RDS DB instance.",
      "C": "Resize the RDS DB instance class to accept more connections.",
      "D": "Migrate the database to Amazon DynamoDB with on-demand scaling."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上托管一个无服务器应用程序。该应用程序使用 Amazon API Gateway、AWS Lambda 和用于 PostgreSQL 数据库的 Amazon RDS。该公司注意到，在高峰流量或不可预测的流量期间，由于数据库连接超时，应用程序错误有所增加。该公司需要一个解决方案，以最少的代码更改来减少应用程序故障。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "降低 Lambda 并发速率。",
      "B": "在 RDS 数据库实例上启用 RDS 代理。",
      "C": "调整 RDS 数据库实例的大小以接受更多连接。",
      "D": "将数据库迁移到具有按需扩展的 Amazon DynamoDB。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "AWS Lambda",
      "API Gateway",
      "PostgreSQL",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查了无服务器应用程序在数据库连接方面的性能优化，重点关注了 RDS 数据库连接管理。与数据库连接池、负载均衡、数据库的扩展性相关。",
      "why_correct": "在 RDS 数据库实例上启用 RDS 代理是解决数据库连接超时问题的有效方法。RDS 代理充当数据库连接池，可以复用数据库连接，从而减少了 Lambda 函数每次调用都必须创建新连接的需求。这显著提高了连接效率，降低了数据库负载，并减少了由于连接耗尽或超时导致的应用程序错误。RDS 代理支持连接多路复用，这意味着多个客户端连接可以复用较少的数据库连接，从而减少数据库负载，提高应用程序的伸缩性，特别是针对并发性高的无服务器应用程序。",
      "why_wrong": "降低 Lambda 并发速率（选项 A）会限制应用程序处理请求的能力，从而影响应用程序的性能，反而可能因为资源闲置而无法解决数据库连接问题。调整 RDS 数据库实例的大小以接受更多连接（选项 C）可能对解决连接问题有一定帮助，但它无法根本性地解决连接创建和关闭的开销问题，也无法有效管理连接激增的情况，并且需要停机维护。将数据库迁移到具有按需扩展的 Amazon DynamoDB（选项 D）虽然可以解决数据库扩展性问题，但涉及了大规模的架构更改，需要重新编写代码，这与题目中“最少代码更改”的要求相矛盾，而且对于 SQL 数据库的操作和复杂查询的支持不如 PostgreSQL 数据库。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon RDS",
      "PostgreSQL",
      "RDS Proxy",
      "DynamoDB"
    ]
  },
  {
    "id": 355,
    "topic": "1",
    "question_en": "A company is migrating an old application to AWS. The application runs a batch job every hour and is CPU intensive. The batch job takes 15 minutes on average with an on-premises server. The server has 64 virtual CPU (vCPU) and 512 GiB of memory. Which solution will run the batch job within 15 minutes with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Lambda with functional scaling.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
      "C": "Use Amazon Lightsail with AWS Auto Scaling.",
      "D": "Use AWS Batch on Amazon EC2."
    },
    "correct_answer": "D",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在将旧应用程序迁移到 AWS。该应用程序每小时运行一个批处理作业，并且是 CPU 密集型作业。批处理作业在本地服务器上平均需要 15 分钟。服务器有 64 个虚拟 CPU (vCPU) 和 512 GiB 内存。哪种解决方案将在 15 分钟内运行批处理作业，并且运营开销最少？",
    "options_cn": {
      "A": "使用 AWS Lambda 和功能扩展。",
      "B": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate。",
      "C": "使用 Amazon Lightsail 和 AWS Auto Scaling。",
      "D": "在 Amazon EC2 上使用 AWS Batch。"
    },
    "tags": [
      "AWS Lambda",
      "Function Scaling",
      "Batch Processing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 96%），解析仅供参考。】\n\n考察如何在 AWS 上运行 CPU 密集型批处理作业，并关注运行时间、成本和运营开销的优化。",
      "why_correct": "AWS Batch 专为运行批处理作业而设计，可以根据作业的资源需求自动预置计算资源。它支持作业并行执行，并且可以利用 Amazon EC2 的多种实例类型，以满足 CPU 密集型作业的需求。由于与题干中的 CPU 密集型作业契合，并能满足15分钟内的运行时间要求，且运营开销相对较少，所以最适合。",
      "why_wrong": "A. AWS Lambda 的最大执行时间限制为 15 分钟，可能无法满足作业需求。虽然 Lambda 可以水平扩展，但对于长时间运行的 CPU 密集型作业，其成本效益不如 AWS Batch。B. Amazon ECS 和 Fargate 适合容器化应用，但对于批处理作业，管理容器和镜像的开销可能比直接使用 AWS Batch 更高。C. Amazon Lightsail 主要面向小型项目和简单应用，不适合大规模、CPU 密集型的批处理作业。Auto Scaling 无法单独解决运行时间问题。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon ECS",
      "AWS Fargate",
      "Amazon Lightsail",
      "AWS Auto Scaling",
      "Amazon EC2",
      "AWS Batch",
      "vCPU",
      "GiB"
    ]
  },
  {
    "id": 356,
    "topic": "1",
    "question_en": "A company stores its data objects in Amazon S3 Standard storage. A solutions architect has found that 75% of the data is rarely accessed after 30 days. The company needs all the data to remain immediately accessible with the same high availability and resiliency, but the company wants to minimize storage costs. Which storage solution will meet these requirements?",
    "options_en": {
      "A": "Move the data objects to S3 Glacier Deep Archive after 30 days.",
      "B": "Move the data objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
      "C": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 30 days.",
      "D": "Move the data objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) immediately."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司将其数据对象存储在 Amazon S3 标准存储中。一位解决方案架构师发现，75% 的数据在 30 天后很少被访问。公司需要所有数据保持立即可访问，并具有相同的高可用性和弹性，但公司希望最大限度地降低存储成本。哪个存储解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 30 天后，将数据对象移动到 S3 Glacier Deep Archive。",
      "B": "在 30 天后，将数据对象移动到 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "C": "在 30 天后，将数据对象移动到 S3 One Zone-Infrequent Access (S3 One Zone-IA)。",
      "D": "立即将数据对象移动到 S3 One Zone-Infrequent Access (S3 One Zone-IA)。"
    },
    "tags": [
      "Amazon S3",
      "S3 Standard-IA",
      "S3 Glacier Deep Archive",
      "S3 One Zone-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 S3 存储类别的选择，主要与存储成本、数据访问频率、可用性、弹性和数据检索时间相关。需要根据访问模式和成本需求，选择合适的 S3 存储类别。本题还涉及了生命周期策略的使用。",
      "why_correct": "S3 Standard-Infrequent Access (S3 Standard-IA) 存储类别是为不常访问的数据设计的，但仍然需要快速访问。它提供了与 S3 Standard 相同的持久性，并且能够立即访问数据。当数据超过 30 天未被访问时，将其转换为 S3 Standard-IA 是一个降低存储成本的有效方法，同时保留数据的即时访问能力。",
      "why_wrong": "A. S3 Glacier Deep Archive 专为长期数据归档而设计，数据检索需要几小时，这不满足“立即可访问”的要求。C 和 D. S3 One Zone-Infrequent Access (S3 One Zone-IA) 存储类别仅将数据存储在一个可用区，虽然成本较低，但可用性低于 S3 Standard 和 S3 Standard-IA。选项 D 立即转移不符合题目中 30 天后转移的要求，且在 30 天内访问频繁的数据不应该被立即转移以节省成本。综上，选项 C 和 D 均不满足题目中对高可用性的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard-IA",
      "S3 Glacier Deep Archive",
      "S3 Standard",
      "S3 One Zone-IA"
    ]
  },
  {
    "id": 357,
    "topic": "1",
    "question_en": "A gaming company is moving its public scoreboard from a data center to the AWS Cloud. The company uses Amazon EC2 Windows Server instances behind an Application Load Balancer to host its dynamic application. The company needs a highly available storage solution for the application. The application consists of static files and dynamic server-side code. Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Store the static files on Amazon S3. Use Amazon CloudFront to cache objects at the edge.",
      "B": "Store the static files on Amazon S3. Use Amazon ElastiCache to cache objects at the edge.",
      "C": "Store the server-side code on Amazon Elastic File System (Amazon EFS). Mount the EFS volume on each EC2 instance to share the files.",
      "D": "Store the server-side code on Amazon FSx for Windows File Server. Mount the FSx for Windows File Server volume on each EC2 instance to share the files",
      "E": "Store the server-side code on a General Purpose SSD (gp2) Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on each EC2 instance to share the files."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "一家游戏公司正在将其公共排行榜从数据中心迁移到 AWS 云。该公司使用 Application Load Balancer 后面的 Amazon EC2 Windows Server 实例来托管其动态应用程序。该公司需要为该应用程序提供高可用性的存储解决方案。该应用程序由静态文件和动态服务器端代码组成。解决方案架构师应采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将静态文件存储在 Amazon S3 中。使用 Amazon CloudFront 在边缘缓存对象。",
      "B": "将静态文件存储在 Amazon S3 中。使用 Amazon ElastiCache 在边缘缓存对象。",
      "C": "将服务器端代码存储在 Amazon Elastic File System (Amazon EFS) 中。 在每个 EC2 实例上挂载 EFS 卷以共享文件。",
      "D": "将服务器端代码存储在 Amazon FSx for Windows File Server 中。 在每个 EC2 实例上挂载 FSx for Windows File Server 卷以共享文件。",
      "E": "将服务器端代码存储在通用 SSD (gp2) Amazon Elastic Block Store (Amazon EBS) 卷中。在每个 EC2 实例上挂载 EBS 卷以共享文件。"
    },
    "tags": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon EC2",
      "High Availability",
      "Storage"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n考查高可用存储解决方案的选型，以及如何为 EC2 实例上的应用提供静态内容缓存。与 Amazon S3、CloudFront、ElastiCache 和 EFS、FSx、EBS 等服务相关。",
      "why_correct": "将静态文件存储在 Amazon S3 中可以提供高可用性、持久性，以及按需扩展。 使用 Amazon CloudFront 作为内容分发网络（CDN）在边缘缓存对象，可以降低延迟，提高访问速度，同时减轻源服务器（EC2）的负载。 S3 和 CloudFront 结合使用是静态内容部署的最佳实践。",
      "why_wrong": {
        "B": "Amazon ElastiCache 主要用于缓存动态数据，例如数据库查询结果，而不是静态文件。虽然 ElastiCache 可以在边缘缓存数据，但它不是为静态文件存储和分发设计的，不适用于该场景。它不能替代 S3 和 CloudFront 的组合。",
        "C": "Amazon Elastic File System (Amazon EFS) 适用于需要共享文件系统的场景，但它不是为存储应用程序代码设计的，不适用。EFS 无法直接解决静态内容的缓存问题。将服务器端代码存储在 EFS 中会增加复杂性。同时，EFS 本身不能提供高可用性，因为它单点故障。另外 EFS 在 EC2 实例挂载，会增加额外的开销，性能不是最佳。",
        "D": "Amazon FSx for Windows File Server 类似于 EFS，设计用于共享文件系统。将服务器端代码存储在 FSx for Windows File Server 中同样不能解决静态文件存储和 CDN 加速的问题。FSx 增加了 EC2 实例间的开销。 FSx for Windows File Server 部署起来比较复杂。",
        "E": "Amazon EBS 卷是为单个 EC2 实例设计的块存储。 在多个 EC2 实例之间共享 EBS 卷是不推荐的，也不提供高可用性。通用 SSD (gp2) EBS 卷也不是为存储服务器端代码提供共享访问的最佳解决方案。如果多个 EC2 实例共享，可能会有数据一致性和性能问题，并且不能提供静态内容的 CDN 加速。"
      }
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "Amazon EC2",
      "Amazon ElastiCache",
      "Amazon EFS",
      "Amazon FSx for Windows File Server",
      "Amazon EBS",
      "CDN",
      "Windows Server",
      "Application Load Balancer"
    ]
  },
  {
    "id": 358,
    "topic": "1",
    "question_en": "A social media company runs its application on Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. The application has more than a billion images stored in an Amazon S3 bucket and processes thousands of images each second. The company wants to resize the images dynamically and serve appropriate formats to clients. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Install an external image management library on an EC2 instance. Use the image management library to process the images.",
      "B": "Create a CloudFront origin request policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request.",
      "C": "Use a Lambda@Edge function with an external image management library. Associate the Lambda@Edge function with the CloudFront behaviors that serve the images.",
      "D": "Create a CloudFront response headers policy. Use the policy to automatically resize images and to serve the appropriate format based on the User-Agent HTTP header in the request."
    },
    "correct_answer": "C",
    "vote_percentage": "88%",
    "question_cn": "一家社交媒体公司在其Application Load Balancer (ALB) 之后，在Amazon EC2实例上运行其应用程序。ALB是Amazon CloudFront分发的源。该应用程序在Amazon S3存储桶中存储了超过十亿张图像，并且每秒处理数千张图像。该公司希望动态调整图像大小并为客户端提供适当的格式。哪个解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在EC2实例上安装外部图像管理库。使用图像管理库处理图像。",
      "B": "创建CloudFront源请求策略。使用该策略根据请求中的User-Agent HTTP标头自动调整图像大小并提供适当的格式。",
      "C": "使用带有外部图像管理库的Lambda@Edge函数。将Lambda@Edge函数与提供图像的CloudFront行为关联。",
      "D": "创建CloudFront响应标头策略。使用该策略根据请求中的User-Agent HTTP标头自动调整图像大小并提供适当的格式。"
    },
    "tags": [
      "Amazon CloudFront",
      "CloudFront Response Headers",
      "Amazon S3",
      "Application Load Balancer",
      "Lambda@Edge",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 88%），解析仅供参考。】\n\n考查利用 Lambda@Edge 和 CloudFront 优化图像处理，以最小化运营开销，满足大规模图像处理需求。",
      "why_correct": "Lambda@Edge 允许在 CloudFront 的边缘位置执行代码，例如图像处理。结合外部图像管理库，可以实现动态图像大小调整和格式转换。这种方案减少了 EC2 实例的负担，并且 CloudFront 可以缓存处理后的图像，从而降低延迟并提高性能。",
      "why_wrong": "选项 A 在 EC2 实例上处理所有图像请求，这会增加 EC2 的负载，难以横向扩展，运营开销大。选项 B 和 D 均使用 CloudFront 策略，但仅通过策略无法直接处理图像大小调整，需要借助 Lambda@Edge 函数。 CloudFront 策略主要用于控制缓存行为，而非图像处理。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "Amazon S3",
      "CloudFront",
      "Lambda@Edge",
      "User-Agent",
      "HTTP",
      "EC2"
    ]
  },
  {
    "id": 359,
    "topic": "1",
    "question_en": "A hospital needs to store patient records in an Amazon S3 bucket. The hospital’s compliance team must ensure that all protected health information (PHI) is encrypted in transit and at rest. The compliance team must administer the encryption key for data at rest. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a public SSL/TLS certificate in AWS Certificate Manager (ACM). Associate the certificate with Amazon S3. Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.",
      "B": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with S3 managed encryption keys (SSE-S3). Assign the compliance team to manage the SSE-S3 keys.",
      "C": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Configure default encryption for each S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Assign the compliance team to manage the KMS keys.",
      "D": "Use the aws:SecureTransport condition on S3 bucket policies to allow only encrypted connections over HTTPS (TLS). Use Amazon Macie to protect the sensitive data that is stored in Amazon S3. Assign the compliance team to manage Macie."
    },
    "correct_answer": "C",
    "vote_percentage": "84%",
    "question_cn": "一家医院需要在 Amazon S3 存储桶中存储患者记录。 医院的合规团队必须确保所有受保护的健康信息 (PHI) 在传输和静态时都被加密。 合规团队必须管理静态数据的加密密钥。 哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Certificate Manager (ACM) 中创建公共 SSL/TLS 证书。 将该证书与 Amazon S3 关联。 配置每个 S3 存储桶的默认加密，以使用具有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密。 分配合规团队来管理 KMS 密钥。",
      "B": "在 S3 存储桶策略上使用 aws:SecureTransport 条件，仅允许通过 HTTPS (TLS) 的加密连接。 配置每个 S3 存储桶的默认加密，以使用具有 S3 托管加密密钥 (SSE-S3) 的服务器端加密。 分配合规团队来管理 SSE-S3 密钥。",
      "C": "在 S3 存储桶策略上使用 aws:SecureTransport 条件，仅允许通过 HTTPS (TLS) 的加密连接。 配置每个 S3 存储桶的默认加密，以使用具有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密。 分配合规团队来管理 KMS 密钥。",
      "D": "在 S3 存储桶策略上使用 aws:SecureTransport 条件，仅允许通过 HTTPS (TLS) 的加密连接。 使用 Amazon Macie 保护存储在 Amazon S3 中的敏感数据。 分配合规团队来管理 Macie。"
    },
    "tags": [
      "Amazon S3",
      "S3",
      "S3 Bucket Policy",
      "SSE-KMS",
      "HTTPS",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 84%），解析仅供参考。】\n\n考查在 Amazon S3 上对静态数据和传输中的数据进行加密，以及密钥管理。与 S3 存储桶策略、服务器端加密 (SSE) 以及密钥管理服务 (KMS) 的选择相关。",
      "why_correct": "选项 C 满足了所有要求。 首先，通过在 S3 存储桶策略中使用 `aws:SecureTransport` 条件，确保了所有客户端连接必须通过 HTTPS (TLS) 建立，从而对传输中的数据进行加密。 其次，配置每个 S3 存储桶的默认加密使用具有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密，符合了静态数据加密的要求，并且允许合规团队管理加密密钥。 KMS 提供了对密钥的精细控制，符合了合规团队的要求。",
      "why_wrong": "选项 A 错误，因为 ACM 用于管理 SSL/TLS 证书，主要用于保护传输中的数据，而不是静态数据加密，并且 ACM 本身不提供静态数据的加密密钥管理。 选项 B 错误，因为 SSE-S3 使用 S3 托管加密密钥，无法满足合规团队管理密钥的要求。 选项 D 错误，因为 Amazon Macie 用于数据安全性和合规性，检测敏感数据，但它本身不提供加密功能或密钥管理，无法满足静态数据加密和密钥管理的需求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "HTTPS",
      "TLS",
      "SSE-KMS",
      "AWS KMS",
      "ACM",
      "SSE-S3",
      "Amazon Macie",
      "S3 Bucket Policy"
    ]
  },
  {
    "id": 360,
    "topic": "1",
    "question_en": "A company uses Amazon API Gateway to run a private gateway with two REST APIs in the same VPC. The BuyStock RESTful web service calls the CheckFunds RESTful web service to ensure that enough funds are available before a stock can be purchased. The company has noticed in the VPC fiow logs that the BuyStock RESTful web service calls the CheckFunds RESTful web service over the internet instead of through the VPC. A solutions architect must implement a solution so that the APIs communicate through the VPC. Which solution will meet these requirements with the FEWEST changes to the code?",
    "options_en": {
      "A": "Add an X-API-Key header in the HTTP header for authorization.",
      "B": "Use an interface endpoint.",
      "C": "Use a gateway endpoint.",
      "D": "Add an Amazon Simple Queue Service (Amazon SQS) queue between the two REST APIs."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家公司使用 Amazon API Gateway 运行一个私有网关，该网关在同一个 VPC 中有两个 REST API。BuyStock RESTful web 服务调用 CheckFunds RESTful web 服务，以确保在购买股票之前有足够的资金可用。该公司在 VPC 流日志中注意到，BuyStock RESTful web 服务通过互联网而不是通过 VPC 调用 CheckFunds RESTful web 服务。一个解决方案架构师必须实现一个解决方案，以便这些 API 通过 VPC 进行通信。以下哪个解决方案将以对代码的更改最少来满足这些要求？",
    "options_cn": {
      "A": "在 HTTP 标头中添加一个 X-API-Key 标头以进行授权。",
      "B": "使用接口 endpoint。",
      "C": "使用网关 endpoint。",
      "D": "在两个 REST API 之间添加一个 Amazon Simple Queue Service (Amazon SQS) 队列。"
    },
    "tags": [
      "Amazon API Gateway",
      "VPC",
      "REST API",
      "HTTP Header"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n考察了API Gateway的VPC内部通信配置方案，以及对现有代码的影响程度。",
      "why_correct": "使用接口 endpoint 可以让 API Gateway 在 VPC 内通过私有 IP 地址进行通信。这种方案无需修改现有代码，只需配置API Gateway的 endpoint，使其能够访问 VPC 内的 CheckFunds REST API，从而解决通过互联网通信的问题。",
      "why_wrong": "选项 A 涉及 API 密钥授权，与解决 API 间 VPC 内通信问题无关，而且不一定能解决根本问题。选项 C 使用网关 endpoint，会产生额外的网络开销，且同样不能解决私有API通过VPC内部通信的问题。选项 D 引入了 SQS 队列，改变了 API 间的同步调用模式，需要修改现有代码，与题干要求“对代码的更改最少”不符。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "VPC",
      "REST API",
      "BuyStock",
      "CheckFunds",
      "VPC Flow Logs",
      "HTTP header",
      "X-API-Key",
      "interface endpoint",
      "gateway endpoint",
      "Amazon Simple Queue Service (Amazon SQS)"
    ]
  },
  {
    "id": 361,
    "topic": "1",
    "question_en": "A company hosts a multiplayer gaming application on AWS. The company wants the application to read data with sub-millisecond latency and run one-time queries on historical data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon RDS for data that is frequently accessed. Run a periodic custom script to export the data to an Amazon S3 bucket.",
      "B": "Store the data directly in an Amazon S3 bucket. Implement an S3 Lifecycle policy to move older data to S3 Glacier Deep Archive for long-term storage. Run one-time queries on the data in Amazon S3 by using Amazon Athena.",
      "C": "Use Amazon DynamoDB with DynamoDB Accelerator (DAX) for data that is frequently accessed. Export the data to an Amazon S3 bucket by using DynamoDB table export. Run one-time queries on the data in Amazon S3 by using Amazon Athena.",
      "D": "Use Amazon DynamoDB for data that is frequently accessed. Turn on streaming to Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to read the data from Kinesis Data Streams. Store the records in an Amazon S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上托管多人游戏应用程序。该公司希望应用程序以亚毫秒级的延迟读取数据，并对历史数据运行一次性查询。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon RDS 存储经常访问的数据。运行一个定期的自定义脚本，将数据导出到 Amazon S3 存储桶。",
      "B": "将数据直接存储在 Amazon S3 存储桶中。实施 S3 生命周期策略，将较旧的数据移动到 S3 Glacier Deep Archive 进行长期存储。使用 Amazon Athena 对 Amazon S3 中的数据运行一次性查询。",
      "C": "使用 Amazon DynamoDB 和 DynamoDB Accelerator (DAX) 存储经常访问的数据。使用 DynamoDB 表导出将数据导出到 Amazon S3 存储桶。使用 Amazon Athena 对 Amazon S3 中的数据运行一次性查询。",
      "D": "使用 Amazon DynamoDB 存储经常访问的数据。打开流式传输到 Amazon Kinesis Data Streams。使用 Amazon Kinesis Data Firehose 从 Kinesis Data Streams 读取数据。将记录存储在 Amazon S3 存储桶中。"
    },
    "tags": [
      "DynamoDB",
      "DAX",
      "S3",
      "Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用低延迟数据访问和一次性查询的解决方案，以及如何最小化运营开销。",
      "why_correct": "Amazon DynamoDB 提供了亚毫秒级的延迟，而 DAX 进一步加速了读取操作。 DynamoDB 表导出功能可以方便地将数据导出到 S3。使用 Athena 对 S3 中的数据进行一次性查询满足了题目的要求，同时降低了运营成本。",
      "why_wrong": "A 选项使用 RDS 用于频繁访问的数据，无法满足亚毫秒级延迟的需求。B 选项直接将数据存储在 S3 中，无法满足亚毫秒级延迟的读取需求。 D 选项虽然使用了 DynamoDB，但引入了 Kinesis Data Streams 和 Kinesis Data Firehose，增加了额外的复杂性和运营开销，并且并非最高效的将数据导出到 S3 的方式。"
    },
    "related_terms": [
      "Amazon RDS",
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "Amazon Athena",
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose"
    ]
  },
  {
    "id": 362,
    "topic": "1",
    "question_en": "A company uses a payment processing system that requires messages for a particular payment ID to be received in the same order that they were sent. Otherwise, the payments might be processed incorrectly. Which actions should a solutions architect take to meet this requirement? (Choose two.)",
    "options_en": {
      "A": "Write the messages to an Amazon DynamoDB table with the payment ID as the partition key.",
      "B": "Write the messages to an Amazon Kinesis data stream with the payment ID as the partition key.",
      "C": "Write the messages to an Amazon ElastiCache for Memcached cluster with the payment ID as the key.",
      "D": "Write the messages to an Amazon Simple Queue Service (Amazon SQS) queu",
      "E": "Set the message attribute to use the payment ID. E. Write the messages to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Set the message group to use the payment ID."
    },
    "correct_answer": "BE",
    "vote_percentage": "74%",
    "question_cn": "一家公司使用一个支付处理系统，该系统要求特定支付 ID 的消息必须按照发送的顺序接收。 否则，支付可能被错误处理。 解决方案架构师应该采取哪些措施来满足此要求？（选择两项。）",
    "options_cn": {
      "A": "将消息写入以支付 ID 作为分区键的 Amazon DynamoDB 表。",
      "B": "将消息写入以支付 ID 作为分区键的 Amazon Kinesis 数据流。",
      "C": "将消息写入以支付 ID 作为键的 Amazon ElastiCache for Memcached 集群。",
      "D": "将消息写入 Amazon Simple Queue Service (Amazon SQS) 队列。 将消息属性设置为使用支付 ID。",
      "E": "将消息写入 Amazon Simple Queue Service (Amazon SQS) FIFO 队列。 将消息组设置为使用支付 ID。"
    },
    "tags": [
      "DynamoDB",
      "Kinesis",
      "SQS",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 74%），解析仅供参考。】\n\n考查保证消息顺序传递的解决方案，需要考虑消息队列或数据流的特性。",
      "why_correct": "选项 B：Amazon Kinesis Data Streams 允许对消息进行分区，保证同一分区键的消息顺序。将支付 ID 作为分区键，可以确保同一支付 ID 的消息按顺序到达。选项 E：Amazon SQS FIFO 队列保证消息的顺序，且可以使用消息组（Message Group ID）来进一步隔离和维护消息的顺序。将支付 ID 作为消息组 ID，可以保证同一支付 ID 的消息按顺序处理。",
      "why_wrong": "选项 A：DynamoDB 主要用于存储和检索数据，不提供消息顺序传递的保证。选项 C：ElastiCache for Memcached 是一个缓存服务，不适合用于消息队列，无法保证消息顺序。选项 D：标准 SQS 队列不保证消息顺序，虽然可以使用消息属性，但无法确保特定支付 ID 消息的顺序。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon Kinesis Data Streams",
      "Amazon ElastiCache for Memcached",
      "Amazon Simple Queue Service (Amazon SQS)",
      "FIFO queue",
      "Message Group ID",
      "partition key",
      "message properties"
    ]
  },
  {
    "id": 363,
    "topic": "1",
    "question_en": "A company is building a game system that needs to send unique events to separate leaderboard, matchmaking, and authentication services concurrently. The company needs an AWS event-driven system that guarantees the order of the events. Which solution will meet these requirements?",
    "options_en": {
      "A": "Amazon EventBridge event bus",
      "B": "Amazon Simple Notification Service (Amazon SNS) FIFO topics",
      "C": "Amazon Simple Notification Service (Amazon SNS) standard topics",
      "D": "Amazon Simple Queue Service (Amazon SQS) FIFO queues"
    },
    "correct_answer": "B",
    "vote_percentage": "73%",
    "question_cn": "一家公司正在构建一个游戏系统，该系统需要将独特的事件并发地发送到单独的排行榜、匹配和身份验证服务。该公司需要一个 AWS 事件驱动系统，以保证事件的顺序。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "Amazon EventBridge 事件总线",
      "B": "Amazon Simple Notification Service (Amazon SNS) FIFO 主题",
      "C": "Amazon Simple Notification Service (Amazon SNS) 标准主题",
      "D": "Amazon Simple Queue Service (Amazon SQS) FIFO 队列"
    },
    "tags": [
      "EventBridge",
      "SNS",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 73%），解析仅供参考。】\n\n此题考察如何构建事件驱动系统，并保证事件顺序。关键在于选择合适的 AWS 服务。",
      "why_correct": "选项 B 正确，因为 Amazon SNS FIFO 主题可以保证消息的顺序。FIFO 主题按消息发布顺序传递消息。",
      "why_wrong": "选项 A 错误，因为 Amazon EventBridge 事件总线不保证事件的顺序。选项 C 错误，因为 Amazon SNS 标准主题不保证消息的顺序。选项 D 错误，因为 Amazon SQS FIFO 队列，适用于消费者直接订阅消息，而不是广播到多个服务。"
    },
    "related_terms": [
      "Amazon EventBridge",
      "Amazon SNS",
      "Amazon SQS"
    ]
  },
  {
    "id": 364,
    "topic": "1",
    "question_en": "A hospital is designing a new application that gathers symptoms from patients. The hospital has decided to use Amazon Simple Queue Service (Amazon SQS) and Amazon Simple Notification Service (Amazon SNS) in the architecture. A solutions architect is reviewing the infrastructure design. Data must be encrypted at rest and in transit. Only authorized personnel of the hospital should be able to access the data. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Turn on server-side encryption on the SQS components. Update the default key policy to restrict key usage to a set of authorized principals.",
      "B": "Turn on server-side encryption on the SNS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals.",
      "C": "Turn on encryption on the SNS components. Update the default key policy to restrict key usage to a set of authorized principals. Set a condition in the topic policy to allow only encrypted connections over TLS.",
      "D": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply a key policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS",
      "E": "Turn on server-side encryption on the SQS components by using an AWS Key Management Service (AWS KMS) customer managed key. Apply an IAM policy to restrict key usage to a set of authorized principals. Set a condition in the queue policy to allow only encrypted connections over TLS."
    },
    "correct_answer": "BD",
    "vote_percentage": "73%",
    "question_cn": "一家医院正在设计一个收集病人症状的新应用程序。医院决定在架构中使用 Amazon Simple Queue Service (Amazon SQS) 和 Amazon Simple Notification Service (Amazon SNS)。一位解决方案架构师正在审查基础设施设计。数据必须在静态和传输过程中加密。只有医院的授权人员才能访问数据。解决方案架构师应采取哪些步骤组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 SQS 组件上打开服务器端加密。更新默认密钥策略，将密钥使用限制为一组授权主体。",
      "B": "通过使用 AWS Key Management Service (AWS KMS) 客户托管密钥，在 SNS 组件上打开服务器端加密。应用密钥策略以限制密钥使用权限为一组授权主体。",
      "C": "在 SNS 组件上打开加密。更新默认密钥策略，将密钥使用限制为一组授权主体。在主题策略中设置一个条件，仅允许通过 TLS 进行加密连接。",
      "D": "通过使用 AWS Key Management Service (AWS KMS) 客户托管密钥，在 SQS 组件上打开服务器端加密。应用密钥策略以限制密钥使用权限为一组授权主体。在队列策略中设置一个条件，仅允许通过 TLS 进行加密连接。",
      "E": "通过使用 AWS Key Management Service (AWS KMS) 客户托管密钥，在 SQS 组件上打开服务器端加密。应用 IAM 策略以限制密钥使用权限为一组授权主体。在队列策略中设置一个条件，仅允许通过 TLS 进行加密连接。"
    },
    "tags": [
      "SQS",
      "SNS",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 73%），解析仅供参考。】\n\n此题考察如何在 SQS 和 SNS 中实现数据加密，并限制访问。重点关注 KMS 的使用和策略配置。",
      "why_correct": "选项 C 正确，因为 SNS 支持服务器端加密 (SSE)。可以通过 KMS 客户托管密钥加密主题，并更新默认密钥策略，将密钥使用限制为一组授权主体，从而满足加密和访问控制的要求。通过在主题策略中设置一个条件，仅允许通过 TLS 进行加密连接，以确保数据传输安全。选项 D 正确，因为 SQS 支持服务器端加密(SSE)。可以通过 KMS 客户托管密钥加密队列，并应用密钥策略以限制密钥使用权限为一组授权主体。通过在队列策略中设置一个条件，仅允许通过 TLS 进行加密连接，以确保数据传输安全。",
      "why_wrong": "选项 A 错误，因为没有提供SNS的加密方法。选项 B 错误，因为 SNS 已经支持服务器端加密(SSE)，无需配置KMS。选项 E 错误，因为 IAM 策略不能用于限制 KMS 密钥的使用，而需要使用密钥策略。"
    },
    "related_terms": [
      "SQS",
      "SNS",
      "AWS KMS",
      "TLS"
    ]
  },
  {
    "id": 365,
    "topic": "1",
    "question_en": "A company runs a web application that is backed by Amazon RDS. A new database administrator caused data loss by accidentally editing information in a database table. To help recover from this type of incident, the company wants the ability to restore the database to its state from 5 minutes before any change within the last 30 days. Which feature should the solutions architect include in the design to meet this requirement?",
    "options_en": {
      "A": "Read replicas",
      "B": "Manual snapshots",
      "C": "Automated backups",
      "D": "Multi-AZ deployments"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司运营一个由 Amazon RDS 支持的 Web 应用程序。一名新的数据库管理员不小心编辑了数据库表中的信息，导致数据丢失。为了帮助从这类事件中恢复，该公司希望能够将数据库恢复到过去 30 天内发生任何更改前 5 分钟的状态。解决方案架构师应该在设计中包含哪个功能来满足此要求？",
    "options_cn": {
      "A": "只读副本",
      "B": "手动快照",
      "C": "自动备份",
      "D": "多可用区部署"
    },
    "tags": [
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何从 RDS 数据库中的数据丢失中恢复。考察点为 RDS 的备份与恢复功能。",
      "why_correct": "选项 C 正确，自动备份功能可以帮助将数据库恢复到过去 30 天内的任何时间点，满足题干要求。",
      "why_wrong": "选项 A 错误，只读副本用于读取扩展，不能用于数据恢复。选项 B 错误，手动快照需要手动创建，不满足快速恢复的需求。选项 D 错误，多可用区部署用于高可用，不能用于数据恢复。"
    },
    "related_terms": [
      "Amazon RDS"
    ]
  },
  {
    "id": 366,
    "topic": "1",
    "question_en": "A company’s web application consists of an Amazon API Gateway API in front of an AWS Lambda function and an Amazon DynamoDB database. The Lambda function handles the business logic, and the DynamoDB table hosts the data. The application uses Amazon Cognito user pools to identify the individual users of the application. A solutions architect needs to update the application so that only users who have a subscription can access premium content. Which solution will meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Enable API caching and throttling on the API Gateway API.",
      "B": "Set up AWS WAF on the API Gateway API. Create a rule to filter users who have a subscription.",
      "C": "Apply fine-grained IAM permissions to the premium content in the DynamoDB table.",
      "D": "Implement API usage plans and API keys to limit the access of users who do not have a subscription."
    },
    "correct_answer": "D",
    "vote_percentage": "85%",
    "question_cn": "一家公司的 Web 应用程序由一个 Amazon API Gateway API、一个 AWS Lambda 函数和一个 Amazon DynamoDB 数据库组成。Lambda 函数处理业务逻辑，DynamoDB 表托管数据。该应用程序使用 Amazon Cognito 用户池来识别应用程序的各个用户。解决方案架构师需要更新应用程序，以便只有订阅用户才能访问高级内容。哪种解决方案将以最少的运营开销满足此要求？",
    "options_cn": {
      "A": "在 API Gateway API 上启用 API 缓存和限制。",
      "B": "在 API Gateway API 上设置 AWS WAF。创建一个规则来过滤未订阅的用户。",
      "C": "将细粒度的 IAM 权限应用于 DynamoDB 表中的高级内容。",
      "D": "实施 API 使用计划和 API 密钥，以限制未订阅用户的访问。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "Cognito"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 85%），解析仅供参考。】\n\n本题考察了如何在 API Gateway 中实现对用户访问的控制，以限制未订阅用户对高级内容的访问，并考察了运营开销的考虑。",
      "why_correct": "选项 D 提供了使用 API 使用计划和 API 密钥的方式来限制访问。API 使用计划可以定义不同的访问级别，并与 Cognito 用户池的用户关联。API 密钥用于识别和验证用户，从而确保只有拥有有效密钥的订阅用户才能访问高级内容。这种方案能够有效控制访问，并且运营成本相对较低。",
      "why_wrong": "选项 A，API 缓存和限制主要用于优化性能和控制流量，不能直接用于实现用户订阅控制。选项 B，AWS WAF 主要用于保护 Web 应用程序免受常见的 Web 攻击，而不是用户订阅管理。WAF 可以用于限制某些恶意访问，但不能直接与 Cognito 的用户关联，实现订阅控制需要额外的逻辑。选项 C，细粒度的 IAM 权限主要应用于控制 Lambda 函数访问 DynamoDB 表的权限，无法直接用于控制 API Gateway 级别的访问，而且会增加运维复杂性。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon Cognito",
      "API Gateway API",
      "API caching",
      "throttling",
      "AWS WAF",
      "IAM",
      "API usage plans",
      "API keys"
    ]
  },
  {
    "id": 367,
    "topic": "1",
    "question_en": "A company is using Amazon Route 53 latency-based routing to route requests to its UDP-based application for users around the world. The application is hosted on redundant servers in the company's on-premises data centers in the United States, Asia, and Europe. The company’s compliance requirements state that the application must be hosted on premises. The company wants to improve the performance and availability of the application. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the NLBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.",
      "B": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. Create an accelerator by using AWS Global Accelerator, and register the ALBs as its endpoints. Provide access to the application by using a CNAME that points to the accelerator DNS.",
      "C": "Configure three Network Load Balancers (NLBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three NLBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS.",
      "D": "Configure three Application Load Balancers (ALBs) in the three AWS Regions to address the on-premises endpoints. In Route 53, create a latency-based record that points to the three ALBs, and use it as an origin for an Amazon CloudFront distribution. Provide access to the application by using a CNAME that points to the CloudFront DNS."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 Amazon Route 53 基于延迟的路由，将请求路由到其面向全球用户的基于 UDP 的应用程序。该应用程序托管在美国、亚洲和欧洲的公司本地数据中心的冗余服务器上。公司的合规性要求规定应用程序必须托管在本地。该公司希望提高应用程序的性能和可用性。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在三个 AWS 区域中配置三个 Network Load Balancer (NLB) 以寻址本地端点。使用 AWS Global Accelerator 创建一个加速器，并将 NLB 注册为其端点。使用指向加速器 DNS 的 CNAME 提供对应用程序的访问。",
      "B": "在三个 AWS 区域中配置三个 Application Load Balancer (ALB) 以寻址本地端点。使用 AWS Global Accelerator 创建一个加速器，并将 ALB 注册为其端点。使用指向加速器 DNS 的 CNAME 提供对应用程序的访问。",
      "C": "在三个 AWS 区域中配置三个 Network Load Balancer (NLB) 以寻址本地端点。在 Route 53 中，创建一个指向这三个 NLB 的基于延迟的记录，并将其用作 Amazon CloudFront 分配的源。使用指向 CloudFront DNS 的 CNAME 提供对应用程序的访问。",
      "D": "在三个 AWS 区域中配置三个 Application Load Balancer (ALB) 以寻址本地端点。在 Route 53 中，创建一个指向这三个 ALB 的基于延迟的记录，并将其用作 Amazon CloudFront 分配的源。使用指向 CloudFront DNS 的 CNAME 提供对应用程序的访问。"
    },
    "tags": [
      "Route 53",
      "Global Accelerator",
      "NLB",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察如何通过延迟路由优化 UDP 应用程序的性能和可用性。重点在于结合 AWS Global Accelerator 和 Network Load Balancer。",
      "why_correct": "选项 A 正确，使用 AWS Global Accelerator 可以将流量路由到最靠近用户的 AWS 边缘站点，从而提高性能。将 NLB 注册为加速器的端点，可以实现对本地端点的访问。Route 53 基于延迟的路由无法解决跨区域负载均衡的问题。该选项既能提高性能，也能提高可用性。",
      "why_wrong": "选项 B 错误，ALB 并不支持 UDP 协议。选项 C 错误，CloudFront 的源需要是 HTTP/HTTPS。选项 D 错误，ALB 并不支持 UDP 协议。"
    },
    "related_terms": [
      "Amazon Route 53",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "Network Load Balancer (NLB)",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 368,
    "topic": "1",
    "question_en": "A solutions architect wants all new users to have specific complexity requirements and mandatory rotation periods for IAM user passwords. What should the solutions architect do to accomplish this?",
    "options_en": {
      "A": "Set an overall password policy for the entire AWS account.",
      "B": "Set a password policy for each IAM user in the AWS account.",
      "C": "Use third-party vendor software to set password requirements.",
      "D": "Attach an Amazon CloudWatch rule to the Create_newuser event to set the password with the appropriate requirements."
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一个解决方案架构师希望所有新用户都具有特定的复杂性要求以及 IAM 用户密码的强制轮换周期。解决方案架构师应该怎么做才能实现此目标？",
    "options_cn": {
      "A": "为整个 AWS 账户设置一个总体的密码策略。",
      "B": "为 AWS 账户中的每个 IAM 用户设置密码策略。",
      "C": "使用第三方供应商软件设置密码要求。",
      "D": "将一个 Amazon CloudWatch 规则附加到 Create_newuser 事件，以使用适当的要求设置密码。"
    },
    "tags": [
      "IAM",
      "password policy",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n此题考察如何在 AWS 中强制执行密码策略。关键在于理解密码策略的配置方式。",
      "why_correct": "选项 A 正确，可以在 AWS 账户级别设置密码策略，以强制执行密码复杂性要求和轮换周期。",
      "why_wrong": "选项 B 错误，IAM 用户级别的密码策略不可行。选项 C 错误，第三方软件并不需要，AWS 本身提供密码策略。选项 D 错误，CloudWatch 规则不能用于设置密码要求。"
    },
    "related_terms": [
      "IAM",
      "CloudWatch"
    ]
  },
  {
    "id": 369,
    "topic": "1",
    "question_en": "A company has migrated an application to Amazon EC2 Linux instances. One of these EC2 instances runs several 1-hour tasks on a schedule. These tasks were written by different teams and have no common programming language. The company is concerned about performance and scalability while these tasks run on a single instance. A solutions architect needs to implement a solution to resolve these concerns. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Batch to run the tasks as jobs. Schedule the jobs by using Amazon EventBridge (Amazon CloudWatch Events).",
      "B": "Convert the EC2 instance to a container. Use AWS App Runner to create the container on demand to run the tasks as jobs.",
      "C": "Copy the tasks into AWS Lambda functions. Schedule the Lambda functions by using Amazon EventBridge (Amazon CloudWatch Events).",
      "D": "Create an Amazon Machine Image (AMI) of the EC2 instance that runs the tasks. Create an Auto Scaling group with the AMI to run multiple copies of the instance."
    },
    "correct_answer": "A",
    "vote_percentage": "65%",
    "question_cn": "一家公司已将应用程序迁移到 Amazon EC2 Linux 实例。其中一个 EC2 实例按计划运行几个 1 小时的任务。这些任务由不同的团队编写，并且没有通用的编程语言。该公司担心这些任务在单个实例上运行时出现的性能和可扩展性问题。一位解决方案架构师需要实施一个解决方案来解决这些问题。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Batch 将任务作为作业运行。使用 Amazon EventBridge（Amazon CloudWatch Events）安排作业。",
      "B": "将 EC2 实例转换为容器。使用 AWS App Runner 按需创建容器以将任务作为作业运行。",
      "C": "将任务复制到 AWS Lambda 函数中。使用 Amazon EventBridge（Amazon CloudWatch Events）安排 Lambda 函数。",
      "D": "创建运行任务的 EC2 实例的 Amazon Machine Image (AMI)。使用 AMI 创建一个 Auto Scaling 组来运行该实例的多个副本。"
    },
    "tags": [
      "EC2",
      "Batch",
      "App Runner",
      "Lambda",
      "AMI",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 65%），解析仅供参考。】\n\n此题考察如何以最小的运营开销运行多个任务，并解决性能和可扩展性问题。关键在于选择最合适的计算服务。",
      "why_correct": "选项 A 正确，AWS Batch 可以用于批量处理，可以方便地运行不同团队编写的任务，并通过 EventBridge 调度作业，实现自动运行，且运维开销最小。",
      "why_wrong": "选项 B 错误，App Runner 虽然可以运行容器化的应用，但并非为批量处理设计。选项 C 错误，Lambda 运行时长有限制，且不适合长时间运行的任务。选项 D 错误，创建 AMI 和 Auto Scaling 组涉及配置和管理，开销较大。"
    },
    "related_terms": [
      "AWS Batch",
      "Amazon EventBridge",
      "AWS Lambda",
      "Auto Scaling",
      "AWS App Runner",
      "Amazon Machine Image (AMI)"
    ]
  },
  {
    "id": 370,
    "topic": "1",
    "question_en": "A company runs a public three-tier web application in a VPC. The application runs on Amazon EC2 instances across multiple Availability Zones. The EC2 instances that run in private subnets need to communicate with a license server over the internet. The company needs a managed solution that minimizes operational maintenance. Which solution meets these requirements?",
    "options_en": {
      "A": "Provision a NAT instance in a public subnet. Modify each private subnet's route table with a default route that points to the NAT instance.",
      "B": "Provision a NAT instance in a private subnet. Modify each private subnet's route table with a default route that points to the NAT instance.",
      "C": "Provision a NAT gateway in a public subnet. Modify each private subnet's route table with a default route that points to the NAT gateway.",
      "D": "Provision a NAT gateway in a private subnet. Modify each private subnet's route table with a default route that points to the NAT gateway."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 VPC 中运行公共三层 Web 应用程序。该应用程序在多个可用区中的 Amazon EC2 实例上运行。在私有子网中运行的 EC2 实例需要通过互联网与许可证服务器通信。该公司需要一个能够最大限度地减少运营维护的托管解决方案。哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "在公有子网中配置一个 NAT 实例。使用指向 NAT 实例的默认路由修改每个私有子网的路由表。",
      "B": "在私有子网中配置一个 NAT 实例。使用指向 NAT 实例的默认路由修改每个私有子网的路由表。",
      "C": "在公有子网中配置一个 NAT 网关。使用指向 NAT 网关的默认路由修改每个私有子网的路由表。",
      "D": "在私有子网中配置一个 NAT 网关。使用指向 NAT 网关的默认路由修改每个私有子网的路由表。"
    },
    "tags": [
      "VPC",
      "NAT",
      "NAT Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何在 VPC 中配置私有子网的互联网访问。关键在于选择 NAT 网关或 NAT 实例。",
      "why_correct": "选项 C 正确，NAT 网关是 AWS 托管服务，能够最大限度地减少运营维护，并且可以实现私有子网与互联网的通信。",
      "why_wrong": "选项 A 错误，NAT 实例需要手动配置和维护。选项 B 错误，NAT 实例不能在私有子网中配置。选项 D 错误，NAT 网关不能在私有子网中配置。"
    },
    "related_terms": [
      "VPC",
      "NAT",
      "NAT Gateway"
    ]
  },
  {
    "id": 371,
    "topic": "1",
    "question_en": "A company needs to create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host a digital media streaming application. The EKS cluster will use a managed node group that is backed by Amazon Elastic Block Store (Amazon EBS) volumes for storage. The company must encrypt all data at rest by using a customer managed key that is stored in AWS Key Management Service (AWS KMS). Which combination of actions will meet this requirement with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Use a Kubernetes plugin that uses the customer managed key to perform data encryption.",
      "B": "After creation of the EKS cluster, locate the EBS volumes. Enable encryption by using the customer managed key.",
      "C": "Enable EBS encryption by default in the AWS Region where the EKS cluster will be created. Select the customer managed key as the default key.",
      "D": "Create the EKS cluster. Create an IAM role that has a policy that grants permission to the customer managed key. Associate the role with the EKS cluster",
      "E": "Store the customer managed key as a Kubernetes secret in the EKS cluster. Use the customer managed key to encrypt the EBS volumes."
    },
    "correct_answer": "CD",
    "vote_percentage": "56%",
    "question_cn": "一家公司需要创建一个 Amazon Elastic Kubernetes Service (Amazon EKS) 集群来托管一个数字媒体流应用程序。EKS 集群将使用一个由 Amazon Elastic Block Store (Amazon EBS) 卷提供存储支持的托管节点组。该公司必须使用存储在 AWS Key Management Service (AWS KMS) 中的客户托管密钥对所有静态数据进行加密。哪种组合的操作将以最少的运营开销满足此要求？（选择两个。）",
    "options_cn": {
      "A": "使用一个 Kubernetes 插件，该插件使用客户托管密钥执行数据加密。",
      "B": "创建 EKS 集群后，找到 EBS 卷。使用客户托管密钥启用加密。",
      "C": "在将创建 EKS 集群的 AWS 区域中，默认启用 EBS 加密。选择客户托管密钥作为默认密钥。",
      "D": "创建 EKS 集群。创建一个 IAM 角色，该角色具有授予客户托管密钥权限的策略。将该角色与 EKS 集群关联。",
      "E": "将客户托管密钥作为 Kubernetes 密钥存储在 EKS 集群中。使用客户托管密钥对 EBS 卷进行加密。"
    },
    "tags": [
      "EKS",
      "EBS",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 56%），解析仅供参考。】\n\n考查如何在 EKS 集群中使用客户托管密钥加密 EBS 卷，以满足静态数据加密需求，并最小化运营开销。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 CD。理由简述：选项 C 正确，通过在 AWS 区域中启用 EBS 默认加密，并指定客户托管密钥，可以确保所有新创建的 EBS 卷都使用该密钥进行加密。选项 D 正确，创建 IAM 角色并授予客户托管密钥的权限，然后将该角色与 EKS 集群关联，确保 EKS 集群有权限使用客户托管密钥。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，使用 Kubernetes 插件进行加密可能引入额外的复杂性和维护开销，且不直接作用于 EBS 卷的加密。选项 B 错误，在创建 EKS 集群后手动找到并加密 EBS 卷需要额外的操作，增加了运营负担。选项 E 错误，将客户托管密钥存储在 Kubernetes 密钥中不安全，且无法直接作用于 EBS 卷的加密。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS Key Management Service (AWS KMS)",
      "EBS encryption",
      "IAM role"
    ]
  },
  {
    "id": 372,
    "topic": "1",
    "question_en": "A company wants to migrate an Oracle database to AWS. The database consists of a single table that contains millions of geographic information systems (GIS) images that are high resolution and are identified by a geographic code. When a natural disaster occurs, tens of thousands of images get updated every few minutes. Each geographic code has a single image or row that is associated with it. The company wants a solution that is highly available and scalable during such events. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store the images and geographic codes in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance.",
      "B": "Store the images in Amazon S3 buckets. Use Amazon DynamoDB with the geographic code as the key and the image S3 URL as the value.",
      "C": "Store the images and geographic codes in an Amazon DynamoDB table. Configure DynamoDB Accelerator (DAX) during times of high load.",
      "D": "Store the images in Amazon S3 buckets. Store geographic codes and image S3 URLs in a database table. Use Oracle running on an Amazon RDS Multi-AZ DB instance."
    },
    "correct_answer": "B",
    "vote_percentage": "66%",
    "question_cn": "一家公司希望将 Oracle 数据库迁移到 AWS。该数据库包含一个包含数百万个高分辨率地理信息系统 (GIS) 图像的表，这些图像由地理代码标识。发生自然灾害时，每隔几分钟就会更新数万张图像。每个地理代码都有一个与之关联的图像或行。该公司希望找到一个在高负载事件期间具有高可用性和可扩展性的解决方案。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "将图像和地理代码存储在数据库表中。使用在 Amazon RDS Multi-AZ DB 实例上运行的 Oracle。",
      "B": "将图像存储在 Amazon S3 存储桶中。使用 Amazon DynamoDB，将地理代码用作键，将图像 S3 URL 用作值。",
      "C": "将图像和地理代码存储在 Amazon DynamoDB 表中。在高负载期间配置 DynamoDB Accelerator (DAX)。",
      "D": "将图像存储在 Amazon S3 存储桶中。将地理代码和图像 S3 URL 存储在数据库表中。使用在 Amazon RDS Multi-AZ DB 实例上运行的 Oracle。"
    },
    "tags": [
      "Oracle",
      "S3",
      "DynamoDB",
      "DAX",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 66%），解析仅供参考。】\n\n此题考察如何高效存储和查询高分辨率 GIS 图像数据。关键在于选择适合的存储方式和查询工具，并满足高可用性和可扩展性需求。",
      "why_correct": "选项 B 正确，将图像存储在 Amazon S3 中，S3 提供了高可用性和可扩展性。将地理代码用作 DynamoDB 的键，将 S3 URL 用作值，可以实现快速查询。这种组合最经济高效，并且可以满足高负载事件期间的需求。",
      "why_wrong": "选项 A 错误，RDS 实例存储图片，性能和可扩展性不足。选项 C 错误，DAX 只能加速 DynamoDB 的读取性能，不能解决存储空间问题。选项 D 错误，RDS 实例存储图片信息，性能和可扩展性不足。"
    },
    "related_terms": [
      "Oracle",
      "Amazon S3",
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon RDS"
    ]
  },
  {
    "id": 373,
    "topic": "1",
    "question_en": "A company has an application that collects data from IoT sensors on automobiles. The data is streamed and stored in Amazon S3 through Amazon Kinesis Data Firehose. The data produces trillions of S3 objects each year. Each morning, the company uses the data from the previous 30 days to retrain a suite of machine learning (ML) models. Four times each year, the company uses the data from the previous 12 months to perform analysis and train other ML models. The data must be available with minimal delay for up to 1 year. After 1 year, the data must be retained for archival purposes. Which storage solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use the S3 Intelligent-Tiering storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.",
      "B": "Use the S3 Intelligent-Tiering storage class. Configure S3 Intelligent-Tiering to automatically move objects to S3 Glacier Deep Archive after 1 year.",
      "C": "Use the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 1 year.",
      "D": "Use the S3 Standard storage class. Create an S3 Lifecycle policy to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days, and then to S3 Glacier Deep Archive after 1 year."
    },
    "correct_answer": "D",
    "vote_percentage": "91%",
    "question_cn": "一家公司有一个应用程序，从汽车上的物联网传感器收集数据。数据通过 Amazon Kinesis Data Firehose 流式传输并存储在 Amazon S3 中。这些数据每年产生数万亿个 S3 对象。每天早上，该公司使用过去 30 天的数据来重新训练一套机器学习 (ML) 模型。每年四次，该公司使用过去 12 个月的数据来执行分析并训练其他 ML 模型。这些数据必须在最多 1 年的时间内以最小的延迟可用。 1 年后，数据必须保留用于归档。哪种存储解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "使用 S3 Intelligent-Tiering 存储类。 创建一个 S3 生命周期策略，在 1 年后将对象转换到 S3 Glacier Deep Archive。",
      "B": "使用 S3 Intelligent-Tiering 存储类。 配置 S3 Intelligent-Tiering，在 1 年后自动将对象移动到 S3 Glacier Deep Archive。",
      "C": "使用 S3 Standard-Infrequent Access (S3 Standard-IA) 存储类。 创建一个 S3 生命周期策略，在 1 年后将对象转换到 S3 Glacier Deep Archive。",
      "D": "使用 S3 Standard 存储类。 创建一个 S3 生命周期策略，在 30 天后将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA)，然后在 1 年后转换为 S3 Glacier Deep Archive。"
    },
    "tags": [
      "S3",
      "Intelligent-Tiering",
      "S3 Standard-IA",
      "S3 Glacier Deep Archive"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 91%），解析仅供参考。】\n\n此题考察 S3 存储类的选择和生命周期策略的配置，以满足数据存储、访问和归档的需求。重点在于选择最经济高效的存储类组合。",
      "why_correct": "选项 D 正确，先将数据存储在 S3 Standard，满足 30 天内需要进行模型重新训练的需求。30 天后转换为 S3 Standard-IA，降低存储成本，满足年度分析需求。1 年后转换为 S3 Glacier Deep Archive，用于归档。",
      "why_wrong": "选项 A 错误，Intelligent-Tiering 的设计目标是根据访问模式自动调整存储层，但它不能满足在 30 天后访问的需求。选项 B 错误，Intelligent-Tiering 不能满足 30 天内进行模型重新训练的需求。选项 C 错误，S3 Standard-IA 不适合频繁访问，数据重新训练会增加成本。"
    },
    "related_terms": [
      "S3 Intelligent-Tiering",
      "S3 Glacier Deep Archive",
      "S3 Standard-Infrequent Access (S3 Standard-IA)"
    ]
  },
  {
    "id": 374,
    "topic": "1",
    "question_en": "A company is running several business applications in three separate VPCs within the us-east-1 Region. The applications must be able to communicate between VPCs. The applications also must be able to consistently send hundreds of gigabytes of data each day to a latency- sensitive application that runs in a single on-premises data center. A solutions architect needs to design a network connectivity solution that maximizes cost-effectiveness. Which solution meets these requirements?",
    "options_en": {
      "A": "Configure three AWS Site-to-Site VPN connections from the data center to AWS. Establish connectivity by configuring one VPN connection for each VPC.",
      "B": "Launch a third-party virtual network appliance in each VPC. Establish an IPsec VPN tunnel between the data center and each virtual appliance.",
      "C": "Set up three AWS Direct Connect connections from the data center to a Direct Connect gateway in us-east-1. Establish connectivity by configuring each VPC to use one of the Direct Connect connections.",
      "D": "Set up one AWS Direct Connect connection from the data center to AWS. Create a transit gateway, and attach each VPC to the transit gateway. Establish connectivity between the Direct Connect connection and the transit gateway."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 us-east-1 区域内的三个独立 VPC 中运行几个业务应用程序。这些应用程序必须能够在 VPC 之间通信。这些应用程序还必须能够每天将数百 GB 的数据一致地发送到在单个本地数据中心运行的延迟敏感型应用程序。一位解决方案架构师需要设计一个网络连接解决方案，以最大限度地提高成本效益。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "从数据中心配置三个 AWS Site-to-Site VPN 连接到 AWS。通过为每个 VPC 配置一个 VPN 连接来建立连接。",
      "B": "在每个 VPC 中启动一个第三方虚拟网络设备。在数据中心和每个虚拟设备之间建立一个 IPsec VPN 隧道。",
      "C": "从数据中心设置三个 AWS Direct Connect 连接到 us-east-1 中的 Direct Connect 网关。通过配置每个 VPC 使用一个 Direct Connect 连接来建立连接。",
      "D": "从数据中心设置一个 AWS Direct Connect 连接到 AWS。创建一个 Transit Gateway，并将每个 VPC 附加到 Transit Gateway。在 Direct Connect 连接和 Transit Gateway 之间建立连接。"
    },
    "tags": [
      "VPC",
      "Direct Connect",
      "Transit Gateway",
      "Site-to-Site VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察如何连接多个 VPC 和本地数据中心，并最大限度地提高成本效益。关键在于比较不同的网络连接方案。",
      "why_correct": "选项 D 正确，使用 Direct Connect 连接到 Transit Gateway，然后将 VPC 连接到 Transit Gateway，可以实现 VPC 之间和本地数据中心的连接，并最大限度地降低成本。Transit Gateway 降低了连接的复杂性，并简化了网络管理。",
      "why_wrong": "选项 A 错误，使用 Site-to-Site VPN 连接每个 VPC，管理成本高，不经济高效。选项 B 错误，使用第三方虚拟网络设备，增加了额外的成本和管理开销。选项 C 错误，使用 Direct Connect 连接每个 VPC，成本较高。"
    },
    "related_terms": [
      "VPC",
      "Direct Connect",
      "Transit Gateway",
      "Site-to-Site VPN"
    ]
  },
  {
    "id": 375,
    "topic": "1",
    "question_en": "An ecommerce company is building a distributed application that involves several serverless functions and AWS services to complete order- processing tasks. These tasks require manual approvals as part of the workfiow. A solutions architect needs to design an architecture for the order-processing application. The solution must be able to combine multiple AWS Lambda functions into responsive serverless applications. The solution also must orchestrate data and services that run on Amazon EC2 instances, containers, or on-premises servers. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Step Functions to build the application.",
      "B": "Integrate all the application components in an AWS Glue job.",
      "C": "Use Amazon Simple Queue Service (Amazon SQS) to build the application.",
      "D": "Use AWS Lambda functions and Amazon EventBridge events to build the application."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司正在构建一个分布式应用程序，该应用程序涉及多个无服务器函数和 AWS 服务来完成订单处理任务。这些任务需要手动批准作为工作流程的一部分。一位解决方案架构师需要为订单处理应用程序设计一个架构。该解决方案必须能够将多个 AWS Lambda 函数组合成响应迅速的无服务器应用程序。该解决方案还必须编排在 Amazon EC2 实例、容器或本地服务器上运行的数据和服务。哪种解决方案能够以最少的运营开销满足这些需求？",
    "options_cn": {
      "A": "使用 AWS Step Functions 构建应用程序。",
      "B": "将所有应用程序组件集成到 AWS Glue 作业中。",
      "C": "使用 Amazon Simple Queue Service (Amazon SQS) 构建应用程序。",
      "D": "使用 AWS Lambda 函数和 Amazon EventBridge 事件构建应用程序。"
    },
    "tags": [
      "Step Functions",
      "Glue",
      "SQS",
      "Lambda",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查无服务器工作流程的编排，要求选择能将多个 Lambda 函数组合并编排，且运营开销最小的解决方案。",
      "why_correct": "AWS Step Functions 是一种完全托管的无服务器编排服务，可让您协调多个 AWS Lambda 函数和其他 AWS 服务。它提供了图形化的工作流程定义，易于理解和维护。Step Functions 可以方便地组合 Lambda 函数，实现人工审批等流程，并且运营开销最小，非常适合本题的需求。",
      "why_wrong": "选项 B，AWS Glue 主要用于 ETL（提取、转换和加载）任务，不适合编排工作流程，且与题目中需要编排EC2实例等的需求不符。选项 C，Amazon SQS 是一种消息队列服务，主要用于解耦应用程序组件，无法直接编排工作流程，并且不支持人工审批。选项 D，AWS Lambda 和 Amazon EventBridge 主要用于事件驱动的架构，虽然可以触发 Lambda 函数，但缺乏工作流程编排的能力，实现复杂工作流程的开销较大，且没有提供人工审批的机制。"
    },
    "related_terms": [
      "AWS Step Functions",
      "AWS Lambda",
      "Amazon EC2",
      "Amazon SQS",
      "Amazon EventBridge",
      "AWS Glue"
    ]
  },
  {
    "id": 376,
    "topic": "1",
    "question_en": "A company has launched an Amazon RDS for MySQL DB instance. Most of the connections to the database come from serverless applications. Application trafic to the database changes significantly at random intervals. At times of high demand, users report that their applications experience database connection rejection errors. Which solution will resolve this issue with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a proxy in RDS Proxy. Configure the users’ applications to use the DB instance through RDS Proxy.",
      "B": "Deploy Amazon ElastiCache for Memcached between the users’ applications and the DB instance.",
      "C": "Migrate the DB instance to a different instance class that has higher I/O capacity. Configure the users’ applications to use the new DB instance.",
      "D": "Configure Multi-AZ for the DB instance. Configure the users’ applications to switch between the DB instances."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司启动了一个 Amazon RDS for MySQL 数据库实例。大多数与数据库的连接来自无服务器应用程序。对数据库的应用程序流量会以随机间隔发生显著变化。在高需求时，用户报告他们的应用程序遇到数据库连接被拒绝的错误。哪种解决方案将以最少的运营开销解决此问题？",
    "options_cn": {
      "A": "在 RDS Proxy 中创建一个代理。将用户应用程序配置为通过 RDS Proxy 使用数据库实例。",
      "B": "在用户应用程序和数据库实例之间部署 Amazon ElastiCache for Memcached。",
      "C": "将数据库实例迁移到具有更高 I/O 容量的不同实例类型。将用户应用程序配置为使用新的数据库实例。",
      "D": "为数据库实例配置多可用区。将用户应用程序配置为在数据库实例之间切换。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "MySQL",
      "ElastiCache",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 RDS Proxy 的应用场景与优势，以及针对数据库连接问题的解决方案。与数据库连接管理、高可用性、性能优化相关。",
      "why_correct": "RDS Proxy 是一种完全托管的数据库代理，可以帮助提高应用程序的可伸缩性、增强数据库的可用性、以及提高应用程序的容错能力。RDS Proxy 能够有效地管理数据库连接，通过连接池复用数据库连接，减少数据库连接建立和断开的开销。对于无服务器应用程序和流量波动大的场景，RDS Proxy 能够显著提升性能，并缓解数据库连接被拒绝的问题。公司启动的数据库实例为 MySQL，且存在数据库连接拒绝的错误，使用 RDS Proxy 可以以最少的运营开销解决此问题。",
      "why_wrong": "B. Amazon ElastiCache for Memcached 主要用于缓存，提升读取性能，无法解决数据库连接被拒绝的问题，并且不能直接替代数据库连接。其主要针对读密集型负载，而不是连接管理。C. 迁移到具有更高 I/O 容量的实例类型，并不能直接解决连接被拒绝的问题。连接问题通常源于连接数超过数据库实例允许的最大连接数限制，或连接建立速度不足。增加 I/O 容量对解决连接问题帮助有限，并且会增加成本。D. 为数据库实例配置多可用区可以提高数据库的可用性，但不能解决连接被拒绝的问题。多可用区方案主要用于故障转移，确保数据库的高可用性，不能解决连接池耗尽或连接建立过慢的问题。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS Proxy",
      "MySQL",
      "ElastiCache",
      "Multi-AZ",
      "EC2",
      "EBS",
      "Memcached",
      "database instance"
    ]
  },
  {
    "id": 377,
    "topic": "1",
    "question_en": "A company recently deployed a new auditing system to centralize information about operating system versions, patching, and installed software for Amazon EC2 instances. A solutions architect must ensure all instances provisioned through EC2 Auto Scaling groups successfully send reports to the auditing system as soon as they are launched and terminated. Which solution achieves these goals MOST eficiently?",
    "options_en": {
      "A": "Use a scheduled AWS Lambda function and run a script remotely on all EC2 instances to send data to the audit system.",
      "B": "Use EC2 Auto Scaling lifecycle hooks to run a custom script to send data to the audit system when instances are launched and terminated.",
      "C": "Use an EC2 Auto Scaling launch configuration to run a custom script through user data to send data to the audit system when instances are launched and terminated.",
      "D": "Run a custom script on the instance operating system to send data to the audit system. Configure the script to be invoked by the EC2 Auto Scaling group when the instance starts and is terminated."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司最近部署了一个新的审计系统，用于集中收集有关 Amazon EC2 实例的操作系统版本、补丁和已安装软件的信息。解决方案架构师必须确保通过 EC2 Auto Scaling 组预置的所有实例在启动和终止后立即将报告成功发送到审计系统。哪种解决方案最有效地实现了这些目标？",
    "options_cn": {
      "A": "使用一个定期的 AWS Lambda 函数，并在所有 EC2 实例上远程运行一个脚本，以将数据发送到审计系统。",
      "B": "使用 EC2 Auto Scaling 生命周期钩子来运行自定义脚本，以便在实例启动和终止时将数据发送到审计系统。",
      "C": "使用 EC2 Auto Scaling 启动配置，通过用户数据运行自定义脚本，以便在实例启动和终止时将数据发送到审计系统。",
      "D": "在实例操作系统上运行自定义脚本，以将数据发送到审计系统。配置该脚本，以便在实例启动和终止时由 EC2 Auto Scaling 组调用。"
    },
    "tags": [
      "EC2 Auto Scaling",
      "Lifecycle Hooks",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 EC2 Auto Scaling 场景下实例生命周期管理以及与审计系统的集成；与 Auto Scaling 的 Lifecycle Hooks、用户数据、实例内部脚本的选型与对比相关。",
      "why_correct": "EC2 Auto Scaling 生命周期钩子（Lifecycle Hooks）允许您在 Auto Scaling 组中的 EC2 实例生命周期内执行自定义操作。使用 Lifecycle Hooks，您可以配置在实例启动或终止期间触发自定义脚本，从而在实例启动和终止时将报告发送到审计系统。这确保了在实例启动后（例如，配置了操作系统版本、补丁和已安装软件）和终止前（收集最终审计数据）立即发送报告，满足了题目对时效性的要求，并且无需轮询或主动探测。",
      "why_wrong": "选项 A 错误，因为使用 Lambda 函数和远程脚本的方案效率较低，需要依赖网络连接，并可能因为网络问题或 Lambda 函数的执行延迟而导致审计报告发送失败或延迟。选项 C 错误，因为用户数据仅在实例启动时执行，无法在终止时执行。此外，用户数据通常用于初始化实例，而非处理实例终止前的清理工作。选项 D 错误，因为在实例操作系统上运行的脚本需要手动配置和维护，并且无法保证在实例终止前成功执行，例如实例直接被强制关闭。此外，这种方式增加了维护复杂性，因为需要在每个实例上单独部署和管理脚本，并且没有与 Auto Scaling 组的集成，无法确保在 Auto Scaling 组自动伸缩时的一致性。"
    },
    "related_terms": [
      "EC2 Auto Scaling",
      "EC2",
      "Auto Scaling",
      "Lambda",
      "Lifecycle Hooks",
      "User Data"
    ]
  },
  {
    "id": 378,
    "topic": "1",
    "question_en": "A company is developing a real-time multiplayer game that uses UDP for communications between the client and servers in an Auto Scaling group. Spikes in demand are anticipated during the day, so the game server platform must adapt accordingly. Developers want to store gamer scores and other non-relational data in a database solution that will scale without intervention. Which solution should a solutions architect recommend?",
    "options_en": {
      "A": "Use Amazon Route 53 for trafic distribution and Amazon Aurora Serverless for data storage.",
      "B": "Use a Network Load Balancer for trafic distribution and Amazon DynamoDB on-demand for data storage.",
      "C": "Use a Network Load Balancer for trafic distribution and Amazon Aurora Global Database for data storage.",
      "D": "Use an Application Load Balancer for trafic distribution and Amazon DynamoDB global tables for data storage."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在开发一款实时多人游戏，该游戏使用 UDP 在 Auto Scaling 组中的客户端和服务器之间进行通信。预计白天会有需求高峰，因此游戏服务器平台必须相应地进行调整。开发人员希望将玩家分数和其他非关系型数据存储在无需干预即可扩展的数据库解决方案中。解决方案架构师应该推荐哪种解决方案？",
    "options_cn": {
      "A": "使用 Amazon Route 53 进行流量分配，并使用 Amazon Aurora Serverless 进行数据存储。",
      "B": "使用 Network Load Balancer 进行流量分配，并使用 Amazon DynamoDB 按需容量模式进行数据存储。",
      "C": "使用 Network Load Balancer 进行流量分配，并使用 Amazon Aurora Global Database 进行数据存储。",
      "D": "使用 Application Load Balancer 进行流量分配，并使用 Amazon DynamoDB 全局表进行数据存储。"
    },
    "tags": [
      "Network Load Balancer",
      "Amazon DynamoDB",
      "DynamoDB On-Demand",
      "Auto Scaling",
      "UDP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查在实时多人游戏场景下，如何设计高可用、可扩展的架构，涉及流量分配、数据存储方案的选择。与 Network Load Balancer (NLB) 和 DynamoDB 的选型与配置，以及对 UDP 协议的支持相关。",
      "why_correct": "Network Load Balancer (NLB) 适用于 UDP 流量，能处理游戏服务器的流量分配需求。DynamoDB 的按需容量模式非常适合应对峰值流量，无需提前规划容量，可以根据实际负载自动扩展，满足游戏平台对数据存储的可扩展性需求。",
      "why_wrong": {
        "A": "Amazon Aurora Serverless 并不直接支持 UDP 协议，且 Aurora 属于关系型数据库，不适用于存储玩家分数等非关系型数据。Route 53 主要用于 DNS 解析和流量路由，与游戏服务器的流量分发需求不直接相关。",
        "C": "Aurora Global Database 针对跨区域数据复制，与本地游戏服务器部署和流量分发的需求不匹配。同样地，Aurora 也是关系型数据库，不适合存储非关系型数据。",
        "D": "Application Load Balancer (ALB) 仅支持 TCP、HTTP 和 HTTPS 协议，无法处理 UDP 流量。虽然 DynamoDB 全局表可以实现跨区域的数据复制，但其与 NLB 的组合更适合此场景，而 ALB 无法支持 UDP 流量。"
      }
    },
    "related_terms": [
      "Network Load Balancer",
      "NLB",
      "Amazon DynamoDB",
      "UDP",
      "Application Load Balancer",
      "ALB",
      "Amazon Route 53",
      "Auto Scaling",
      "DynamoDB On-Demand",
      "Amazon Aurora Serverless",
      "Amazon Aurora Global Database"
    ]
  },
  {
    "id": 379,
    "topic": "1",
    "question_en": "A company hosts a frontend application that uses an Amazon API Gateway API backend that is integrated with AWS Lambda. When the API receives requests, the Lambda function loads many libraries. Then the Lambda function connects to an Amazon RDS database, processes the data, and returns the data to the frontend application. The company wants to ensure that response latency is as low as possible for all its users with the fewest number of changes to the company's operations. Which solution will meet these requirements?",
    "options_en": {
      "A": "Establish a connection between the frontend application and the database to make queries faster by bypassing the API.",
      "B": "Configure provisioned concurrency for the Lambda function that handles the requests.",
      "C": "Cache the results of the queries in Amazon S3 for faster retrieval of similar datasets.",
      "D": "Increase the size of the database to increase the number of connections Lambda can establish at one time."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管一个前端应用程序，该应用程序使用与 AWS Lambda 集成的 Amazon API Gateway API 后端。 当 API 收到请求时，Lambda 函数会加载许多库。 然后，Lambda 函数连接到 Amazon RDS 数据库，处理数据，并将数据返回给前端应用程序。该公司希望确保所有用户的响应延迟尽可能低，同时对公司的运营变更最少。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在前端应用程序和数据库之间建立连接，通过绕过 API 来加快查询速度。",
      "B": "为处理请求的 Lambda 函数配置预置并发。",
      "C": "将查询结果缓存在 Amazon S3 中，以便更快地检索类似数据集。",
      "D": "增加数据库的大小，以增加 Lambda 可以一次建立的连接数。"
    },
    "tags": [
      "Amazon S3",
      "API Gateway",
      "Lambda",
      "RDS",
      "Caching"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n题目考察 API Gateway、Lambda 函数性能优化，以及对延迟敏感的应用场景的解决方案。",
      "why_correct": "为 Lambda 函数配置预置并发（Provisioned Concurrency）可以提前初始化 Lambda 函数，并将它们保持就绪状态以处理请求。 这消除了冷启动的延迟，从而减少了响应时间，满足了降低延迟的要求，并且对现有基础设施的变动最小。",
      "why_wrong": "选项 A 绕过 API Gateway 会导致安全性和管理上的问题，并且前端直接连接数据库也可能导致连接管理困难，这与减少运营变更的需求相悖。选项 C 将查询结果缓存在 S3 中适用于读取密集型场景，但无法解决 Lambda 函数加载库和数据库连接导致的延迟。选项 D 增加数据库大小虽然可以支持更多的并发连接，但无法直接解决 Lambda 函数冷启动和处理过程中的延迟问题，并且改变数据库规模带来的成本和运维变动较大。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon RDS",
      "Amazon S3",
      "Provisioned Concurrency"
    ]
  },
  {
    "id": 380,
    "topic": "1",
    "question_en": "A company is migrating its on-premises workload to the AWS Cloud. The company already uses several Amazon EC2 instances and Amazon RDS DB instances. The company wants a solution that automatically starts and stops the EC2 instances and DB instances outside of business hours. The solution must minimize cost and infrastructure maintenance. Which solution will meet these requirements?",
    "options_en": {
      "A": "Scale the EC2 instances by using elastic resize. Scale the DB instances to zero outside of business hours.",
      "B": "Explore AWS Marketplace for partner solutions that will automatically start and stop the EC2 instances and DB instances on a schedule.",
      "C": "Launch another EC2 instance. Configure a crontab schedule to run shell scripts that will start and stop the existing EC2 instances and DB instances on a schedule.",
      "D": "Create an AWS Lambda function that will start and stop the EC2 instances and DB instances. Configure Amazon EventBridge to invoke the Lambda function on a schedule."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其本地工作负载迁移到 AWS 云。该公司已经使用了多个 Amazon EC2 实例和 Amazon RDS 数据库实例。该公司希望有一个解决方案，可以在非工作时间自动启动和停止 EC2 实例和数据库实例。该解决方案必须最大限度地降低成本并减少基础设施维护。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过使用弹性调整大小来扩展 EC2 实例。在非工作时间将数据库实例缩减到零。",
      "B": "在 AWS Marketplace 中探索合作伙伴解决方案，这些解决方案将按计划自动启动和停止 EC2 实例和数据库实例。",
      "C": "启动另一个 EC2 实例。配置一个 crontab 计划，以运行 shell 脚本，这些脚本将按计划启动和停止现有的 EC2 实例和数据库实例。",
      "D": "创建一个 AWS Lambda 函数，该函数将启动和停止 EC2 实例和数据库实例。配置 Amazon EventBridge 以按计划调用 Lambda 函数。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon RDS",
      "Cost Optimization",
      "AWS Lambda",
      "Amazon EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查在云上自动化启动和停止 EC2 实例和 RDS 数据库实例，以达到成本优化和减少维护的目的。与 Lambda 函数、EventBridge、以及不同实例类型的扩展能力相关。",
      "why_correct": "选项 A 提出了一个经济高效的解决方案。通过使用弹性调整大小（如 Auto Scaling）来扩展 EC2 实例，可以在工作负载高峰期提供更高的计算能力，而在非高峰期保持较低的实例数量。同时，将数据库实例缩减到零，意味着在非工作时间完全停止数据库，从而避免了不必要的运行成本。这种方法既能满足需求，又能最大限度地降低成本。",
      "why_wrong": "选项 B 依赖于第三方解决方案，增加了复杂性，并且可能涉及额外的订阅费用，这与成本最小化的目标相悖。选项 C 描述了一个手动管理的解决方案，需要维护和管理 shell 脚本及定时任务，增加了维护工作量，且不如使用 AWS 提供的原生服务（如 Lambda 和 EventBridge）高效和可靠。选项 D 虽然使用了 AWS Lambda 和 EventBridge，但未能充分利用 EC2 实例的弹性扩展能力，并且直接启动和停止 EC2 实例的操作不如 A 方案更灵活。A 方案既能自动化，又能实现实例的弹性伸缩，成本控制更优。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "AWS Lambda",
      "Amazon EventBridge",
      "Auto Scaling",
      "AWS Marketplace",
      "crontab"
    ]
  },
  {
    "id": 381,
    "topic": "1",
    "question_en": "A company hosts a three-tier web application that includes a PostgreSQL database. The database stores the metadata from documents. The company searches the metadata for key terms to retrieve documents that the company reviews in a report each month. The documents are stored in Amazon S3. The documents are usually written only once, but they are updated frequently. The reporting process takes a few hours with the use of relational queries. The reporting process must not prevent any document modifications or the addition of new documents. A solutions architect needs to implement a solution to speed up the reporting process. Which solution will meet these requirements with the LEAST amount of change to the application code?",
    "options_en": {
      "A": "Set up a new Amazon DocumentDB (with MongoDB compatibility) cluster that includes a read replica. Scale the read replica to generate the reports.",
      "B": "Set up a new Amazon Aurora PostgreSQL DB cluster that includes an Aurora Replica. Issue queries to the Aurora Replica to generate the reports.",
      "C": "Set up a new Amazon RDS for PostgreSQL Multi-AZ DB instance. Configure the reporting module to query the secondary RDS node so that the reporting module does not affect the primary node.",
      "D": "Set up a new Amazon DynamoDB table to store the documents. Use a fixed write capacity to support new document entries. Automatically scale the read capacity to support the reports."
    },
    "correct_answer": "B",
    "vote_percentage": "94%",
    "question_cn": "一家公司托管一个三层 Web 应用程序，该应用程序包括一个 PostgreSQL 数据库。该数据库存储文档的元数据。该公司搜索元数据中的关键术语，以检索该公司每月在报告中审查的文档。这些文档存储在 Amazon S3 中。这些文档通常只写入一次，但会经常更新。使用关系查询的报告过程需要几个小时。报告过程不得阻止任何文档修改或新文档的添加。解决方案架构师需要实施一个解决方案来加速报告过程。哪个解决方案将满足这些要求，并且对应用程序代码的更改最少？",
    "options_cn": {
      "A": "设置一个新的 Amazon DocumentDB（与 MongoDB 兼容）集群，其中包括一个只读副本。扩展只读副本以生成报告。",
      "B": "设置一个新的 Amazon Aurora PostgreSQL 数据库集群，其中包括一个 Aurora 副本。向 Aurora 副本发出查询以生成报告。",
      "C": "设置一个新的 Amazon RDS for PostgreSQL 多可用区数据库实例。将报告模块配置为查询辅助 RDS 节点，以便报告模块不影响主节点。",
      "D": "设置一个新的 Amazon DynamoDB 表来存储文档。使用固定的写入容量来支持新的文档条目。自动扩展读取容量以支持报告。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon S3",
      "Read Capacity",
      "Write Capacity",
      "Aurora PostgreSQL",
      "Amazon RDS for PostgreSQL",
      "Amazon DocumentDB",
      "MongoDB",
      "PostgreSQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 94%），解析仅供参考。】\n\n考查了如何通过设置数据库的只读副本，加速数据库报表生成，同时不影响主数据库的性能以及对数据的读写操作。",
      "why_correct": "Amazon Aurora PostgreSQL 数据库集群是高度可扩展的，并且支持创建 Aurora 副本，这些副本可以独立于主实例运行。通过向 Aurora 副本发出查询，可以将报告负载从主数据库实例中卸载，从而提高报告性能，并确保报表过程不影响文档的修改或新文档的添加。Aurora 副本提供了与主实例相同的数据，并且查询 Aurora 副本不会影响主实例的性能。",
      "why_wrong": "选项 A，虽然 DocumentDB 兼容 MongoDB，但其设计并非专门针对关系型查询，性能可能无法满足需求。选项 C，虽然 RDS for PostgreSQL 的多可用区配置包含一个辅助节点，但其主要目的是提供高可用性而非专门优化报告性能，且查询辅助节点可能无法完全满足性能需求。选项 D，将数据迁移到 DynamoDB 会涉及应用程序代码的重大更改，这与题目中“对应用程序代码的更改最少”的要求相悖，并且 DynamoDB 并不擅长复杂的关系型查询。"
    },
    "related_terms": [
      "Amazon S3",
      "PostgreSQL",
      "Amazon DocumentDB",
      "MongoDB",
      "Amazon Aurora PostgreSQL",
      "Aurora Replica",
      "Amazon RDS for PostgreSQL",
      "DynamoDB"
    ]
  },
  {
    "id": 382,
    "topic": "1",
    "question_en": "A company has a three-tier application on AWS that ingests sensor data from its users’ devices. The trafic fiows through a Network Load Balancer (NLB), then to Amazon EC2 instances for the web tier, and finally to EC2 instances for the application tier. The application tier makes calls to a database. What should a solutions architect do to improve the security of the data in transit?",
    "options_en": {
      "A": "Configure a TLS listener. Deploy the server certificate on the NLB.",
      "B": "Configure AWS Shield Advanced. Enable AWS WAF on the NLB.",
      "C": "Change the load balancer to an Application Load Balancer (ALB). Enable AWS WAF on the ALB.",
      "D": "Encrypt the Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instances by using AWS Key Management Service (AWS KMS)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上有一个三层应用程序，该应用程序从其用户的设备摄取传感器数据。流量通过 Network Load Balancer (NLB)，然后到用于 Web 层的 Amazon EC2 实例，最后到用于应用程序层的 EC2 实例。应用程序层调用一个数据库。解决方案架构师应该怎么做才能提高传输中数据的安全性？",
    "options_cn": {
      "A": "配置 TLS 监听器。 在 NLB 上部署服务器证书。",
      "B": "配置 AWS Shield Advanced。在 NLB 上启用 AWS WAF。",
      "C": "将负载均衡器更改为 Application Load Balancer (ALB)。在 ALB 上启用 AWS WAF。",
      "D": "使用 AWS Key Management Service (AWS KMS) 加密 EC2 实例上的 Amazon Elastic Block Store (Amazon EBS) 卷。"
    },
    "tags": [
      "Network Load Balancer",
      "TLS",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在传输过程中保护数据的安全措施。与网络负载均衡器的配置、TLS 协议以及服务器证书的部署相关。",
      "why_correct": "配置 TLS 监听器并在 NLB 上部署服务器证书可以为传输中的数据提供加密。NLB 支持 TLS 终止，这意味着它可以处理 TLS 加密和解密，保护客户端和 NLB 之间的数据安全。部署服务器证书保证了客户端与 NLB 之间的身份验证和安全连接。",
      "why_wrong": "B 选项：AWS Shield Advanced 主要是针对 DDoS 攻击的保护，不能直接加密数据。AWS WAF 可以防御 Web 应用程序层面的攻击，但不是用于保护传输中数据的核心方案。\nC 选项：ALB 同样可以与 AWS WAF 配合使用，但问题核心在于数据传输加密。ALB 与 NLB 的主要区别在于应用层功能，而不是针对数据传输加密的功能。更改负载均衡器并不能直接解决题干中的安全需求。\nD 选项：使用 AWS KMS 加密 EBS 卷可以保护静态数据，而本题需求是保护传输中的数据，两者关注点不同。EBS 加密无法保证数据在传输过程中的安全，并且与 NLB 的配置无关。"
    },
    "related_terms": [
      "Network Load Balancer",
      "TLS",
      "Amazon EC2",
      "AWS Shield Advanced",
      "AWS WAF",
      "Application Load Balancer",
      "AWS Key Management Service",
      "Amazon Elastic Block Store",
      "AWS KMS",
      "NLB",
      "ALB",
      "EBS"
    ]
  },
  {
    "id": 383,
    "topic": "1",
    "question_en": "A company is planning to migrate a commercial off-the-shelf application from its on-premises data center to AWS. The software has a software licensing model using sockets and cores with predictable capacity and uptime requirements. The company wants to use its existing licenses, which were purchased earlier this year. Which Amazon EC2 pricing option is the MOST cost-effective?",
    "options_en": {
      "A": "Dedicated Reserved Hosts",
      "B": "Dedicated On-Demand Hosts",
      "C": "Dedicated Reserved Instances",
      "D": "Dedicated On-Demand Instances"
    },
    "correct_answer": "A",
    "vote_percentage": "89%",
    "question_cn": "一家公司计划将其商业现成的应用程序从其本地数据中心迁移到 AWS。该软件具有使用套接字和内核的软件许可模式，具有可预测的容量和正常运行时间要求。该公司希望使用其现有的许可证，这些许可证是今年早些时候购买的。哪种 Amazon EC2 定价选项最具成本效益？",
    "options_cn": {
      "A": "Dedicated Reserved Hosts",
      "B": "Dedicated On-Demand Hosts",
      "C": "Dedicated Reserved Instances",
      "D": "Dedicated On-Demand Instances"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Pricing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 89%），解析仅供参考。】\n\n考查 EC2 定价模式的选择；与软件许可、容量需求、正常运行时间等因素相关。",
      "why_correct": "Dedicated Reserved Hosts 最具成本效益，因为该公司需要使用现有的软件许可证，这些许可证与特定的物理服务器绑定。Reserved Hosts 允许将许可证分配给专用主机，并提供最大的灵活性，能够控制底层物理服务器，从而满足软件许可要求。此外，Reserved Hosts 还提供了预留容量，满足了可预测的容量和正常运行时间要求。",
      "why_wrong": "Dedicated On-Demand Hosts 提供的成本高于 Dedicated Reserved Hosts，因为没有预留实例的折扣，且仍然是按需付费。Dedicated Reserved Instances 无法满足软件许可的特定需求，因为它们虽然提供了预留容量和折扣，但许可证通常绑定到主机，而不是实例。Dedicated On-Demand Instances 提供的成本高于 Dedicated Reserved Hosts，且无法满足许可要求，因为它不提供控制底层物理服务器的能力，无法绑定特定许可证。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "Dedicated Reserved Hosts",
      "Dedicated On-Demand Hosts",
      "Dedicated Reserved Instances",
      "Dedicated On-Demand Instances"
    ]
  },
  {
    "id": 384,
    "topic": "1",
    "question_en": "A company runs an application on Amazon EC2 Linux instances across multiple Availability Zones. The application needs a storage layer that is highly available and Portable Operating System Interface (POSIX)-compliant. The storage layer must provide maximum data durability and must be shareable across the EC2 instances. The data in the storage layer will be accessed frequently for the first 30 days and will be accessed infrequently after that time. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Glacier.",
      "B": "Use the Amazon S3 Standard storage class. Create an S3 Lifecycle policy to move infrequently accessed data to S3 Standard-Infrequent Access (S3 Standard-IA).",
      "C": "Use the Amazon Elastic File System (Amazon EFS) Standard storage class. Create a lifecycle management policy to move infrequently accessed data to EFS Standard-Infrequent Access (EFS Standard-IA).",
      "D": "Use the Amazon Elastic File System (Amazon EFS) One Zone storage class. Create a lifecycle management policy to move infrequently accessed data to EFS One Zone-Infrequent Access (EFS One Zone-IA)."
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一家公司在多个可用区运行 Amazon EC2 Linux 实例上的应用程序。该应用程序需要一个高可用性且符合可移植操作系统接口 (POSIX) 标准的存储层。该存储层必须提供最大的数据持久性，并且必须在 EC2 实例之间共享。存储层中的数据将在前 30 天内频繁访问，此后将不经常访问。哪种解决方案可以最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 标准存储类。创建 S3 生命周期策略，将不经常访问的数据移动到 S3 Glacier。",
      "B": "使用 Amazon S3 标准存储类。创建 S3 生命周期策略，将不经常访问的数据移动到 S3 标准 - 不频繁访问 (S3 Standard-IA)。",
      "C": "使用 Amazon Elastic File System (Amazon EFS) 标准存储类。创建生命周期管理策略，将不经常访问的数据移动到 EFS 标准 - 不频繁访问 (EFS Standard-IA)。",
      "D": "使用 Amazon Elastic File System (Amazon EFS) 单区存储类。创建生命周期管理策略，将不经常访问的数据移动到 EFS 单区 - 不频繁访问 (EFS One Zone-IA)。"
    },
    "tags": [
      "Amazon S3",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier",
      "Amazon EFS",
      "EFS Standard",
      "EFS Standard-IA",
      "EFS One Zone-IA",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n考察如何在满足高可用性、POSIX 兼容性和数据持久性要求的前提下，使用 EFS 和 S3 的生命周期策略，以实现最具成本效益的存储解决方案。",
      "why_correct": "Amazon EFS 标准存储类提供了高可用性和 POSIX 兼容性，非常适合在多个可用区运行的 EC2 实例之间共享文件。生命周期管理策略可以将不经常访问的数据迁移到 EFS Standard-IA，从而降低存储成本。这种组合方案在保证数据持久性的同时，实现了成本优化。",
      "why_wrong": "选项 A 错误，因为 S3 不提供 POSIX 兼容性，不适用于应用程序的存储需求。选项 B 错误，虽然 S3 Standard-IA 提供成本效益，但它同样不提供 POSIX 兼容性。选项 D 错误，虽然 EFS 单区存储类也支持生命周期管理，但它不提供高可用性，这与题目的要求相悖，并且它的可用性不如 EFS 标准存储类。"
    },
    "related_terms": [
      "Amazon EC2",
      "POSIX",
      "Amazon S3",
      "S3 Standard",
      "S3 Glacier",
      "S3 Standard-IA",
      "Amazon Elastic File System (Amazon EFS)",
      "EFS Standard",
      "EFS Standard-IA",
      "EFS One Zone-IA"
    ]
  },
  {
    "id": 385,
    "topic": "1",
    "question_en": "A solutions architect is creating a new VPC design. There are two public subnets for the load balancer, two private subnets for web servers, and two private subnets for MySQL. The web servers use only HTTPS. The solutions architect has already created a security group for the load balancer allowing port 443 from 0.0.0.0/0. Company policy requires that each resource has the least access required to still be able to perform its tasks. Which additional configuration strategy should the solutions architect use to meet these requirements?",
    "options_en": {
      "A": "Create a security group for the web servers and allow port 443 from 0.0.0.0/0. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
      "B": "Create a network ACL for the web servers and allow port 443 from 0.0.0.0/0. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group.",
      "C": "Create a security group for the web servers and allow port 443 from the load balancer. Create a security group for the MySQL servers and allow port 3306 from the web servers security group.",
      "D": "Create a network ACL for the web servers and allow port 443 from the load balancer. Create a network ACL for the MySQL servers and allow port 3306 from the web servers security group."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在创建新的 VPC 设计。 有两个用于 Application Load Balancer 的公有子网，两个用于 Web 服务器的私有子网，以及两个用于 MySQL 的私有子网。 Web 服务器仅使用 HTTPS。 解决方案架构师已经为 Application Load Balancer 创建了一个安全组，允许来自 0.0.0.0/0 的 443 端口。 公司策略要求每个资源都具有完成其任务所需的最低访问权限。 解决方案架构师应该使用哪种额外的配置策略来满足这些要求？",
    "options_cn": {
      "A": "为 Web 服务器创建一个安全组，并允许来自 0.0.0.0/0 的 443 端口。 为 MySQL 服务器创建一个安全组，并允许来自 Web 服务器安全组的 3306 端口。",
      "B": "为 Web 服务器创建一个网络 ACL，并允许来自 0.0.0.0/0 的 443 端口。 为 MySQL 服务器创建一个网络 ACL，并允许来自 Web 服务器安全组的 3306 端口。",
      "C": "为 Web 服务器创建一个安全组，并允许来自 Application Load Balancer 的 443 端口。 为 MySQL 服务器创建一个安全组，并允许来自 Web 服务器安全组的 3306 端口。",
      "D": "为 Web 服务器创建一个网络 ACL，并允许来自 Application Load Balancer 的 443 端口。 为 MySQL 服务器创建一个网络 ACL，并允许来自 Web 服务器安全组的 3306 端口。"
    },
    "tags": [
      "Security Group",
      "Amazon VPC",
      "Application Load Balancer",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察 VPC 内的安全组配置，以及最小权限原则的实践。涉及 Application Load Balancer、Web 服务器和 MySQL 数据库的安全隔离，需要理解安全组的作用和配置方式，与 Network ACL 的区别与联系。",
      "why_correct": "选项 C 遵循了最小权限原则，且配置正确。它为 Web 服务器创建了一个安全组，仅允许来自 Application Load Balancer 的 443 端口（HTTPS）。 这限制了 Web 服务器的外部访问，符合公司策略。此外，MySQL 服务器的安全组允许来自 Web 服务器安全组的 3306 端口（MySQL 默认端口），保证了 Web 服务器对 MySQL 数据库的访问，同时限制了 MySQL 数据库的访问源，满足了最小权限原则的要求。",
      "why_wrong": "选项 A 错误的原因在于，它允许 Web 服务器从 0.0.0.0/0 访问 443 端口。这实际上向互联网开放了 Web 服务器的 HTTPS 访问，违反了最小权限原则，因为 Web 服务器的 HTTPS 访问应该仅仅来自于 Application Load Balancer。选项 B 和 D 错误的原因在于，它们使用了 Network ACL 而不是安全组。Network ACL 是在子网级别进行流量控制的，而安全组是在实例级别进行流量控制的。虽然 Network ACL 也可以实现流量控制，但它们不具备安全组的灵活性和细粒度控制。 此外，选项 D 与选项 B 一样，都将 443 端口的访问限制为 Application Load Balancer，这不符合题目要求，因为 Web 服务器的 HTTPS 访问应该仅仅来自于 Application Load Balancer。"
    },
    "related_terms": [
      "Application Load Balancer",
      "MySQL",
      "Network ACL",
      "VPC",
      "HTTPS",
      "Web server",
      "Security Group",
      "0.0.0.0/0"
    ]
  },
  {
    "id": 386,
    "topic": "1",
    "question_en": "An ecommerce company is running a multi-tier application on AWS. The front-end and backend tiers both run on Amazon EC2, and the database runs on Amazon RDS for MySQL. The backend tier communicates with the RDS instance. There are frequent calls to return identical datasets from the database that are causing performance slowdowns. Which action should be taken to improve the performance of the backend?",
    "options_en": {
      "A": "Implement Amazon SNS to store the database calls.",
      "B": "Implement Amazon ElastiCache to cache the large datasets.",
      "C": "Implement an RDS for MySQL read replica to cache database calls.",
      "D": "Implement Amazon Kinesis Data Firehose to stream the calls to the database."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司正在 AWS 上运行一个多层应用程序。前端和后端层都在 Amazon EC2 上运行，数据库在 Amazon RDS for MySQL 上运行。后端层与 RDS 实例通信。经常调用从数据库返回相同的数据集，导致性能下降。应采取什么行动来提高后端的性能？",
    "options_cn": {
      "A": "实施 Amazon SNS 来存储数据库调用。",
      "B": "实施 Amazon ElastiCache 来缓存大型数据集。",
      "C": "实施 RDS for MySQL 只读副本以缓存数据库调用。",
      "D": "实施 Amazon Kinesis Data Firehose 将调用流式传输到数据库。"
    },
    "tags": [
      "Amazon ElastiCache",
      "RDS for MySQL",
      "Amazon EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 ElastiCache 的使用场景以及缓存机制，与 RDS for MySQL、EC2 上的应用性能优化相关。",
      "why_correct": "Amazon ElastiCache 是一种内存缓存服务，可以用于缓存数据库查询结果。通过将经常访问的数据集缓存在 ElastiCache 中，可以减少对 RDS for MySQL 数据库的访问，从而提高后端应用程序的性能。在这种情况下，ElastiCache 缓存了大型数据集，后端层可以直接从 ElastiCache 读取数据，避免了重复查询数据库的开销。",
      "why_wrong": "A. Amazon SNS（Simple Notification Service）是一个发布/订阅消息服务，用于发送通知和消息，与数据库调用缓存无关，无法提高数据库性能。\nC. RDS for MySQL 只读副本主要用于提高读取性能和实现高可用性，它复制了数据库的数据，但并不直接用于缓存数据库调用。虽然可以减少主数据库的负载，但其主要用途是读负载分担，而非针对重复查询进行缓存。\nD. Amazon Kinesis Data Firehose 是一个数据流服务，用于将数据流式传输到数据仓库或其它目的地，与缓存数据库调用无关。Kinesis Data Firehose 主要用于数据摄取，而非优化数据库查询性能。"
    },
    "related_terms": [
      "Amazon EC2",
      "ElastiCache",
      "Amazon SNS",
      "Kinesis Data Firehose",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "id": 387,
    "topic": "1",
    "question_en": "A new employee has joined a company as a deployment engineer. The deployment engineer will be using AWS CloudFormation templates to create multiple AWS resources. A solutions architect wants the deployment engineer to perform job activities while following the principle of least privilege. Which combination of actions should the solutions architect take to accomplish this goal? (Choose two.)",
    "options_en": {
      "A": "Have the deployment engineer use AWS account root user credentials for performing AWS CloudFormation stack operations.",
      "B": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the PowerUsers IAM policy attached.",
      "C": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has the AdministratorAccess IAM policy attached.",
      "D": "Create a new IAM user for the deployment engineer and add the IAM user to a group that has an IAM policy that allows AWS CloudFormation actions only",
      "E": "Create an IAM role for the deployment engineer to explicitly define the permissions specific to the AWS CloudFormation stack and launch stacks using that IAM role."
    },
    "correct_answer": "DE",
    "vote_percentage": "",
    "question_cn": "一位新员工作为部署工程师加入了公司。部署工程师将使用 AWS CloudFormation 模板来创建多个 AWS 资源。一位解决方案架构师希望部署工程师在执行工作活动时遵循最小权限原则。解决方案架构师应采取哪些组合措施来完成此目标？（选择两个。）",
    "options_cn": {
      "A": "让部署工程师使用 AWS 账户根用户凭证来执行 AWS CloudFormation 堆栈操作。",
      "B": "为部署工程师创建一个新的 IAM 用户，并将该 IAM 用户添加到附加了 PowerUsers IAM 策略的组中。",
      "C": "为部署工程师创建一个新的 IAM 用户，并将该 IAM 用户添加到附加了 AdministratorAccess IAM 策略的组中。",
      "D": "为部署工程师创建一个新的 IAM 用户，并将该 IAM 用户添加到具有仅允许 AWS CloudFormation 操作的 IAM 策略的组中。",
      "E": "为部署工程师创建一个 IAM 角色，以明确定义特定于 AWS CloudFormation 堆栈的权限，并使用该 IAM 角色启动堆栈。"
    },
    "tags": [
      "IAM",
      "AWS CloudFormation",
      "least privilege"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 —），解析仅供参考。】\n\n考查 IAM 权限配置，以及在 AWS CloudFormation 中的应用，特别是最小权限原则的实践。这与 IAM 用户、IAM 组、IAM 策略和 IAM 角色的选择和配置相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 DE。理由简述：创建一个新的 IAM 用户，并将其添加到具有仅允许 AWS CloudFormation 操作的 IAM 策略的组中，是实现最小权限原则的最佳实践。这种方法允许部署工程师仅拥有执行 CloudFormation 堆栈操作所需的最低权限，从而限制了潜在的安全风险。通过这种方式，可以精确控制部署工程师对 AWS 资源的访问，避免了不必要的权限授予。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项：使用根用户凭证违反了最小权限原则，根用户具有对账户的完全访问权限，一旦凭证泄露将导致严重的安全风险。B 选项：PowerUsers 策略授予了超出 CloudFormation 操作所需的广泛权限，同样违反了最小权限原则。C 选项：AdministratorAccess 策略授予了对账户的完全访问权限，是最不推荐的做法，与最小权限原则背道而驰。E 选项：虽然使用 IAM 角色启动堆栈能够实现最小权限原则，但题目要求选择两个措施，选项 D 提供了更好的方案，且结合了最小权限原则；另外，E 选项缺少了为部署工程师配置身份的步骤，仅仅配置角色不足以满足要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM",
      "AWS CloudFormation",
      "IAM User",
      "IAM Group",
      "IAM Policy",
      "root user",
      "PowerUsers",
      "AdministratorAccess",
      "IAM Role"
    ]
  },
  {
    "id": 388,
    "topic": "1",
    "question_en": "A company is deploying a two-tier web application in a VPC. The web tier is using an Amazon EC2 Auto Scaling group with public subnets that span multiple Availability Zones. The database tier consists of an Amazon RDS for MySQL DB instance in separate private subnets. The web tier requires access to the database to retrieve product information. The web application is not working as intended. The web application reports that it cannot connect to the database. The database is confirmed to be up and running. All configurations for the network ACLs, security groups, and route tables are still in their default states. What should a solutions architect recommend to fix the application?",
    "options_en": {
      "A": "Add an explicit rule to the private subnet’s network ACL to allow trafic from the web tier’s EC2 instances.",
      "B": "Add a route in the VPC route table to allow trafic between the web tier’s EC2 instances and the database tier.",
      "C": "Deploy the web tier's EC2 instances and the database tier’s RDS instance into two separate VPCs, and configure VPC peering.",
      "D": "Add an inbound rule to the security group of the database tier’s RDS instance to allow trafic from the web tiers security group."
    },
    "correct_answer": "D",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在 VPC 中部署一个两层 Web 应用程序。Web 层使用一个 Amazon EC2 Auto Scaling 组，该组具有跨多个可用区的公共子网。数据库层由一个位于单独私有子网中的 Amazon RDS for MySQL 数据库实例组成。Web 层需要访问数据库以检索产品信息。Web 应用程序没有按预期工作。Web 应用程序报告它无法连接到数据库。已确认数据库已启动并正在运行。网络 ACL、安全组和路由表的所有配置仍处于其默认状态。解决方案架构师应该建议采取什么措施来修复应用程序？",
    "options_cn": {
      "A": "将一个明确的规则添加到私有子网的网络 ACL 中，以允许来自 Web 层 EC2 实例的流量。",
      "B": "在 VPC 路由表中添加一条路由，以允许 Web 层的 EC2 实例和数据库层之间的流量。",
      "C": "将 Web 层的 EC2 实例和数据库层的 RDS 实例部署到两个单独的 VPC 中，并配置 VPC 对等连接。",
      "D": "将一条入站规则添加到数据库层 RDS 实例的安全组中，以允许来自 Web 层安全组的流量。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon RDS",
      "Security Group",
      "VPC",
      "Network ACL",
      "Route Table"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 96%），解析仅供参考。】\n\n考查 VPC 内的 EC2 实例与 RDS 数据库之间的连接问题排查，涉及安全组、网络 ACL 和路由表的配置。也涉及到对默认安全组规则的理解和网络流量的控制。",
      "why_correct": "选项 D 正确。Web 层和数据库层位于同一 VPC 内。默认情况下，安全组是无状态的。 为了使 Web 层 EC2 实例能够访问数据库层的 RDS 实例，需要在 RDS 实例的安全组中添加一条入站规则，允许来自 Web 层安全组的流量。这允许 Web 应用程序通过其安全组定义的端口（通常是 MySQL 的 3306 端口）与数据库建立连接。此方法是解决此连接问题的最直接和最有效的方法，因为问题描述中已经确认了网络 ACL、路由表都处于默认状态，只需要调整安全组。",
      "why_wrong": "选项 A 错误。网络 ACL 默认是允许所有出站和入站流量，即使需要添加规则，在私有子网的入站规则中配置也是不正确的，因为流量是从 Web 层的 EC2 实例流向数据库层的 RDS 实例。网络 ACL 主要是为子网级别的流量提供安全控制，而本题更侧重于实例级别的安全控制，这应该通过安全组来完成。\n选项 B 错误。VPC 路由表定义了流量在 VPC 内的路由方式，但题干中提到路由表处于默认状态，这意味着配置应该是正确的，不应影响连接。即使需要修改，也无法解决安全组所阻止的流量。路由表主要关注于流量的走向，而安全组则控制流量是否被允许。\n选项 C 错误。将 Web 层和数据库层部署到两个独立的 VPC 中并通过 VPC 对等连接来连接，会增加架构的复杂性。更重要的是，题干的故障表现是应用无法连接数据库，如果数据库已经启动，则更可能的是网络或安全组配置的问题，而不是 VPC 隔离的问题。在这种情况下，修改安全组设置就能解决问题，不需要创建新的 VPC。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "MySQL",
      "VPC",
      "Network ACL",
      "Security Group",
      "Route Table"
    ]
  },
  {
    "id": 389,
    "topic": "1",
    "question_en": "A company has a large dataset for its online advertising business stored in an Amazon RDS for MySQL DB instance in a single Availability Zone. The company wants business reporting queries to run without impacting the write operations to the production DB instance. Which solution meets these requirements?",
    "options_en": {
      "A": "Deploy RDS read replicas to process the business reporting queries.",
      "B": "Scale out the DB instance horizontally by placing it behind an Elastic Load Balancer.",
      "C": "Scale up the DB instance to a larger instance type to handle write operations and queries.",
      "D": "Deploy the DB instance in multiple Availability Zones to process the business reporting queries."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个大型数据集，用于其在线广告业务，该数据集存储在单个可用区中的 Amazon RDS for MySQL 数据库实例中。该公司希望业务报告查询在不影响对生产数据库实例的写入操作的情况下运行。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "部署 RDS 读取副本以处理业务报告查询。",
      "B": "通过将数据库实例置于 Elastic Load Balancer 之后来横向扩展数据库实例。",
      "C": "将数据库实例扩展到更大的实例类型以处理写入操作和查询。",
      "D": "在多个可用区中部署数据库实例以处理业务报告查询。"
    },
    "tags": [
      "Amazon RDS",
      "RDS for MySQL",
      "Multi-AZ",
      "Read Replicas",
      "Elastic Load Balancer",
      "Database Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 RDS for MySQL 的高可用性和读写分离能力，以及如何在不影响写入性能的情况下运行业务报告查询。与 RDS 的多可用区部署、读取副本、负载均衡和实例扩展的选型/对比相关。",
      "why_correct": "在多个可用区中部署数据库实例（即 Multi-AZ）为数据库提供了高可用性，但默认情况下，它不会提供读写分离。虽然 Multi-AZ 部署的主要目的是实现故障转移和提高可用性，但通过结合使用 Multi-AZ 和读取副本，可以更好地满足业务报告查询的需求。Multi-AZ 架构能够确保即使主数据库实例所在的可用区发生故障，也能通过故障转移来维持业务的持续运行，从而保证高可用性。",
      "why_wrong": "A. 部署 RDS 读取副本可以用于分担读取负载，实现读写分离，以不影响主数据库的写入操作。但题目提到数据库位于单个可用区，这种部署方式无法满足高可用性需求。B. 通过 Elastic Load Balancer 之后来横向扩展数据库实例，这通常不是 RDS 的最佳实践，并且无法直接解决读取负载的问题；负载均衡主要用于分发请求，而不是实现数据库的读写分离或高可用性。C. 将数据库实例扩展到更大的实例类型只能提高数据库的整体性能，包括写入和查询性能，但无法实现读写分离，并且对于业务报告查询来说，可能仍然会影响写入操作，且不能提供高可用性。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "Elastic Load Balancer",
      "RDS for MySQL",
      "Read Replicas",
      "Database Instance"
    ]
  },
  {
    "id": 390,
    "topic": "1",
    "question_en": "A company hosts a three-tier ecommerce application on a fieet of Amazon EC2 instances. The instances run in an Auto Scaling group behind an Application Load Balancer (ALB). All ecommerce data is stored in an Amazon RDS for MariaDB Multi-AZ DB instance. The company wants to optimize customer session management during transactions. The application must store session data durably. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Turn on the sticky sessions feature (session afinity) on the ALB.",
      "B": "Use an Amazon DynamoDB table to store customer session information.",
      "C": "Deploy an Amazon Cognito user pool to manage user session information.",
      "D": "Deploy an Amazon ElastiCache for Redis cluster to store customer session information",
      "E": "Use AWS Systems Manager Application Manager in the application to manage user session information."
    },
    "correct_answer": "AD",
    "vote_percentage": "44%",
    "question_cn": "一家公司在其 Amazon EC2 实例集群上托管一个三层电子商务应用程序。这些实例在 Application Load Balancer (ALB) 之后，运行在一个 Auto Scaling 组中。所有电子商务数据都存储在 Amazon RDS for MariaDB Multi-AZ 数据库实例中。该公司希望在交易期间优化客户会话管理。该应用程序必须持久地存储会话数据。哪些解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 ALB 上打开粘性会话功能（会话亲和性）。",
      "B": "使用 Amazon DynamoDB 表来存储客户会话信息。",
      "C": "部署一个 Amazon Cognito 用户池来管理用户会话信息。",
      "D": "部署一个 Amazon ElastiCache for Redis 集群来存储客户会话信息。",
      "E": "在应用程序中使用 AWS Systems Manager Application Manager 来管理用户会话信息。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon RDS",
      "Amazon DynamoDB",
      "Amazon Cognito",
      "Amazon ElastiCache",
      "MariaDB",
      "Session management"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 44%），解析仅供参考。】\n\n本题考察了如何在 EC2 集群和 RDS 数据库的架构下，持久化存储客户会话数据的两种有效方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AD。理由简述：选项 A，ALB 的粘性会话（Sticky Sessions）功能，可以确保来自特定客户端的请求始终被路由到同一 EC2 实例，从而维持会话持久性，满足了应用程序的会话管理需求。选项 D，ElastiCache for Redis 提供了高性能的内存中数据存储，非常适合存储和检索会话数据，能够快速响应客户的会话请求，实现会话的持久性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B，虽然 DynamoDB 可以存储数据，但其延迟性不适合对性能要求极高的会话管理。选项 C，Amazon Cognito 主要用于用户身份验证和授权，而不是存储和管理会话数据。选项 E，AWS Systems Manager Application Manager 主要用于应用程序的管理和监控，与会话数据的存储无关。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Application Load Balancer (ALB)",
      "Auto Scaling group",
      "Amazon RDS for MariaDB",
      "Multi-AZ",
      "Sticky Sessions",
      "Amazon DynamoDB",
      "Amazon Cognito",
      "Amazon ElastiCache for Redis",
      "AWS Systems Manager Application Manager"
    ]
  },
  {
    "id": 391,
    "topic": "1",
    "question_en": "A company needs a backup strategy for its three-tier stateless web application. The web application runs on Amazon EC2 instances in an Auto Scaling group with a dynamic scaling policy that is configured to respond to scaling events. The database tier runs on Amazon RDS for PostgreSQL. The web application does not require temporary local storage on the EC2 instances. The company’s recovery point objective (RPO) is 2 hours. The backup strategy must maximize scalability and optimize resource utilization for this environment. Which solution will meet these requirements?",
    "options_en": {
      "A": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances and database every 2 hours to meet the RPO.",
      "B": "Configure a snapshot lifecycle policy to take Amazon Elastic Block Store (Amazon EBS) snapshots. Enable automated backups in Amazon RDS to meet the RPO.",
      "C": "Retain the latest Amazon Machine Images (AMIs) of the web and application tiers. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO.",
      "D": "Take snapshots of Amazon Elastic Block Store (Amazon EBS) volumes of the EC2 instances every 2 hours. Enable automated backups in Amazon RDS and use point-in-time recovery to meet the RPO."
    },
    "correct_answer": "C",
    "vote_percentage": "87%",
    "question_cn": "一家公司需要为其三层无状态 Web 应用程序制定备份策略。该 Web 应用程序在 Auto Scaling 组中的 Amazon EC2 实例上运行，该组具有动态伸缩策略，该策略配置为响应伸缩事件。数据库层在 Amazon RDS for PostgreSQL 上运行。Web 应用程序不需要在 EC2 实例上使用临时本地存储。该公司的恢复点目标 (RPO) 为 2 小时。备份策略必须最大程度地提高可伸缩性并优化此环境的资源利用率。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "每 2 小时拍摄 EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 卷和数据库的快照，以满足 RPO。",
      "B": "配置快照生命周期策略以拍摄 Amazon Elastic Block Store (Amazon EBS) 快照。在 Amazon RDS 中启用自动备份以满足 RPO。",
      "C": "保留 Web 和应用程序层的最新 Amazon Machine Images (AMI)。在 Amazon RDS 中启用自动备份并使用时间点恢复以满足 RPO。",
      "D": "每 2 小时拍摄 EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 卷的快照。在 Amazon RDS 中启用自动备份并使用时间点恢复以满足 RPO。"
    },
    "tags": [
      "EC2",
      "EBS",
      "RDS",
      "PostgreSQL",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 87%），解析仅供参考。】\n\n考查 Web 应用程序的备份策略设计，包括 EC2 实例的备份、RDS 数据库的备份，以及如何满足恢复点目标 (RPO) 和优化资源利用率。",
      "why_correct": "选项 C 提供了最适合的解决方案。创建并保留最新的 Amazon Machine Images (AMI) 可以快速恢复 Web 和应用程序层。启用 Amazon RDS 的自动备份和时间点恢复，可以满足 RPO 为 2 小时的要求，同时提高可伸缩性并优化资源利用率。",
      "why_wrong": "选项 A 的问题在于，手动每 2 小时拍摄 EBS 快照，难以实现自动化，不利于伸缩性和资源利用率。选项 B 虽然使用快照生命周期策略来管理 EBS 快照，但缺少对 Web 应用程序层的保护。选项 D 虽然在 RDS 中启用自动备份和时间点恢复，但对 Web 应用程序层还是使用 EBS 快照，恢复速度不如使用 AMI 快速，且可能导致实例启动时间过长。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS for PostgreSQL",
      "RPO",
      "Amazon EBS",
      "Amazon Machine Images (AMI)",
      "EBS snapshots",
      "RDS automatic backups",
      "point-in-time recovery",
      "snapshot lifecycle policy"
    ]
  },
  {
    "id": 392,
    "topic": "1",
    "question_en": "A company wants to deploy a new public web application on AWS. The application includes a web server tier that uses Amazon EC2 instances. The application also includes a database tier that uses an Amazon RDS for MySQL DB instance. The application must be secure and accessible for global customers that have dynamic IP addresses. How should a solutions architect configure the security groups to meet these requirements?",
    "options_en": {
      "A": "Configure the security group for the web servers to allow inbound trafic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trafic on port 3306 from the security group of the web servers.",
      "B": "Configure the security group for the web servers to allow inbound trafic on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trafic on port 3306 from the security group of the web servers.",
      "C": "Configure the security group for the web servers to allow inbound trafic on port 443 from the IP addresses of the customers. Configure the security group for the DB instance to allow inbound trafic on port 3306 from the IP addresses of the customers.",
      "D": "Configure the security group for the web servers to allow inbound trafic on port 443 from 0.0.0.0/0. Configure the security group for the DB instance to allow inbound trafic on port 3306 from 0.0.0.0/0."
    },
    "correct_answer": "A",
    "vote_percentage": "83%",
    "question_cn": "一家公司希望在 AWS 上部署一个新的公共 Web 应用程序。该应用程序包含一个使用 Amazon EC2 实例的 Web 服务器层。该应用程序还包含一个使用 Amazon RDS for MySQL 数据库实例的数据库层。该应用程序必须安全且可供具有动态 IP 地址的全球客户访问。解决方案架构师应如何配置安全组以满足这些要求？",
    "options_cn": {
      "A": "配置 Web 服务器的安全组，允许从 0.0.0.0/0 上的端口 443 传入流量。配置数据库实例的安全组，允许从 Web 服务器的安全组上的端口 3306 传入流量。",
      "B": "配置 Web 服务器的安全组，允许从客户的 IP 地址上的端口 443 传入流量。配置数据库实例的安全组，允许从 Web 服务器的安全组上的端口 3306 传入流量。",
      "C": "配置 Web 服务器的安全组，允许从客户的 IP 地址上的端口 443 传入流量。配置数据库实例的安全组，允许从客户的 IP 地址上的端口 3306 传入流量。",
      "D": "配置 Web 服务器的安全组，允许从 0.0.0.0/0 上的端口 443 传入流量。配置数据库实例的安全组，允许从 0.0.0.0/0 上的端口 3306 传入流量。"
    },
    "tags": [
      "EC2",
      "RDS",
      "MySQL",
      "Security Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 83%），解析仅供参考。】\n\n本题考察 Web 应用的安全组配置。对于公共 Web 应用程序，需要允许来自任何地方的 HTTPS 流量。数据库访问则需要限制到 Web 服务器，以增强安全性。",
      "why_correct": "A 选项正确配置了 Web 服务器的安全组，允许来自互联网的 HTTPS 流量，并限制了数据库访问仅允许 Web 服务器的流量，满足了安全性和访问性的要求。",
      "why_wrong": "B 选项错误地认为客户的 IP 地址是固定的；C 选项也错误地认为客户的 IP 地址是固定的，并且数据库访问限制不正确；D 选项配置了错误的数据库访问规则，允许来自任何地方的 MySQL 流量，存在安全风险。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "MySQL",
      "HTTPS",
      "Security Group"
    ]
  },
  {
    "id": 393,
    "topic": "1",
    "question_en": "A payment processing company records all voice communication with its customers and stores the audio files in an Amazon S3 bucket. The company needs to capture the text from the audio files. The company must remove from the text any personally identifiable information (PII) that belongs to customers. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Process the audio files by using Amazon Kinesis Video Streams. Use an AWS Lambda function to scan for known PII patterns.",
      "B": "When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start an Amazon Textract task to analyze the call recordings.",
      "C": "Configure an Amazon Transcribe transcription job with PII redaction turned on. When an audio file is uploaded to the S3 bucket, invoke an AWS Lambda function to start the transcription job. Store the output in a separate S3 bucket.",
      "D": "Create an Amazon Connect contact fiow that ingests the audio files with transcription turned on. Embed an AWS Lambda function to scan for known PII patterns. Use Amazon EventBridge to start the contact fiow when an audio file is uploaded to the S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家支付处理公司会记录其与客户的所有语音通信，并将音频文件存储在 Amazon S3 存储桶中。该公司需要从音频文件中提取文本。该公司必须从文本中删除属于客户的任何个人身份信息 (PII)。解决方案架构师应如何满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Kinesis Video Streams 处理音频文件。使用 AWS Lambda 函数扫描已知的 PII 模式。",
      "B": "当音频文件上传到 S3 存储桶时，调用 AWS Lambda 函数以启动 Amazon Textract 任务来分析通话录音。",
      "C": "配置 Amazon Transcribe 转录作业，并打开 PII 隐藏。当音频文件上传到 S3 存储桶时，调用 AWS Lambda 函数以启动转录作业。将输出存储在单独的 S3 存储桶中。",
      "D": "创建一个 Amazon Connect 联系流程，该流程通过转录来摄取音频文件。嵌入一个 AWS Lambda 函数来扫描已知的 PII 模式。当音频文件上传到 S3 存储桶时，使用 Amazon EventBridge 启动联系流程。"
    },
    "tags": [
      "S3",
      "Lambda",
      "Transcribe",
      "PII"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查语音数据处理及 PII 保护。需要考虑从音频文件中提取文本，并删除个人敏感信息。最佳实践是使用 AWS Transcribe 转录音频，并结合 PII 隐藏功能。",
      "why_correct": "C 选项使用了 Amazon Transcribe，并开启了 PII 隐藏功能，满足了转录和 PII 保护的要求，是最优的解决方案。",
      "why_wrong": "A 选项使用了 Amazon Kinesis Video Streams，这并非处理音频文件的最佳方式；B 选项错误地使用 Textract 处理语音文件；D 选项使用了 Amazon Connect，这对于此场景来说过于复杂，且没有提供 PII 隐藏功能。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "Transcribe",
      "PII",
      "Kinesis Video Streams",
      "Textract",
      "Connect"
    ]
  },
  {
    "id": 394,
    "topic": "1",
    "question_en": "A company is running a multi-tier ecommerce web application in the AWS Cloud. The application runs on Amazon EC2 instances with an Amazon RDS for MySQL Multi-AZ DB instance. Amazon RDS is configured with the latest generation DB instance with 2,000 GB of storage in a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume. The database performance affects the application during periods of high demand. A database administrator analyzes the logs in Amazon CloudWatch Logs and discovers that the application performance always degrades when the number of read and write IOPS is higher than 20,000. What should a solutions architect do to improve the application performance?",
    "options_en": {
      "A": "Replace the volume with a magnetic volume.",
      "B": "Increase the number of IOPS on the gp3 volume.",
      "C": "Replace the volume with a Provisioned IOPS SSD (io2) volume.",
      "D": "Replace the 2,000 GB gp3 volume with two 1,000 GB gp3 volumes."
    },
    "correct_answer": "D",
    "vote_percentage": "40%",
    "question_cn": "一家公司在 AWS 云中运行多层电商 Web 应用程序。该应用程序运行在 Amazon EC2 实例上，并带有 Amazon RDS for MySQL Multi-AZ DB 实例。Amazon RDS 配置了最新一代的数据库实例，该实例具有 2,000 GB 的存储空间，位于通用 SSD (gp3) Amazon Elastic Block Store (Amazon EBS) 卷中。在需求量大的时期，数据库性能会影响应用程序。数据库管理员在 Amazon CloudWatch Logs 中分析日志，发现当读写 IOPS 数量高于 20,000 时，应用程序性能总会下降。解决方案架构师应该怎么做来提高应用程序性能？",
    "options_cn": {
      "A": "用磁性卷替换该卷。",
      "B": "增加 gp3 卷上的 IOPS 数量。",
      "C": "用预置 IOPS SSD (io2) 卷替换该卷。",
      "D": "用两个 1,000 GB 的 gp3 卷替换 2,000 GB 的 gp3 卷。"
    },
    "tags": [
      "EC2",
      "RDS",
      "MySQL",
      "CloudWatch Logs",
      "IOPS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 40%），解析仅供参考。】\n\n考查如何通过调整 EBS 卷配置，优化 RDS for MySQL 数据库性能，解决 IOPS 瓶颈问题。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。根据题干描述，数据库性能在 IOPS 超过 20,000 时下降。gp3 卷允许独立配置 IOPS 和存储空间。通过将一个 2,000 GB 的 gp3 卷替换为两个 1,000 GB 的 gp3 卷，可以在不增加总体存储空间的情况下，提升 IOPS 性能。两个卷可以实现更高的并发能力，从而可能提升应用程序性能。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。磁性卷性能远低于 gp3，无法满足数据库的性能需求。选项 B 错误。虽然增加 gp3 卷的 IOPS 可以提升性能，但题干并未明确说明当前 IOPS 的具体配置，增加 IOPS 不一定是最佳的优化策略。选项 C 错误。虽然 io2 卷提供了更高的 IOPS 和更低的延迟，但需要手动配置 IOPS，且成本高于 gp3。而题目中并未明确指出 io2 卷可以有效提升性能，且 gp3 的调整更灵活，所以没有必要直接替换为 io2。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS for MySQL",
      "Multi-AZ DB",
      "Amazon CloudWatch Logs",
      "IOPS",
      "Amazon EBS",
      "gp3",
      "io2",
      "SSD",
      "MySQL"
    ]
  },
  {
    "id": 395,
    "topic": "1",
    "question_en": "An IAM user made several configuration changes to AWS resources in their company's account during a production deployment last week. A solutions architect learned that a couple of security group rules are not configured as desired. The solutions architect wants to confirm which IAM user was responsible for making changes. Which service should the solutions architect use to find the desired information?",
    "options_en": {
      "A": "Amazon GuardDuty",
      "B": "Amazon Inspector",
      "C": "AWS CloudTrail",
      "D": "AWS Config"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一名 IAM 用户在上周的生产部署期间对其公司的账户中的 AWS 资源进行了一些配置更改。一位解决方案架构师了解到，几个安全组规则未按预期配置。解决方案架构师希望确认是哪个 IAM 用户负责进行更改。解决方案架构师应该使用哪项服务来查找所需信息？",
    "options_cn": {
      "A": "Amazon GuardDuty",
      "B": "Amazon Inspector",
      "C": "AWS CloudTrail",
      "D": "AWS Config"
    },
    "tags": [
      "IAM",
      "CloudTrail"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS CloudTrail 审计日志追踪 IAM 用户对 AWS 资源配置更改的操作。",
      "why_correct": "AWS CloudTrail 记录了账户中 API 调用的历史记录，包括 IAM 用户对安全组规则的修改操作。通过 CloudTrail 事件，可以确定执行修改操作的 IAM 用户，以及修改的时间和具体内容。这满足了题目中查找 IAM 用户进行配置更改的需求。",
      "why_wrong": "Amazon GuardDuty 用于检测潜在的安全威胁，如恶意活动或未经授权的行为，与查找特定 IAM 用户的配置更改无关。Amazon Inspector 用于评估应用程序的漏洞和安全合规性，而非审计用户操作。AWS Config 用于跟踪资源配置更改，但并不直接提供 IAM 用户的信息，需要与其他服务（如 CloudTrail）配合使用，在此场景中不如 CloudTrail 直接。"
    },
    "related_terms": [
      "IAM",
      "AWS CloudTrail",
      "Amazon GuardDuty",
      "Amazon Inspector",
      "AWS Config",
      "Security Group"
    ]
  },
  {
    "id": 396,
    "topic": "1",
    "question_en": "A company has implemented a self-managed DNS service on AWS. The solution consists of the following: • Amazon EC2 instances in different AWS Regions • Endpoints of a standard accelerator in AWS Global Accelerator The company wants to protect the solution against DDoS attacks. What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Subscribe to AWS Shield Advanced. Add the accelerator as a resource to protect.",
      "B": "Subscribe to AWS Shield Advanced. Add the EC2 instances as resources to protect.",
      "C": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the accelerator.",
      "D": "Create an AWS WAF web ACL that includes a rate-based rule. Associate the web ACL with the EC2 instances."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "一家公司已在 AWS 上实施了自管理 DNS 服务。 解决方案包括以下内容：\n\n- 不同 AWS 区域中的 Amazon EC2 实例\n- AWS Global Accelerator 中标准加速器的端点\n\n该公司希望保护其解决方案免受 DDoS 攻击。 解决方案架构师应采取什么措施来满足此要求？",
    "options_cn": {
      "A": "订阅 AWS Shield Advanced。将加速器添加为要保护的资源。",
      "B": "订阅 AWS Shield Advanced。将 EC2 实例添加为要保护的资源。",
      "C": "创建一个包含基于速率规则的 AWS WAF Web ACL。将 Web ACL 与加速器关联。",
      "D": "创建一个包含基于速率规则的 AWS WAF Web ACL。将 Web ACL 与 EC2 实例关联。"
    },
    "tags": [
      "Shield Advanced",
      "Global Accelerator",
      "DDoS",
      "WAF"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n本题考查 DDoS 攻击防护。针对 Global Accelerator 的部署，需要结合 Shield Advanced 和 WAF。",
      "why_correct": "A 选项订阅 AWS Shield Advanced 可以提供 DDoS 保护。并且将 Global Accelerator 作为保护对象，可以防御 DDoS 攻击。",
      "why_wrong": "B 选项虽然订阅了 AWS Shield Advanced，但将 EC2 实例作为保护对象，无法直接保护 Global Accelerator；C 选项和 D 选项中仅使用 WAF，无法完全防御 DDoS 攻击，并且与 Global Accelerator 结合使用效果不好。"
    },
    "related_terms": [
      "Global Accelerator",
      "DDoS",
      "WAF",
      "Web ACL",
      "Shield Advanced"
    ]
  },
  {
    "id": 397,
    "topic": "1",
    "question_en": "An ecommerce company needs to run a scheduled daily job to aggregate and filter sales records for analytics. The company stores the sales records in an Amazon S3 bucket. Each object can be up to 10 GB in size. Based on the number of sales events, the job can take up to an hour to complete. The CPU and memory usage of the job are constant and are known in advance. A solutions architect needs to minimize the amount of operational effort that is needed for the job to run. Which solution meets these requirements?",
    "options_en": {
      "A": "Create an AWS Lambda function that has an Amazon EventBridge notification. Schedule the EventBridge event to run once a day.",
      "B": "Create an AWS Lambda function. Create an Amazon API Gateway HTTP API, and integrate the API with the function. Create an Amazon EventBridge scheduled event that calls the API and invokes the function.",
      "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an AWS Fargate launch type. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job.",
      "D": "Create an Amazon Elastic Container Service (Amazon ECS) cluster with an Amazon EC2 launch type and an Auto Scaling group with at least one EC2 instance. Create an Amazon EventBridge scheduled event that launches an ECS task on the cluster to run the job."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司需要运行一个每日计划任务，以聚合和过滤销售记录进行分析。该公司将销售记录存储在 Amazon S3 存储桶中。每个对象的大小最大为 10 GB。根据销售事件的数量，该作业可能需要长达一个小时才能完成。该作业的 CPU 和内存使用率是恒定的，并且提前已知。一个解决方案架构师需要最大限度地减少运行该作业所需的运营工作量。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，该函数具有 Amazon EventBridge 通知。将 EventBridge 事件计划为每天运行一次。",
      "B": "创建一个 AWS Lambda 函数。创建一个 Amazon API Gateway HTTP API，并将该 API 与该函数集成。创建一个 Amazon EventBridge 计划事件，该事件调用 API 并调用该函数。",
      "C": "创建一个具有 AWS Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群。创建一个 Amazon EventBridge 计划事件，该事件在集群上启动一个 ECS 任务来运行该作业。",
      "D": "创建一个具有 Amazon EC2 启动类型和具有至少一个 EC2 实例的 Auto Scaling 组的 Amazon Elastic Container Service (Amazon ECS) 集群。创建一个 Amazon EventBridge 计划事件，该事件在集群上启动一个 ECS 任务来运行该作业。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查任务调度和批量处理。需要选择一个能够支持定时任务，并能够有效执行批量处理的解决方案。Lambda 函数、ECS 和 Fargate 都是可选项。",
      "why_correct": "C 选项创建了 ECS 集群，并使用 Fargate 启动类型，简化了运维，降低了成本，同时可以利用 EventBridge 调度任务，满足了题目的要求。",
      "why_wrong": "A 选项使用了 Lambda 函数，单次执行时间长，且 Lambda 的执行时间和成本不可控；B 选项，虽然可以调度 Lambda 函数，但是对大数据处理的成本较高，并且涉及 API Gateway，增加了复杂性；D 选项，虽然使用了 ECS 和 Auto Scaling，但是使用了 EC2 启动类型，增加了运维的复杂性。"
    },
    "related_terms": [
      "ECS",
      "Fargate",
      "EventBridge",
      "Lambda",
      "API Gateway",
      "EC2",
      "Auto Scaling"
    ]
  },
  {
    "id": 398,
    "topic": "1",
    "question_en": "A company needs to transfer 600 TB of data from its on-premises network-attached storage (NAS) system to the AWS Cloud. The data transfer must be complete within 2 weeks. The data is sensitive and must be encrypted in transit. The company’s internet connection can support an upload speed of 100 Mbps. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use Amazon S3 multi-part upload functionality to transfer the files over HTTPS.",
      "B": "Create a VPN connection between the on-premises NAS system and the nearest AWS Region. Transfer the data over the VPN connection.",
      "C": "Use the AWS Snow Family console to order several AWS Snowball Edge Storage Optimized devices. Use the devices to transfer the data to Amazon S3.",
      "D": "Set up a 10 Gbps AWS Direct Connect connection between the company location and the nearest AWS Region. Transfer the data over a VPN connection into the Region to store the data in Amazon S3."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要将其本地网络附加存储 (NAS) 系统中的 600 TB 数据传输到 AWS 云。数据传输必须在 2 周内完成。数据是敏感的，并且必须在传输过程中加密。该公司的互联网连接可以支持 100 Mbps 的上传速度。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 多部分上传功能通过 HTTPS 传输文件。",
      "B": "在本地 NAS 系统和最近的 AWS 区域之间创建 VPN 连接。通过 VPN 连接传输数据。",
      "C": "使用 AWS Snow Family 控制台订购多个 AWS Snowball Edge 存储优化设备。使用这些设备将数据传输到 Amazon S3。",
      "D": "在公司所在地和最近的 AWS 区域之间设置 10 Gbps 的 AWS Direct Connect 连接。通过 VPN 连接将数据传输到该区域，以将数据存储在 Amazon S3 中。"
    },
    "tags": [
      "S3",
      "Snowball Edge",
      "VPN",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在带宽受限、数据量大、时效性要求高、以及数据安全需求下的数据迁移方案选择。重点考察 AWS Snow Family 的使用场景。",
      "why_correct": "AWS Snowball Edge Storage Optimized 设备专为大规模数据传输设计，可以离线方式将数据导入 Amazon S3，避免了受限的互联网带宽影响。Snowball Edge 设备支持数据加密，满足了数据安全的需求。多个 Snowball Edge 设备可以并行使用，以满足 2 周内完成 600 TB 数据传输的时效性要求。这种方式在成本效益上通常优于其他方案。",
      "why_wrong": "选项 A，使用 Amazon S3 多部分上传虽然支持数据加密，但受限于 100 Mbps 的上传速度，传输 600 TB 数据需要很长时间，无法满足 2 周的时限要求。选项 B，通过 VPN 连接传输数据也受限于 100 Mbps 的上传速度，速度太慢。选项 D，使用 10 Gbps 的 AWS Direct Connect 连接，虽然速度快，但 Direct Connect 的成本远高于 Snowball Edge，并且 VPN 的额外设置也增加了复杂性，不符合经济高效的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "HTTPS",
      "VPN",
      "AWS Snow Family",
      "AWS Snowball Edge",
      "AWS Direct Connect"
    ]
  },
  {
    "id": 399,
    "topic": "1",
    "question_en": "A financial company hosts a web application on AWS. The application uses an Amazon API Gateway Regional API endpoint to give users the ability to retrieve current stock prices. The company’s security team has noticed an increase in the number of API requests. The security team is concerned that HTTP fiood attacks might take the application ofiine. A solutions architect must design a solution to protect the application from this type of attack. Which solution meets these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon CloudFront distribution in front of the API Gateway Regional API endpoint with a maximum TTL of 24 hours.",
      "B": "Create a Regional AWS WAF web ACL with a rate-based rule. Associate the web ACL with the API Gateway stage.",
      "C": "Use Amazon CloudWatch metrics to monitor the Count metric and alert the security team when the predefined rate is reached.",
      "D": "Create an Amazon CloudFront distribution with Lambda@Edge in front of the API Gateway Regional API endpoint. Create an AWS Lambda function to block requests from IP addresses that exceed the predefined rate."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家金融公司在 AWS 上托管一个 Web 应用程序。该应用程序使用 Amazon API Gateway Regional API 终端节点，使用户能够检索当前的股票价格。该公司的安全团队注意到 API 请求数量有所增加。安全团队担心 HTTP 泛洪攻击可能会导致应用程序离线。解决方案架构师必须设计一个解决方案来保护应用程序免受此类攻击。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 API Gateway Regional API 终端节点前创建一个 Amazon CloudFront 分配，最大 TTL 为 24 小时。",
      "B": "创建一个 Regional AWS WAF Web ACL，其中包含基于速率的规则。将 Web ACL 与 API Gateway 阶段关联。",
      "C": "使用 Amazon CloudWatch 指标来监视 Count 指标，并在达到预定义速率时提醒安全团队。",
      "D": "在 API Gateway Regional API 终端节点前创建一个带有 Lambda@Edge 的 Amazon CloudFront 分配。创建一个 AWS Lambda 函数以阻止来自超出预定义速率的 IP 地址的请求。"
    },
    "tags": [
      "API Gateway",
      "CloudFront",
      "WAF",
      "DDoS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查 API Gateway 的 DDoS 防护。需要考虑解决方案的运营开销。",
      "why_correct": "B 选项，使用 Regional AWS WAF Web ACL，并配置基于速率的规则，可以有效拦截 DDoS 攻击，并且运维开销小。",
      "why_wrong": "A 选项使用 CloudFront 无法有效防止 DDoS 攻击；C 选项，仅监视指标无法阻止攻击；D 选项使用 Lambda@Edge 增加了复杂性，且成本较高。"
    },
    "related_terms": [
      "API Gateway",
      "CloudFront",
      "WAF",
      "DDoS",
      "Web ACL"
    ]
  },
  {
    "id": 400,
    "topic": "1",
    "question_en": "A meteorological startup company has a custom web application to sell weather data to its users online. The company uses Amazon DynamoDB to store its data and wants to build a new service that sends an alert to the managers of four internal teams every time a new weather event is recorded. The company does not want this new service to affect the performance of the current application. What should a solutions architect do to meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Use DynamoDB transactions to write new event data to the table. Configure the transactions to notify internal teams.",
      "B": "Have the current application publish a message to four Amazon Simple Notification Service (Amazon SNS) topics. Have each team subscribe to one topic.",
      "C": "Enable Amazon DynamoDB Streams on the table. Use triggers to write to a single Amazon Simple Notification Service (Amazon SNS) topic to which the teams can subscribe.",
      "D": "Add a custom attribute to each record to fiag new items. Write a cron job that scans the table every minute for items that are new and notifies an Amazon Simple Queue Service (Amazon SQS) queue to which the teams can subscribe."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家气象初创公司拥有一款自定义 Web 应用程序，用于在线向其用户销售天气数据。该公司使用 Amazon DynamoDB 存储其数据，并希望构建一项新服务，以便在每次记录新的天气事件时向四个内部团队的经理发送警报。该公司不希望这项新服务影响当前应用程序的性能。解决方案架构师应该怎么做才能以最少的运营开销来满足这些要求？",
    "options_cn": {
      "A": "使用 DynamoDB 事务将新的事件数据写入表。配置事务以通知内部团队。",
      "B": "让当前应用程序向四个 Amazon Simple Notification Service (Amazon SNS) 主题发布消息。让每个团队订阅一个主题。",
      "C": "在表上启用 Amazon DynamoDB Streams。使用触发器写入单个 Amazon Simple Notification Service (Amazon SNS) 主题，团队可以订阅该主题。",
      "D": "为每条记录添加一个自定义属性来标记新项目。编写一个 cron 作业，每分钟扫描一次表以查找新项目，并通知一个 Amazon Simple Queue Service (Amazon SQS) 队列，团队可以订阅该队列。"
    },
    "tags": [
      "DynamoDB",
      "SNS",
      "DynamoDB Streams"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查 DynamoDB 事件通知。需要选择一个对现有应用程序影响最小的方案，同时满足通知需求。",
      "why_correct": "C 选项，启用 DynamoDB Streams，然后使用 Lambda 触发器将消息发送到 SNS，这种方式对现有应用程序的影响最小。",
      "why_wrong": "A 选项使用 DynamoDB 事务，增加了复杂性；B 选项，要求修改应用程序，增加了维护成本；D 选项，使用 cron 作业效率低，并且增加了 SQS 的管理成本。"
    },
    "related_terms": [
      "DynamoDB",
      "SNS",
      "DynamoDB Streams",
      "SQS"
    ]
  },
  {
    "id": 401,
    "topic": "1",
    "question_en": "A company wants to use the AWS Cloud to make an existing application highly available and resilient. The current version of the application resides in the company's data center. The application recently experienced data loss after a database server crashed because of an unexpected power outage. The company needs a solution that avoids any single points of failure. The solution must give the application the ability to scale to meet user demand. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance in a Multi-AZ configuration.",
      "B": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group in a single Availability Zone. Deploy the database on an EC2 instance. Enable EC2 Auto Recovery.",
      "C": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Use an Amazon RDS DB instance with a read replica in a single Availability Zone. Promote the read replica to replace the primary DB instance if the primary DB instance fails.",
      "D": "Deploy the application servers by using Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy the primary and secondary database servers on EC2 instances across multiple Availability Zones. Use Amazon Elastic Block Store (Amazon EBS) Multi-Attach to create shared storage between the instances."
    },
    "correct_answer": "A",
    "vote_percentage": "90%",
    "question_cn": "一家公司希望使用 AWS Cloud 来使现有应用程序具有高可用性和弹性。该应用程序的当前版本位于公司的数据中心中。由于意外停电，数据库服务器崩溃后，该应用程序最近发生了数据丢失。公司需要一个解决方案来避免任何单点故障。该解决方案必须使应用程序能够扩展以满足用户需求。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过在多个可用区中使用 Auto Scaling 组中的 Amazon EC2 实例来部署应用程序服务器。 在 Multi-AZ 配置中使用 Amazon RDS DB 实例。",
      "B": "通过在单个可用区中使用 Auto Scaling 组中的 Amazon EC2 实例来部署应用程序服务器。在 EC2 实例上部署数据库。启用 EC2 自动恢复。",
      "C": "通过在多个可用区中使用 Auto Scaling 组中的 Amazon EC2 实例来部署应用程序服务器。 使用在单个可用区中具有只读副本的 Amazon RDS DB 实例。如果主数据库实例发生故障，将只读副本提升以替换主数据库实例。",
      "D": "通过在多个可用区中使用 Auto Scaling 组中的 Amazon EC2 实例来部署应用程序服务器。在跨多个可用区的 EC2 实例上部署主数据库和辅助数据库服务器。使用 Amazon Elastic Block Store (Amazon EBS) Multi-Attach 在实例之间创建共享存储。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Auto Scaling",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 90%），解析仅供参考。】\n\n本题考察高可用性架构。需要考虑避免单点故障，并支持弹性伸缩。",
      "why_correct": "A 选项，在多个可用区中使用 Auto Scaling 组中的 EC2 实例，并且使用 Multi-AZ 配置的 RDS 实例，可以实现高可用性和弹性伸缩。",
      "why_wrong": "B 选项，EC2 实例在单个可用区，数据库也在 EC2 实例上部署，不满足高可用性；C 选项，虽然 RDS 使用了 Multi-AZ，但是数据库只读副本无法完全替换主实例；D 选项，使用 Multi-Attach 和共享存储存在单点故障，并且管理复杂。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "Auto Scaling",
      "Multi-AZ",
      "EBS",
      "High Availability"
    ]
  },
  {
    "id": 402,
    "topic": "1",
    "question_en": "A company needs to ingest and handle large amounts of streaming data that its application generates. The application runs on Amazon EC2 instances and sends data to Amazon Kinesis Data Streams, which is configured with default settings. Every other day, the application consumes the data and writes the data to an Amazon S3 bucket for business intelligence (BI) processing. The company observes that Amazon S3 is not receiving all the data that the application sends to Kinesis Data Streams. What should a solutions architect do to resolve this issue?",
    "options_en": {
      "A": "Update the Kinesis Data Streams default settings by modifying the data retention period.",
      "B": "Update the application to use the Kinesis Producer Library (KPL) to send the data to Kinesis Data Streams.",
      "C": "Update the number of Kinesis shards to handle the throughput of the data that is sent to Kinesis Data Streams.",
      "D": "Turn on S3 Versioning within the S3 bucket to preserve every version of every object that is ingested in the S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "64%",
    "question_cn": "一家公司需要摄取和处理其应用程序生成的大量流数据。该应用程序运行在 Amazon EC2 实例上，并将数据发送到 Amazon Kinesis Data Streams，后者配置了默认设置。每隔一天，应用程序就会使用数据，并将数据写入 Amazon S3 存储桶以进行商业智能 (BI) 处理。该公司观察到 Amazon S3 没有收到应用程序发送到 Kinesis Data Streams 的所有数据。解决方案架构师应该怎么做才能解决这个问题？",
    "options_cn": {
      "A": "通过修改数据保留期来更新 Kinesis Data Streams 默认设置。",
      "B": "更新应用程序以使用 Kinesis Producer Library (KPL) 将数据发送到 Kinesis Data Streams。",
      "C": "更新 Kinesis 分片的数量，以处理发送到 Kinesis Data Streams 的数据吞吐量。",
      "D": "在 S3 存储桶中打开 S3 版本控制，以保留摄取到 S3 存储桶中的每个对象的每个版本。"
    },
    "tags": [
      "Kinesis Data Streams",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 64%），解析仅供参考。】\n\n本题考查 Kinesis Data Streams 数据丢失问题。通常由于 Kinesis Data Streams 配置不正确导致。",
      "why_correct": "A 选项，调整 Kinesis Data Streams 的数据保留时间可以解决问题。",
      "why_wrong": "B 选项，使用 Kinesis Producer Library (KPL) 无法解决数据丢失问题；C 选项，调整 Kinesis 分片数量与数据丢失无关；D 选项，开启 S3 版本控制也无法解决数据丢失问题。"
    },
    "related_terms": [
      "Kinesis Data Streams",
      "S3",
      "KPL"
    ]
  },
  {
    "id": 403,
    "topic": "1",
    "question_en": "A developer has an application that uses an AWS Lambda function to upload files to Amazon S3 and needs the required permissions to perform the task. The developer already has an IAM user with valid IAM credentials required for Amazon S3. What should a solutions architect do to grant the permissions?",
    "options_en": {
      "A": "Add required IAM permissions in the resource policy of the Lambda function.",
      "B": "Create a signed request using the existing IAM credentials in the Lambda function.",
      "C": "Create a new IAM user and use the existing IAM credentials in the Lambda function.",
      "D": "Create an IAM execution role with the required permissions and attach the IAM role to the Lambda function."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "开发人员有一个应用程序，该应用程序使用 AWS Lambda 函数将文件上传到 Amazon S3，并且需要执行此任务所需的权限。开发人员已经拥有一个 IAM 用户，该用户具有 Amazon S3 所需的有效 IAM 凭证。解决方案架构师应该怎么做才能授予权限？",
    "options_cn": {
      "A": "在 Lambda 函数的资源策略中添加所需的 IAM 权限。",
      "B": "使用 Lambda 函数中现有的 IAM 凭证创建签名请求。",
      "C": "创建一个新的 IAM 用户，并在 Lambda 函数中使用现有的 IAM 凭证。",
      "D": "创建一个具有所需权限的 IAM 执行角色，并将该 IAM 角色附加到 Lambda 函数。"
    },
    "tags": [
      "Lambda",
      "IAM",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查如何为 Lambda 函数授予访问 S3 资源的权限，需要理解 IAM 角色在 Lambda 函数中的应用。",
      "why_correct": "选项 D 正确。使用 IAM 执行角色是为 Lambda 函数提供访问其他 AWS 服务（如 S3）权限的标准方法。创建一个具有 S3 访问权限的 IAM 角色，并将该角色附加到 Lambda 函数，可以让函数以该角色的身份执行操作，从而安全地访问 S3。",
      "why_wrong": "选项 A 错误，资源策略用于控制谁可以调用 Lambda 函数，而不是控制函数访问其他 AWS 资源的权限。选项 B 错误，直接在代码中使用 IAM 凭证不符合最佳实践，且难以管理。选项 C 错误，创建一个新的 IAM 用户并将其凭证硬编码到 Lambda 函数中，同样不符合安全最佳实践，且无法实现角色分离。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon S3",
      "IAM",
      "IAM role",
      "IAM user",
      "Resource policy",
      "IAM credentials"
    ]
  },
  {
    "id": 404,
    "topic": "1",
    "question_en": "A company has deployed a serverless application that invokes an AWS Lambda function when new documents are uploaded to an Amazon S3 bucket. The application uses the Lambda function to process the documents. After a recent marketing campaign, the company noticed that the application did not process many of the documents. What should a solutions architect do to improve the architecture of this application?",
    "options_en": {
      "A": "Set the Lambda function's runtime timeout value to 15 minutes.",
      "B": "Configure an S3 bucket replication policy. Stage the documents in the S3 bucket for later processing.",
      "C": "Deploy an additional Lambda function. Load balance the processing of the documents across the two Lambda functions.",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Send the requests to the queue. Configure the queue as an event source for Lambda."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司部署了一个无服务器应用程序，当新文档上传到 Amazon S3 存储桶时，该应用程序会调用一个 AWS Lambda 函数。该应用程序使用 Lambda 函数来处理这些文档。在最近的市场营销活动之后，该公司注意到该应用程序没有处理许多文档。解决方案架构师应该怎么做才能改进此应用程序的架构？",
    "options_cn": {
      "A": "将 Lambda 函数的运行时超时值设置为 15 分钟。",
      "B": "配置一个 S3 存储桶复制策略。将文档分阶段存储在 S3 存储桶中，以便稍后处理。",
      "C": "部署一个额外的 Lambda 函数。在两个 Lambda 函数之间负载均衡处理文档。",
      "D": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。将请求发送到队列。将队列配置为 Lambda 的事件源。"
    },
    "tags": [
      "Lambda",
      "S3",
      "SQS",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查 Lambda 函数处理 S3 事件的架构优化。如果处理能力不足，可以考虑异步处理或者负载均衡。",
      "why_correct": "D 选项，使用 SQS 队列作为 Lambda 的事件源，将异步处理文档，缓解流量压力。",
      "why_wrong": "A 选项，增加 Lambda 函数的超时时间并不能根本解决问题；B 选项，S3 存储桶复制策略无法解决处理能力不足的问题；C 选项，部署额外的 Lambda 函数无法解决事件高峰时的问题。"
    },
    "related_terms": [
      "Lambda",
      "S3",
      "SQS",
      "EventBridge"
    ]
  },
  {
    "id": 405,
    "topic": "1",
    "question_en": "A solutions architect is designing the architecture for a software demonstration environment. The environment will run on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The system will experience significant increases in trafic during working hours but is not required to operate on weekends. Which combination of actions should the solutions architect take to ensure that the system can scale to meet demand? (Choose two.)",
    "options_en": {
      "A": "Use AWS Auto Scaling to adjust the ALB capacity based on request rate.",
      "B": "Use AWS Auto Scaling to scale the capacity of the VPC internet gateway.",
      "C": "Launch the EC2 instances in multiple AWS Regions to distribute the load across Regions.",
      "D": "Use a target tracking scaling policy to scale the Auto Scaling group based on instance CPU utilization",
      "E": "Use scheduled scaling to change the Auto Scaling group minimum, maximum, and desired capacity to zero for weekends. Revert to the default values at the start of the week."
    },
    "correct_answer": "DE",
    "vote_percentage": "63%",
    "question_cn": "一位解决方案架构师正在为软件演示环境设计架构。 该环境将在位于 Application Load Balancer (ALB) 之后，Auto Scaling 组中的 Amazon EC2 实例上运行。 该系统在工作时间会经历流量的大幅增加，但不需要在周末运行。 解决方案架构师应采取哪些组合措施来确保系统能够扩展以满足需求？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Auto Scaling 根据请求速率调整 ALB 容量。",
      "B": "使用 AWS Auto Scaling 来扩展 VPC 互联网网关的容量。",
      "C": "在多个 AWS 区域中启动 EC2 实例，以跨区域分配负载。",
      "D": "使用目标跟踪扩展策略，根据实例的 CPU 利用率来扩展 Auto Scaling 组。",
      "E": "使用计划扩展将 Auto Scaling 组的最小、最大和所需容量设置为周末的零。 在一周开始时恢复为默认值。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "ALB",
      "CPU Utilization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 63%），解析仅供参考。】\n\n考察了在弹性伸缩场景下，如何通过Auto Scaling 和扩展策略来优化EC2实例的容量，满足流量峰值需求，并有效控制成本。",
      "why_correct": "选项 D 正确，目标跟踪扩展策略能根据 CPU 利用率自动调整 Auto Scaling 组的实例数量，从而应对工作时间流量增加的情况。选项 E 正确，计划扩展允许根据时间表设置 Auto Scaling 组的最小、最大和所需容量，在周末设置为零，避免不必要的成本，并在一周开始时恢复默认值。",
      "why_wrong": "选项 A 错误，ALB 本身是完全托管服务，AWS Auto Scaling 无法直接调整其容量。选项 B 错误，VPC 互联网网关的容量是固定的，不能被扩展。选项 C 错误，虽然跨区域部署可以提高可用性，但题目场景中主要考虑的是弹性伸缩和成本控制，且未提及可用性需求，因此不适用。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Auto Scaling",
      "Amazon EC2",
      "VPC",
      "Internet Gateway",
      "CPU utilization"
    ]
  },
  {
    "id": 406,
    "topic": "1",
    "question_en": "A solutions architect is designing a two-tiered architecture that includes a public subnet and a database subnet. The web servers in the public subnet must be open to the internet on port 443. The Amazon RDS for MySQL DB instance in the database subnet must be accessible only to the web servers on port 3306. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create a network ACL for the public subnet. Add a rule to deny outbound trafic to 0.0.0.0/0 on port 3306.",
      "B": "Create a security group for the DB instance. Add a rule to allow trafic from the public subnet CIDR block on port 3306.",
      "C": "Create a security group for the web servers in the public subnet. Add a rule to allow trafic from 0.0.0.0/0 on port 443.",
      "D": "Create a security group for the DB instanc",
      "E": "Add a rule to allow trafic from the web servers’ security group on port 3306. E. Create a security group for the DB instance. Add a rule to deny all trafic except trafic from the web servers’ security group on port 3306."
    },
    "correct_answer": "CD",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在设计一个两层架构，其中包括一个公有子网和一个数据库子网。公有子网中的 Web 服务器必须在端口 443 上向互联网开放。数据库子网中的 Amazon RDS for MySQL DB 实例必须只能通过端口 3306 被 Web 服务器访问。解决方案架构师应采取哪些步骤组合来满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "为公有子网创建网络 ACL。添加一条规则，拒绝向 0.0.0.0/0 在端口 3306 上的出站流量。",
      "B": "为 DB 实例创建安全组。添加一条规则，允许来自公有子网 CIDR 块在端口 3306 上的流量。",
      "C": "为公有子网中的 Web 服务器创建安全组。添加一条规则，允许来自 0.0.0.0/0 在端口 443 上的流量。",
      "D": "为 DB 实例创建安全组。添加一条规则，允许来自 Web 服务器的安全组在端口 3306 上的流量。",
      "E": "为 DB 实例创建安全组。添加一条规则，拒绝所有流量，除了来自 Web 服务器的安全组在端口 3306 上的流量。"
    },
    "tags": [
      "VPC",
      "EC2",
      "Security Group",
      "Network ACL",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 100%），解析仅供参考。】\n\n此题考察 VPC 网络安全配置。Web 服务器需要通过 443 端口对外开放，RDS 数据库只能被 Web 服务器通过 3306 端口访问。安全组是针对实例级别的防火墙，而网络 ACL 是子网级别的防火墙。",
      "why_correct": "C 选项正确，配置 Web 服务器的安全组，允许来自 0.0.0.0/0 在 443 端口上的流量，满足了 Web 服务器对外提供服务的需求。",
      "why_wrong": "A 选项错误，网络 ACL 拒绝 3306 端口的出站流量会阻止 Web 服务器访问数据库。B 选项错误，允许来自公有子网 CIDR 块在 3306 端口的流量，相当于允许所有来自公网的流量访问数据库，不符合安全要求。D 选项错误，没有配置允许来自 443 端口的流量，Web 服务器无法对外提供服务，且 DB 实例的安全组没有明确允许来自 Web 服务器的安全组在 3306 端口的流量，可能会被阻止。E 选项错误，只允许来自 Web 服务器的安全组在 3306 端口上的流量，没有允许 443 端口的流量，导致 Web 服务器无法对外提供服务。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "Network ACL",
      "RDS",
      "CIDR",
      "MySQL",
      "Security Group"
    ]
  },
  {
    "id": 407,
    "topic": "1",
    "question_en": "A company is implementing a shared storage solution for a gaming application that is hosted in the AWS Cloud. The company needs the ability to use Lustre clients to access data. The solution must be fully managed. Which solution meets these requirements?",
    "options_en": {
      "A": "Create an AWS DataSync task that shares the data as a mountable file system. Mount the file system to the application server.",
      "B": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. Attach the file system to the origin server. Connect the application server to the file system.",
      "D": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect the application server to the file system."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为其托管在 AWS 云中的游戏应用程序实施共享存储解决方案。该公司需要使用 Lustre 客户端访问数据的能力。该解决方案必须是完全托管的。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "创建一个 AWS DataSync 任务，将数据共享为可挂载的文件系统。将文件系统挂载到应用程序服务器。",
      "B": "创建一个 AWS Storage Gateway 文件网关。创建一个使用所需客户端协议的文件共享。将应用程序服务器连接到文件共享。",
      "C": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统，并将其配置为支持 Lustre。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。",
      "D": "创建一个 Amazon FSx for Lustre 文件系统。将文件系统连接到源服务器。将应用程序服务器连接到文件系统。"
    },
    "tags": [
      "FSx for Lustre",
      "EFS",
      "Storage Gateway",
      "DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察完全托管的 Lustre 文件系统解决方案的选择。",
      "why_correct": "Amazon FSx for Lustre 专为高性能计算（HPC）工作负载而设计，并提供完全托管的 Lustre 文件系统。它允许使用 Lustre 客户端直接访问数据，满足了题目的要求。FSx for Lustre 简化了文件系统的部署和管理，不需要手动配置和维护 Lustre 环境。",
      "why_wrong": "A. AWS DataSync 用于数据传输和同步，而非提供可挂载的文件系统，无法满足Lustre客户端访问数据的需求。B. AWS Storage Gateway 文件网关主要用于连接本地存储与云端存储，不是一个完全托管的 Lustre 文件系统解决方案。C. Amazon EFS 无法原生支持 Lustre，与题目要求不符。"
    },
    "related_terms": [
      "AWS DataSync",
      "AWS Storage Gateway",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon FSx for Lustre",
      "Lustre"
    ]
  },
  {
    "id": 408,
    "topic": "1",
    "question_en": "A company runs an application that receives data from thousands of geographically dispersed remote devices that use UDP. The application processes the data immediately and sends a message back to the device if necessary. No data is stored. The company needs a solution that minimizes latency for the data transmission from the devices. The solution also must provide rapid failover to another AWS Region. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an Amazon Route 53 failover routing policy. Create a Network Load Balancer (NLB) in each of the two Regions. Configure the NLB to invoke an AWS Lambda function to process the data.",
      "B": "Use AWS Global Accelerator. Create a Network Load Balancer (NLB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the NLProcess the data in Amazon ECS.",
      "C": "Use AWS Global Accelerator. Create an Application Load Balancer (ALB) in each of the two Regions as an endpoint. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS.",
      "D": "Configure an Amazon Route 53 failover routing policy. Create an Application Load Balancer (ALB) in each of the two Regions. Create an Amazon Elastic Container Service (Amazon ECS) cluster with the Fargate launch type. Create an ECS service on the cluster. Set the ECS service as the target for the ALB. Process the data in Amazon ECS."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司运行一个应用程序，该应用程序从数千个使用 UDP 的地理位置分散的远程设备接收数据。应用程序立即处理数据，并在必要时向设备发送消息。不存储任何数据。该公司需要一个解决方案，以最大限度地减少从设备传输数据的延迟。该解决方案还必须提供到另一个 AWS 区域的快速故障转移。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Route 53 故障转移路由策略。在两个区域中的每一个中创建一个 Network Load Balancer (NLB)。配置 NLB 以调用 AWS Lambda 函数来处理数据。",
      "B": "使用 AWS Global Accelerator。在两个区域中的每一个中创建一个 Network Load Balancer (NLB) 作为端点。创建一个带有 Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群。在集群上创建一个 ECS 服务。将 ECS 服务设置为 NL 的目标。在 Amazon ECS 中处理数据。",
      "C": "使用 AWS Global Accelerator。在两个区域中的每一个中创建一个 Application Load Balancer (ALB) 作为端点。创建一个带有 Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群。在集群上创建一个 ECS 服务。将 ECS 服务设置为 ALB 的目标。在 Amazon ECS 中处理数据。",
      "D": "配置 Amazon Route 53 故障转移路由策略。在两个区域中的每一个中创建一个 Application Load Balancer (ALB)。创建一个带有 Fargate 启动类型的 Amazon Elastic Container Service (Amazon ECS) 集群。在集群上创建一个 ECS 服务。将 ECS 服务设置为 ALB 的目标。在 Amazon ECS 中处理数据。"
    },
    "tags": [
      "Global Accelerator",
      "Route 53",
      "NLB",
      "ALB",
      "ECS",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察低延迟数据传输和快速故障转移的方案。AWS Global Accelerator 通过全球加速，可以有效降低延迟， Route 53 可以实现 DNS 级别的故障转移。",
      "why_correct": "B 选项正确，Global Accelerator 配合 Network Load Balancer (NLB) 可以提供低延迟，并且 NLB 后面挂载 ECS 服务，利用 Global Accelerator 实现快速故障转移。",
      "why_wrong": "A 选项错误，Route 53 故障转移路由策略虽然可以实现故障转移，但是无法优化延迟， NLB 不能调用 Lambda 函数处理数据。 C 选项错误，Application Load Balancer (ALB) 性能不如 NLB，且 ALB 无法满足 UDP 协议的需求。 D 选项错误，Route 53 故障转移路由策略虽然可以实现故障转移，但是无法优化延迟，且 ALB 性能不如 NLB，ALB 无法满足 UDP 协议的需求。"
    },
    "related_terms": [
      "Global Accelerator",
      "Route 53",
      "NLB",
      "ALB",
      "ECS",
      "Fargate",
      "UDP"
    ]
  },
  {
    "id": 409,
    "topic": "1",
    "question_en": "A solutions architect must migrate a Windows Internet Information Services (IIS) web application to AWS. The application currently relies on a file share hosted in the user's on-premises network-attached storage (NAS). The solutions architect has proposed migrating the IIS web servers to Amazon EC2 instances in multiple Availability Zones that are connected to the storage solution, and configuring an Elastic Load Balancer attached to the instances. Which replacement to the on-premises file share is MOST resilient and durable?",
    "options_en": {
      "A": "Migrate the file share to Amazon RDS.",
      "B": "Migrate the file share to AWS Storage Gateway.",
      "C": "Migrate the file share to Amazon FSx for Windows File Server.",
      "D": "Migrate the file share to Amazon Elastic File System (Amazon EFS)."
    },
    "correct_answer": "C",
    "vote_percentage": "96%",
    "question_cn": "一个解决方案架构师必须将 Windows Internet Information Services (IIS) Web 应用程序迁移到 AWS。该应用程序当前依赖于用户本地网络附加存储 (NAS) 中托管的文件共享。解决方案架构师已建议将 IIS Web 服务器迁移到多个可用区中的 Amazon EC2 实例，这些实例连接到存储解决方案，并配置一个连接到这些实例的 Elastic Load Balancer。对本地文件共享的哪种替换方案的弹性和持久性最强？",
    "options_cn": {
      "A": "将文件共享迁移到 Amazon RDS。",
      "B": "将文件共享迁移到 AWS Storage Gateway。",
      "C": "将文件共享迁移到 Amazon FSx for Windows File Server。",
      "D": "将文件共享迁移到 Amazon Elastic File System (Amazon EFS)。"
    },
    "tags": [
      "RDS",
      "FSx for Windows File Server",
      "EFS",
      "Storage Gateway",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 96%），解析仅供参考。】\n\n考察将本地 NAS 迁移到 AWS 的文件存储解决方案，以及针对 IIS Web 应用程序的弹性与持久性需求。",
      "why_correct": "Amazon FSx for Windows File Server 提供了完全托管的 Windows 文件服务器，专为 Windows 工作负载设计，与 IIS 应用程序兼容。它提供与本地文件共享相似的 SMB 协议支持，易于迁移。FSx for Windows File Server 在多个可用区提供高可用性和持久性，满足题目的弹性与持久性要求。",
      "why_wrong": "选项 A 错误，Amazon RDS 是关系型数据库服务，不适用于文件共享。选项 B 错误，AWS Storage Gateway 是一种混合云存储服务，虽然可以连接到本地 NAS，但本身并非高可用且弹性的文件存储。选项 D 错误，Amazon EFS 专为 Linux 工作负载设计，与 Windows IIS 应用程序的兼容性较差，且SMB协议的支持不如FSx for Windows File Server完善。"
    },
    "related_terms": [
      "Windows Internet Information Services (IIS)",
      "AWS",
      "Amazon EC2",
      "Elastic Load Balancer",
      "Amazon RDS",
      "AWS Storage Gateway",
      "Amazon FSx for Windows File Server",
      "Amazon Elastic File System (Amazon EFS)",
      "NAS",
      "SMB"
    ]
  },
  {
    "id": 410,
    "topic": "1",
    "question_en": "A company is deploying a new application on Amazon EC2 instances. The application writes data to Amazon Elastic Block Store (Amazon EBS) volumes. The company needs to ensure that all data that is written to the EBS volumes is encrypted at rest. Which solution will meet this requirement?",
    "options_en": {
      "A": "Create an IAM role that specifies EBS encryption. Attach the role to the EC2 instances.",
      "B": "Create the EBS volumes as encrypted volumes. Attach the EBS volumes to the EC2 instances.",
      "C": "Create an EC2 instance tag that has a key of Encrypt and a value of True. Tag all instances that require encryption at the EBS level.",
      "D": "Create an AWS Key Management Service (AWS KMS) key policy that enforces EBS encryption in the account. Ensure that the key policy is active."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 Amazon EC2 实例上部署一个新的应用程序。该应用程序将数据写入 Amazon Elastic Block Store (Amazon EBS) 卷。公司需要确保写入 EBS 卷的所有数据在静态时都被加密。哪种解决方案将满足此要求？",
    "options_cn": {
      "A": "创建一个 IAM 角色，指定 EBS 加密。将该角色附加到 EC2 实例。",
      "B": "创建加密的 EBS 卷。将 EBS 卷附加到 EC2 实例。",
      "C": "创建一个 EC2 实例标签，其键为 Encrypt，值为 True。标记所有需要在 EBS 级别进行加密的实例。",
      "D": "创建一个 AWS Key Management Service (AWS KMS) 密钥策略，在账户中强制执行 EBS 加密。确保密钥策略处于活动状态。"
    },
    "tags": [
      "EBS",
      "KMS",
      "IAM",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察 EBS 卷的加密。在 EBS 创建时启用加密即可实现静态加密。",
      "why_correct": "B 选项正确，创建加密的 EBS 卷，并将 EBS 卷附加到 EC2 实例上，满足了加密需求。",
      "why_wrong": "A 选项错误，IAM 角色用于授予访问权限，不能指定 EBS 加密。 C 选项错误，EC2 实例标签不能用于控制 EBS 的加密。 D 选项错误，KMS 密钥策略用于管理密钥，但不是加密 EBS 的直接方法。"
    },
    "related_terms": [
      "EBS",
      "KMS",
      "IAM",
      "EC2"
    ]
  },
  {
    "id": 411,
    "topic": "1",
    "question_en": "A company has a web application with sporadic usage patterns. There is heavy usage at the beginning of each month, moderate usage at the start of each week, and unpredictable usage during the week. The application consists of a web server and a MySQL database server running inside the data center. The company would like to move the application to the AWS Cloud, and needs to select a cost-effective database platform that will not require database modifications. Which solution will meet these requirements?",
    "options_en": {
      "A": "Amazon DynamoDB",
      "B": "Amazon RDS for MySQL",
      "C": "MySQL-compatible Amazon Aurora Serverless",
      "D": "MySQL deployed on Amazon EC2 in an Auto Scaling group"
    },
    "correct_answer": "C",
    "vote_percentage": "89%",
    "question_cn": "一家公司有一个使用模式不固定的 Web 应用程序。每月月初会有大量使用，每周初会有适度使用，而一周内使用情况无法预测。该应用程序由一个 Web 服务器和一个在数据中心内运行的 MySQL 数据库服务器组成。该公司希望将应用程序迁移到 AWS 云，并且需要选择一个经济高效的数据库平台，该平台不需要修改数据库。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "Amazon DynamoDB",
      "B": "Amazon RDS for MySQL",
      "C": "MySQL 兼容的 Amazon Aurora Serverless",
      "D": "在 Amazon EC2 的 Auto Scaling 组中部署 MySQL"
    },
    "tags": [
      "DynamoDB",
      "RDS",
      "Aurora Serverless",
      "EC2",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 89%），解析仅供参考。】\n\n考察数据库选择。Aurora Serverless 在不改变数据库的情况下，提供了按需扩展的能力。",
      "why_correct": "C 选项正确，MySQL 兼容的 Amazon Aurora Serverless 具有按需扩展和 MySQL 兼容的特性，不需要修改数据库，且经济高效。",
      "why_wrong": "A 选项错误，Amazon DynamoDB 是一种 NoSQL 数据库，可能需要修改应用程序。 B 选项错误，Amazon RDS for MySQL 在负载不固定的时候，成本可能较高。 D 选项错误，在 Amazon EC2 的 Auto Scaling 组中部署 MySQL，需要自己管理，成本较高，且没有 Aurora Serverless 灵活。"
    },
    "related_terms": [
      "DynamoDB",
      "RDS",
      "Aurora Serverless",
      "EC2",
      "MySQL"
    ]
  },
  {
    "id": 412,
    "topic": "1",
    "question_en": "An image-hosting company stores its objects in Amazon S3 buckets. The company wants to avoid accidental exposure of the objects in the S3 buckets to the public. All S3 objects in the entire AWS account need to remain private. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon GuardDuty to monitor S3 bucket policies. Create an automatic remediation action rule that uses an AWS Lambda function to remediate any change that makes the objects public.",
      "B": "Use AWS Trusted Advisor to find publicly accessible S3 buckets. Configure email notifications in Trusted Advisor when a change is detected. Manually change the S3 bucket policy if it allows public access.",
      "C": "Use AWS Resource Access Manager to find publicly accessible S3 buckets. Use Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function when a change is detected. Deploy a Lambda function that programmatically remediates the change.",
      "D": "Use the S3 Block Public Access feature on the account level. Use AWS Organizations to create a service control policy (SCP) that prevents IAM users from changing the setting. Apply the SCP to the account."
    },
    "correct_answer": "D",
    "vote_percentage": "94%",
    "question_cn": "一家图片托管公司将其对象存储在 Amazon S3 存储桶中。该公司希望避免 S3 存储桶中的对象意外向公众暴露。整个 AWS 账户中的所有 S3 对象都需要保持私有。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon GuardDuty 监控 S3 存储桶策略。创建自动修复操作规则，该规则使用 AWS Lambda 函数来修复任何使对象公开的更改。",
      "B": "使用 AWS Trusted Advisor 查找可公开访问的 S3 存储桶。在 Trusted Advisor 中配置电子邮件通知，以便在检测到更改时发送通知。如果 S3 存储桶策略允许公共访问，请手动更改该策略。",
      "C": "使用 AWS Resource Access Manager 查找可公开访问的 S3 存储桶。使用 Amazon Simple Notification Service (Amazon SNS) 在检测到更改时调用 AWS Lambda 函数。部署一个 Lambda 函数，以编程方式修复更改。",
      "D": "在账户级别使用 S3 阻止公共访问功能。使用 AWS Organizations 创建服务控制策略 (SCP)，以防止 IAM 用户更改该设置。将 SCP 应用于账户。"
    },
    "tags": [
      "S3",
      "GuardDuty",
      "Trusted Advisor",
      "Resource Access Manager",
      "Organizations",
      "SCP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 94%），解析仅供参考。】\n\n考察 S3 阻止公共访问的最佳实践。使用阻止公共访问可以防止对象被意外公开。",
      "why_correct": "D 选项正确，在账户级别使用 S3 阻止公共访问功能，并使用 Organizations 的 SCP 来防止 IAM 用户更改该设置，可以确保所有 S3 对象保持私有。",
      "why_wrong": "A 选项错误，GuardDuty 用于监控，不能阻止对象暴露。 B 选项错误，Trusted Advisor 用于建议，不能阻止对象暴露。 C 选项错误，Resource Access Manager 与此场景无关。"
    },
    "related_terms": [
      "S3",
      "GuardDuty",
      "Trusted Advisor",
      "SCP",
      "Resource Access Manager",
      "Organizations"
    ]
  },
  {
    "id": 413,
    "topic": "1",
    "question_en": "An ecommerce company is experiencing an increase in user trafic. The company’s store is deployed on Amazon EC2 instances as a two-tier web application consisting of a web tier and a separate database tier. As trafic increases, the company notices that the architecture is causing significant delays in sending timely marketing and order confirmation email to users. The company wants to reduce the time it spends resolving complex email delivery issues and minimize operational overhead. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a separate application tier using EC2 instances dedicated to email processing.",
      "B": "Configure the web instance to send email through Amazon Simple Email Service (Amazon SES).",
      "C": "Configure the web instance to send email through Amazon Simple Notification Service (Amazon SNS).",
      "D": "Create a separate application tier using EC2 instances dedicated to email processing. Place the instances in an Auto Scaling group."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个电商公司正在经历用户流量的增长。该公司的商店部署在 Amazon EC2 实例上，作为一个由 Web 层和一个单独的数据库层组成的双层 Web 应用程序。随着流量的增加，该公司注意到这种架构导致发送及时的营销和订单确认电子邮件给用户时出现显著的延迟。该公司希望减少解决复杂电子邮件传递问题所花费的时间，并最大限度地减少运营开销。解决方案架构师应采取什么措施来满足这些要求？",
    "options_cn": {
      "A": "使用专门用于电子邮件处理的 EC2 实例创建一个单独的应用程序层。",
      "B": "配置 Web 实例通过 Amazon Simple Email Service (Amazon SES) 发送电子邮件。",
      "C": "配置 Web 实例通过 Amazon Simple Notification Service (Amazon SNS) 发送电子邮件。",
      "D": "使用专门用于电子邮件处理的 EC2 实例创建一个单独的应用程序层。将实例放置在 Auto Scaling 组中。"
    },
    "tags": [
      "SES",
      "SNS",
      "EC2",
      "Auto Scaling",
      "Simple Email Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察邮件发送服务。 Amazon SES 是专门用于发送邮件的服务。",
      "why_correct": "B 选项正确，通过 Amazon Simple Email Service (Amazon SES) 发送电子邮件是发送电子邮件最简单、成本最低的方式。",
      "why_wrong": "A 选项错误，使用专门用于电子邮件处理的 EC2 实例，增加了复杂性和运营开销。 C 选项错误，Amazon SNS 主要用于发布/订阅消息，不适用于直接发送电子邮件。 D 选项错误，虽然使用专门用于电子邮件处理的 EC2 实例可以处理邮件，但不如 SES 方便。"
    },
    "related_terms": [
      "SES",
      "SNS",
      "EC2",
      "Auto Scaling",
      "Simple Email Service"
    ]
  },
  {
    "id": 414,
    "topic": "1",
    "question_en": "A company has a business system that generates hundreds of reports each day. The business system saves the reports to a network share in CSV format. The company needs to store this data in the AWS Cloud in near-real time for analysis. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Use AWS DataSync to transfer the files to Amazon S3. Create a scheduled task that runs at the end of each day.",
      "B": "Create an Amazon S3 File Gateway. Update the business system to use a new network share from the S3 File Gateway.",
      "C": "Use AWS DataSync to transfer the files to Amazon S3. Create an application that uses the DataSync API in the automation workfiow.",
      "D": "Deploy an AWS Transfer for SFTP endpoint. Create a script that checks for new files on the network share and uploads the new files by using SFTP."
    },
    "correct_answer": "B",
    "vote_percentage": "85%",
    "question_cn": "一家公司有一个业务系统，每天生成数百份报告。该业务系统将报告保存为 CSV 格式的网络共享。该公司需要在 AWS 云中近乎实时地存储这些数据以供分析。哪种解决方案将以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 将文件传输到 Amazon S3。创建一个定期任务，每天结束时运行。",
      "B": "创建一个 Amazon S3 File Gateway。更新业务系统以使用来自 S3 File Gateway 的新网络共享。",
      "C": "使用 AWS DataSync 将文件传输到 Amazon S3。创建一个应用程序，该应用程序在自动化工作流程中使用 DataSync API。",
      "D": "部署一个 AWS Transfer for SFTP 端点。创建一个脚本，该脚本检查网络共享上的新文件，并通过使用 SFTP 上传新文件。"
    },
    "tags": [
      "DataSync",
      "S3",
      "S3 File Gateway",
      "SFTP",
      "AWS Transfer Family"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 85%），解析仅供参考。】\n\n考察使用 S3 File Gateway 简化数据传输和近乎实时的数据分析场景。",
      "why_correct": "Amazon S3 File Gateway 提供了本地文件共享接口，可以无缝地将数据存储到 Amazon S3。业务系统可以将其报告直接写入 File Gateway，从而将其存储在 S3 中。File Gateway 提供了近乎实时的传输，并降低了管理开销，因为不需要手动配置数据同步或编写应用程序。",
      "why_wrong": "选项 A 的 DataSync 需要配置定期任务，不满足近乎实时的要求。选项 C 的 DataSync 方案需要编写应用程序，增加了管理复杂性。选项 D 的 SFTP 需要部署 Transfer for SFTP，且需要创建脚本来检测新文件并上传，这增加了额外的管理负担，不如 File Gateway 简洁。"
    },
    "related_terms": [
      "Amazon S3",
      "CSV",
      "AWS DataSync",
      "Amazon S3 File Gateway",
      "AWS Transfer for SFTP",
      "SFTP"
    ]
  },
  {
    "id": 415,
    "topic": "1",
    "question_en": "A company is storing petabytes of data in Amazon S3 Standard. The data is stored in multiple S3 buckets and is accessed with varying frequency. The company does not know access patterns for all the data. The company needs to implement a solution for each S3 bucket to optimize the cost of S3 usage. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Intelligent-Tiering.",
      "B": "Use the S3 storage class analysis tool to determine the correct tier for each object in the S3 bucket. Move each object to the identified storage tier.",
      "C": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 Glacier Instant Retrieval.",
      "D": "Create an S3 Lifecycle configuration with a rule to transition the objects in the S3 bucket to S3 One Zone-Infrequent Access (S3 One Zone-IA)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 Amazon S3 Standard 中存储数 PB 的数据。数据存储在多个 S3 存储桶中，并且访问频率各不相同。该公司不了解所有数据的访问模式。该公司需要为每个 S3 存储桶实施一个解决方案，以优化 S3 使用的成本。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "创建一个 S3 生命周期配置，其中包含一条规则，将 S3 存储桶中的对象转换为 S3 Intelligent-Tiering。",
      "B": "使用 S3 存储类别分析工具确定 S3 存储桶中每个对象的正确层。将每个对象移动到已确定的存储层。",
      "C": "创建一个 S3 生命周期配置，其中包含一条规则，将 S3 存储桶中的对象转换为 S3 Glacier Instant Retrieval。",
      "D": "创建一个 S3 生命周期配置，其中包含一条规则，将 S3 存储桶中的对象转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)。"
    },
    "tags": [
      "S3",
      "Lifecycle Configuration",
      "Intelligent-Tiering",
      "Glacier",
      "One Zone-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考察 S3 存储类别优化。 S3 Intelligent-Tiering 可以自动调整存储类别，从而优化成本。",
      "why_correct": "A 选项正确，创建 S3 生命周期配置，将对象转换为 S3 Intelligent-Tiering，可以根据访问频率自动调整存储类别，从而优化成本，并且无需提前了解数据的访问模式。",
      "why_wrong": "B 选项错误，虽然 S3 存储类别分析工具可以确定对象的层，但是需要手动操作。 C 选项错误，S3 Glacier Instant Retrieval 适用于归档，不适用于不清楚访问模式的情况。 D 选项错误，S3 One Zone-IA 适用于不经常访问的数据，不适用于不清楚访问模式的情况。"
    },
    "related_terms": [
      "S3",
      "Intelligent-Tiering",
      "Glacier",
      "Lifecycle Configuration",
      "One Zone-IA"
    ]
  },
  {
    "id": 416,
    "topic": "1",
    "question_en": "A rapidly growing global ecommerce company is hosting its web application on AWS. The web application includes static content and dynamic content. The website stores online transaction processing (OLTP) data in an Amazon RDS database The website’s users are experiencing slow page loads. Which combination of actions should a solutions architect take to resolve this issue? (Choose two.)",
    "options_en": {
      "A": "Configure an Amazon Redshift cluster.",
      "B": "Set up an Amazon CloudFront distribution.",
      "C": "Host the dynamic web content in Amazon S3.",
      "D": "Create a read replica for the RDS DB instanc",
      "E": "E. Configure a Multi-AZ deployment for the RDS DB instance."
    },
    "correct_answer": "BD",
    "vote_percentage": "87%",
    "question_cn": "一家快速增长的全球电子商务公司正在 AWS 上托管其 Web 应用程序。该 Web 应用程序包括静态内容和动态内容。该网站将在线事务处理 (OLTP) 数据存储在 Amazon RDS 数据库中。该网站的用户遇到了页面加载缓慢的问题。解决方案架构师应采取哪些组合操作来解决此问题？（选择两项。）",
    "options_cn": {
      "A": "配置一个 Amazon Redshift 集群。",
      "B": "设置一个 Amazon CloudFront 分发。",
      "C": "将动态 Web 内容托管在 Amazon S3 中。",
      "D": "为 RDS 数据库实例创建一个只读副本。",
      "E": "为 RDS 数据库实例配置多可用区部署。"
    },
    "tags": [
      "CloudFront",
      "RDS",
      "S3",
      "Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 87%），解析仅供参考。】\n\n考察 Web 应用程序优化。CloudFront 用于缓存静态内容，可以提高页面加载速度。",
      "why_correct": "B 选项正确，设置一个 Amazon CloudFront 分发，可以缓存静态内容，提高页面加载速度。",
      "why_wrong": "A 选项错误，Amazon Redshift 主要用于数据仓库，与页面加载速度无关。 C 选项错误，将动态 Web 内容托管在 Amazon S3 中无法解决问题。 D 选项错误，RDS 数据库实例的只读副本对于提高页面加载速度的帮助不大。 E 选项错误，为 RDS 数据库实例配置多可用区部署，和提高页面加载速度关系不大。"
    },
    "related_terms": [
      "CloudFront",
      "RDS",
      "S3",
      "Redshift"
    ]
  },
  {
    "id": 417,
    "topic": "1",
    "question_en": "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?",
    "options_en": {
      "A": "Purchase an EC2 Instance Savings Plan Optimize the Lambda functions’ duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.",
      "B": "Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.",
      "C": "Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.",
      "D": "Purchase a Compute Savings Plan. Optimize the Lambda functions’ duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon EC2 实例和 AWS Lambda 函数来运行其应用程序。该公司在其 AWS 账户中拥有具有公有子网和私有子网的 VPC。 EC2 实例在其中一个 VPC 的私有子网中运行。Lambda 函数需要直接网络访问 EC2 实例才能使应用程序正常工作。该应用程序将运行至少 1 年。该公司预计在此期间应用程序使用的 Lambda 函数数量会增加。该公司希望最大程度地节省所有应用程序资源并保持服务之间的低网络延迟。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "购买 EC2 实例储蓄计划。优化 Lambda 函数的持续时间和内存使用量以及调用的次数。将 Lambda 函数连接到包含 EC2 实例的私有子网。",
      "B": "购买 EC2 实例储蓄计划。优化 Lambda 函数的持续时间、内存使用量、调用的次数以及传输的数据量。将 Lambda 函数连接到 EC2 实例运行所在的同一 VPC 中的公有子网。",
      "C": "购买计算储蓄计划。优化 Lambda 函数的持续时间、内存使用量、调用的次数以及传输的数据量。将 Lambda 函数连接到包含 EC2 实例的私有子网。",
      "D": "购买计算储蓄计划。优化 Lambda 函数的持续时间、内存使用量、调用的次数以及传输的数据量。将 Lambda 函数保留在 Lambda 服务 VPC 中。"
    },
    "tags": [
      "Lambda",
      "EC2",
      "VPC",
      "Savings Plans",
      "Compute Savings Plans"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察 Lambda 函数和 EC2 之间的网络连接。通过计算储蓄计划，可以降低成本。",
      "why_correct": "C 选项正确，使用计算储蓄计划可以最大程度地节省资源，优化 Lambda 函数的持续时间、内存使用量、调用的次数以及传输的数据量，并且将 Lambda 函数连接到包含 EC2 实例的私有子网，满足了所有要求。",
      "why_wrong": "A 选项错误，EC2 储蓄计划与 Lambda 函数的网络延迟无关。 B 选项错误，EC2 储蓄计划与 Lambda 函数的网络延迟无关。D 选项错误，将 Lambda 函数保留在 Lambda 服务 VPC 中，网络延迟较高。"
    },
    "related_terms": [
      "Lambda",
      "EC2",
      "VPC",
      "Compute Savings Plans",
      "Savings Plans"
    ]
  },
  {
    "id": 418,
    "topic": "1",
    "question_en": "A solutions architect needs to allow team members to access Amazon S3 buckets in two different AWS accounts: a development account and a production account. The team currently has access to S3 buckets in the development account by using unique IAM users that are assigned to an IAM group that has appropriate permissions in the account. The solutions architect has created an IAM role in the production account. The role has a policy that grants access to an S3 bucket in the production account. Which solution will meet these requirements while complying with the principle of least privilege?",
    "options_en": {
      "A": "Attach the Administrator Access policy to the development account users.",
      "B": "Add the development account as a principal in the trust policy of the role in the production account.",
      "C": "Turn off the S3 Block Public Access feature on the S3 bucket in the production account.",
      "D": "Create a user in the production account with unique credentials for each team member."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要允许团队成员访问两个不同 AWS 账户中的 Amazon S3 存储桶：一个开发账户和一个生产账户。团队目前通过使用唯一的 IAM 用户访问开发账户中的 S3 存储桶，这些用户被分配到一个在账户中具有适当权限的 IAM 组。解决方案架构师已经在生产账户中创建了一个 IAM 角色。该角色具有一个策略，授予对生产账户中的 S3 存储桶的访问权限。哪个解决方案将满足这些要求，同时符合最小权限原则？",
    "options_cn": {
      "A": "将 Administrator Access 策略附加到开发账户用户。",
      "B": "将开发账户添加为生产账户中角色的信任策略中的委托人。",
      "C": "关闭生产账户中 S3 存储桶上的 S3 阻止公共访问功能。",
      "D": "为每个团队成员在生产账户中创建一个具有唯一凭据的用户。"
    },
    "tags": [
      "IAM",
      "S3",
      "Organizations",
      "STS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察跨账户访问 S3。IAM 角色通过信任策略允许跨账户访问 S3。",
      "why_correct": "B 选项正确，将开发账户添加为生产账户中角色的信任策略中的委托人，允许开发账户的用户担任此角色，进而访问生产账户中的 S3 存储桶。",
      "why_wrong": "A 选项错误，将 Administrator Access 策略附加到开发账户用户，会授予用户过多权限，不符合最小权限原则。 C 选项错误，关闭生产账户中 S3 存储桶上的 S3 阻止公共访问功能，会使数据暴露在公网上，不安全。 D 选项错误，为每个团队成员在生产账户中创建一个具有唯一凭据的用户，增加了管理复杂性，不符合最小权限原则。"
    },
    "related_terms": [
      "IAM",
      "S3",
      "STS",
      "Organizations"
    ]
  },
  {
    "id": 419,
    "topic": "1",
    "question_en": "A company uses AWS Organizations with all features enabled and runs multiple Amazon EC2 workloads in the ap-southeast-2 Region. The company has a service control policy (SCP) that prevents any resources from being created in any other Region. A security policy requires the company to encrypt all data at rest. An audit discovers that employees have created Amazon Elastic Block Store (Amazon EBS) volumes for EC2 instances without encrypting the volumes. The company wants any new EC2 instances that any IAM user or root user launches in ap-southeast-2 to use encrypted EBS volumes. The company wants a solution that will have minimal effect on employees who create EBS volumes. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "In the Amazon EC2 console, select the EBS encryption account attribute and define a default encryption key.",
      "B": "Create an IAM permission boundary. Attach the permission boundary to the root organizational unit (OU). Define the boundary to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals false.",
      "C": "Create an SCP. Attach the SCP to the root organizational unit (OU). Define the SCP to deny the ec2:CreateVolume action whenthe ec2:Encrypted condition equals false.",
      "D": "Update the IAM policies for each account to deny the ec2:CreateVolume action when the ec2:Encrypted condition equals fals",
      "E": "E. In the Organizations management account, specify the Default EBS volume encryption setting."
    },
    "correct_answer": "CE",
    "vote_percentage": "75%",
    "question_cn": "一家公司使用启用了所有功能的 AWS Organizations，并在 ap-southeast-2 区域运行多个 Amazon EC2 工作负载。该公司有一个服务控制策略 (SCP)，禁止在任何其他区域创建任何资源。一项安全策略要求公司对所有静态数据进行加密。审计发现，员工为 EC2 实例创建了 Amazon Elastic Block Store (Amazon EBS) 卷，但没有加密这些卷。该公司希望任何 IAM 用户或根用户在 ap-southeast-2 中启动的任何新 EC2 实例都使用加密的 EBS 卷。该公司希望一个对创建 EBS 卷的员工影响最小的解决方案。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 Amazon EC2 控制台中，选择 EBS 加密账户属性并定义一个默认加密密钥。",
      "B": "创建 IAM 权限边界。将权限边界附加到根组织单元 (OU)。定义边界以拒绝 ec2:CreateVolume 操作，当 ec2:Encrypted 条件等于 false 时。",
      "C": "创建 SCP。将 SCP 附加到根组织单元 (OU)。定义 SCP 以拒绝 ec2:CreateVolume 操作，当 ec2:Encrypted 条件等于 false 时。",
      "D": "更新每个账户的 IAM 策略，以拒绝 ec2:CreateVolume 操作，当 ec2:Encrypted 条件等于 false 时。",
      "E": "在 Organizations 管理账户中，指定默认的 EBS 卷加密设置。"
    },
    "tags": [
      "EBS",
      "Organizations",
      "SCP",
      "IAM",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 75%），解析仅供参考。】\n\n考察使用 SCP 和默认加密设置来强制对 EBS 卷进行加密，以满足安全策略要求，并最小化对现有员工的影响。",
      "why_correct": "选项 C 正确，因为 SCP 允许在组织级别强制执行策略。通过拒绝未加密的 EBS 卷创建操作，SCP 确保了所有新创建的 EBS 卷都必须加密。选项 E 正确，通过在 Organizations 管理账户中设置默认的 EBS 卷加密设置，可以确保所有新创建的 EBS 卷默认使用加密。这两个方法结合使用，可以满足安全要求，同时尽可能减少对员工操作的影响。",
      "why_wrong": "选项 A 错误，因为在 EC2 控制台中设置默认加密密钥只影响该账户本身，无法对整个组织强制执行加密策略。选项 B 错误，因为权限边界限制 IAM 用户的权限，但不能控制组织范围内的资源创建，且会影响员工的操作。选项 D 错误，更新每个账户的 IAM 策略会导致管理成本过高，无法统一管理，且影响员工的操作。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS Organizations",
      "IAM",
      "SCP",
      "OU",
      "ec2:CreateVolume",
      "ec2:Encrypted"
    ]
  },
  {
    "id": 420,
    "topic": "1",
    "question_en": "A company wants to use an Amazon RDS for PostgreSQL DB cluster to simplify time-consuming database administrative tasks for production database workloads. The company wants to ensure that its database is highly available and will provide automatic failover support in most scenarios in less than 40 seconds. The company wants to ofioad reads off of the primary instance and keep costs as low as possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an Amazon RDS Multi-AZ DB instance deployment. Create one read replica and point the read workload to the read replica.",
      "B": "Use an Amazon RDS Multi-AZ DB duster deployment Create two read replicas and point the read workload to the read replicas.",
      "C": "Use an Amazon RDS Multi-AZ DB instance deployment. Point the read workload to the secondary instances in the Multi-AZ pair.",
      "D": "Use an Amazon RDS Multi-AZ DB cluster deployment Point the read workload to the reader endpoint."
    },
    "correct_answer": "D",
    "vote_percentage": "82%",
    "question_cn": "一家公司希望使用 Amazon RDS for PostgreSQL DB 集群来简化生产数据库工作负载中耗时的数据库管理任务。该公司希望确保其数据库具有高可用性，并在大多数情况下在 40 秒内提供自动故障转移支持。该公司希望将读取从主实例中卸载，并尽可能降低成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon RDS Multi-AZ DB 实例部署。创建一个读取副本并将读取工作负载指向读取副本。",
      "B": "使用 Amazon RDS Multi-AZ DB 集群部署。创建两个读取副本，并将读取工作负载指向读取副本。",
      "C": "使用 Amazon RDS Multi-AZ DB 实例部署。将读取工作负载指向 Multi-AZ 对中的辅助实例。",
      "D": "使用 Amazon RDS Multi-AZ DB 集群部署。将读取工作负载指向读取器端点。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "Multi-AZ",
      "Read Replica"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 82%），解析仅供参考。】\n\n考查 Amazon RDS for PostgreSQL 的高可用性、读取扩展和成本优化，以及自动故障转移策略。",
      "why_correct": "选项 D 使用 RDS Multi-AZ DB 集群，提供高可用性并支持快速故障转移（通常在 35 秒内）。读取器端点允许将读取工作负载分发到读取副本，实现读取扩展。Multi-AZ DB 集群在成本方面优于 Multi-AZ DB 实例，因为它们支持多个读取副本。",
      "why_wrong": "选项 A 使用 RDS Multi-AZ DB 实例，故障转移时间通常需要 60-120 秒，不满足40秒的要求。选项 B 使用 RDS Multi-AZ DB 集群，但未说明故障转移时间，但满足高可用性。然而，该选项没有说明如何使用 RDS 的读取器端点。选项 C 使用 RDS Multi-AZ DB 实例，但将读取工作负载指向辅助实例，这不是最佳实践。辅助实例主要用于故障转移，不用于读取扩展，并且无法实现成本优化。"
    },
    "related_terms": [
      "Amazon RDS for PostgreSQL",
      "DB cluster",
      "Multi-AZ",
      "DB instance",
      "Read replica",
      "Reader endpoint",
      "Multi-AZ DB instance",
      "Multi-AZ DB cluster"
    ]
  },
  {
    "id": 421,
    "topic": "1",
    "question_en": "A company runs a highly available SFTP service. The SFTP service uses two Amazon EC2 Linux instances that run with elastic IP addresses to accept trafic from trusted IP sources on the internet. The SFTP service is backed by shared storage that is attached to the instances. User accounts are created and managed as Linux users in the SFTP servers. The company wants a serverless option that provides high IOPS performance and highly configurable security. The company also wants to maintain control over user permissions. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the EBS volume to the SFTP service endpoint. Grant users access to the SFTP service.",
      "B": "Create an encrypted Amazon Elastic File System (Amazon EFS) volume. Create an AWS Transfer Family SFTP service with elastic IP addresses and a VPC endpoint that has internet-facing access. Attach a security group to the endpoint that allows only trusted IP addresses. Attach the EFS volume to the SFTP service endpoint. Grant users access to the SFTP service.",
      "C": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a public endpoint that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service.",
      "D": "Create an Amazon S3 bucket with default encryption enabled. Create an AWS Transfer Family SFTP service with a VPC endpoint that has internal access in a private subnet. Attach a security group that allows only trusted IP addresses. Attach the S3 bucket to the SFTP service endpoint. Grant users access to the SFTP service."
    },
    "correct_answer": "B",
    "vote_percentage": "82%",
    "question_cn": "一家公司运营一项高可用 SFTP 服务。SFTP 服务使用两个 Amazon EC2 Linux 实例，这些实例使用弹性 IP 地址运行，以接受来自互联网上受信任 IP 源的流量。SFTP 服务由连接到实例的共享存储提供支持。用户账户在 SFTP 服务器中作为 Linux 用户创建和管理。该公司想要一个无服务器选项，该选项提供高 IOPS 性能和高度可配置的安全性。该公司还希望保持对用户权限的控制。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个加密的 Amazon Elastic Block Store (Amazon EBS) 卷。创建一个 AWS Transfer Family SFTP 服务，该服务具有一个公共端点，仅允许受信任的 IP 地址。将 EBS 卷附加到 SFTP 服务端点。授予用户访问 SFTP 服务的权限。",
      "B": "创建一个加密的 Amazon Elastic File System (Amazon EFS) 卷。创建一个 AWS Transfer Family SFTP 服务，该服务具有弹性 IP 地址和具有面向 Internet 访问的 VPC 端点。将安全组附加到端点，该安全组仅允许受信任的 IP 地址。将 EFS 卷附加到 SFTP 服务端点。授予用户访问 SFTP 服务的权限。",
      "C": "创建一个已启用默认加密的 Amazon S3 存储桶。创建一个 AWS Transfer Family SFTP 服务，该服务具有一个公共端点，仅允许受信任的 IP 地址。将 S3 存储桶附加到 SFTP 服务端点。授予用户访问 SFTP 服务的权限。",
      "D": "创建一个已启用默认加密的 Amazon S3 存储桶。创建一个 AWS Transfer Family SFTP 服务，该服务具有 VPC 端点，该端点在私有子网中具有内部访问权限。附加一个仅允许受信任 IP 地址的安全组。将 S3 存储桶附加到 SFTP 服务端点。授予用户访问 SFTP 服务的权限。"
    },
    "tags": [
      "Transfer Family",
      "EC2",
      "EBS",
      "EFS",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 82%），解析仅供参考。】\n\n考察使用 AWS Transfer Family 创建无服务器 SFTP 服务，并配置存储和安全组来实现高可用性、高 IOPS 性能、高度可配置的安全性以及对用户权限的控制。",
      "why_correct": "选项 B 提供了最符合要求的解决方案。它使用加密的 Amazon Elastic File System (Amazon EFS) 卷，满足了高 IOPS 性能的需求。AWS Transfer Family SFTP 服务配置了具有弹性 IP 地址和面向 Internet 访问的 VPC 端点，并使用安全组限制来自受信任 IP 地址的访问，从而保证了安全性。EFS 的共享存储特性也符合 SFTP 服务需求。",
      "why_wrong": "选项 A 错误在于其使用了 EBS。EBS 卷是单实例存储，不适合于高可用性的 SFTP 服务，因为这与题干中的高可用性要求相悖。选项 C 错误在于其使用了 S3。虽然 S3 提供了存储功能，但它不支持直接的文件系统操作，性能和对用户权限的控制不如 EFS。选项 D 错误在于使用了私有子网中的 VPC 端点，这会限制来自互联网的访问，无法满足题干中来自互联网上受信任 IP 源的需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "SFTP",
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS Transfer Family",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon S3",
      "VPC",
      "Elastic IP",
      "Security Group",
      "IOPS",
      "Linux"
    ]
  },
  {
    "id": 422,
    "topic": "1",
    "question_en": "A company is developing a new machine learning (ML) model solution on AWS. The models are developed as independent microservices that fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the models through an asynchronous API. Users can send a request or a batch of requests and specify where the results should be sent. The company provides models to hundreds of users. The usage patterns for the models are irregular. Some models could be unused for days or weeks. Other models could receive batches of thousands of requests at a time. Which design should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the models as AWS Lambda functions that are invoked by the NLB.",
      "B": "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from an Amazon Simple Queue Service (Amazon SQS) queue. Use AWS App Mesh to scale the instances of the ECS cluster based on the SQS queue size.",
      "C": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as AWS Lambda functions that are invoked by SQS events. Use AWS Auto Scaling to increase the number of vCPUs for the Lambda functions based on the SQS queue size.",
      "D": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Enable AWS Auto Scaling on Amazon ECS for both the cluster and copies of the service based on the queue size."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上开发新的机器学习 (ML) 模型解决方案。这些模型被开发为独立的微服务，这些微服务在启动时从 Amazon S3 获取大约 1 GB 的模型数据，并将数据加载到内存中。用户通过异步 API 访问这些模型。用户可以发送一个请求或一批请求，并指定结果的发送位置。该公司向数百名用户提供模型。这些模型的使用模式是不规则的。某些模型可能几天或几周未使用。其他模型一次可能收到数千个请求。解决方案架构师应该推荐哪种设计来满足这些要求？",
    "options_cn": {
      "A": "将来自 API 的请求定向到 Network Load Balancer (NLB)。将这些模型部署为由 NLB 调用的 AWS Lambda 函数。",
      "B": "将来自 API 的请求定向到 Application Load Balancer (ALB)。将这些模型部署为从 Amazon Simple Queue Service (Amazon SQS) 队列读取的 Amazon Elastic Container Service (Amazon ECS) 服务。使用 AWS App Mesh 根据 SQS 队列大小扩展 ECS 集群的实例。",
      "C": "将来自 API 的请求定向到 Amazon Simple Queue Service (Amazon SQS) 队列。将这些模型部署为由 SQS 事件调用的 AWS Lambda 函数。使用 AWS Auto Scaling 根据 SQS 队列大小增加 Lambda 函数的 vCPU 数量。",
      "D": "将来自 API 的请求定向到 Amazon Simple Queue Service (Amazon SQS) 队列。将这些模型部署为从队列读取的 Amazon Elastic Container Service (Amazon ECS) 服务。在 Amazon ECS 上启用 AWS Auto Scaling，以便根据队列大小扩展集群和服务的副本。"
    },
    "tags": [
      "Lambda",
      "SQS",
      "ECS",
      "NLB",
      "ALB",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察如何为机器学习模型构建可扩展的架构。D 选项使用 SQS 作为请求队列，ECS 作为模型服务，并使用 Auto Scaling 机制来动态调整 ECS 实例的数量，以应对用户请求量的变化，是最佳的解决方案。",
      "why_correct": "选项 D 正确，因为 SQS 队列能够解耦用户请求和模型服务的执行，ECS 服务可以运行模型微服务，Auto Scaling 可以根据队列中的消息数量自动调整 ECS 实例的数量，以实现弹性伸缩。",
      "why_wrong": "选项 A 错误，NLB 用于分发负载，不能实现异步处理和弹性伸缩；选项 B 错误，ALB 和 ECS 结合虽然能实现一定的弹性，但 SQS 队列的作用被削弱，且没有使用 Auto Scaling 来根据队列大小进行伸缩；选项 C 错误，Lambda 函数不适合运行需要加载大量数据的模型。"
    },
    "related_terms": [
      "Lambda",
      "SQS",
      "ECS",
      "NLB",
      "ALB",
      "Auto Scaling",
      "ML"
    ]
  },
  {
    "id": 423,
    "topic": "1",
    "question_en": "A solutions architect wants to use the following JSON text as an identity-based policy to grant specific permissions: Which IAM principals can the solutions architect attach this policy to? (Choose two.)",
    "question_image": "/data/images/423.png",
    "options_en": {
      "A": "Role",
      "B": "Group",
      "C": "Organization",
      "D": "Amazon Elastic Container Service (Amazon ECS) resource",
      "E": "Amazon EC2 resource"
    },
    "correct_answer": "AB",
    "vote_percentage": "100%",
    "question_cn": "解决方案架构师希望使用以下 JSON 文本作为基于身份的策略来授予特定权限：解决方案架构师可以将此策略附加到哪些 IAM 主体？（选择两个。）",
    "options_cn": {
      "A": "角色",
      "B": "组",
      "C": "组织",
      "D": "Amazon Elastic Container Service (Amazon ECS) 资源",
      "E": "Amazon EC2 资源"
    },
    "tags": [
      "IAM",
      "Role",
      "Group"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 100%），解析仅供参考。】\n\n以下是这些为何是正确选择的解释:\n\n1. IAM角色 (IAM Role)\nIAM角色是定义了一组用于进行AWS服务请求的权限的实体。基于身份的策略可以附加到角色,以指定该角色可以执行哪些操作。角色通常用于授予对AWS资源的临时访问权限。\n\n2. IAM组 (IAM Group)\nIAM组是IAM用户的集合。基于身份的策略可以附加到组,然后这些权限将应用于组中的所有用户。这是为具有相似访问需求的多个用户管理权限的有效方法。\n\n需要注意的是:\n• 组织 (选项C) 不是IAM主体,不能直接将基于身份的策略附加到它们。\n• 亚马逊ECS资源和亚马逊EC2资源 (选项D和E) 是AWS资源,而不是IAM主体。对于这些资源,将使用基于资源的策略,而不是基于身份的策略。\n\n在使用IAM策略时,始终遵循最小权限原则。这意味着只授予执行任务所需的权限。定期审查和审计您的IAM策略,以确保它们符合您的安全要求和业务需求。\n\n请记住,在将任何新的或修改后的IAM策略部署到生产环境之前,先在非生产环境中进行测试。",
      "why_correct": "A 角色、B 组均为 IAM 主体,基于身份的策略可以附加到角色以指定该角色可执行的操作,也可以附加到组并使权限应用于组内所有用户。",
      "why_wrong": "C 组织不是 IAM 主体,不能直接附加基于身份的策略。D、E 是 AWS 资源而非 IAM 主体,应使用基于资源的策略。"
    },
    "related_terms": [
      "IAM",
      "ECS",
      "EC2",
      "Role",
      "Group"
    ]
  },
  {
    "id": 424,
    "topic": "1",
    "question_en": "A company is running a custom application on Amazon EC2 On-Demand Instances. The application has frontend nodes that need to run 24 hours a day, 7 days a week and backend nodes that need to run only for a short time based on workload. The number of backend nodes varies during the day. The company needs to scale out and scale in more instances based on workload. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use Reserved Instances for the frontend nodes. Use AWS Fargate for the backend nodes.",
      "B": "Use Reserved Instances for the frontend nodes. Use Spot Instances for the backend nodes.",
      "C": "Use Spot Instances for the frontend nodes. Use Reserved Instances for the backend nodes.",
      "D": "Use Spot Instances for the frontend nodes. Use AWS Fargate for the backend nodes."
    },
    "correct_answer": "B",
    "vote_percentage": "64%",
    "question_cn": "一家公司正在 Amazon EC2 On-Demand 实例上运行一个自定义应用程序。该应用程序具有需要每天 24 小时、每周 7 天运行的前端节点和仅根据工作负载在短时间内运行的后端节点。后端节点的数量在一天中变化。该公司需要根据工作负载扩展和缩减更多实例。哪种解决方案将以最具成本效益的方式满足这些需求？",
    "options_cn": {
      "A": "为前端节点使用 Reserved Instances。为后端节点使用 AWS Fargate。",
      "B": "为前端节点使用 Reserved Instances。为后端节点使用 Spot Instances。",
      "C": "为前端节点使用 Spot Instances。为后端节点使用 Reserved Instances。",
      "D": "为前端节点使用 Spot Instances。为后端节点使用 AWS Fargate。"
    },
    "tags": [
      "EC2",
      "Spot Instances",
      "Reserved Instances",
      "AWS Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 64%），解析仅供参考。】\n\n该题考查 EC2 实例的购买选项。 对于需要 24/7 运行的节点，使用 Reserved Instances 能够降低成本。对于工作负载可变的后端节点，使用 Spot Instances 具有成本效益。",
      "why_correct": "选项 B 正确，Reserved Instances 适合前端节点，Spot Instances 适合后端节点。",
      "why_wrong": "选项 A 错误，AWS Fargate 无法满足高可用性前端节点；选项 C 错误，Spot Instances 不适合 24/7 运行的前端节点；选项 D 错误，AWS Fargate 无法满足高可用性前端节点。"
    },
    "related_terms": [
      "EC2",
      "AWS Fargate",
      "Spot Instances",
      "Reserved Instances"
    ]
  },
  {
    "id": 425,
    "topic": "1",
    "question_en": "A company uses high block storage capacity to runs its workloads on premises. The company's daily peak input and output transactions per second are not more than 15,000 IOPS. The company wants to migrate the workloads to Amazon EC2 and to provision disk performance independent of storage capacity. Which Amazon Elastic Block Store (Amazon EBS) volume type will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "GP2 volume type",
      "B": "io2 volume type",
      "C": "GP3 volume type",
      "D": "io1 volume type"
    },
    "correct_answer": "C",
    "vote_percentage": "94%",
    "question_cn": "一家公司使用高块存储容量在其本地运行工作负载。该公司每天的峰值每秒输入和输出事务不超过 15,000 IOPS。该公司希望将工作负载迁移到 Amazon EC2，并配置与存储容量无关的磁盘性能。哪种 Amazon Elastic Block Store (Amazon EBS) 卷类型将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "GP2 卷类型",
      "B": "io2 卷类型",
      "C": "GP3 卷类型",
      "D": "io1 卷类型"
    },
    "tags": [
      "EBS",
      "IOPS",
      "GP2",
      "GP3",
      "io1",
      "io2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 94%），解析仅供参考。】\n\n此题考查 EBS 卷类型。GP3 卷提供了独立的 IOPS 和吞吐量配置，可以满足高性能需求，且更具成本效益。",
      "why_correct": "选项 C 正确，GP3 卷能满足题目中存储容量无关的磁盘性能的需求，且更具成本效益。",
      "why_wrong": "选项 A 错误，GP2 卷的性能与卷大小相关，不符合题目要求；选项 B 错误，io2 卷适用于极高性能需求，成本较高；选项 D 错误，io1 卷适用于极高性能需求，成本较高。"
    },
    "related_terms": [
      "EBS",
      "IOPS",
      "GP2",
      "GP3",
      "io2",
      "io1"
    ]
  },
  {
    "id": 426,
    "topic": "1",
    "question_en": "A company needs to store data from its healthcare application. The application’s data frequently changes. A new regulation requires audit access at all levels of the stored data. The company hosts the application on an on-premises infrastructure that is running out of storage capacity. A solutions architect must securely migrate the existing data to AWS while satisfying the new regulation. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS DataSync to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.",
      "B": "Use AWS Snowcone to move the existing data to Amazon S3. Use AWS CloudTrail to log management events.",
      "C": "Use Amazon S3 Transfer Acceleration to move the existing data to Amazon S3. Use AWS CloudTrail to log data events.",
      "D": "Use AWS Storage Gateway to move the existing data to Amazon S3. Use AWS CloudTrail to log management events."
    },
    "correct_answer": "A",
    "vote_percentage": "57%",
    "question_cn": "一家公司需要存储其医疗保健应用程序的数据。该应用程序的数据经常变化。一项新法规要求对存储数据的各个级别进行审计访问。该公司将应用程序托管在本地基础设施上，该基础设施的存储容量即将耗尽。一位解决方案架构师必须安全地将现有数据迁移到 AWS，同时满足新法规的要求。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 将现有数据移动到 Amazon S3。使用 AWS CloudTrail 记录数据事件。",
      "B": "使用 AWS Snowcone 将现有数据移动到 Amazon S3。使用 AWS CloudTrail 记录管理事件。",
      "C": "使用 Amazon S3 Transfer Acceleration 将现有数据移动到 Amazon S3。使用 AWS CloudTrail 记录数据事件。",
      "D": "使用 AWS Storage Gateway 将现有数据移动到 Amazon S3。使用 AWS CloudTrail 记录管理事件。"
    },
    "tags": [
      "S3",
      "DataSync",
      "Snowcone",
      "CloudTrail",
      "Storage Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 57%），解析仅供参考。】\n\n考查数据迁移方案的选择，以及如何满足数据审计需求，同时兼顾存储容量和安全性。需要选择既能进行高效数据迁移，又能记录数据访问事件的方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：AWS DataSync 适用于在线数据迁移，能够快速、安全地将数据从本地存储迁移到 Amazon S3。结合 AWS CloudTrail 记录数据事件，可以满足法规对数据审计的要求，完整记录数据的读写操作，提供访问日志。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 中 AWS Snowcone 适用于大批量离线数据迁移，不适合数据经常变化的应用。Snowcone 的数据访问审计功能有限，且迁移过程需要实体设备，效率较低。选项 C 中 Amazon S3 Transfer Acceleration 主要用于加速数据上传，并非完整的数据迁移方案，且未说明如何进行数据审计。选项 D 中 AWS Storage Gateway 适用于混合云架构，并非最佳的数据迁移方案，且存储网关的审计功能可能不完全满足法规要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon S3",
      "AWS CloudTrail",
      "AWS Snowcone",
      "Amazon S3 Transfer Acceleration",
      "AWS Storage Gateway"
    ]
  },
  {
    "id": 427,
    "topic": "1",
    "question_en": "A solutions architect is implementing a complex Java application with a MySQL database. The Java application must be deployed on Apache Tomcat and must be highly available. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Deploy the application in AWS Lambda. Configure an Amazon API Gateway API to connect with the Lambda functions.",
      "B": "Deploy the application by using AWS Elastic Beanstalk. Configure a load-balanced environment and a rolling deployment policy.",
      "C": "Migrate the database to Amazon ElastiCache. Configure the ElastiCache security group to allow access from the application.",
      "D": "Launch an Amazon EC2 instance. Install a MySQL server on the EC2 instance. Configure the application on the server. Create an AMI. Use the AMI to create a launch template with an Auto Scaling group."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在实施一个具有 MySQL 数据库的复杂 Java 应用程序。Java 应用程序必须部署在 Apache Tomcat 上，并且必须具有高可用性。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在 AWS Lambda 中部署应用程序。配置 Amazon API Gateway API 以连接到 Lambda 函数。",
      "B": "使用 AWS Elastic Beanstalk 部署应用程序。配置负载均衡环境和滚动部署策略。",
      "C": "将数据库迁移到 Amazon ElastiCache。配置 ElastiCache 安全组，以允许从应用程序进行访问。",
      "D": "启动 Amazon EC2 实例。在 EC2 实例上安装 MySQL 服务器。在服务器上配置应用程序。创建一个 AMI。使用 AMI 创建一个具有 Auto Scaling group 的启动模板。"
    },
    "tags": [
      "Elastic Beanstalk",
      "EC2",
      "MySQL",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察如何部署 Java 应用。Elastic Beanstalk 提供了便捷的部署和管理 Java 应用的方式，支持负载均衡和滚动部署。",
      "why_correct": "选项 B 正确，Elastic Beanstalk 提供了自动负载均衡和滚动部署，满足高可用性要求。",
      "why_wrong": "选项 A 错误，Lambda 不适合部署 Java 应用；选项 C 错误，ElastiCache 和 MySQL 无直接关联，无法满足高可用性；选项 D 错误，启动 EC2 实例过于复杂，不便于维护和部署。"
    },
    "related_terms": [
      "Elastic Beanstalk",
      "EC2",
      "MySQL",
      "Auto Scaling",
      "API Gateway",
      "Lambda",
      "ElastiCache"
    ]
  },
  {
    "id": 428,
    "topic": "1",
    "question_en": "A serverless application uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. The Lambda function needs permissions to read and write to the DynamoDB table. Which solution will give the Lambda function access to the DynamoDB table MOST securely?",
    "options_en": {
      "A": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters as part of the Lambda environment variables. Ensure that other AWS users do not have read and write access to the Lambda function configuration.",
      "B": "Create an IAM role that includes Lambda as a trusted service. Attach a policy to the role that allows read and write access to the DynamoDB table. Update the configuration of the Lambda function to use the new role as the execution role.",
      "C": "Create an IAM user with programmatic access to the Lambda function. Attach a policy to the user that allows read and write access to the DynamoDB table. Store the access_key_id and secret_access_key parameters in AWS Systems Manager Parameter Store as secure string parameters. Update the Lambda function code to retrieve the secure string parameters before connecting to the DynamoDB table.",
      "D": "Create an IAM role that includes DynamoDB as a trusted service. Attach a policy to the role that allows read and write access from the Lambda function. Update the code of the Lambda function to attach to the new role as an execution role."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个无服务器应用程序使用 Amazon API Gateway、AWS Lambda 和 Amazon DynamoDB。Lambda 函数需要权限来读取和写入 DynamoDB 表。哪种解决方案将最安全地赋予 Lambda 函数访问 DynamoDB 表的权限？",
    "options_cn": {
      "A": "创建一个 IAM 用户，使用编程访问权限访问 Lambda 函数。为该用户附加一个策略，允许对 DynamoDB 表进行读写访问。将 access_key_id 和 secret_access_key 参数存储为 Lambda 环境变量的一部分。确保其他 AWS 用户没有对 Lambda 函数配置的读写访问权限。",
      "B": "创建一个 IAM 角色，其中包括 Lambda 作为受信任的服务。为该角色附加一个策略，允许对 DynamoDB 表进行读写访问。更新 Lambda 函数的配置以使用新角色作为执行角色。",
      "C": "创建一个 IAM 用户，使用编程访问权限访问 Lambda 函数。为该用户附加一个策略，允许对 DynamoDB 表进行读写访问。将 access_key_id 和 secret_access_key 参数作为安全字符串参数存储在 AWS Systems Manager Parameter Store 中。更新 Lambda 函数代码以在连接到 DynamoDB 表之前检索安全字符串参数。",
      "D": "创建一个 IAM 角色，其中包括 DynamoDB 作为受信任的服务。为该角色附加一个策略，允许来自 Lambda 函数的读写访问。更新 Lambda 函数的代码以附加到新角色作为执行角色。"
    },
    "tags": [
      "IAM",
      "Lambda",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察如何安全地赋予 Lambda 函数访问 DynamoDB 的权限。使用 IAM 角色是最佳实践。",
      "why_correct": "选项 B 正确，通过 IAM 角色，可以安全地为 Lambda 函数赋予 DynamoDB 的访问权限，避免了密钥泄露的风险。",
      "why_wrong": "选项 A 错误，将密钥存储在环境变量中不安全；选项 C 错误，将密钥存储在 Parameter Store 中增加了复杂性；选项 D 错误，DynamoDB 无法包含 Lambda 作为受信任的服务。"
    },
    "related_terms": [
      "IAM",
      "Lambda",
      "DynamoDB"
    ]
  },
  {
    "id": 429,
    "topic": "1",
    "question_en": "The following IAM policy is attached to an IAM group. This is the only policy applied to the group. What are the effective IAM permissions of this policy for group members?",
    "question_image": "/data/images/429.png",
    "options_en": {
      "A": "Group members are permitted any Amazon EC2 action within the us-east-1 Region. Statements after the Allow permission are not applied.",
      "B": "Group members are denied any Amazon EC2 permissions in the us-east-1 Region unless they are logged in with multi-factor authentication (MFA).",
      "C": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for all Regions when logged in with multi- factor authentication (MFA). Group members are permitted any other Amazon EC2 action.",
      "D": "Group members are allowed the ec2:StopInstances and ec2:TerminateInstances permissions for the us-east-1 Region only when logged in with multi-factor authentication (MFA). Group members are permitted any other Amazon EC2 action within the us-east-1 Region."
    },
    "correct_answer": "D",
    "vote_percentage": "85%",
    "question_cn": "以下 IAM 策略附加到一个 IAM 组。这是应用于该组的唯一策略。此策略对组成员的有效 IAM 权限是什么？",
    "options_cn": {
      "A": "组成员被允许在 us-east-1 区域内执行任何 Amazon EC2 操作。Allow 权限之后的语句不适用。",
      "B": "除非使用多因素身份验证 (MFA) 登录，否则组成员在 us-east-1 区域中被拒绝任何 Amazon EC2 权限。",
      "C": "组成员在使用多因素身份验证 (MFA) 登录时，被允许在所有区域执行 ec2:StopInstances 和 ec2:TerminateInstances 权限。组成员被允许任何其他 Amazon EC2 操作。",
      "D": "组成员仅在使用多因素身份验证 (MFA) 登录时，被允许在 us-east-1 区域执行 ec2:StopInstances 和 ec2:TerminateInstances 权限。组成员被允许在 us-east-1 区域内执行任何其他 Amazon EC2 操作。"
    },
    "tags": [
      "IAM",
      "EC2",
      "MFA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 85%），解析仅供参考。】\n\n答案解析:\n\n根据所描述的IAM策略,组成员的有效权限如下:\n\nD. 组成员只有在通过多因素身份验证(MFA)登录时,才被允许在美东1区(us-east-1)拥有 ec2:StopInstances(停止实例)和 ec2:TerminateInstances(终止实例)权限。组成员在美东1区被允许执行任何其他亚马逊EC2操作。\n\n该策略通过以下方式实现了最小权限原则:\n1. 限制敏感的EC2操作(停止和终止实例)需要MFA身份验证。\n2. 将这些操作的范围限制在特定区域(美东1区)。\n3. 允许在同一区域内执行其他EC2操作而无需额外限制。\n\n关键安全影响和建议:\n1. MFA要求为潜在的破坏性操作增加了一层额外的安全保障。\n2. 该策略是特定于区域的,有助于控制潜在滥用的影响范围。\n3. 考虑通过指定确切的资源ARN而不是对资源使用「*」来进一步限制权限。\n4. 定期审查和审计该策略,以确保其符合最小权限原则。\n5. 监控使用模式,并考虑实施额外的控制措施,如使用AWS CloudTrail来审计EC2操作。\n\n请记住,在将此策略部署到生产账户之前,先在非生产环境中进行测试。始终根据您的特定用例和安全要求来定制IAM策略。",
      "why_correct": "D. 仅当使用 MFA 登录时,组成员在 us-east-1 才拥有 ec2:StopInstances 与 ec2:TerminateInstances 权限;在 us-east-1 内可执行任何其他 EC2 操作。",
      "why_wrong": "A 与策略条件不符;B 表述过于绝对(并非完全拒绝);C 错误地将敏感操作允许范围扩大到所有区域。"
    },
    "related_terms": [
      "IAM",
      "EC2",
      "MFA"
    ]
  },
  {
    "id": 430,
    "topic": "1",
    "question_en": "A manufacturing company has machine sensors that upload .csv files to an Amazon S3 bucket. These .csv files must be converted into images and must be made available as soon as possible for the automatic generation of graphical reports. The images become irrelevant after 1 month, but the .csv files must be kept to train machine learning (ML) models twice a year. The ML trainings and audits are planned weeks in advance. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Launch an Amazon EC2 Spot Instance that downloads the .csv files every hour, generates the image files, and uploads the images to the S3 bucket.",
      "B": "Design an AWS Lambda function that converts the .csv files into images and stores the images in the S3 bucket. Invoke the Lambda function when a .csv file is uploaded.",
      "C": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Glacier 1 day after they are uploaded. Expire the image files after 30 days.",
      "D": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 One Zone- Infrequent Access (S3 One Zone-IA) 1 day after they are uploaded. Expire the image files after 30 days",
      "E": "Create S3 Lifecycle rules for .csv files and image files in the S3 bucket. Transition the .csv files from S3 Standard to S3 Standard- Infrequent Access (S3 Standard-IA) 1 day after they are uploaded. Keep the image files in Reduced Redundancy Storage (RRS)."
    },
    "correct_answer": "BC",
    "vote_percentage": "89%",
    "question_cn": "一家制造公司拥有将 .csv 文件上传到 Amazon S3 存储桶的机器传感器。这些 .csv 文件必须转换为图像，并尽快提供这些图像以自动生成图形报告。这些图像在 1 个月后变得无关紧要，但 .csv 文件必须保留以每年两次训练机器学习 (ML) 模型。ML 训练和审计提前几周计划。哪种步骤组合将以最具成本效益的方式满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "启动一个 Amazon EC2 Spot 实例，该实例每小时下载 .csv 文件，生成图像文件，并将图像上传到 S3 存储桶。",
      "B": "设计一个 AWS Lambda 函数，将 .csv 文件转换为图像，并将图像存储在 S3 存储桶中。在上传 .csv 文件时调用 Lambda 函数。",
      "C": "为 S3 存储桶中的 .csv 文件和图像文件创建 S3 生命周期规则。在上传 .csv 文件 1 天后，将 .csv 文件从 S3 Standard 转换为 S3 Glacier。在 30 天后删除图像文件。",
      "D": "为 S3 存储桶中的 .csv 文件和图像文件创建 S3 生命周期规则。在上传 .csv 文件 1 天后，将 .csv 文件从 S3 Standard 转换为 S3 One Zone- Infrequent Access (S3 One Zone-IA)。在 30 天后删除图像文件。",
      "E": "为 S3 存储桶中的 .csv 文件和图像文件创建 S3 生命周期规则。在上传 .csv 文件 1 天后，将 .csv 文件从 S3 Standard 转换为 S3 Standard- Infrequent Access (S3 Standard-IA)。将图像文件保存在 Reduced Redundancy Storage (RRS) 中。"
    },
    "tags": [
      "S3",
      "Lambda",
      "S3 Lifecycle",
      "S3 Glacier",
      "S3 Standard-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 89%），解析仅供参考。】\n\n此题考查如何存储数据和优化成本。 使用 Lambda 将 CSV 转换成图片，可以实现自动化。S3 生命周期规则可以根据时间将数据迁移到不同的存储类型，以优化成本。",
      "why_correct": "选项 B 正确，使用 Lambda 转换文件可以实现自动化；选项 D 正确，S3 生命周期规则允许将文件移动到 S3 Standard-IA，降低存储成本。",
      "why_wrong": "选项 A 错误，Spot 实例不适合持续运行；选项 C 错误，S3 Glacier 不适合频繁访问；选项 E 错误， Reduced Redundancy Storage (RRS) 不推荐使用。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "S3 Glacier",
      "S3 Standard-IA",
      "S3 Lifecycle"
    ]
  },
  {
    "id": 431,
    "topic": "1",
    "question_en": "A company has developed a new video game as a web application. The application is in a three-tier architecture in a VPC with Amazon RDS for MySQL in the database layer. Several players will compete concurrently online. The game’s developers want to display a top-10 scoreboard in near-real time and offer the ability to stop and restore the game while preserving the current scores. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Set up an Amazon ElastiCache for Memcached cluster to cache the scores for the web application to display.",
      "B": "Set up an Amazon ElastiCache for Redis cluster to compute and cache the scores for the web application to display.",
      "C": "Place an Amazon CloudFront distribution in front of the web application to cache the scoreboard in a section of the application.",
      "D": "Create a read replica on Amazon RDS for MySQL to run queries to compute the scoreboard and serve the read trafic to the web application."
    },
    "correct_answer": "B",
    "vote_percentage": "96%",
    "question_cn": "一家公司开发了一款新的视频游戏，作为Web应用程序。该应用程序在VPC中使用三层架构，数据库层使用Amazon RDS for MySQL。几名玩家将同时在线竞争。游戏开发人员希望近乎实时地显示前10名排行榜，并提供暂停和恢复游戏的功能，同时保留当前分数。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "设置一个Amazon ElastiCache for Memcached集群，以缓存Web应用程序的分数供其显示。",
      "B": "设置一个Amazon ElastiCache for Redis集群，以计算和缓存Web应用程序的分数供其显示。",
      "C": "在Web应用程序前面放置一个Amazon CloudFront分发，以缓存应用程序一部分的排行榜。",
      "D": "在Amazon RDS for MySQL上创建一个只读副本，以运行查询来计算排行榜并将读取流量提供给Web应用程序。"
    },
    "tags": [
      "ElastiCache",
      "Redis",
      "Memcached",
      "CloudFront",
      "RDS",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 96%），解析仅供参考。】\n\n此题考察实时排行榜的实现。使用 ElastiCache for Redis 能够提供低延迟的读写操作，非常适合排行榜这种需要频繁更新的应用场景。",
      "why_correct": "选项 B 正确，ElastiCache for Redis 可以计算和缓存分数，提供实时更新和高效的读取性能。",
      "why_wrong": "选项 A 错误，Memcached 不支持排行榜计算；选项 C 错误，CloudFront 无法动态计算排行榜；选项 D 错误，RDS for MySQL 读取性能有限，不适合高并发读取排行榜。"
    },
    "related_terms": [
      "ElastiCache",
      "Redis",
      "CloudFront",
      "RDS",
      "MySQL",
      "Memcached"
    ]
  },
  {
    "id": 432,
    "topic": "1",
    "question_en": "An ecommerce company wants to use machine learning (ML) algorithms to build and train models. The company will use the models to visualize complex scenarios and to detect trends in customer data. The architecture team wants to integrate its ML models with a reporting platform to analyze the augmented data and use the data directly in its business intelligence dashboards. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Glue to create an ML transform to build and train models. Use Amazon OpenSearch Service to visualize the data.",
      "B": "Use Amazon SageMaker to build and train models. Use Amazon QuickSight to visualize the data.",
      "C": "Use a pre-built ML Amazon Machine Image (AMI) from the AWS Marketplace to build and train models. Use Amazon OpenSearch Service to visualize the data.",
      "D": "Use Amazon QuickSight to build and train models by using calculated fields. Use Amazon QuickSight to visualize the data."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司希望使用机器学习 (ML) 算法来构建和训练模型。该公司将使用这些模型来可视化复杂场景并检测客户数据中的趋势。架构团队希望将其 ML 模型与报告平台集成，以分析增强的数据，并在其商业智能仪表板中直接使用这些数据。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Glue 创建 ML 转换来构建和训练模型。使用 Amazon OpenSearch Service 可视化数据。",
      "B": "使用 Amazon SageMaker 构建和训练模型。使用 Amazon QuickSight 可视化数据。",
      "C": "使用 AWS Marketplace 中预构建的 ML Amazon Machine Image (AMI) 来构建和训练模型。使用 Amazon OpenSearch Service 可视化数据。",
      "D": "通过使用计算字段，使用 Amazon QuickSight 构建和训练模型。使用 Amazon QuickSight 可视化数据。"
    },
    "tags": [
      "SageMaker",
      "QuickSight",
      "Glue",
      "OpenSearch Service",
      "AWS Marketplace"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查机器学习解决方案。SageMaker 提供了构建和训练 ML 模型的完整解决方案，而 QuickSight 提供了方便的 BI 工具，可以满足需求。",
      "why_correct": "选项 B 正确，SageMaker 可以构建和训练模型，QuickSight 可以可视化数据。",
      "why_wrong": "选项 A 错误，Glue 并非专业的机器学习平台；选项 C 错误，AWS Marketplace 的 AMI 需要额外的配置；选项 D 错误，QuickSight 本身无法构建和训练模型。"
    },
    "related_terms": [
      "SageMaker",
      "QuickSight",
      "Glue",
      "OpenSearch Service",
      "AWS Marketplace"
    ]
  },
  {
    "id": 433,
    "topic": "1",
    "question_en": "A company is running its production and nonproduction environment workloads in multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to design a solution that will prevent the modification of cost usage tags. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a custom AWS Config rule to prevent tag modification except by authorized principals.",
      "B": "Create a custom trail in AWS CloudTrail to prevent tag modification.",
      "C": "Create a service control policy (SCP) to prevent tag modification except by authorized principals.",
      "D": "Create custom Amazon CloudWatch logs to prevent tag modification."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在多个 AWS 账户中运行其生产和非生产环境工作负载。这些账户位于 AWS Organizations 中的一个组织中。该公司需要设计一个解决方案，以防止修改成本使用标签。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个自定义 AWS Config 规则，以防止标签修改，除非由授权主体执行。",
      "B": "在 AWS CloudTrail 中创建一个自定义跟踪，以防止标签修改。",
      "C": "创建一个服务控制策略 (SCP)，以防止标签修改，除非由授权主体执行。",
      "D": "创建自定义 Amazon CloudWatch 日志以防止标签修改。"
    },
    "tags": [
      "Organizations",
      "SCP",
      "CloudTrail",
      "Config"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查防止修改成本使用标签。Service Control Policies (SCP) 允许集中控制组织中账户的权限，可以用于限制 IAM 权限，包括标签修改。",
      "why_correct": "选项 C 正确，SCP 可以用于限制标签修改，实现组织级别的管控。",
      "why_wrong": "选项 A 错误，自定义 AWS Config 规则不能直接阻止修改标签；选项 B 错误，CloudTrail 只能记录，不能阻止修改；选项 D 错误，CloudWatch Logs 也只能记录，不能阻止修改。"
    },
    "related_terms": [
      "SCP",
      "CloudTrail",
      "Organizations",
      "Config"
    ]
  },
  {
    "id": 434,
    "topic": "1",
    "question_en": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in anotherAWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?",
    "options_en": {
      "A": "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
      "B": "Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be launched when needed Configure DNS failover to point to the new disaster recovery Region's load balancer.",
      "C": "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be launched when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
      "D": "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger an AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer."
    },
    "correct_answer": "A",
    "vote_percentage": "52%",
    "question_cn": "一家公司在 AWS 云中托管其应用程序。该应用程序在 Auto Scaling 组中的 Application Load Balancer 后面的 Amazon EC2 实例上运行，并带有一个 Amazon DynamoDB 表。公司希望确保该应用程序可以在另一个 AWS 区域中使用，且停机时间最短。解决方案架构师应该怎么做才能以最少的停机时间满足这些要求？",
    "options_cn": {
      "A": "在灾难恢复区域中创建一个 Auto Scaling 组和一个负载均衡器。将 DynamoDB 表配置为全局表。配置 DNS 故障转移以指向新的灾难恢复区域的负载均衡器。",
      "B": "创建一个 AWS CloudFormation 模板，以创建 EC2 实例、负载均衡器和 DynamoDB 表，以便在需要时启动。配置 DNS 故障转移以指向新的灾难恢复区域的负载均衡器。",
      "C": "创建一个 AWS CloudFormation 模板，以创建 EC2 实例和负载均衡器，以便在需要时启动。将 DynamoDB 表配置为全局表。配置 DNS 故障转移以指向新的灾难恢复区域的负载均衡器。",
      "D": "在灾难恢复区域中创建一个 Auto Scaling 组和负载均衡器。将 DynamoDB 表配置为全局表。创建一个 Amazon CloudWatch 警报，以触发一个 AWS Lambda 函数，该函数更新 Amazon Route 53，指向灾难恢复负载均衡器。"
    },
    "tags": [
      "DynamoDB",
      "Auto Scaling",
      "Application Load Balancer",
      "Route 53",
      "CloudFormation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 52%），解析仅供参考。】\n\n此题考察多区域架构的灾难恢复。最佳实践是使用全局表，通过 Route 53 故障转移实现快速切换。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，使用 DynamoDB 全局表、Auto Scaling 和 DNS 故障转移，可以实现最少的停机时间。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，CloudFormation 创建需要时间，停机时间较长；选项 C 错误，CloudFormation 创建需要时间，停机时间较长；选项 D 错误，CloudWatch 警报和 Lambda 函数更新 Route 53 存在延迟。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "DynamoDB",
      "Auto Scaling",
      "Application Load Balancer",
      "Route 53",
      "CloudFormation"
    ]
  },
  {
    "id": 435,
    "topic": "1",
    "question_en": "A company needs to migrate a MySQL database from its on-premises data center to AWS within 2 weeks. The database is 20 TB in size. The company wants to complete the migration with minimal downtime. Which solution will migrate the database MOST cost-effectively?",
    "options_en": {
      "A": "Order an AWS Snowball Edge Storage Optimized device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes. Send the Snowball Edge device to AWS to finish the migration and continue the ongoing replication.",
      "B": "Order an AWS Snowmobile vehicle. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowmobile vehicle back to AWS to finish the migration and continue the ongoing replication.",
      "C": "Order an AWS Snowball Edge Compute Optimized with GPU device. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with ongoing changes. Send the Snowball device to AWS to finish the migration and continue the ongoing replication",
      "D": "Order a 1 GB dedicated AWS Direct Connect connection to establish a connection with the data center. Use AWS Database Migration Service (AWS DMS) with AWS Schema Conversion Tool (AWS SCT) to migrate the database with replication of ongoing changes."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司需要在 2 周内将其 MySQL 数据库从其本地数据中心迁移到 AWS。数据库大小为 20 TB。该公司希望以最小的停机时间完成迁移。哪个解决方案将以最具成本效益的方式迁移数据库？",
    "options_cn": {
      "A": "订购一个 AWS Snowball Edge 存储优化设备。使用 AWS Database Migration Service (AWS DMS) 和 AWS Schema Conversion Tool (AWS SCT) 迁移数据库，并复制持续更改。将 Snowball Edge 设备发送到 AWS 以完成迁移并继续进行持续复制。",
      "B": "订购一辆 AWS Snowmobile 车辆。使用 AWS Database Migration Service (AWS DMS) 和 AWS Schema Conversion Tool (AWS SCT) 迁移数据库，并复制持续更改。将 Snowmobile 车辆发送回 AWS 以完成迁移并继续进行持续复制。",
      "C": "订购一个带有 GPU 的 AWS Snowball Edge Compute Optimized 设备。使用 AWS Database Migration Service (AWS DMS) 和 AWS Schema Conversion Tool (AWS SCT) 迁移数据库，并复制持续更改。将 Snowball 设备发送到 AWS 以完成迁移并继续进行持续复制。",
      "D": "订购 1 GB 专用 AWS Direct Connect 连接以与数据中心建立连接。使用 AWS Database Migration Service (AWS DMS) 和 AWS Schema Conversion Tool (AWS SCT) 迁移数据库，并复制持续更改。"
    },
    "tags": [
      "DMS",
      "SCT",
      "Snowball Edge",
      "Snowmobile",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n考查在限定时间内，迁移大型数据库（20TB）到 AWS 的最具成本效益的解决方案。重点考察 AWS Snow 系列设备与数据迁移服务的结合使用。",
      "why_correct": "AWS Snowball Edge 存储优化设备是为大型数据集迁移设计的。结合 AWS DMS 和 AWS SCT，可以实现数据库的迁移和持续复制，从而最大限度地减少停机时间。此方案提供了一种成本效益高且便捷的物理数据传输方式，特别适合于数据库大小为 20 TB 的场景。",
      "why_wrong": "选项 B 中，AWS Snowmobile 车辆虽然适用于更大规模的数据迁移，但其成本较高，对于 20 TB 的数据库来说，性价比不高。选项 C 中，GPU 优化的 Snowball Edge 设备通常用于计算密集型任务，而非存储优化。选项 D 中，1 GB 的 AWS Direct Connect 连接带宽不足以在两周内迁移 20 TB 的数据库，且成本相对较高，迁移速度会非常慢，不符合题干中的时间要求。"
    },
    "related_terms": [
      "AWS Snowball Edge",
      "AWS Snowball Edge Compute Optimized",
      "AWS Snowmobile",
      "AWS Database Migration Service (AWS DMS)",
      "AWS Schema Conversion Tool (AWS SCT)",
      "AWS Direct Connect",
      "MySQL"
    ]
  },
  {
    "id": 436,
    "topic": "1",
    "question_en": "A company moved its on-premises PostgreSQL database to an Amazon RDS for PostgreSQL DB instance. The company successfully launched a new product. The workload on the database has increased. The company wants to accommodate the larger workload without adding infrastructure. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Buy reserved DB instances for the total workload. Make the Amazon RDS for PostgreSQL DB instance larger.",
      "B": "Make the Amazon RDS for PostgreSQL DB instance a Multi-AZ DB instance.",
      "C": "Buy reserved DB instances for the total workload. Add another Amazon RDS for PostgreSQL DB instance.",
      "D": "Make the Amazon RDS for PostgreSQL DB instance an on-demand DB instance."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司将其本地 PostgreSQL 数据库迁移到了 Amazon RDS for PostgreSQL 数据库实例。该公司成功发布了一个新产品。数据库上的工作负载增加了。该公司希望在不增加基础设施的情况下适应更大的工作负载。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "为总工作负载购买预留数据库实例。 将 Amazon RDS for PostgreSQL 数据库实例变得更大。",
      "B": "将 Amazon RDS for PostgreSQL 数据库实例设置为多可用区数据库实例。",
      "C": "为总工作负载购买预留数据库实例。添加另一个 Amazon RDS for PostgreSQL 数据库实例。",
      "D": "将 Amazon RDS for PostgreSQL 数据库实例设置为按需数据库实例。"
    },
    "tags": [
      "Amazon RDS",
      "RDS for PostgreSQL",
      "Reserved Instances",
      "Cost Optimization",
      "Performance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n考察 Amazon RDS 数据库实例的成本优化和性能扩展能力，以及预留实例的购买策略。与数据库实例类型选择、多可用区部署、按需实例和水平扩展等概念相关。",
      "why_correct": "为总工作负载购买预留数据库实例可以显著降低 RDS 实例的成本。同时，将 RDS for PostgreSQL 数据库实例变得更大可以提升单个实例的处理能力，从而在不增加基础设施的前提下适应更大的工作负载。这种方案兼顾了成本效益和性能提升。",
      "why_wrong": "B 选项将实例设置为多可用区实例，虽然提高了可用性和可靠性，但主要目的是实现故障转移和数据冗余，并不能直接解决负载增加导致的性能问题，也无法实现成本优化。C 选项购买预留实例并添加另一个实例实现了水平扩展，可以提高性能，但会增加基础设施成本，不符合题目中“最具成本效益”的要求。 D 选项使用按需实例，按需实例比预留实例更贵，不满足“最具成本效益”的要求，并且按需实例本身并不能提升性能。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "RDS for PostgreSQL",
      "Reserved Instances",
      "On-Demand Instances"
    ]
  },
  {
    "id": 437,
    "topic": "1",
    "question_en": "A company operates an ecommerce website on Amazon EC2 instances behind an Application Load Balancer (ALB) in an Auto Scaling group. The site is experiencing performance issues related to a high request rate from illegitimate external systems with changing IP addresses. The security team is worried about potential DDoS attacks against the website. The company must block the illegitimate incoming requests in a way that has a minimal impact on legitimate users. What should a solutions architect recommend?",
    "options_en": {
      "A": "Deploy Amazon Inspector and associate it with the ALB.",
      "B": "Deploy AWS WAF, associate it with the ALB, and configure a rate-limiting rule.",
      "C": "Deploy rules to the network ACLs associated with the ALB to block the incomingtrafic.",
      "D": "Deploy Amazon GuardDuty and enable rate-limiting protection when configuring GuardDuty."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司在其 Application Load Balancer (ALB) 之后，在 Auto Scaling 组中的 Amazon EC2 实例上运营一个电子商务网站。该网站正经历与来自具有变化 IP 地址的非法外部系统的高请求速率相关的性能问题。安全团队担心针对该网站的潜在 DDoS 攻击。该公司必须以对合法用户影响最小的方式阻止非法传入请求。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "部署 Amazon Inspector 并将其与 ALB 关联。",
      "B": "部署 AWS WAF，将其与 ALB 关联，并配置一个速率限制规则。",
      "C": "将规则部署到与 ALB 关联的网络 ACL，以阻止传入流量。",
      "D": "部署 Amazon GuardDuty 并在配置 GuardDuty 时启用速率限制保护。"
    },
    "tags": [
      "AWS WAF",
      "Application Load Balancer",
      "Rate limiting",
      "DDoS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n考查如何应对 DDoS 攻击；与 AWS WAF、ALB、速率限制等相关。要求阻止非法请求，同时最小化对合法用户的干扰。",
      "why_correct": "AWS WAF 是一种 Web 应用程序防火墙，可以保护 Web 应用程序免受常见的 Web 攻击，如 DDoS 攻击。通过将 AWS WAF 与 ALB 关联，可以拦截并检查进入 ALB 的流量。配置速率限制规则允许限制特定 IP 地址的请求速率，从而有效阻止来自恶意源的过高请求速率，减轻 DDoS 攻击，同时最大程度地减少对合法用户的潜在影响。这种方案结合了攻击检测和缓解措施。",
      "why_wrong": "选项 A 错误，因为 Amazon Inspector 主要用于评估 AWS 资源的安全性，并不具备直接阻止请求的能力，无法有效应对 DDoS 攻击。选项 C 错误，因为网络 ACL（Network ACL）是在子网级别进行流量控制的，其配置较为粗粒度，难以针对特定 IP 地址进行细粒度的速率限制，且配置不当可能导致合法用户流量也被阻止。选项 D 错误，因为 GuardDuty 主要用于威胁检测，虽然可以检测潜在的恶意活动，但其不具备直接阻止请求的能力，而速率限制保护是 WAF 的功能。GuardDuty 无法直接与 ALB 关联以实现速率限制，从而无法有效阻止 DDoS 攻击。"
    },
    "related_terms": [
      "AWS WAF",
      "ALB",
      "Application Load Balancer",
      "DDoS",
      "Amazon Inspector",
      "Network ACL",
      "Amazon GuardDuty",
      "EC2"
    ]
  },
  {
    "id": 438,
    "topic": "1",
    "question_en": "A company wants to share accounting data with an external auditor. The data is stored in an Amazon RDS DB instance that resides in a private subnet. The auditor has its own AWS account and requires its own copy of the database. What is the MOST secure way for the company to share the database with the auditor?",
    "options_en": {
      "A": "Create a read replica of the database. Configure IAM standard database authentication to grant the auditor access.",
      "B": "Export the database contents to text files. Store the files in an Amazon S3 bucket. Create a new IAM user for the auditor. Grant the user access to the S3 bucket.",
      "C": "Copy a snapshot of the database to an Amazon S3 bucket. Create an IAM user. Share the user's keys with the auditor to grant access to the object in the S3 bucket.",
      "D": "Create an encrypted snapshot of the database. Share the snapshot with the auditor. Allow access to the AWS Key Management Service (AWS KMS) encryption key."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望与外部审计员共享会计数据。数据存储在位于私有子网中的 Amazon RDS 数据库实例中。审计员有自己的 AWS 账户，并且需要自己的数据库副本。公司与审计员共享数据库的最安全方式是什么？",
    "options_cn": {
      "A": "创建数据库的只读副本。配置 IAM 标准数据库身份验证以授予审计员访问权限。",
      "B": "将数据库内容导出到文本文件。将文件存储在 Amazon S3 存储桶中。为审计员创建一个新的 IAM 用户。授予该用户访问 S3 存储桶的权限。",
      "C": "将数据库的快照复制到 Amazon S3 存储桶。创建一个 IAM 用户。与审计员共享用户的密钥，以授予其访问 S3 存储桶中对象的权限。",
      "D": "创建数据库的加密快照。与审计员共享快照。允许访问 AWS Key Management Service (AWS KMS) 加密密钥。"
    },
    "tags": [
      "Amazon RDS",
      "RDS",
      "Database Snapshot",
      "AWS KMS",
      "IAM",
      "Amazon S3",
      "Database Encryption",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查共享 RDS 数据库数据的安全方法，以及 RDS 快照、数据库加密、IAM 授权和密钥管理之间的关系。重点关注安全性和数据保护。",
      "why_correct": "选项 D 提供了最安全的方案。它创建了数据库的加密快照，确保数据在传输和存储过程中都受到保护。通过允许审计员访问 AWS KMS 加密密钥，审计员能够解密快照并访问数据库内容。这种方法既保证了数据的机密性，又满足了审计员需要独立数据库副本的需求。",
      "why_wrong": "选项 A 存在安全隐患，使用 IAM 标准数据库身份验证可能会降低安全性，并且 RDS 只读副本可能无法满足审计员需要独立数据库副本的要求。选项 B 通过将数据库内容导出为文本文件并存储在 Amazon S3 中，降低了数据的安全性；文本文件容易泄露，而且未加密存储。选项 C 存在安全风险，共享 IAM 用户密钥给审计员是不安全的做法，违反了最小权限原则，且快照未加密，数据在传输过程中可能被窃取。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS",
      "AWS KMS",
      "IAM",
      "Amazon S3",
      "Database Snapshot",
      "Database Encryption",
      "Security",
      "Encryption Key"
    ]
  },
  {
    "id": 439,
    "topic": "1",
    "question_en": "A solutions architect configured a VPC that has a small range of IP addresses. The number of Amazon EC2 instances that are in the VPC is increasing, and there is an insuficient number of IP addresses for future workloads. Which solution resolves this issue with the LEAST operational overhead?",
    "options_en": {
      "A": "Add an additional IPv4 CIDR block to increase the number of IP addresses and create additional subnets in the VPC. Create new resources in the new subnets by using the new CIDR.",
      "B": "Create a second VPC with additional subnets. Use a peering connection to connect the second VPC with the first VPC Update the routes and create new resources in the subnets of the second VPC.",
      "C": "Use AWS Transit Gateway to add a transit gateway and connect a second VPC with the first VPUpdate the routes of the transit gateway and VPCs. Create new resources in the subnets of the second VPC.",
      "D": "Create a second VPC. Create a Site-to-Site VPN connection between the first VPC and the second VPC by using a VPN-hosted solution on Amazon EC2 and a virtual private gateway. Update the route between VPCs to the trafic through the VPN. Create new resources in the subnets of the second VPC."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师配置了一个具有小范围 IP 地址的 VPC。VPC 中 Amazon EC2 实例的数量正在增加，并且没有足够的 IP 地址来满足未来的工作负载。哪种解决方案以最小的运营开销解决了这个问题？",
    "options_cn": {
      "A": "添加一个额外的 IPv4 CIDR 块以增加 IP 地址的数量，并在 VPC 中创建额外的子网。使用新的 CIDR 在新的子网中创建新资源。",
      "B": "创建一个具有额外子网的第二个 VPC。使用对等连接将第二个 VPC 与第一个 VPC 连接起来。更新路由并在第二个 VPC 的子网中创建新资源。",
      "C": "使用 AWS Transit Gateway 添加一个 transit gateway，并将第二个 VPC 与第一个 VPC 连接起来。更新 transit gateway 和 VPC 的路由。在第二个 VPC 的子网中创建新资源。",
      "D": "创建第二个 VPC。通过使用 Amazon EC2 上的 VPN 托管解决方案和虚拟专用网关，在第一个 VPC 和第二个 VPC 之间创建 Site-to-Site VPN 连接。更新 VPC 之间的路由以通过 VPN 传输流量。在第二个 VPC 的子网中创建新资源。"
    },
    "tags": [
      "VPC",
      "CIDR",
      "Subnet",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 VPC 的 IP 地址扩展方案；与 VPC 设计、子网规划、CIDR 块管理相关。",
      "why_correct": "选项 A 通过增加新的 IPv4 CIDR 块来扩展 VPC 的 IP 地址空间，并在新的子网中部署资源。这是最直接、最简单的解决方案，能够直接增加可用的 IP 数量，并且运营开销最小。它不需要引入额外的复杂性，如 VPC 对等连接、Transit Gateway 或 VPN 连接。",
      "why_wrong": "选项 B 涉及创建第二个 VPC，并使用 VPC 对等连接。虽然可行，但管理多个 VPC 会增加运营复杂性。而且 VPC 对等连接没有 NAT Gateway, 如果跨 VPC 的通信需要访问 Internet，会变得复杂。选项 C 使用 AWS Transit Gateway，进一步增加了复杂性，虽然 Transit Gateway 提供了更强大的网络管理和连接能力，但对于简单的 IP 地址扩展需求来说，过度设计了。选项 D 引入了 VPN 连接，增加了配置和维护的复杂性，并且 VPN 会引入额外的延迟，不如直接扩展 VPC 内部的 IP 地址池高效。"
    },
    "related_terms": [
      "VPC",
      "CIDR",
      "EC2",
      "VPC Peering",
      "AWS Transit Gateway",
      "VPN",
      "Site-to-Site VPN",
      "NAT Gateway",
      "IPv4",
      "Subnet"
    ]
  },
  {
    "id": 440,
    "topic": "1",
    "question_en": "A company used an Amazon RDS for MySQL DB instance during application testing. Before terminating the DB instance at the end of the test cycle, a solutions architect created two backups. The solutions architect created the first backup by using the mysqldump utility to create a database dump. The solutions architect created the second backup by enabling the final DB snapshot option on RDS termination. The company is now planning for a new test cycle and wants to create a new DB instance from the most recent backup. The company has chosen a MySQL-compatible edition ofAmazon Aurora to host the DB instance. Which solutions will create the new DB instance? (Choose two.)",
    "options_en": {
      "A": "Import the RDS snapshot directly into Aurora.",
      "B": "Upload the RDS snapshot to Amazon S3. Then import the RDS snapshot into Aurora.",
      "C": "Upload the database dump to Amazon S3. Then import the database dump into Aurora.",
      "D": "Use AWS Database Migration Service (AWS DMS) to import the RDS snapshot into Aurora",
      "E": "Upload the database dump to Amazon S3. Then use AWS Database Migration Service (AWS DMS) to import the database dump into Aurora."
    },
    "correct_answer": "AC",
    "vote_percentage": "80%",
    "question_cn": "一家公司在应用程序测试期间使用了 Amazon RDS for MySQL 数据库实例。在测试周期结束时终止数据库实例之前，一位解决方案架构师创建了两个备份。解决方案架构师使用 mysqldump 实用程序创建数据库转储来创建第一个备份。解决方案架构师通过在 RDS 终止时启用最终数据库快照选项来创建第二个备份。该公司现在计划进行新的测试周期，并希望从最新的备份中创建新的数据库实例。该公司选择了与 MySQL 兼容的 Amazon Aurora 版本来托管数据库实例。哪些解决方案将创建新的数据库实例？（选择两项。）",
    "options_cn": {
      "A": "将 RDS 快照直接导入 Aurora。",
      "B": "将 RDS 快照上传到 Amazon S3，然后将 RDS 快照导入 Aurora。",
      "C": "将数据库转储上传到 Amazon S3，然后将数据库转储导入 Aurora。",
      "D": "使用 AWS Database Migration Service (AWS DMS) 将 RDS 快照导入 Aurora。",
      "E": "将数据库转储上传到 Amazon S3，然后使用 AWS Database Migration Service (AWS DMS) 将数据库转储导入 Aurora。"
    },
    "tags": [
      "Amazon RDS",
      "Amazon Aurora",
      "Database Migration",
      "mysqldump",
      "AWS DMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 80%），解析仅供参考。】\n\n考查从 RDS for MySQL 备份恢复到 Aurora MySQL 的方法，包括直接导入 RDS 快照和通过 S3 导入数据库转储。",
      "why_correct": "选项 A 正确，可以直接将 RDS 快照导入 Aurora。Aurora 支持从 RDS 快照创建数据库实例，这是直接且有效的方法。选项 C 正确，可以使用 `mysqldump` 创建的数据库转储文件，将其上传到 S3 后，再从 S3 导入 Aurora。这适用于从数据库转储恢复数据。",
      "why_wrong": "选项 B 错误，RDS 快照无法直接上传到 S3。 RDS 快照是 AWS 内部管理的对象，不能像普通文件一样上传。选项 D 错误，AWS DMS 无法直接从 RDS 快照迁移。DMS 主要用于数据库迁移，而非从快照恢复。选项 E 错误，DMS 无法直接处理数据库转储文件。DMS 需要连接到源数据库并进行数据迁移，而不是导入数据库转储。"
    },
    "related_terms": [
      "Amazon RDS for MySQL",
      "Aurora MySQL",
      "mysqldump",
      "Amazon S3",
      "AWS Database Migration Service (AWS DMS)",
      "RDS snapshot"
    ]
  },
  {
    "id": 441,
    "topic": "1",
    "question_en": "A company hosts a multi-tier web application on Amazon Linux Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company observes that the Auto Scaling group launches more On-Demand Instances when the application's end users access high volumes of static web content. The company wants to optimize cost. What should a solutions architect do to redesign the application MOST cost-effectively?",
    "options_en": {
      "A": "Update the Auto Scaling group to use Reserved Instances instead of On-Demand Instances.",
      "B": "Update the Auto Scaling group to scale by launching Spot Instances instead of On-Demand Instances.",
      "C": "Create an Amazon CloudFront distribution to host the static web contents from an Amazon S3 bucket.",
      "D": "Create an AWS Lambda function behind an Amazon API Gateway API to host the static website contents."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在Application Load Balancer后面的Amazon Linux Amazon EC2实例上托管一个多层Web应用程序。这些实例在多个可用区中的Auto Scaling组中运行。该公司观察到，当应用程序的最终用户访问大量静态Web内容时，Auto Scaling组会启动更多按需实例。该公司希望优化成本。解决方案架构师应该怎么做才能以最具成本效益的方式重新设计应用程序？",
    "options_cn": {
      "A": "更新Auto Scaling组以使用Reserved Instances而不是On-Demand Instances。",
      "B": "更新Auto Scaling组以通过启动Spot Instances而不是On-Demand Instances进行扩展。",
      "C": "创建一个Amazon CloudFront分布，以从Amazon S3存储桶托管静态Web内容。",
      "D": "在Amazon API Gateway API后面创建一个AWS Lambda函数来托管静态网站内容。"
    },
    "tags": [
      "Amazon S3",
      "Amazon CloudFront",
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Web Application"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察 Web 应用程序的成本优化，以及使用 CloudFront 和 S3 托管静态内容的方案。与 Auto Scaling、EC2 实例类型选择、负载均衡等相关。",
      "why_correct": "创建一个 Amazon CloudFront 分布，将静态 Web 内容存储在 Amazon S3 存储桶中，可以有效降低成本。CloudFront 是一个内容分发网络（CDN），它将内容缓存在全球边缘站点，从而减少了源服务器（EC2 实例）的负载。这意味着当最终用户访问静态内容时，他们从 CloudFront 的边缘站点获取内容，而不是从 EC2 实例获取，从而减少了对 EC2 实例的需求，降低了 Auto Scaling 组启动更多 On-Demand 实例的可能性，最终达到成本优化的目的。这种架构也提高了网站的性能，因为内容离最终用户更近。",
      "why_wrong": "A. 使用 Reserved Instances 可以降低 EC2 实例的成本，但它不能直接解决因静态内容访问导致的 Auto Scaling 组启动更多 On-Demand 实例的问题。Reserved Instances 仅适用于 EC2 实例的计算成本，不能减少 EC2 实例本身的需求。\nB. 使用 Spot Instances 可以进一步降低 EC2 实例的成本，但是 Spot 实例的可用性是不可预测的，不适用于所有工作负载，且并不能解决因静态内容访问导致的问题，反而可能因为实例中断影响用户体验。\nD. 使用 Lambda 和 API Gateway 托管静态 Web 内容，通常比 S3 和 CloudFront 成本更高，并且 API Gateway 的调用费用会增加成本。虽然 Lambda 可以按需扩展，但它不适合托管大量的静态内容。此外，这也不是最佳实践，S3 和 CloudFront 搭配是托管静态网站的最佳解决方案。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "AWS Lambda",
      "Amazon API Gateway",
      "On-Demand Instances",
      "Reserved Instances",
      "Spot Instances"
    ]
  },
  {
    "id": 442,
    "topic": "1",
    "question_en": "A company stores several petabytes of data across multiple AWS accounts. The company uses AWS Lake Formation to manage its data lake. The company's data science team wants to securely share selective data from its accounts with the company's engineering team for analytical purposes. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Copy the required data to a common account. Create an IAM access role in that account. Grant access by specifying a permission policy that includes users from the engineering team accounts as trusted entities.",
      "B": "Use the Lake Formation permissions Grant command in each account where the data is stored to allow the required engineering team users to access the data.",
      "C": "Use AWS Data Exchange to privately publish the required data to the required engineering team accounts.",
      "D": "Use Lake Formation tag-based access control to authorize and grant cross-account permissions for the required data to the engineering team accounts."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司跨多个 AWS 账户存储了数 PB 的数据。该公司使用 AWS Lake Formation 来管理其数据湖。该公司的数据科学团队希望安全地与公司的工程团队共享其账户中的选择性数据，用于分析目的。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将所需数据复制到一个公共账户。在该账户中创建 IAM 访问角色。通过指定一个包括工程团队账户中的用户作为受信任实体的权限策略来授予访问权限。",
      "B": "在存储数据的每个账户中使用 Lake Formation 权限 Grant 命令，以允许所需的工程团队用户访问数据。",
      "C": "使用 AWS Data Exchange 私下将所需数据发布到所需的工程团队账户。",
      "D": "使用 Lake Formation 基于标签的访问控制，为工程团队账户授权并授予对所需数据的跨账户权限。"
    },
    "tags": [
      "AWS Lake Formation",
      "IAM",
      "Cross-account access",
      "Data sharing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查跨账户数据共享的解决方案选择，以及 Lake Formation 的使用，涉及 IAM 角色、权限管理、以及数据安全和运营开销的权衡。正确答案侧重于使用 Lake Formation 简化跨账户数据访问，降低管理负担。",
      "why_correct": "选项 D 使用 Lake Formation 基于标签的访问控制，为工程团队账户授权并授予对所需数据的跨账户权限。Lake Formation 允许集中管理数据访问，通过使用标签，可以精细控制哪些数据可以被哪些用户访问。这种方法减少了管理开销，因为访问控制可以在 Lake Formation 中集中管理，而不是在每个账户中单独配置。此外，它提供了更好的数据治理和安全控制，因为所有数据访问都通过 Lake Formation 集中处理。",
      "why_wrong": "选项 A 涉及将数据复制到公共账户，这会增加数据泄露的风险，并违反了安全最佳实践。此外，管理公共账户中的 IAM 角色和权限会带来额外的运营开销。选项 B 要求在存储数据的每个账户中使用 Lake Formation 权限 Grant 命令，这会增加管理复杂性，特别是在跨多个账户共享数据时。手动管理每个账户的权限容易出错，且难以维护。选项 C 使用 AWS Data Exchange 私下共享数据，虽然提供了安全的数据共享机制，但它更适用于将数据发布给外部客户或合作伙伴，而不是内部团队。对于内部团队的数据共享，Lake Formation 提供了更有效和更精细的控制。"
    },
    "related_terms": [
      "AWS Lake Formation",
      "IAM",
      "AWS Data Exchange",
      "Cross-account access",
      "Data sharing",
      "Permissions",
      "Account",
      "Data lake"
    ]
  },
  {
    "id": 443,
    "topic": "1",
    "question_en": "A company wants to host a scalable web application on AWS. The application will be accessed by users from different geographic regions of the world. Application users will be able to download and upload unique data up to gigabytes in size. The development team wants a cost- effective solution to minimize upload and download latency and maximize performance. What should a solutions architect do to accomplish this?",
    "options_en": {
      "A": "Use Amazon S3 with Transfer Acceleration to host the application.",
      "B": "Use Amazon S3 with CacheControl headers to host the application.",
      "C": "Use Amazon EC2 with Auto Scaling and Amazon CloudFront to host the application.",
      "D": "Use Amazon EC2 with Auto Scaling and Amazon ElastiCache to host the application."
    },
    "correct_answer": "A",
    "vote_percentage": "65%",
    "question_cn": "一家公司希望在 AWS 上托管一个可扩展的 Web 应用程序。该应用程序将由来自世界不同地理区域的用户访问。应用程序用户将能够下载和上传大小高达千兆字节的唯一数据。开发团队希望有一个经济高效的解决方案，以最大限度地减少上传和下载延迟并最大限度地提高性能。解决方案架构师应该怎么做才能实现这一目标？",
    "options_cn": {
      "A": "使用 Amazon S3 以及 S3 Transfer Acceleration 来托管应用程序。",
      "B": "使用 Amazon S3 以及 CacheControl 标头来托管应用程序。",
      "C": "使用 Amazon EC2 以及 Auto Scaling 和 Amazon CloudFront 来托管应用程序。",
      "D": "使用 Amazon EC2 以及 Auto Scaling 和 Amazon ElastiCache 来托管应用程序。"
    },
    "tags": [
      "Amazon S3",
      "S3 Transfer Acceleration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 65%），解析仅供参考。】\n\n考查 Web 应用程序的架构设计，以及如何通过优化数据传输来提高性能、降低延迟。主要涉及 Amazon S3 的使用，以及 S3 Transfer Acceleration 的应用场景。也需要考虑数据传输的效率和成本，并与 CloudFront 等服务进行对比。",
      "why_correct": "使用 Amazon S3 结合 S3 Transfer Acceleration 是最合适的方案。S3 Transfer Acceleration 通过利用 Amazon 全球分布的边缘站点来加速数据传输，优化了上传和下载过程。它通过将数据传输到最近的边缘站点，然后通过 Amazon 优化网络传输到 S3 存储桶，从而减少延迟并提高性能。这种方案特别适合于全球用户访问、大文件上传下载的场景，符合题目需求。",
      "why_wrong": "选项 B 仅使用 CacheControl 标头，无法有效解决全球用户的上传下载延迟问题。CacheControl 标头主要用于控制浏览器缓存，虽然可以优化客户端的下载速度，但无法加速数据上传，并且对于大文件传输的优化效果有限。\n选项 C 使用 Amazon EC2、Auto Scaling 和 Amazon CloudFront，虽然 CloudFront 可以加速内容的下载，但 EC2 实例需要管理和维护，成本相对较高。同时，EC2 实例需要手动配置和管理上传下载逻辑，不如 S3 提供的原生功能便捷，且无法完全解决上传延迟问题。\n选项 D 使用 Amazon EC2、Auto Scaling 和 Amazon ElastiCache。ElastiCache 主要用于缓存，优化读取性能，对于解决大文件上传下载的延迟问题无直接帮助。与选项 C 类似，EC2 的维护成本较高，且无法有效解决上传延迟问题。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Transfer Acceleration",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon CloudFront",
      "Amazon ElastiCache",
      "CacheControl"
    ]
  },
  {
    "id": 444,
    "topic": "1",
    "question_en": "A company has hired a solutions architect to design a reliable architecture for its application. The application consists of one Amazon RDS DB instance and two manually provisioned Amazon EC2 instances that run web servers. The EC2 instances are located in a single Availability Zone. An employee recently deleted the DB instance, and the application was unavailable for 24 hours as a result. The company is concerned with the overall reliability of its environment. What should the solutions architect do to maximize reliability of the application's infrastructure?",
    "options_en": {
      "A": "Delete one EC2 instance and enable termination protection on the other EC2 instance. Update the DB instance to be Multi-AZ, and enable deletion protection.",
      "B": "Update the DB instance to be Multi-AZ, and enable deletion protection. Place the EC2 instances behind an Application Load Balancer, and run them in an EC2 Auto Scaling group across multiple Availability Zones.",
      "C": "Create an additional DB instance along with an Amazon API Gateway and an AWS Lambda function. Configure the application to invoke the Lambda function through API Gateway. Have the Lambda function write the data to the two DB instances.",
      "D": "Place the EC2 instances in an EC2 Auto Scaling group that has multiple subnets located in multiple Availability Zones. Use Spot Instances instead of On-Demand Instances. Set up Amazon CloudWatch alarms to monitor the health of the instances Update the DB instance to be Multi-AZ, and enable deletion protection."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司聘请了一位解决方案架构师来为其应用程序设计可靠的架构。 该应用程序由一个 Amazon RDS 数据库实例和两个手动配置的运行 Web 服务器的 Amazon EC2 实例组成。 EC2 实例位于单个可用区中。 一名员工最近删除了数据库实例，导致应用程序停用 24 小时。 公司担心其环境的整体可靠性。 解决方案架构师应该怎么做才能最大限度地提高应用程序基础设施的可靠性？",
    "options_cn": {
      "A": "删除一个 EC2 实例，并在另一个 EC2 实例上启用终止保护。 将数据库实例更新为 Multi-AZ，并启用删除保护。",
      "B": "将数据库实例更新为 Multi-AZ，并启用删除保护。 将 EC2 实例放置在 Application Load Balancer 后面，并在多个可用区中运行 EC2 Auto Scaling 组。",
      "C": "创建另一个数据库实例以及一个 Amazon API Gateway 和一个 AWS Lambda 函数。 将应用程序配置为通过 API Gateway 调用 Lambda 函数。 让 Lambda 函数将数据写入两个数据库实例。",
      "D": "将 EC2 实例放置在具有多个子网的 EC2 Auto Scaling 组中，这些子网位于多个可用区中。 使用 Spot 实例而不是按需实例。 设置 Amazon CloudWatch 警报以监控实例的运行状况。 将数据库实例更新为 Multi-AZ，并启用删除保护。"
    },
    "tags": [
      "Amazon RDS",
      "Multi-AZ",
      "Amazon EC2",
      "EC2 Auto Scaling",
      "Application Load Balancer",
      "Availability Zone",
      "Delete Protection"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了如何通过架构设计来提高应用程序的可靠性，特别是针对数据库的灾备和Web服务器的高可用。与 RDS Multi-AZ、EC2 Auto Scaling、负载均衡等服务的功能和应用场景相关。",
      "why_correct": "将 RDS 数据库实例配置为 Multi-AZ 可以提供数据库的容错能力。如果主数据库实例发生故障，RDS 会自动在备用数据库实例上进行故障转移，从而最大程度地减少停机时间。启用删除保护可以防止意外删除数据库实例。将 EC2 实例放置在 Application Load Balancer (ALB) 后面可以实现流量的负载均衡和高可用性。在多个 Availability Zone 中运行 EC2 Auto Scaling 组可以确保即使单个 Availability Zone 发生故障，应用程序仍然可用，从而提高了整体应用程序的可靠性。",
      "why_wrong": "选项 A 仅在单个可用区中操作 EC2 实例，这无法提供高可用性。终止保护无法防止意外删除数据库。选项 C 引入了额外的复杂性，包括 API Gateway、Lambda 函数和两个数据库实例，增加了维护成本，且在数据同步上可能存在问题，并不能直接解决 Web 服务器的高可用性问题。选项 D 使用 Spot 实例虽然可以降低成本，但其不确定性不适用于需要高可用性的场景，且在 EC2 实例的部署上，仅提供了 Auto Scaling 和 CloudWatch 监控，没有提供 ALB，因此无法直接实现高可用性，而数据库的可用性仅体现在配置了 Multi-AZ 和删除保护。"
    },
    "related_terms": [
      "Amazon RDS",
      "Multi-AZ",
      "EC2",
      "Application Load Balancer",
      "Availability Zone",
      "EC2 Auto Scaling",
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon CloudWatch",
      "Delete Protection",
      "Spot Instance"
    ]
  },
  {
    "id": 445,
    "topic": "1",
    "question_en": "A company is storing 700 terabytes of data on a large network-attached storage (NAS) system in its corporate data center. The company has a hybrid environment with a 10 Gbps AWS Direct Connect connection. After an audit from a regulator, the company has 90 days to move the data to the cloud. The company needs to move the data eficiently and without disruption. The company still needs to be able to access and update the data during the transfer window. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an AWS DataSync agent in the corporate data center. Create a data transfer task Start the transfer to an Amazon S3 bucket.",
      "B": "Back up the data to AWS Snowball Edge Storage Optimized devices. Ship the devices to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system.",
      "C": "Use rsync to copy the data directly from local storage to a designated Amazon S3 bucket over the Direct Connect connection.",
      "D": "Back up the data on tapes. Ship the tapes to an AWS data center. Mount a target Amazon S3 bucket on the on-premises file system."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司将其 700 TB 的数据存储在其公司数据中心的大型网络附加存储 (NAS) 系统上。该公司拥有混合环境，具有 10 Gbps 的 AWS Direct Connect 连接。在监管机构的审计之后，该公司有 90 天的时间将数据迁移到云端。该公司需要高效地迁移数据且不中断。该公司在传输窗口期间仍然需要能够访问和更新数据。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在公司数据中心创建 AWS DataSync 代理。创建一个数据传输任务。开始传输到 Amazon S3 存储桶。",
      "B": "将数据备份到 AWS Snowball Edge 存储优化设备。将设备运送到 AWS 数据中心。在本地文件系统上挂载一个目标 Amazon S3 存储桶。",
      "C": "使用 rsync 通过 Direct Connect 连接将数据直接从本地存储复制到指定的 Amazon S3 存储桶。",
      "D": "将数据备份到磁带上。将磁带运送到 AWS 数据中心。在本地文件系统上挂载一个目标 Amazon S3 存储桶。"
    },
    "tags": [
      "AWS DataSync",
      "Amazon S3",
      "AWS Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查大规模数据迁移方案，侧重考查数据迁移效率、持续访问能力以及对现有基础设施的影响。与 AWS DataSync、Amazon S3、AWS Direct Connect 及数据同步技术相关。",
      "why_correct": "AWS DataSync 专门用于将本地数据快速、安全地迁移到 AWS 服务，例如 Amazon S3。它通过部署 DataSync 代理到本地环境，并通过网络复制数据。DataSync 支持持续数据传输，在数据迁移期间可以保持对数据的访问和更新。结合 10 Gbps 的 Direct Connect 连接，DataSync 能够高效地完成 700 TB 数据的迁移任务，并在 90 天的时间窗口内满足监管要求。",
      "why_wrong": "选项 B 使用 AWS Snowball Edge，虽然适用于大规模数据迁移，但涉及到物理设备运输，耗时较长，无法在 90 天内完成任务，且不支持持续访问和更新数据。选项 C 使用 rsync 通过 Direct Connect 进行数据迁移，这种方式虽然可行，但对于 700 TB 的数据量，效率较低，而且 rsync 在网络中断时需要重新传输，对网络稳定性要求高，难以满足高效迁移的需求。选项 D 涉及磁带备份，物理运输耗时更长，且不支持在数据迁移过程中对数据的访问和更新，完全无法满足题目的需求。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon S3",
      "AWS Direct Connect",
      "Snowball Edge",
      "rsync"
    ]
  },
  {
    "id": 446,
    "topic": "1",
    "question_en": "A company stores data in PDF format in an Amazon S3 bucket. The company must follow a legal requirement to retain all new and existing data in Amazon S3 for 7 years. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Turn on the S3 Versioning feature for the S3 bucket. Configure S3 Lifecycle to delete the data after 7 years. Configure multi-factor authentication (MFA) delete for all S3 objects.",
      "B": "Turn on S3 Object Lock with governance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.",
      "C": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Recopy all existing objects to bring the existing data into compliance.",
      "D": "Turn on S3 Object Lock with compliance retention mode for the S3 bucket. Set the retention period to expire after 7 years. Use S3 Batch Operations to bring the existing data into compliance."
    },
    "correct_answer": "D",
    "vote_percentage": "83%",
    "question_cn": "一家公司将 PDF 格式的数据存储在 Amazon S3 存储桶中。该公司必须遵守法律要求，将 Amazon S3 中所有新的和现有数据保留 7 年。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "为 S3 存储桶打开 S3 版本控制功能。配置 S3 生命周期以在 7 年后删除数据。为所有 S3 对象配置多因素身份验证 (MFA) 删除。",
      "B": "为 S3 存储桶打开 S3 对象锁定，并使用治理保留模式。将保留期设置为 7 年后到期。重新复制所有现有对象以使现有数据符合要求。",
      "C": "为 S3 存储桶打开 S3 对象锁定，并使用合规保留模式。将保留期设置为 7 年后到期。重新复制所有现有对象以使现有数据符合要求。",
      "D": "为 S3 存储桶打开 S3 对象锁定，并使用合规保留模式。将保留期设置为 7 年后到期。使用 S3 批量操作使现有数据符合要求。"
    },
    "tags": [
      "Amazon S3",
      "S3 Object Lock",
      "S3 Versioning",
      "S3 Lifecycle",
      "S3 Batch Operations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 83%），解析仅供参考。】\n\n考查 Amazon S3 对象锁定的合规保留模式，以及批量操作满足数据保留合规性的解决方案。",
      "why_correct": "选项 D 提供了最合适的解决方案。它使用了 S3 对象锁定的合规保留模式，确保数据在 7 年内不可被删除或修改。此外，通过 S3 批量操作可以高效地将现有数据应用相同的保留策略，从而满足合规性要求。",
      "why_wrong": "选项 A 错误，因为仅启用 S3 版本控制和生命周期策略无法满足数据保留 7 年的要求，且 MFA 删除并非强制的保留策略。选项 B 错误，治理保留模式允许特定权限的用户绕过保留限制，不满足题目对数据完整性的要求。选项 C 错误，虽然合规保留模式正确，但采用重新复制所有现有对象效率低下，批量操作更优。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Bucket",
      "S3 Versioning",
      "S3 Lifecycle",
      "Multi-Factor Authentication (MFA)",
      "S3 Object Lock",
      "Governance Retention Mode",
      "Compliance Retention Mode",
      "S3 Batch Operations"
    ]
  },
  {
    "id": 447,
    "topic": "1",
    "question_en": "A company has a stateless web application that runs on AWS Lambda functions that are invoked by Amazon API Gateway. The company wants to deploy the application across multiple AWS Regions to provide Regional failover capabilities. What should a solutions architect do to route trafic to multiple Regions?",
    "options_en": {
      "A": "Create Amazon Route 53 health checks for each Region. Use an active-active failover configuration.",
      "B": "Create an Amazon CloudFront distribution with an origin for each Region. Use CloudFront health checks to route trafic.",
      "C": "Create a transit gateway. Attach the transit gateway to the API Gateway endpoint in each Region. Configure the transit gateway to route requests.",
      "D": "Create an Application Load Balancer in the primary Region. Set the target group to point to the API Gateway endpoint hostnames in each Region."
    },
    "correct_answer": "A",
    "vote_percentage": "87%",
    "question_cn": "一家公司有一个无状态的 Web 应用程序，该应用程序在由 Amazon API Gateway 调用的 AWS Lambda 函数上运行。该公司希望跨多个 AWS 区域部署该应用程序，以提供区域故障转移功能。解决方案架构师应该怎么做才能将流量路由到多个区域？",
    "options_cn": {
      "A": "为每个区域创建 Amazon Route 53 运行状况检查。使用主动-主动故障转移配置。",
      "B": "创建一个 Amazon CloudFront 分发，其中包含每个区域的源。使用 CloudFront 运行状况检查来路由流量。",
      "C": "创建一个 transit gateway。将 transit gateway 附加到每个区域中的 API Gateway 端点。配置 transit gateway 以路由请求。",
      "D": "在主要区域创建一个 Application Load Balancer。将目标组设置为指向每个区域中的 API Gateway 端点主机名。"
    },
    "tags": [
      "Amazon Route 53",
      "API Gateway",
      "AWS Lambda",
      "Regional Failover"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 87%），解析仅供参考。】\n\n考查多区域流量路由及故障转移的实现方案；与 Route 53 运行状况检查、API Gateway 和 Lambda 的部署模式相关。",
      "why_correct": "Amazon Route 53 是 AWS 的 DNS 服务，Route 53 运行状况检查可用于监控后端服务的可用性。通过配置主动-主动故障转移，Route 53 可以根据后端服务的健康状况将流量路由到多个区域中的 API Gateway。当某个区域的 API Gateway 出现故障时，Route 53 会自动将流量路由到其他健康区域，从而实现区域故障转移。此方案简化了配置，并且与 API Gateway 和 Lambda 的无状态特性很好地兼容，无需在应用层面进行复杂的负载均衡或状态管理。",
      "why_wrong": "B. Amazon CloudFront 是内容分发网络（CDN），用于加速内容分发，而非设计用于跨区域的流量路由和故障转移。虽然 CloudFront 可以配置多个源，并进行健康检查，但它主要用于优化内容分发和缓存，而不是实现跨区域故障转移；它不直接与 API Gateway 集成来实现流量的智能路由。\nC. Transit Gateway 主要用于连接 VPC、不同 AWS 账户和本地网络。它不直接用于路由流量到 API Gateway 端点，也不是实现跨区域故障转移的正确方案。Transit Gateway 的主要作用是网络互联，而不是流量路由和故障转移。\nD. Application Load Balancer (ALB) 主要用于负载均衡应用程序流量。将 ALB 部署在主要区域，并指向其他区域的 API Gateway 主机名，会导致主要区域成为单点故障，无法实现区域故障转移。此外，通过主机名进行流量转发，可能导致性能下降和延迟增加，不如直接由 DNS 服务进行流量管理效率高。"
    },
    "related_terms": [
      "Amazon Route 53",
      "API Gateway",
      "AWS Lambda",
      "CloudFront",
      "Transit Gateway",
      "Application Load Balancer",
      "DNS",
      "VPC"
    ]
  },
  {
    "id": 448,
    "topic": "1",
    "question_en": "A company has two VPCs named Management and Production. The Management VPC uses VPNs through a customer gateway to connect to a single device in the data center. The Production VPC uses a virtual private gateway with two attached AWS Direct Connect connections. The Management and Production VPCs both use a single VPC peering connection to allow communication between the applications. What should a solutions architect do to mitigate any single point of failure in this architecture?",
    "options_en": {
      "A": "Add a set of VPNs between the Management and Production VPCs.",
      "B": "Add a second virtual private gateway and attach it to the Management VPC.",
      "C": "Add a second set of VPNs to the Management VPC from a second customer gateway device.",
      "D": "Add a second VPC peering connection between the Management VPC and the Production VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有两个名为 Management 和 Production 的 VPC。 Management VPC 通过客户网关使用 VPN 连接到数据中心中的单个设备。 Production VPC 使用一个虚拟私有网关，该网关连接了两个 AWS Direct Connect 连接。 Management 和 Production VPC 都使用单个 VPC 对等连接来允许应用程序之间的通信。 解决方案架构师应该怎么做以减轻此架构中的任何单点故障？",
    "options_cn": {
      "A": "在 Management 和 Production VPC 之间添加一组 VPN。",
      "B": "添加第二个虚拟私有网关并将其连接到 Management VPC。",
      "C": "从第二个客户网关设备向 Management VPC 添加第二组 VPN。",
      "D": "在 Management VPC 和 Production VPC 之间添加第二个 VPC 对等连接。"
    },
    "tags": [
      "VPC",
      "VPN",
      "Direct Connect",
      "VPC Peering",
      "Customer Gateway",
      "Virtual Private Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查 VPC 架构中单点故障的缓解措施，特别是针对 VPN 连接和 Direct Connect 冗余的考量。与高可用性设计、网络冗余以及 VPC 对等互连的适用场景相关。",
      "why_correct": "选项 C 正确。题目描述的管理 VPC 通过客户网关（Customer Gateway）连接到数据中心，存在单点故障。为了提高可用性，应该增加 VPN 连接的冗余。从第二个客户网关设备向管理 VPC 添加第二组 VPN，可以提供 VPN 连接的冗余，如果其中一个 VPN 连接发生故障，流量可以切换到另一个 VPN 连接，从而避免单点故障，保证 Management VPC 的网络连通性。",
      "why_wrong": "选项 A 错误。在 Management 和 Production VPC 之间添加 VPN 并不能解决 Management VPC 访问数据中心存在的单点故障问题，反而会使 VPC 之间的通信更加复杂。 选项 B 错误。添加第二个虚拟私有网关（Virtual Private Gateway）并将其连接到 Management VPC 无法解决 VPN 连接的单点故障问题，VPG 主要用于 Direct Connect 和 VPN 连接。 选项 D 错误。在 Management VPC 和 Production VPC 之间添加第二个 VPC 对等连接，无法解决 Management VPC 访问数据中心的单点故障问题。VPC 对等连接用于 VPC 之间的通信，不涉及与数据中心的连接。"
    },
    "related_terms": [
      "VPC",
      "VPN",
      "Direct Connect",
      "VPC Peering",
      "Customer Gateway",
      "Virtual Private Gateway",
      "Management VPC",
      "Production VPC"
    ]
  },
  {
    "id": 449,
    "topic": "1",
    "question_en": "A company runs its application on an Oracle database. The company plans to quickly migrate to AWS because of limited resources for the database, backup administration, and data center maintenance. The application uses third-party database features that require privileged access. Which solution will help the company migrate the database to AWS MOST cost-effectively?",
    "options_en": {
      "A": "Migrate the database to Amazon RDS for Oracle. Replace third-party features with cloud services.",
      "B": "Migrate the database to Amazon RDS Custom for Oracle. Customize the database settings to support third-party features.",
      "C": "Migrate the database to an Amazon EC2 Amazon Machine Image (AMI) for Oracle. Customize the database settings to support third- party features.",
      "D": "Migrate the database to Amazon RDS for PostgreSQL by rewriting the application code to remove dependency on Oracle APEX."
    },
    "correct_answer": "B",
    "vote_percentage": "94%",
    "question_cn": "一家公司在其应用程序上运行 Oracle 数据库。由于数据库、备份管理和数据中心维护的资源有限，该公司计划快速迁移到 AWS。该应用程序使用需要特权访问的第三方数据库功能。哪种解决方案将帮助该公司以最具成本效益的方式将数据库迁移到 AWS？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon RDS for Oracle。用云服务替换第三方功能。",
      "B": "将数据库迁移到 Amazon RDS Custom for Oracle。自定义数据库设置以支持第三方功能。",
      "C": "将数据库迁移到 Amazon EC2 Amazon Machine Image (AMI) for Oracle。自定义数据库设置以支持第三方功能。",
      "D": "通过重写应用程序代码以删除对 Oracle APEX 的依赖性，将数据库迁移到 Amazon RDS for PostgreSQL。"
    },
    "tags": [
      "Amazon EC2",
      "Oracle",
      "AMI",
      "Amazon RDS",
      "Amazon RDS Custom",
      "Amazon RDS for PostgreSQL",
      "Oracle APEX"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 94%），解析仅供参考。】\n\n本题考查 Oracle 数据库迁移至 AWS 的方案选择，重点在于对第三方数据库功能的兼容性以及成本效益的考量。",
      "why_correct": "Amazon RDS Custom for Oracle 允许用户自定义数据库环境，从而支持第三方数据库功能。这满足了题目中应用程序依赖特权访问的第三方数据库功能的需求。RDS Custom 提供了数据库的管理便利性，比 EC2 更具成本效益，适合快速迁移。",
      "why_wrong": "选项 A 无法直接支持第三方数据库功能，因为它强制使用云服务替代，这可能与应用程序的需求不符。选项 C 需要用户自行管理数据库，涉及更多的运维工作，且 RDS Custom 通常比 EC2 更具成本效益。选项 D 建议迁移至 PostgreSQL，这需要修改应用程序代码，改造成本和风险较高，不符合快速迁移的要求。"
    },
    "related_terms": [
      "Oracle",
      "AWS",
      "Amazon RDS for Oracle",
      "Amazon RDS Custom for Oracle",
      "Amazon EC2",
      "Amazon Machine Image (AMI)",
      "Oracle APEX",
      "Amazon RDS for PostgreSQL"
    ]
  },
  {
    "id": 450,
    "topic": "1",
    "question_en": "A company has a three-tier web application that is in a single server. The company wants to migrate the application to the AWS Cloud. The company also wants the application to align with the AWS Well-Architected Framework and to be consistent with AWS recommended best practices for security, scalability, and resiliency. Which combination of solutions will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Create a VPC across two Availability Zones with the application's existing architecture. Host the application with existing architecture on an Amazon EC2 instance in a private subnet in each Availability Zone with EC2 Auto Scaling groups. Secure the EC2 instance with security groups and network access control lists (network ACLs).",
      "B": "Set up security groups and network access control lists (network ACLs) to control access to the database layer. Set up a single Amazon RDS database in a private subnet.",
      "C": "Create a VPC across two Availability Zones. Refactor the application to host the web tier, application tier, and database tier. Host each tier on its own private subnet with Auto Scaling groups for the web tier and application tier.",
      "D": "Use a single Amazon RDS databas",
      "E": "Allow database access only from the application tier security group. E. Use Elastic Load Balancers in front of the web tier. Control access by using security groups containing references to each layer's security groups",
      "F": "Use an Amazon RDS database Multi-AZ cluster deployment in private subnets. Allow database access only from application tier security groups."
    },
    "correct_answer": "ACF",
    "vote_percentage": "",
    "question_cn": "一家公司有一个三层 Web 应用程序，该应用程序位于单个服务器上。该公司希望将应用程序迁移到 AWS 云。该公司还希望应用程序与 AWS 架构完善框架保持一致，并与 AWS 推荐的安全、可扩展性和弹性最佳实践保持一致。哪种解决方案组合将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "在两个可用区之间创建一个 VPC，并使用应用程序的现有架构。使用现有架构在每个可用区中的私有子网中的 Amazon EC2 实例上托管应用程序，并使用 EC2 Auto Scaling 组。使用安全组和网络访问控制列表（网络 ACL）保护 EC2 实例。",
      "B": "设置安全组和网络访问控制列表（网络 ACL）以控制对数据库层的访问。在私有子网中设置单个 Amazon RDS 数据库。",
      "C": "在两个可用区之间创建一个 VPC。重构应用程序以托管 Web 层、应用程序层和数据库层。将每一层托管在其自己的私有子网中，并为 Web 层和应用程序层配置 Auto Scaling 组。",
      "D": "使用单个 Amazon RDS 数据库。仅允许从应用程序层安全组访问数据库。",
      "E": "在 Web 层前面使用 Elastic Load Balancer。通过使用包含对每一层安全组引用的安全组来控制访问。",
      "F": "在私有子网中使用 Amazon RDS 数据库 Multi-AZ 集群部署。仅允许从应用程序层安全组访问数据库。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "VPC",
      "Security Groups",
      "Network ACL",
      "Amazon RDS",
      "Elastic Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACF（社区 —），解析仅供参考。】\n\n考查在 AWS 上迁移单服务器三层 Web 应用程序，并满足安全、可扩展性和弹性的要求。题目考察了 VPC、子网、EC2、Auto Scaling、安全组、RDS、Elastic Load Balancer 等服务在应用迁移中的应用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACF。理由简述：选项 A 正确：在两个可用区创建 VPC，在私有子网中用 EC2 Auto Scaling 组托管应用，并用安全组与网络 ACL 保护，满足弹性与安全。选项 C 正确：重构为 Web/应用/数据库三层，每层在各自私有子网，Web 层与应用层配置 Auto Scaling，符合 Well-Architected 最佳实践。选项 F 正确：在私有子网中部署 RDS Multi-AZ，仅允许应用层安全组访问，实现数据库高可用与安全。A+C+F 组合满足安全、可扩展性与弹性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 错误，单 RDS 无 Multi-AZ，不满足高可用。D 错误，单 RDS 且未体现 Web/应用层架构与弹性。E 错误，仅 ELB 与安全组引用不足以构成完整三层架构与数据库高可用。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "VPC",
      "Network ACL",
      "Amazon RDS",
      "Elastic Load Balancer",
      "Multi-AZ",
      "Security Groups"
    ]
  },
  {
    "id": 451,
    "topic": "1",
    "question_en": "A company is migrating its applications and databases to the AWS Cloud. The company will use Amazon Elastic Container Service (Amazon ECS), AWS Direct Connect, and Amazon RDS. Which activities will be managed by the company's operational team? (Choose three.)",
    "options_en": {
      "A": "Management of the Amazon RDS infrastructure layer, operating system, and platforms",
      "B": "Creation of an Amazon RDS DB instance and configuring the scheduled maintenance window",
      "C": "Configuration of additional software components on Amazon ECS for monitoring, patch management, log management, and host intrusion detection",
      "D": "Installation of patches for all minor and major database versions for Amazon RDS",
      "E": "Ensure the physical security of the Amazon RDS infrastructure in the data center",
      "F": "Encryption of the data that moves in transit through Direct Connect"
    },
    "correct_answer": "BCF",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其应用程序和数据库迁移到 AWS 云。该公司将使用 Amazon Elastic Container Service (Amazon ECS)、AWS Direct Connect 和 Amazon RDS。公司的运营团队将管理哪些活动？（选择三个。）",
    "options_cn": {
      "A": "管理 Amazon RDS 基础设施层、操作系统和平台",
      "B": "创建 Amazon RDS 数据库实例并配置计划维护窗口",
      "C": "在 Amazon ECS 上配置额外的软件组件，用于监控、补丁管理、日志管理和主机入侵检测",
      "D": "为 Amazon RDS 安装所有次要和主要数据库版本的补丁",
      "E": "确保数据中心内 Amazon RDS 基础设施的物理安全",
      "F": "加密通过 Direct Connect 传输的数据"
    },
    "tags": [
      "ECS",
      "Direct Connect",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BCF（社区 —），解析仅供参考。】\n\n本题考察了公司运营团队在 AWS 云环境中，使用 Amazon ECS、AWS Direct Connect 和 Amazon RDS 时需要负责的管理活动。重点在于区分公司层面运维责任与 AWS 提供的服务管理责任。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BCF。理由简述：选项 B 正确：运营团队负责创建 RDS 实例并配置计划维护窗口。选项 C 正确：在 ECS 上配置监控、补丁、日志、主机入侵检测等额外软件组件属于运营团队职责。选项 F 正确：确保通过 Direct Connect 传输的数据加密（如 TLS）由运营团队配置与维护。B+C+F 均为运营团队管理活动。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 错误，RDS 基础设施、OS 与平台由 AWS 托管。D 错误，RDS 补丁安装由 AWS 负责。E 错误，数据中心物理安全由 AWS 负责。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ECS",
      "Direct Connect",
      "RDS"
    ]
  },
  {
    "id": 452,
    "topic": "1",
    "question_en": "A company runs a Java-based job on an Amazon EC2 instance. The job runs every hour and takes 10 seconds to run. The job runs on a scheduled interval and consumes 1 GB of memory. The CPU utilization of the instance is low except for short surges during which the job uses the maximum CPU available. The company wants to optimize the costs to run the job. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS App2Container (A2C) to containerize the job. Run the job as an Amazon Elastic Container Service (Amazon ECS) task on AWS Fargate with 0.5 virtual CPU (vCPU) and 1 GB of memory.",
      "B": "Copy the code into an AWS Lambda function that has 1 GB of memory. Create an Amazon EventBridge scheduled rule to run the code each hour.",
      "C": "Use AWS App2Container (A2C) to containerize the job. Install the container in the existing Amazon Machine Image (AMI). Ensure that the schedule stops the container when the task finishes.",
      "D": "Configure the existing schedule to stop the EC2 instance at the completion of the job and restart the EC2 instance when the next job starts."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Amazon EC2 实例上运行一个基于 Java 的作业。该作业每小时运行一次，需要 10 秒钟。该作业在预定的时间间隔内运行，并消耗 1 GB 内存。除了作业使用最大可用 CPU 的短时峰值外，实例的 CPU 利用率较低。该公司希望优化运行该作业的成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS App2Container (A2C) 将作业容器化。在 AWS Fargate 上将该作业作为 Amazon Elastic Container Service (Amazon ECS) 任务运行，配置 0.5 个虚拟 CPU (vCPU) 和 1 GB 内存。",
      "B": "将代码复制到具有 1 GB 内存的 AWS Lambda 函数中。创建一个 Amazon EventBridge 计划规则，以便每小时运行代码。",
      "C": "使用 AWS App2Container (A2C) 将作业容器化。将容器安装在现有的 Amazon Machine Image (AMI) 中。确保该计划在任务完成后停止容器。",
      "D": "配置现有计划，以便在作业完成后停止 EC2 实例，并在下一个作业开始时重新启动 EC2 实例。"
    },
    "tags": [
      "EC2",
      "Java",
      "Lambda",
      "EventBridge",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查了优化运行 Java 作业的成本方案。作业定时运行，且 CPU 利用率较低，表明可以使用 Serverless 方案来降低成本。",
      "why_correct": "选项 B 正确，使用 AWS Lambda 函数运行作业，并结合 Amazon EventBridge 计划规则，可以实现按需计算，并降低成本。",
      "why_wrong": "选项 A 错误，虽然 Fargate 也是 Serverless 的，但使用 App2Container 将作业容器化，再用 ECS 运行，相对于 Lambda 而言，配置较为复杂，成本也相对较高。选项 C 错误，将作业容器化后安装在 AMI 中，并不能有效优化成本，没有充分利用 Serverless 的优势。选项 D 错误，停止和重新启动 EC2 实例虽然能降低成本，但每次启动 EC2 实例会带来冷启动时间，不适合需要定时运行的作业。"
    },
    "related_terms": [
      "EC2",
      "Java",
      "Lambda",
      "EventBridge",
      "Fargate",
      "AMI",
      "A2C",
      "vCPU"
    ]
  },
  {
    "id": 453,
    "topic": "1",
    "question_en": "A company wants to implement a backup strategy for Amazon EC2 data and multiple Amazon S3 buckets. Because of regulatory requirements, the company must retain backup files for a specific time period. The company must not alter the files for the duration of the retention period. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Backup to create a backup vault that has a vault lock in governance mode. Create the required backup plan.",
      "B": "Use Amazon Data Lifecycle Manager to create the required automated snapshot policy.",
      "C": "Use Amazon S3 File Gateway to create the backup. Configure the appropriate S3 Lifecycle management.",
      "D": "Use AWS Backup to create a backup vault that has a vault lock in compliance mode. Create the required backup plan."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望为其 Amazon EC2 数据和多个 Amazon S3 存储桶实施备份策略。由于监管要求，该公司必须将备份文件保留一段特定的时间。在保留期内，公司不得更改文件。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Backup 创建一个备份库，该库在治理模式下具有库锁。创建所需的备份计划。",
      "B": "使用 Amazon Data Lifecycle Manager 创建所需的自动快照策略。",
      "C": "使用 Amazon S3 File Gateway 创建备份。配置适当的 S3 生命周期管理。",
      "D": "使用 AWS Backup 创建一个备份库，该库在合规模式下具有库锁。创建所需的备份计划。"
    },
    "tags": [
      "EC2",
      "S3",
      "AWS Backup",
      "Data Lifecycle Manager",
      "File Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查 AWS Backup 的使用，以及在满足监管要求下的备份策略实施，重点是 Backup Vault 的 Lock 功能。",
      "why_correct": "选项 D 正确。通过 AWS Backup 创建备份库 (Backup Vault)，并启用合规模式下的库锁 (Vault Lock)。合规模式下的库锁可以防止在指定的保留期内删除或修改备份文件，满足了题目中不得更改文件的要求。然后创建备份计划以备份 EC2 数据和 S3 存储桶，实现备份策略。",
      "why_wrong": "选项 A 错误。治理模式下的库锁虽然可以防止意外删除或修改备份，但无法完全满足监管要求，因为管理员仍有权限删除备份。选项 B 错误。Amazon Data Lifecycle Manager 主要用于管理 EBS 快照的生命周期，无法直接用于备份 S3 存储桶。选项 C 错误。虽然 S3 生命周期管理可以用于管理 S3 对象的生命周期，但 S3 File Gateway 并非专门设计用于备份，且无法满足不得更改文件的要求，无法完全保证备份的完整性。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "AWS Backup",
      "Backup Vault",
      "Vault Lock",
      "Governance mode",
      "Compliance mode",
      "Amazon Data Lifecycle Manager",
      "EBS",
      "S3 File Gateway",
      "S3"
    ]
  },
  {
    "id": 454,
    "topic": "1",
    "question_en": "A company has resources across multiple AWS Regions and accounts. A newly hired solutions architect discovers a previous employee did not provide details about the resources inventory. The solutions architect needs to build and map the relationship details of the various workloads across all accounts. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Use AWS Systems Manager Inventory to generate a map view from the detailed view report.",
      "B": "Use AWS Step Functions to collect workload details. Build architecture diagrams of the workloads manually.",
      "C": "Use Workload Discovery on AWS to generate architecture diagrams of the workloads.",
      "D": "Use AWS X-Ray to view the workload details. Build architecture diagrams with relationships."
    },
    "correct_answer": "C",
    "vote_percentage": "95%",
    "question_cn": "一家公司在多个 AWS 区域和账户中拥有资源。一位新聘用的解决方案架构师发现，之前的员工没有提供关于资源清单的详细信息。该解决方案架构师需要构建并映射所有账户中各种工作负载的关系细节。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Systems Manager Inventory 从详细视图报告生成地图视图。",
      "B": "使用 AWS Step Functions 收集工作负载详细信息。手动构建工作负载的架构图。",
      "C": "使用 AWS Workload Discovery 生成工作负载的架构图。",
      "D": "使用 AWS X-Ray 查看工作负载详细信息。构建具有关系的架构图。"
    },
    "tags": [
      "AWS Systems Manager",
      "Step Functions",
      "Workload Discovery",
      "X-Ray"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 95%），解析仅供参考。】\n\n考查在多账户多区域环境中，以最具运营效率的方式发现和映射工作负载关系的方法。",
      "why_correct": "AWS Workload Discovery 专门设计用于自动发现和可视化 AWS 资源及其相互关系。它可以跨多个 AWS 账户和区域运行，并生成工作负载的架构图。这种自动化和集成的特性使得 Workload Discovery 成为最有效率的解决方案。",
      "why_wrong": "A. AWS Systems Manager Inventory 主要用于收集和管理服务器的清单信息，生成地图视图的功能有限，无法直接构建工作负载的架构图，且不具备跨账户和区域的发现能力。 B. 使用 AWS Step Functions 收集信息需要手动构建流程，手动构建架构图效率低下，且不具备自动化发现能力。 D. AWS X-Ray 用于跟踪和分析应用程序的请求，提供性能分析和问题排查，并不直接用于发现和映射工作负载间的关系，也无法自动生成架构图。"
    },
    "related_terms": [
      "AWS Systems Manager Inventory",
      "AWS Step Functions",
      "AWS Workload Discovery",
      "AWS X-Ray"
    ]
  },
  {
    "id": 455,
    "topic": "1",
    "question_en": "A company uses AWS Organizations. The company wants to operate some of its AWS accounts with different budgets. The company wants to receive alerts and automatically prevent provisioning of additional resources on AWS accounts when the allocated budget threshold is met during a specific period. Which combination of solutions will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Use AWS Budgets to create a budget. Set the budget amount under the Cost and Usage Reports section of the required AWS accounts.",
      "B": "Use AWS Budgets to create a budget. Set the budget amount under the Billing dashboards of the required AWS accounts.",
      "C": "Create an IAM user for AWS Budgets to run budget actions with the required permissions.",
      "D": "Create an IAM role for AWS Budgets to run budget actions with the required permissions",
      "E": "Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate config rule to prevent provisioning of additional resources",
      "F": "Add an alert to notify the company when each account meets its budget threshold. Add a budget action that selects the IAM identity created with the appropriate service control policy (SCP) to prevent provisioning of additional resources."
    },
    "correct_answer": "BDF",
    "vote_percentage": "",
    "question_cn": "一家公司使用 AWS Organizations。该公司希望使用不同的预算来操作其部分 AWS 账户。当在特定时期内达到分配的预算阈值时，该公司希望接收警报并自动阻止在 AWS 账户上配置其他资源。哪些解决方案组合将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "使用 AWS Budgets 创建预算。在所需 AWS 账户的“成本和使用报告”部分设置预算金额。",
      "B": "使用 AWS Budgets 创建预算。在所需 AWS 账户的“计费控制面板”下设置预算金额。",
      "C": "为 AWS Budgets 创建一个 IAM 用户，以使用所需的权限运行预算操作。",
      "D": "为 AWS Budgets 创建一个 IAM 角色，以使用所需的权限运行预算操作。",
      "E": "添加警报，在每个账户达到其预算阈值时通知公司。添加一个预算操作，该操作选择使用适当的配置规则创建的 IAM 身份，以防止配置其他资源。",
      "F": "添加警报，在每个账户达到其预算阈值时通知公司。添加一个预算操作，该操作选择使用适当的服务控制策略 (SCP) 创建的 IAM 身份，以防止配置其他资源。"
    },
    "tags": [
      "AWS Budgets",
      "AWS Organizations",
      "IAM",
      "SCP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BDF（社区 —），解析仅供参考。】\n\n考查 AWS Organizations 中使用 AWS Budgets 进行成本控制，并实现预算超额报警和资源配置阻止的功能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BDF。理由简述：选项 B 正确：使用 AWS Budgets 在计费控制面板下为各账户设置预算金额。选项 D 正确：为 Budgets 创建 IAM 角色以执行预算操作，符合最佳实践。选项 F 正确：添加预算阈值告警，并添加预算操作，选择由 SCP 创建的 IAM 身份以阻止超额后继续配置资源。B+D+F 满足预算、告警与自动阻止配置的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 错误，预算金额在计费控制面板设置，非成本和使用报告。C 错误，应使用 IAM 角色而非 IAM 用户执行预算操作。E 错误，使用配置规则创建的 IAM 身份与本题「阻止配置」场景不符，SCP 更适用于组织级阻止。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Budgets",
      "AWS Organizations",
      "IAM",
      "SCP",
      "Cost and Usage Report"
    ]
  },
  {
    "id": 456,
    "topic": "1",
    "question_en": "A company runs applications on Amazon EC2 instances in one AWS Region. The company wants to back up the EC2 instances to a second Region. The company also wants to provision EC2 resources in the second Region and manage the EC2 instances centrally from one AWS account. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a disaster recovery (DR) plan that has a similar number of EC2 instances in the second Region. Configure data replication.",
      "B": "Create point-in-time Amazon Elastic Block Store (Amazon EBS) snapshots of the EC2 instances. Copy the snapshots to the second Region periodically.",
      "C": "Create a backup plan by using AWS Backup. Configure cross-Region backup to the second Region for the EC2 instances.",
      "D": "Deploy a similar number of EC2 instances in the second Region. Use AWS DataSync to transfer the data from the source Region to the second Region."
    },
    "correct_answer": "C",
    "vote_percentage": "79%",
    "question_cn": "一家公司在一个 AWS 区域的 Amazon EC2 实例上运行应用程序。该公司希望将 EC2 实例备份到第二个区域。该公司还希望在第二个区域配置 EC2 资源，并从一个 AWS 账户集中管理 EC2 实例。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "创建一个灾难恢复 (DR) 计划，该计划在第二个区域具有相似数量的 EC2 实例。配置数据复制。",
      "B": "创建 EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 时间点快照。定期将快照复制到第二个区域。",
      "C": "使用 AWS Backup 创建一个备份计划。为 EC2 实例配置跨区域备份到第二个区域。",
      "D": "在第二个区域部署相似数量的 EC2 实例。使用 AWS DataSync 将数据从源区域传输到第二个区域。"
    },
    "tags": [
      "EC2",
      "EBS",
      "AWS Backup",
      "AWS DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 79%），解析仅供参考。】\n\n此题考查如何以最具成本效益的方式备份 EC2 实例到另一个区域。重点在于灾备和集中管理。",
      "why_correct": "选项 C 正确，使用 AWS Backup 可以配置跨区域备份，从而实现备份到第二个区域并集中管理 EC2 实例。",
      "why_wrong": "选项 A 错误，创建一个灾难恢复 (DR) 计划，并配置数据复制，没有体现出成本效益。选项 B 错误，使用 Amazon EBS 快照，需要手动管理快照的复制，成本相对较高。选项 D 错误，AWS DataSync 用于数据同步，需要部署 EC2 实例并传输数据，成本较高。"
    },
    "related_terms": [
      "EC2",
      "EBS",
      "AWS Backup",
      "AWS DataSync"
    ]
  },
  {
    "id": 457,
    "topic": "1",
    "question_en": "A company that uses AWS is building an application to transfer data to a product manufacturer. The company has its own identity provider (IdP). The company wants the IdP to authenticate application users while the users use the application to transfer data. The company must use Applicability Statement 2 (AS2) protocol. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS DataSync to transfer the data. Create an AWS Lambda function for IdP authentication.",
      "B": "Use Amazon AppFlow fiows to transfer the data. Create an Amazon Elastic Container Service (Amazon ECS) task for IdP authentication.",
      "C": "Use AWS Transfer Family to transfer the data. Create an AWS Lambda function for IdP authentication.",
      "D": "Use AWS Storage Gateway to transfer the data. Create an Amazon Cognito identity pool for IdP authentication."
    },
    "correct_answer": "C",
    "vote_percentage": "86%",
    "question_cn": "一家使用 AWS 的公司正在构建一个应用程序，用于将数据传输给产品制造商。该公司拥有自己的身份提供商 (IdP)。该公司希望 IdP 对应用程序用户进行身份验证，同时用户使用该应用程序传输数据。该公司必须使用 Applicability Statement 2 (AS2) 协议。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 传输数据。创建一个 AWS Lambda 函数进行 IdP 身份验证。",
      "B": "使用 Amazon AppFlow 流传输数据。创建一个 Amazon Elastic Container Service (Amazon ECS) 任务进行 IdP 身份验证。",
      "C": "使用 AWS Transfer Family 传输数据。创建一个 AWS Lambda 函数进行 IdP 身份验证。",
      "D": "使用 AWS Storage Gateway 传输数据。创建一个 Amazon Cognito 身份池进行 IdP 身份验证。"
    },
    "tags": [
      "AWS DataSync",
      "AppFlow",
      "AWS Transfer Family",
      "Storage Gateway",
      "AS2",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 86%），解析仅供参考。】\n\n此题考查使用 AS2 协议进行数据传输。需要考虑使用 AS2 协议的数据传输服务，并结合 IdP 身份验证。",
      "why_correct": "选项 C 正确，AWS Transfer Family 支持 AS2 协议，并允许使用 AWS Lambda 函数进行身份验证。",
      "why_wrong": "选项 A 错误，AWS DataSync 不支持 AS2 协议。选项 B 错误，Amazon AppFlow 不支持 AS2 协议。选项 D 错误，AWS Storage Gateway 不支持 AS2 协议，且使用 Cognito 不正确。"
    },
    "related_terms": [
      "AWS DataSync",
      "AWS Transfer Family",
      "Storage Gateway",
      "Lambda",
      "Cognito",
      "AppFlow",
      "AS2",
      "IdP"
    ]
  },
  {
    "id": 458,
    "topic": "1",
    "question_en": "A solutions architect is designing a RESTAPI in Amazon API Gateway for a cash payback service. The application requires 1 GB of memory and 2 GB of storage for its computation resources. The application will require that the data is in a relational format. Which additional combination ofAWS services will meet these requirements with the LEAST administrative effort? (Choose two.)",
    "options_en": {
      "A": "Amazon EC2",
      "B": "AWS Lambda",
      "C": "Amazon RDS",
      "D": "Amazon DynamoDB",
      "E": "Amazon Elastic Kubernetes Services (Amazon EKS)"
    },
    "correct_answer": "BC",
    "vote_percentage": "86%",
    "question_cn": "一个解决方案架构师正在为现金返还服务设计 Amazon API Gateway 中的 REST API。该应用程序需要 1 GB 内存和 2 GB 存储空间用于其计算资源。 该应用程序将要求数据采用关系格式。 哪种额外的 AWS 服务组合将以最少的管理工作量满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "Amazon EC2",
      "B": "AWS Lambda",
      "C": "Amazon RDS",
      "D": "Amazon DynamoDB",
      "E": "Amazon Elastic Kubernetes Services (Amazon EKS)"
    },
    "tags": [
      "API Gateway",
      "EC2",
      "Lambda",
      "RDS",
      "DynamoDB",
      "EKS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 86%），解析仅供参考。】\n\n此题考查如何结合 API Gateway 选择合适的计算和存储服务，以满足应用程序的需求。需要考虑应用程序的内存和存储需求，以及关系型数据的需求。",
      "why_correct": "选项 B 正确，AWS Lambda 提供了计算资源，可以满足内存需求，且能够方便地与 API Gateway 集成。选项 C 正确，Amazon RDS 可以存储关系型数据。",
      "why_wrong": "选项 A 错误，Amazon EC2 需要用户管理，无法满足最小管理工作量的要求。选项 D 错误，Amazon DynamoDB 是 NoSQL 数据库，不满足关系型数据的需求。选项 E 错误，Amazon EKS 需要用户管理，也无法满足最小管理工作量的要求。"
    },
    "related_terms": [
      "API Gateway",
      "EC2",
      "Lambda",
      "RDS",
      "DynamoDB",
      "EKS",
      "REST API"
    ]
  },
  {
    "id": 459,
    "topic": "1",
    "question_en": "A company uses AWS Organizations to run workloads within multiple AWS accounts. A tagging policy adds department tags to AWS resources when the company creates tags. An accounting team needs to determine spending on Amazon EC2 consumption. The accounting team must determine which departments are responsible for the costs regardless ofAWS account. The accounting team has access to AWS Cost Explorer for all AWS accounts within the organization and needs to access all reports from Cost Explorer. Which solution meets these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "From the Organizations management account billing console, activate a user-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.",
      "B": "From the Organizations management account billing console, activate an AWS-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2.",
      "C": "From the Organizations member account billing console, activate a user-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by the tag name, and filter by EC2.",
      "D": "From the Organizations member account billing console, activate an AWS-defined cost allocation tag named department. Create one cost report in Cost Explorer grouping by tag name, and filter by EC2."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS Organizations 在多个 AWS 账户中运行工作负载。当公司创建标签时，一个标签策略会将部门标签添加到 AWS 资源。一个会计团队需要确定 Amazon EC2 消耗的支出。会计团队必须确定哪些部门负责这些成本，而无论 AWS 账户如何。会计团队可以访问组织内的所有 AWS 账户的 AWS Cost Explorer，并需要访问 Cost Explorer 中的所有报告。哪种解决方案以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "从 Organizations 管理账户的账单控制台，激活一个名为 department 的用户定义成本分配标签。在 Cost Explorer 中创建一个成本报告，按标签名称分组，并按 EC2 筛选。",
      "B": "从 Organizations 管理账户的账单控制台，激活一个名为 department 的 AWS 定义成本分配标签。在 Cost Explorer 中创建一个成本报告，按标签名称分组，并按 EC2 筛选。",
      "C": "从 Organizations 成员账户的账单控制台，激活一个名为 department 的用户定义成本分配标签。在 Cost Explorer 中创建一个成本报告，按标签名称分组，并按 EC2 筛选。",
      "D": "从 Organizations 成员账户的账单控制台，激活一个名为 department 的 AWS 定义成本分配标签。在 Cost Explorer 中创建一个成本报告，按标签名称分组，并按 EC2 筛选。"
    },
    "tags": [
      "Cost Explorer",
      "AWS Organizations",
      "Cost Allocation Tags",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查通过 Cost Explorer 分析跨 AWS 账户的 EC2 成本，并根据部门标签进行分组。",
      "why_correct": "在 Organizations 管理账户的账单控制台中，可以激活用户定义的成本分配标签。激活之后，所有成员账户的资源产生的成本都会打上标签。在 Cost Explorer 中创建成本报告，可以按标签分组，筛选 EC2 实例，从而确定不同部门的 EC2 成本，满足题目的需求。",
      "why_wrong": "选项 B 错误在于，AWS 定义的成本分配标签是 AWS 预定义的，无法自定义 department 标签。选项 C 和 D 错误在于，标签应该在 Organizations 的管理账户中激活，而不是成员账户，才能影响所有账户的成本分析。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS Account",
      "Amazon EC2",
      "AWS Cost Explorer",
      "Cost Allocation Tag"
    ]
  },
  {
    "id": 460,
    "topic": "1",
    "question_en": "A company wants to securely exchange data between its software as a service (SaaS) application Salesforce account and Amazon S3. The company must encrypt the data at rest by using AWS Key Management Service (AWS KMS) customer managed keys (CMKs). The company must also encrypt the data in transit. The company has enabled API access for the Salesforce account.",
    "options_en": {
      "A": "Create AWS Lambda functions to transfer the data securely from Salesforce to Amazon S3.",
      "B": "Create an AWS Step Functions workfiow. Define the task to transfer the data securely from Salesforce to Amazon S3.",
      "C": "Create Amazon AppFlow fiows to transfer the data securely from Salesforce to Amazon S3.",
      "D": "Create a custom connector for Salesforce to transfer the data securely from Salesforce to Amazon S3."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在其软件即服务 (SaaS) 应用程序 Salesforce 帐户和 Amazon S3 之间安全地交换数据。该公司必须使用 AWS Key Management Service (AWS KMS) 客户托管密钥 (CMK) 加密静态数据。该公司还必须加密传输中的数据。该公司已为 Salesforce 帐户启用了 API 访问。",
    "options_cn": {
      "A": "创建 AWS Lambda 函数以将数据从 Salesforce 安全地传输到 Amazon S3。",
      "B": "创建一个 AWS Step Functions 工作流程。定义将数据从 Salesforce 安全地传输到 Amazon S3 的任务。",
      "C": "创建 Amazon AppFlow 流以将数据从 Salesforce 安全地传输到 Amazon S3。",
      "D": "为 Salesforce 创建一个自定义连接器，以安全地将数据从 Salesforce 传输到 Amazon S3。"
    },
    "tags": [
      "Salesforce",
      "S3",
      "KMS",
      "AppFlow",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查如何安全地在 Salesforce 帐户和 Amazon S3 之间交换数据。需要使用 KMS 客户托管密钥加密静态数据和传输中的数据。",
      "why_correct": "选项 C 正确，Amazon AppFlow 支持与 Salesforce 和 Amazon S3 的集成，并支持数据加密，满足安全交换数据的要求。",
      "why_wrong": "选项 A 错误，AWS Lambda 函数需要用户手动编写代码来实现数据传输和加密，比较复杂。选项 B 错误，AWS Step Functions 工作流程也需要用户手动管理数据传输和加密，较为复杂。选项 D 错误，自定义连接器需要额外开发，不方便，且难以集成 KMS 加密。"
    },
    "related_terms": [
      "S3",
      "KMS",
      "Lambda",
      "Salesforce",
      "AppFlow",
      "CMK"
    ]
  },
  {
    "id": 461,
    "topic": "1",
    "question_en": "A company is developing a mobile gaming app in a single AWS Region. The app runs on multiple Amazon EC2 instances in an Auto Scaling group. The company stores the app data in Amazon DynamoDB. The app communicates by using TCP trafic and UDP trafic between the users and the servers. The application will be used globally. The company wants to ensure the lowest possible latency for all users. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Global Accelerator to create an accelerator. Create an Application Load Balancer (ALB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB.",
      "B": "Use AWS Global Accelerator to create an accelerator. Create a Network Load Balancer (NLB) behind an accelerator endpoint that uses Global Accelerator integration and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB.",
      "C": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create a Network Load Balancer (NLB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the NLB. Update CloudFront to use the NLB as the origin.",
      "D": "Create an Amazon CloudFront content delivery network (CDN) endpoint. Create an Application Load Balancer (ALB) behind the endpoint and listening on the TCP and UDP ports. Update the Auto Scaling group to register instances on the ALB. Update CloudFront to use the ALB as the origin."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在单个 AWS 区域中开发移动游戏应用程序。该应用程序在 Auto Scaling 组中的多个 Amazon EC2 实例上运行。该公司将应用程序数据存储在 Amazon DynamoDB 中。该应用程序通过使用 TCP 和 UDP 流量在用户和服务器之间进行通信。该应用程序将在全球范围内使用。该公司希望确保所有用户的延迟尽可能低。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Global Accelerator 创建一个加速器。在加速器端点后面创建一个 Application Load Balancer (ALB)，该端点使用 Global Accelerator 集成并在 TCP 和 UDP 端口上侦听。更新 Auto Scaling 组以在 ALB 上注册实例。",
      "B": "使用 AWS Global Accelerator 创建一个加速器。在加速器端点后面创建一个 Network Load Balancer (NLB)，该端点使用 Global Accelerator 集成并在 TCP 和 UDP 端口上侦听。更新 Auto Scaling 组以在 NLB 上注册实例。",
      "C": "创建一个 Amazon CloudFront 内容分发网络 (CDN) 端点。在端点后面创建一个 Network Load Balancer (NLB)，并在 TCP 和 UDP 端口上侦听。更新 Auto Scaling 组以在 NLB 上注册实例。更新 CloudFront 以使用 NLB 作为源。",
      "D": "创建一个 Amazon CloudFront 内容分发网络 (CDN) 端点。在端点后面创建一个 Application Load Balancer (ALB)，并在 TCP 和 UDP 端口上侦听。更新 Auto Scaling 组以在 ALB 上注册实例。更新 CloudFront 以使用 ALB 作为源。"
    },
    "tags": [
      "Global Accelerator",
      "ALB",
      "NLB",
      "CloudFront",
      "EC2",
      "DynamoDB",
      "TCP",
      "UDP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察使用 AWS Global Accelerator 优化全球用户延迟，并结合 Network Load Balancer (NLB) 和 Auto Scaling 组实现应用程序高可用性。",
      "why_correct": "选项 B 提供了最佳解决方案。Global Accelerator 优化了全球流量的路由，降低了延迟。NLB 支持 TCP 和 UDP 流量，满足了应用程序的需求，并与 Auto Scaling 组集成，确保了应用程序的高可用性和弹性。",
      "why_wrong": "选项 A 错误在于使用了 Application Load Balancer (ALB)。ALB 主要用于 HTTP/HTTPS 流量，虽然 Global Accelerator 可以与 ALB 集成，但对于 TCP 和 UDP 流量的支持不如 NLB。选项 C 和 D 错误在于 CloudFront 主要用于内容分发，虽然可以加速静态内容，但对于 TCP 和 UDP 流量的应用程序，其加速效果不如 Global Accelerator。此外，选项 C 和 D 使用 CloudFront 需要配置 NLB 或 ALB 作为源，增加了复杂性，且无法直接提供 Global Accelerator 的加速效果。"
    },
    "related_terms": [
      "AWS Global Accelerator",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon DynamoDB",
      "TCP",
      "UDP",
      "Application Load Balancer (ALB)",
      "Network Load Balancer (NLB)",
      "Amazon CloudFront",
      "CDN"
    ]
  },
  {
    "id": 462,
    "topic": "1",
    "question_en": "A company has an application that processes customer orders. The company hosts the application on an Amazon EC2 instance that saves the orders to an Amazon Aurora database. Occasionally when trafic is high the workload does not process orders fast enough. What should a solutions architect do to write the orders reliably to the database as quickly as possible?",
    "options_en": {
      "A": "Increase the instance size of the EC2 instance when trafic is high. Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic.",
      "B": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database.",
      "C": "Write orders to Amazon Simple Notification Service (Amazon SNS). Subscribe the database endpoint to the SNS topic. Use EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SNS topic.",
      "D": "Write orders to an Amazon Simple Queue Service (Amazon SQS) queue when the EC2 instance reaches CPU threshold limits. Use scheduled scaling of EC2 instances in an Auto Scaling group behind an Application Load Balancer to read from the SQS queue and process orders into the database."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个处理客户订单的应用程序。该公司将该应用程序托管在 Amazon EC2 实例上，该实例将订单保存到 Amazon Aurora 数据库。偶尔，当流量很高时，工作负载无法足够快地处理订单。解决方案架构师应该怎么做才能尽快可靠地将订单写入数据库？",
    "options_cn": {
      "A": "当流量很高时，增加 EC2 实例的大小。将订单写入 Amazon Simple Notification Service (Amazon SNS)。将数据库端点订阅到 SNS 主题。",
      "B": "将订单写入 Amazon Simple Queue Service (Amazon SQS) 队列。使用 Auto Scaling 组中的 EC2 实例（位于 Application Load Balancer 之后）从 SQS 队列中读取并将订单处理到数据库中。",
      "C": "将订单写入 Amazon Simple Notification Service (Amazon SNS)。将数据库端点订阅到 SNS 主题。使用 Auto Scaling 组中的 EC2 实例（位于 Application Load Balancer 之后）从 SNS 主题中读取。",
      "D": "当 EC2 实例达到 CPU 阈值限制时，将订单写入 Amazon Simple Queue Service (Amazon SQS) 队列。使用 Auto Scaling 组中的 EC2 实例（位于 Application Load Balancer 之后）的计划缩放，以从 SQS 队列中读取并将订单处理到数据库中。"
    },
    "tags": [
      "EC2",
      "SQS",
      "SNS",
      "Auto Scaling",
      "Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查处理高流量时如何写入数据库。需要考虑解耦、弹性伸缩和可靠性。",
      "why_correct": "选项 B 正确，使用 SQS 队列解耦应用程序和数据库，确保订单写入队列，使用 Auto Scaling 组中的 EC2 实例消费队列消息，可以实现弹性和可靠性。",
      "why_wrong": "选项 A 错误，直接将订单写入 SNS 主题，并不能保证写入数据库的可靠性，且扩展性较差。选项 C 错误，直接将订单写入 SNS 主题，并不能保证写入数据库的可靠性。选项 D 错误，使用计划缩放可能会导致资源利用率不足，无法及时处理订单。"
    },
    "related_terms": [
      "EC2",
      "SQS",
      "SNS",
      "Auto Scaling",
      "Aurora",
      "Load Balancer"
    ]
  },
  {
    "id": 463,
    "topic": "1",
    "question_en": "An IoT company is releasing a mattress that has sensors to collect data about a user’s sleep. The sensors will send data to an Amazon S3 bucket. The sensors collect approximately 2 MB of data every night for each mattress. The company must process and summarize the data for each mattress. The results need to be available as soon as possible. Data processing will require 1 GB of memory and will finish within 30 seconds. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use AWS Glue with a Scala job",
      "B": "Use Amazon EMR with an Apache Spark script",
      "C": "Use AWS Lambda with a Python script",
      "D": "Use AWS Glue with a PySpark job"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家物联网公司正在发布一款带有传感器的床垫，用于收集有关用户睡眠的数据。传感器将把数据发送到 Amazon S3 存储桶。传感器每晚为每个床垫收集大约 2 MB 的数据。该公司必须处理和汇总每个床垫的数据。结果需要尽快可用。数据处理将需要 1 GB 的内存，并在 30 秒内完成。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Glue 和一个 Scala 作业",
      "B": "使用 Amazon EMR 和一个 Apache Spark 脚本",
      "C": "使用 AWS Lambda 和一个 Python 脚本",
      "D": "使用 AWS Glue 和一个 PySpark 作业"
    },
    "tags": [
      "S3",
      "Lambda",
      "Glue",
      "EMR",
      "Spark"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查如何处理和汇总物联网数据。需要考虑成本效益和处理时间。",
      "why_correct": "选项 C 正确，使用 AWS Lambda 和 Python 脚本可以在最短的时间内以较低的成本处理数据。",
      "why_wrong": "选项 A 错误，AWS Glue 和 Scala 作业相对 Lambda 来说，启动时间较长，成本较高。选项 B 错误，Amazon EMR 和 Apache Spark 脚本的启动时间和成本都相对较高。选项 D 错误，AWS Glue 和 PySpark 作业的启动时间和成本也相对较高。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "Glue",
      "EMR",
      "Spark",
      "PySpark"
    ]
  },
  {
    "id": 464,
    "topic": "1",
    "question_en": "A company hosts an online shopping application that stores all orders in an Amazon RDS for PostgreSQL Single-AZ DB instance. Management wants to eliminate single points of failure and has asked a solutions architect to recommend an approach to minimize database downtime without requiring any changes to the application code. Which solution meets these requirements?",
    "options_en": {
      "A": "Convert the existing database instance to a Multi-AZ deployment by modifying the database instance and specifying the Multi-AZ option.",
      "B": "Create a new RDS Multi-AZ deployment. Take a snapshot of the current RDS instance and restore the new Multi-AZ deployment with the snapshot.",
      "C": "Create a read-only replica of the PostgreSQL database in another Availability Zone. Use Amazon Route 53 weighted record sets to distribute requests across the databases.",
      "D": "Place the RDS for PostgreSQL database in an Amazon EC2 Auto Scaling group with a minimum group size of two. Use Amazon Route 53 weighted record sets to distribute requests across instances."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管了一个在线购物应用程序，该应用程序将所有订单存储在 Amazon RDS for PostgreSQL 单可用区数据库实例中。管理层希望消除单点故障，并要求解决方案架构师推荐一种方法，以最大限度地减少数据库停机时间，而无需对应用程序代码进行任何更改。哪个解决方案符合这些要求？",
    "options_cn": {
      "A": "通过修改数据库实例并指定多可用区选项，将现有的数据库实例转换为多可用区部署。",
      "B": "创建一个新的 RDS 多可用区部署。拍摄当前 RDS 实例的快照，并使用该快照还原新的多可用区部署。",
      "C": "在另一个可用区中创建 PostgreSQL 数据库的只读副本。使用 Amazon Route 53 加权记录集将请求分发到各个数据库。",
      "D": "将 RDS for PostgreSQL 数据库放置在 Amazon EC2 自动伸缩组中，最小组大小为两个。使用 Amazon Route 53 加权记录集将请求分发到各个实例。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "Multi-AZ",
      "Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查如何最大限度地减少数据库停机时间，而无需更改应用程序代码。重点在于数据库的高可用性。",
      "why_correct": "选项 A 正确，将现有的数据库实例转换为多可用区部署，可以在不更改应用程序代码的情况下，实现数据库的高可用性。",
      "why_wrong": "选项 B 错误，创建一个新的 RDS 多可用区部署并恢复快照，会导致数据库停机。选项 C 错误，创建只读副本无法消除单点故障。选项 D 错误，RDS for PostgreSQL 数据库放置在 Amazon EC2 自动伸缩组中是错误的。"
    },
    "related_terms": [
      "RDS",
      "PostgreSQL",
      "Multi-AZ",
      "Route 53",
      "Read Replica"
    ]
  },
  {
    "id": 465,
    "topic": "1",
    "question_en": "A company is developing an application to support customer demands. The company wants to deploy the application on multiple Amazon EC2 Nitro-based instances within the same Availability Zone. The company also wants to give the application the ability to write to multiple block storage volumes in multiple EC2 Nitro-based instances simultaneously to achieve higher application availability. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use General Purpose SSD (gp3) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "B": "Use Throughput Optimized HDD (st1) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "C": "Use Provisioned IOPS SSD (io2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "D": "Use General Purpose SSD (gp2) EBS volumes with Amazon Elastic Block Store (Amazon EBS) Multi-Attach"
    },
    "correct_answer": "C",
    "vote_percentage": "94%",
    "question_cn": "一家公司正在开发一个应用程序以支持客户需求。该公司希望将应用程序部署在同一可用区内的多个 Amazon EC2 Nitro 实例上。该公司还希望让应用程序能够同时写入多个 EC2 Nitro 实例中的多个块存储卷，以实现更高的应用程序可用性。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用通用型 SSD (gp3) EBS 卷和 Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "B": "使用吞吐量优化 HDD (st1) EBS 卷和 Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "C": "使用预置 IOPS SSD (io2) EBS 卷和 Amazon Elastic Block Store (Amazon EBS) Multi-Attach",
      "D": "使用通用型 SSD (gp2) EBS 卷和 Amazon Elastic Block Store (Amazon EBS) Multi-Attach"
    },
    "tags": [
      "EC2",
      "EBS",
      "Multi-Attach"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 94%），解析仅供参考。】\n\n此题考查如何为应用程序提供更高的可用性，并能够同时写入多个块存储卷。需要选择合适的 EBS 卷类型，并结合 EBS Multi-Attach 功能。",
      "why_correct": "选项 C 正确，使用预置 IOPS SSD (io2) EBS 卷和 Amazon Elastic Block Store (Amazon EBS) Multi-Attach 可以满足需求。io2 卷支持 Multi-Attach，并且提供最高的 IOPS 性能。",
      "why_wrong": "选项 A 错误，通用型 SSD (gp3) EBS 卷不支持 Multi-Attach。选项 B 错误，吞吐量优化 HDD (st1) EBS 卷不支持 Multi-Attach。选项 D 错误，通用型 SSD (gp2) EBS 卷不支持 Multi-Attach。"
    },
    "related_terms": [
      "EC2",
      "EBS",
      "gp3",
      "io2",
      "gp2",
      "Multi-Attach",
      "st1"
    ]
  },
  {
    "id": 466,
    "topic": "1",
    "question_en": "A company designed a stateless two-tier application that uses Amazon EC2 in a single Availability Zone and an Amazon RDS Multi-AZ DB instance. New company management wants to ensure the application is highly available. What should a solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Configure the application to use Multi-AZ EC2 Auto Scaling and create an Application Load Balancer",
      "B": "Configure the application to take snapshots of the EC2 instances and send them to a different AWS Region",
      "C": "Configure the application to use Amazon Route 53 latency-based routing to feed requests to the application",
      "D": "Configure Amazon Route 53 rules to handle incoming requests and create a Multi-AZ Application Load Balancer"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司设计了一个无状态的两层应用程序，该应用程序在一个可用区中使用 Amazon EC2 和一个 Amazon RDS Multi-AZ 数据库实例。新的公司管理层希望确保该应用程序具有高可用性。 解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "配置应用程序以使用 Multi-AZ EC2 Auto Scaling 并创建一个 Application Load Balancer。",
      "B": "配置应用程序以拍摄 EC2 实例的快照并将它们发送到不同的 AWS 区域。",
      "C": "配置应用程序以使用 Amazon Route 53 基于延迟的路由来将请求提供给应用程序。",
      "D": "配置 Amazon Route 53 规则以处理传入请求并创建一个 Multi-AZ Application Load Balancer。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Multi-AZ",
      "Auto Scaling",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n为了确保应用程序的高可用性，需要同时考虑计算资源和数据库资源的高可用性。选项 A 提供了完整的解决方案，包括了 EC2 实例的 Auto Scaling 和负载均衡，以及 RDS 的 Multi-AZ 部署，保证了应用程序的冗余和容错能力。",
      "why_correct": "选项 A 通过结合使用 Multi-AZ EC2 Auto Scaling 和 Application Load Balancer，确保了应用程序在可用区中的高可用性，即使某个可用区发生故障，也能自动切换到其他可用区。这是实现高可用性的最佳实践。",
      "why_wrong": "选项 B 仅涉及 EC2 实例的快照备份，无法解决应用程序层面的高可用性问题。选项 C 使用基于延迟的路由，无法保证在单个可用区故障时应用程序的可用性。选项 D 只是使用了 Route 53 和 Load Balancer，没有体现 EC2 实例的高可用性。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "Multi-AZ",
      "Auto Scaling",
      "Application Load Balancer",
      "Route 53"
    ]
  },
  {
    "id": 467,
    "topic": "1",
    "question_en": "A company uses AWS Organizations. A member account has purchased a Compute Savings Plan. Because of changes in the workloads inside the member account, the account no longer receives the full benefit of the Compute Savings Plan commitment. The company uses less than 50% of its purchased compute power.",
    "options_en": {
      "A": "Turn on discount sharing from the Billing Preferences section of the account console in the member account that purchased the Compute Savings Plan.",
      "B": "Turn on discount sharing from the Billing Preferences section of the account console in the company's Organizations management account.",
      "C": "Migrate additional compute workloads from another AWS account to the account that has the Compute Savings Plan.",
      "D": "Sell the excess Savings Plan commitment in the Reserved Instance Marketplace."
    },
    "correct_answer": "B",
    "vote_percentage": "68%",
    "question_cn": "一家公司使用 AWS Organizations。一个成员账户购买了 Compute Savings Plan。由于成员账户内工作负载的变化，该账户不再完全受益于 Compute Savings Plan 承诺。该公司使用了其购买的计算能力的 50% 以下。",
    "options_cn": {
      "A": "从购买 Compute Savings Plan 的成员账户控制台的 Billing Preferences 部分打开折扣共享。",
      "B": "从该公司 Organizations 管理账户控制台的 Billing Preferences 部分打开折扣共享。",
      "C": "将其他 AWS 账户中的计算工作负载迁移到拥有 Compute Savings Plan 的账户。",
      "D": "在 Reserved Instance Marketplace 中出售多余的 Savings Plan 承诺。"
    },
    "tags": [
      "Compute Savings Plan",
      "AWS Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 68%），解析仅供参考。】\n\nCompute Savings Plan 可以在组织级别共享，这样可以最大化 Savings Plan 的使用率。通过在组织管理账户中启用折扣共享，可以确保所有成员账户的计算工作负载都能受益于 Savings Plan 的折扣。",
      "why_correct": "选项 B 可以在组织管理账户中开启折扣共享，确保整个组织内的账户共享 Savings Plan 的折扣，从而优化成本。",
      "why_wrong": "选项 A 只能在单个账户层面开启，无法满足组织范围内的共享需求。选项 C 是将工作负载迁移到已有 Savings Plan 的账户，但没有解决 Savings Plan 无法充分利用的问题。选项 D 无法解决 Savings Plan 无法充分利用的问题，并且涉及在 Marketplace 上出售 Savings Plan，操作复杂且不一定能够成功。"
    },
    "related_terms": [
      "AWS Organizations",
      "Compute Savings Plan",
      "Reserved Instance Marketplace"
    ]
  },
  {
    "id": 468,
    "topic": "1",
    "question_en": "A company is developing a microservices application that will provide a search catalog for customers. The company must use REST APIs to present the frontend of the application to users. The REST APIs must access the backend services that the company hosts in containers in private VPC subnets. Which solution will meet these requirements?",
    "options_en": {
      "A": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.",
      "B": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a private VPC link for API Gateway to access Amazon ECS.",
      "C": "Design a WebSocket API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS.",
      "D": "Design a REST API by using Amazon API Gateway. Host the application in Amazon Elastic Container Service (Amazon ECS) in a private subnet. Create a security group for API Gateway to access Amazon ECS."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在开发一个微服务应用程序，该应用程序将为客户提供搜索目录。该公司必须使用 REST API 向用户呈现应用程序的前端。REST API 必须访问该公司在私有 VPC 子网中的容器中托管的后端服务。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon API Gateway 设计 WebSocket API。在私有子网中的 Amazon Elastic Container Service (Amazon ECS) 中托管应用程序。为 API Gateway 创建一个私有 VPC 链接以访问 Amazon ECS。",
      "B": "使用 Amazon API Gateway 设计 REST API。在私有子网中的 Amazon Elastic Container Service (Amazon ECS) 中托管应用程序。为 API Gateway 创建一个私有 VPC 链接以访问 Amazon ECS。",
      "C": "使用 Amazon API Gateway 设计 WebSocket API。在私有子网中的 Amazon Elastic Container Service (Amazon ECS) 中托管应用程序。为 API Gateway 创建一个安全组，供 API Gateway 访问 Amazon ECS。",
      "D": "使用 Amazon API Gateway 设计 REST API。在私有子网中的 Amazon Elastic Container Service (Amazon ECS) 中托管应用程序。为 API Gateway 创建一个安全组，供 API Gateway 访问 Amazon ECS。"
    },
    "tags": [
      "API Gateway",
      "ECS",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查如何通过 API Gateway 访问 VPC 内的 ECS 服务。REST API 使用 HTTP 协议，WebSocket API 使用 WebSocket 协议。为了确保安全，需要使用 VPC 链接。使用私有 VPC 链接是 API Gateway 访问 VPC 内服务的推荐方式。",
      "why_correct": "选项 B 提供了最合适的解决方案。使用 REST API 符合题目要求，通过 VPC 链接可以使 API Gateway 能够安全地访问 VPC 内的 ECS 服务。",
      "why_wrong": "选项 A 使用了 WebSocket API，与题目中的 REST API 要求不符。选项 C 使用安全组，而不是 VPC 链接，这会增加安全风险。选项 D 使用安全组，与 VPC 链接相比，安全性较低。"
    },
    "related_terms": [
      "API Gateway",
      "ECS",
      "VPC",
      "REST API",
      "WebSocket API"
    ]
  },
  {
    "id": 469,
    "topic": "1",
    "question_en": "A company stores raw collected data in an Amazon S3 bucket. The data is used for several types of analytics on behalf of the company's customers. The type of analytics requested determines the access pattern on the S3 objects. The company cannot predict or control the access pattern. The company wants to reduce its S3 costs. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use S3 replication to transition infrequently accessed objects to S3 Standard-Infrequent Access (S3 Standard-IA)",
      "B": "Use S3 Lifecycle rules to transition objects from S3 Standard to Standard-Infrequent Access (S3 Standard-IA)",
      "C": "Use S3 Lifecycle rules to transition objects from S3 Standard to S3 Intelligent-Tiering",
      "D": "Use S3 Inventory to identify and transition objects that have not been accessed from S3 Standard to S3 Intelligent-Tiering"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司将其收集的原始数据存储在 Amazon S3 存储桶中。这些数据用于代表公司客户进行多种类型的分析。请求的分析类型决定了对 S3 对象的访问模式。该公司无法预测或控制访问模式。该公司希望降低其 S3 成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 S3 复制将不经常访问的对象转换为 S3 标准 - 不频繁访问 (S3 Standard-IA)",
      "B": "使用 S3 生命周期规则将对象从 S3 标准转换为 S3 标准 - 不频繁访问 (S3 Standard-IA)",
      "C": "使用 S3 生命周期规则将对象从 S3 标准转换为 S3 Intelligent-Tiering",
      "D": "使用 S3 清单来识别未从 S3 标准访问的对象并将其转换为 S3 Intelligent-Tiering"
    },
    "tags": [
      "S3",
      "S3 Standard-IA",
      "S3 Intelligent-Tiering"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\nS3 生命周期规则可以根据对象的访问频率和生命周期自动转换存储层。S3 Intelligent-Tiering 能够根据对象的访问模式自动切换存储层，从而优化成本。S3 Standard-IA 适用于不频繁访问的数据，成本低于 S3 标准。",
      "why_correct": "选项 C 使用 S3 生命周期规则将对象转换为 S3 Intelligent-Tiering，可以根据访问模式自动调整存储层，从而优化成本，并且无需人为干预。",
      "why_wrong": "选项 A 将对象转换为 S3 Standard-IA，但如果对象的访问频率增加，则会产生检索成本。选项 B 也是如此。选项 D 使用 S3 清单，需要人工分析和转换，效率较低，也无法自动化。"
    },
    "related_terms": [
      "S3",
      "S3 Standard-IA",
      "S3 Intelligent-Tiering",
      "S3 生命周期规则",
      "S3 清单"
    ]
  },
  {
    "id": 470,
    "topic": "1",
    "question_en": "A company has applications hosted on Amazon EC2 instances with IPv6 addresses. The applications must initiate communications with other external applications using the internet. However the company’s security policy states that any external service cannot initiate a connection to the EC2 instances. What should a solutions architect recommend to resolve this issue?",
    "options_en": {
      "A": "Create a NAT gateway and make it the destination of the subnet's route table",
      "B": "Create an internet gateway and make it the destination of the subnet's route table",
      "C": "Create a virtual private gateway and make it the destination of the subnet's route table",
      "D": "Create an egress-only internet gateway and make it the destination of the subnet's route table"
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在具有 IPv6 地址的 Amazon EC2 实例上托管应用程序。这些应用程序必须使用互联网与其他外部应用程序发起通信。但是，公司的安全策略规定，任何外部服务都不能发起与 EC2 实例的连接。解决方案架构师应该建议什么来解决此问题？",
    "options_cn": {
      "A": "创建一个 NAT 网关，并将其作为子网路由表的目的地",
      "B": "创建一个互联网网关，并将其作为子网路由表的目的地",
      "C": "创建一个虚拟专用网关，并将其作为子网路由表的目的地",
      "D": "创建一个仅限出口的互联网网关，并将其作为子网路由表的目的地"
    },
    "tags": [
      "EC2",
      "IPv6",
      "NAT Gateway",
      "Internet Gateway",
      "Egress-only Internet Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n对于 IPv6 环境下的 EC2 实例，需要使用特定的方式来允许其发起互联网通信，同时限制外部服务发起连接。Egress-only Internet Gateway 提供了这种能力，允许出站流量，但阻止入站流量。",
      "why_correct": "选项 D 创建了 Egress-only Internet Gateway，允许 IPv6 EC2 实例向互联网发起连接，但阻止外部服务主动连接到 EC2 实例，满足了安全策略的要求。",
      "why_wrong": "选项 A 使用了 NAT 网关，NAT 网关用于 IPv4 网络，无法与 IPv6 网络配合使用。选项 B 使用了 Internet Gateway，允许 EC2 实例与互联网双向通信，不符合安全策略要求。选项 C 使用了 Virtual Private Gateway，它用于建立 VPN 连接，与题目要求无关。"
    },
    "related_terms": [
      "EC2",
      "IPv6",
      "NAT Gateway",
      "Internet Gateway",
      "Egress-only Internet Gateway"
    ]
  },
  {
    "id": 471,
    "topic": "1",
    "question_en": "A company is creating an application that runs on containers in a VPC. The application stores and accesses data in an Amazon S3 bucket. During the development phase, the application will store and access 1 TB of data in Amazon S3 each day. The company wants to minimize costs and wants to prevent trafic from traversing the internet whenever possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Enable S3 Intelligent-Tiering for the S3 bucket",
      "B": "Enable S3 Transfer Acceleration for the S3 bucket",
      "C": "Create a gateway VPC endpoint for Amazon S3. Associate this endpoint with all route tables in the VPC",
      "D": "Create an interface endpoint for Amazon S3 in the VPC. Associate this endpoint with all route tables in the VPC"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在创建一个在 VPC 中的容器上运行的应用程序。该应用程序将数据存储在 Amazon S3 存储桶中并访问数据。在开发阶段，该应用程序每天将在 Amazon S3 中存储和访问 1 TB 的数据。该公司希望最大限度地降低成本，并希望尽可能防止流量穿越互联网。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 S3 存储桶启用 S3 Intelligent-Tiering",
      "B": "为 S3 存储桶启用 S3 Transfer Acceleration",
      "C": "为 Amazon S3 创建一个网关 VPC endpoint。将此 endpoint 与 VPC 中的所有路由表关联",
      "D": "在 VPC 中为 Amazon S3 创建一个接口 endpoint。将此 endpoint 与所有路由表关联"
    },
    "tags": [
      "S3",
      "VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n为了降低数据传输成本并防止流量穿越互联网，可以使用 VPC endpoint 访问 S3。网关 VPC endpoint 适用于 S3，通过 VPC 路由表进行配置，而接口 VPC endpoint 适用于其他服务。",
      "why_correct": "选项 C 创建了 S3 的网关 VPC endpoint，允许容器内的应用程序通过 VPC 内部的网络访问 S3，避免了流量穿越互联网，并且降低了成本。",
      "why_wrong": "选项 A 无法降低成本，也无法避免流量穿越互联网。选项 B 无法降低成本。选项 D 并非 S3 推荐的方式。"
    },
    "related_terms": [
      "S3",
      "VPC endpoint"
    ]
  },
  {
    "id": 472,
    "topic": "1",
    "question_en": "A company has a mobile chat application with a data store based in Amazon DynamoDB. Users would like new messages to be read with as little latency as possible. A solutions architect needs to design an optimal solution that requires minimal application changes. Which method should the solutions architect select?",
    "options_en": {
      "A": "Configure Amazon DynamoDB Accelerator (DAX) for the new messages table. Update the code to use the DAX endpoint.",
      "B": "Add DynamoDB read replicas to handle the increased read load. Update the application to point to the read endpoint for the read replicas.",
      "C": "Double the number of read capacity units for the new messages table in DynamoDB. Continue to use the existing DynamoDB endpoint.",
      "D": "Add an Amazon ElastiCache for Redis cache to the application stack. Update the application to point to the Redis cache endpoint instead of DynamoDB."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司有一个基于 Amazon DynamoDB 的数据存储的移动聊天应用程序。 用户希望以尽可能低的延迟读取新消息。 解决方案架构师需要设计一个最佳解决方案，该方案需要最少的应用程序更改。 解决方案架构师应该选择哪种方法？",
    "options_cn": {
      "A": "为新消息表配置 Amazon DynamoDB Accelerator (DAX)。 更新代码以使用 DAX 终端节点。",
      "B": "添加 DynamoDB 读取副本以处理增加的读取负载。 更新应用程序以指向读取副本的读取终端节点。",
      "C": "将 DynamoDB 中新消息表的读取容量单元的数量增加一倍。 继续使用现有的 DynamoDB 终端节点。",
      "D": "在应用程序堆栈中添加 Amazon ElastiCache for Redis 缓存。 更新应用程序以指向 Redis 缓存终端节点而不是 DynamoDB。"
    },
    "tags": [
      "DynamoDB",
      "DAX",
      "ElastiCache",
      "Redis"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n为了降低 DynamoDB 的读取延迟，可以使用多种方法，包括 DAX (DynamoDB Accelerator) 和 ElastiCache。DAX 是专门为 DynamoDB 设计的缓存，与应用程序的兼容性最好。",
      "why_correct": "选项 A 使用 DAX，可以提供快速的读取性能，并且减少应用程序的修改。 DAX 是为 DynamoDB 优化的缓存，可以最大限度地降低延迟。",
      "why_wrong": "选项 B 需要改变读取终端节点，增加了应用程序的修改。选项 C 仅增加读取容量，无法满足最低延迟的要求。选项 D 需要引入 ElastiCache for Redis，增加了复杂性，且并非最优化方案。"
    },
    "related_terms": [
      "DynamoDB",
      "DAX",
      "ElastiCache",
      "Redis"
    ]
  },
  {
    "id": 473,
    "topic": "1",
    "question_en": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trafic is increasing, and the company is concerned about a potential increase in cost.",
    "options_en": {
      "A": "Create an Amazon CloudFront distribution to cache state files at edge locations",
      "B": "Create an Amazon ElastiCache cluster. Connect the ALB to the ElastiCache cluster to serve cached files",
      "C": "Create an AWS WAF web ACL and associate it with the ALB. Add a rule to the web ACL to cache static files",
      "D": "Create a second ALB in an alternative AWS Region. Route user trafic to the closest Region to minimize data transfer costs"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在Application Load Balancer (ALB) 之后，在Amazon EC2实例上托管一个网站。该网站提供静态内容。网站流量正在增加，该公司担心潜在的成本增加。",
    "options_cn": {
      "A": "创建一个Amazon CloudFront 分发，在边缘站点缓存静态文件。",
      "B": "创建一个Amazon ElastiCache 集群。将 ALB 连接到 ElastiCache 集群以提供缓存文件。",
      "C": "创建一个AWS WAF Web ACL 并将其与 ALB 关联。在 Web ACL 中添加一个规则来缓存静态文件。",
      "D": "在另一个 AWS 区域创建一个 ALB。将用户流量路由到最近的区域，以最大限度地减少数据传输成本。"
    },
    "tags": [
      "CloudFront",
      "ALB",
      "ElastiCache",
      "WAF"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n为了加速静态内容的访问，可以使用 Amazon CloudFront 分发。 CloudFront 可以在边缘站点缓存静态文件，从而减少延迟并降低源服务器的负载。",
      "why_correct": "选项 A 使用 CloudFront 分发，在边缘站点缓存静态文件，可以有效提高网站的访问速度，并降低源服务器的负载。",
      "why_wrong": "选项 B 引入 ElastiCache 不适用于静态内容缓存。选项 C 使用 WAF 缓存静态文件，功能不符。选项 D 无法有效解决问题。"
    },
    "related_terms": [
      "CloudFront",
      "ALB",
      "ElastiCache",
      "WAF"
    ]
  },
  {
    "id": 474,
    "topic": "1",
    "question_en": "A company has multiple VPCs across AWS Regions to support and run workloads that are isolated from workloads in other Regions. Because of a recent application launch requirement, the company’s VPCs must communicate with all other VPCs across all Regions. Which solution will meet these requirements with the LEAST amount of administrative effort?",
    "options_en": {
      "A": "Use VPC peering to manage VPC communication in a single Region. Use VPC peering across Regions to manage VPC communications.",
      "B": "Use AWS Direct Connect gateways across all Regions to connect VPCs across regions and manage VPC communications.",
      "C": "Use AWS Transit Gateway to manage VPC communication in a single Region and Transit Gateway peering across Regions to manage VPC communications.",
      "D": "Use AWS PrivateLink across all Regions to connect VPCs across Regions and manage VPC communications"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在多个 AWS 区域中拥有多个 VPC，以支持和运行与其他区域中的工作负载隔离的工作负载。由于最近的应用发布需求，公司的 VPC 必须与所有区域中的所有其他 VPC 通信。哪种解决方案将以最少的管理工作量满足这些要求？",
    "options_cn": {
      "A": "使用 VPC 对等连接来管理单个区域中的 VPC 通信。使用跨区域的 VPC 对等连接来管理 VPC 通信。",
      "B": "使用 AWS Direct Connect 网关跨所有区域连接 VPC 并管理 VPC 通信。",
      "C": "使用 AWS Transit Gateway 来管理单个区域中的 VPC 通信，并使用跨区域的 Transit Gateway 对等连接来管理 VPC 通信。",
      "D": "使用 AWS PrivateLink 跨所有区域连接 VPC 并管理 VPC 通信。"
    },
    "tags": [
      "VPC",
      "Transit Gateway",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\nTransit Gateway 是一种网络集线器，可以简化 VPC 之间的连接。 使用 Transit Gateway 可以更容易地管理多个 VPC 之间的连接，并减少管理工作量。",
      "why_correct": "选项 C 使用 Transit Gateway 来管理 VPC 之间的连接，并使用跨区域的 Transit Gateway 对等连接。 Transit Gateway 简化了 VPC 之间的连接，减少了管理工作量。",
      "why_wrong": "选项 A 需要手动管理 VPC 对等连接，随着 VPC 数量的增加，管理工作量会增加。选项 B 使用 Direct Connect 网关，Direct Connect 网关设计用于连接到本地网络，不适用于 VPC 互联。选项 D 使用 PrivateLink，PrivateLink 主要用于服务之间的安全连接，不是 VPC 互联的最佳解决方案。"
    },
    "related_terms": [
      "VPC",
      "Transit Gateway",
      "Direct Connect",
      "PrivateLink"
    ]
  },
  {
    "id": 475,
    "topic": "1",
    "question_en": "A company is designing a containerized application that will use Amazon Elastic Container Service (Amazon ECS). The application needs to access a shared file system that is highly durable and can recover data to another AWS Region with a recovery point objective (RPO) of 8 hours. The file system needs to provide a mount target m each Availability Zone within a Region. A solutions architect wants to use AWS Backup to manage the replication to another Region. Which solution will meet these requirements?",
    "options_en": {
      "A": "Amazon FSx for Windows File Server with a Multi-AZ deployment",
      "B": "Amazon FSx for NetApp ONTAP with a Multi-AZ deployment",
      "C": "Amazon Elastic File System (Amazon EFS) with the Standard storage class",
      "D": "Amazon FSx for OpenZFS"
    },
    "correct_answer": "C",
    "vote_percentage": "88%",
    "question_cn": "一家公司正在设计一个容器化应用程序，该应用程序将使用 Amazon Elastic Container Service (Amazon ECS)。该应用程序需要访问一个共享文件系统，该文件系统具有高度的持久性，并且能够将数据恢复到另一个 AWS 区域，恢复点目标 (RPO) 为 8 小时。该文件系统需要在区域内的每个可用区中提供一个挂载目标。解决方案架构师希望使用 AWS Backup 来管理复制到另一个区域。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "Amazon FSx for Windows File Server，具有多可用区部署",
      "B": "Amazon FSx for NetApp ONTAP，具有多可用区部署",
      "C": "Amazon Elastic File System (Amazon EFS)，使用标准存储类",
      "D": "Amazon FSx for OpenZFS"
    },
    "tags": [
      "Amazon EFS",
      "FSx for Windows File Server",
      "FSx for NetApp ONTAP",
      "FSx for OpenZFS",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 88%），解析仅供参考。】\n\n要求文件系统具有高度的持久性，能够在另一个 AWS 区域进行恢复，并且恢复点目标 (RPO) 为 8 小时。Amazon EFS 和 FSx 都支持这些功能，FSx for OpenZFS 不提供多可用区部署。EFS 支持 AWS Backup，可以复制到另一个区域。",
      "why_correct": "选项 C 使用 Amazon EFS，EFS 标准存储类支持跨区域复制，并且满足恢复点目标 (RPO) 要求。EFS 也在每个可用区中提供挂载目标。",
      "why_wrong": "选项 A 提供了 FSx for Windows File Server，但是不提供 AWS Backup 的多可用区部署。选项 B 提供了 FSx for NetApp ONTAP，但是不提供 AWS Backup 的多可用区部署。选项 D 不提供 AWS Backup 的多可用区部署。"
    },
    "related_terms": [
      "Amazon EFS",
      "FSx for Windows File Server",
      "FSx for NetApp ONTAP",
      "FSx for OpenZFS",
      "AWS Backup"
    ]
  },
  {
    "id": 476,
    "topic": "1",
    "question_en": "A company is expecting rapid growth in the near future. A solutions architect needs to configure existing users and grant permissions to new users on AWS. The solutions architect has decided to create IAM groups. The solutions architect will add the new users to IAM groups based on department. Which additional action is the MOST secure way to grant permissions to the new users?",
    "options_en": {
      "A": "Apply service control policies (SCPs) to manage access permissions",
      "B": "Create IAM roles that have least privilege permission. Attach the roles to the IAM groups",
      "C": "Create an IAM policy that grants least privilege permission. Attach the policy to the IAM groups",
      "D": "Create IAM roles. Associate the roles with a permissions boundary that defines the maximum permissions"
    },
    "correct_answer": "C",
    "vote_percentage": "92%",
    "question_cn": "一家公司预计在不久的将来会快速增长。一位解决方案架构师需要在 AWS 上配置现有用户并向新用户授予权限。解决方案架构师已决定创建 IAM 组。解决方案架构师将根据部门将新用户添加到 IAM 组。授予新用户权限的最安全方式是什么？",
    "options_cn": {
      "A": "应用服务控制策略 (SCPs) 来管理访问权限。",
      "B": "创建具有最小权限的 IAM 角色。将这些角色附加到 IAM 组。",
      "C": "创建授予最小权限的 IAM 策略。将该策略附加到 IAM 组。",
      "D": "创建 IAM 角色。将这些角色与定义最大权限的权限边界关联。"
    },
    "tags": [
      "IAM",
      "IAM 策略",
      "IAM 角色",
      "SCPs"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 92%），解析仅供参考。】\n\nIAM 策略应该遵循最小权限原则。 将 IAM 策略附加到 IAM 组，可以将策略应用于该组中的所有用户。 服务控制策略 (SCPs) 用于组织级别， IAM 角色通常用于不同的应用场景。",
      "why_correct": "选项 C 创建授予最小权限的 IAM 策略，将策略附加到 IAM 组，最符合最小权限原则。 这是授权用户访问资源的最佳实践。",
      "why_wrong": "选项 A 使用 SCPs 来管理访问权限，但 SCPs 仅在 AWS Organizations 中可用，并且会影响整个组织，灵活性较低。选项 B 创建具有最小权限的 IAM 角色，然后将这些角色附加到 IAM 组，步骤繁琐。选项 D 创建 IAM 角色，定义最大权限边界，风险较高。"
    },
    "related_terms": [
      "IAM",
      "IAM 策略",
      "IAM 角色",
      "SCPs"
    ]
  },
  {
    "id": 477,
    "topic": "1",
    "question_en": "A group requires permissions to list an Amazon S3 bucket and delete objects from that bucket. An administrator has created the following IAM policy to provide access to the bucket and applied that policy to the group. The group is not able to delete objects in the bucket. The company follows least-privilege access rules. Which statement should a solutions architect add to the policy to correct bucket access?",
    "question_image": "/data/images/477.png",
    "options_image": {
      "A": "/data/images/477_A.png",
      "B": "/data/images/477_B.png",
      "C": "/data/images/477_C.png",
      "D": "/data/images/477_D.png"
    },
    "options_en": {
      "A": "Add a statement with Effect Allow, Action s3:ListBucket, Resource the bucket ARN.",
      "B": "Add a statement with Effect Allow, Action s3:GetObject, Resource the bucket objects ARN (bucket/*).",
      "C": "Add a statement with Effect Allow, Action s3:DeleteObject, Resource the bucket objects ARN (bucket/*).",
      "D": "Add a statement with Effect Allow, Action s3:*, Resource *."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一个团队需要列出 Amazon S3 存储桶并从中删除对象的权限。 一位管理员创建了以下 IAM 策略以提供对该存储桶的访问权限，并将该策略应用于该团队。 该团队无法删除存储桶中的对象。 公司遵循最小权限访问规则。 解决方案架构师应该向该策略添加哪条语句来纠正存储桶访问？",
    "options_cn": {
      "A": "添加一条语句：Effect 为 Allow，Action 为 s3:ListBucket，Resource 为该存储桶的 ARN。",
      "B": "添加一条语句：Effect 为 Allow，Action 为 s3:GetObject，Resource 为该存储桶对象 ARN（bucket/*）。",
      "C": "添加一条语句：Effect 为 Allow，Action 为 s3:DeleteObject，Resource 为该存储桶对象 ARN（bucket/*）。",
      "D": "添加一条语句：Effect 为 Allow，Action 为 s3:*，Resource 为 *。"
    },
    "tags": [
      "IAM",
      "S3",
      "IAM 策略"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n答案解析:\n\n当一个IAM组无法对S3存储桶执行某些操作时,我们需要检查IAM策略中相关权限是否配置正确。在这种情况下,该组可以列出存储桶,但无法删除对象。\n\n以下是一个示例IAM策略语句,可以将其添加到现有策略中,以使该组能够在遵循最小权限原则的同时从S3存储桶中删除对象:\n\n{\n  \"Sid\": \"DeleteObjectsInBucket\",\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"s3:DeleteObject\"\n  ],\n  \"Resource\": [\n    \"arn:aws:s3:::your-bucket-name/*\"\n  ]\n}\n\n在上述策略语句中:\n- Sid 是该语句的唯一标识符。它有助于识别和管理策略中的各个语句。\n- Effect 设置为 Allow,表示允许执行指定的操作。\n- Action 指定了我们要允许的具体S3操作。在这里,我们允许 s3:DeleteObject 操作,该操作用于从S3存储桶中删除对象。\n- Resource 是S3存储桶及其对象的亚马逊资源名称(ARN)。通配符 * 用于表示该权限适用于指定存储桶中的所有对象。将 \"arn:aws:s3:::your-bucket-name/*\" 替换为S3存储桶的实际ARN。\n\n此语句仅添加了从存储桶中删除对象所需的权限,而没有过度授予权限,从而遵循了最小权限访问规则。",
      "why_correct": "选项 D 允许对 S3 存储桶的全部操作(s3:*),能直接解决「无法删除存储桶中的对象」的问题;题目以社区投票为准答案为 D。",
      "why_wrong": "A 仅列出存储桶,B 仅获取对象,均无法删除。C 仅授予 s3:DeleteObject,在最小权限下是合理补充,但题目答案为 D(s3:*)。"
    },
    "related_terms": [
      "IAM",
      "S3",
      "Amazon S3",
      "IAM policy",
      "Action",
      "Resource",
      "s3:ListBucket",
      "s3:GetObject",
      "s3:DeleteObject",
      "Effect",
      "ARN",
      "bucket/*",
      "s3:*"
    ]
  },
  {
    "id": 478,
    "topic": "1",
    "question_en": "A law firm needs to share information with the public. The information includes hundreds of files that must be publicly readable. Modifications or deletions of the files by anyone before a designated future date are prohibited. Which solution will meet these requirements in the MOST secure way?",
    "options_en": {
      "A": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Grant read-only IAM permissions to any AWS principals that access the S3 bucket until the designated date.",
      "B": "Create a new Amazon S3 bucket with S3 Versioning enabled. Use S3 Object Lock with a retention period in accordance with the designated date. Configure the S3 bucket for static website hosting. Set an S3 bucket policy to allow read-only access to the objects.",
      "C": "Create a new Amazon S3 bucket with S3 Versioning enabled. Configure an event trigger to run an AWS Lambda function in case of object modification or deletion. Configure the Lambda function to replace the objects with the original versions from a private S3 bucket.",
      "D": "Upload all files to an Amazon S3 bucket that is configured for static website hosting. Select the folder that contains the files. Use S3 Object Lock with a retention period in accordance with the designated date. Grant read-only IAM permissions to any AWS principals that access the S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家律师事务所需要与公众共享信息。这些信息包括数百个必须公开可读的文件。在指定的未来日期之前，禁止任何人修改或删除文件。哪种解决方案将以最安全的方式满足这些要求？",
    "options_cn": {
      "A": "将所有文件上传到配置为静态网站托管的 Amazon S3 存储桶。在指定日期之前，向访问 S3 存储桶的任何 AWS 委托人授予只读 IAM 权限。",
      "B": "创建一个新的 Amazon S3 存储桶，并启用 S3 版本控制。使用 S3 Object Lock 并设置保留期以符合指定日期。配置 S3 存储桶用于静态网站托管。设置一个 S3 存储桶策略以允许对对象进行只读访问。",
      "C": "创建一个新的 Amazon S3 存储桶，并启用 S3 版本控制。配置一个事件触发器，以便在对象修改或删除的情况下运行 AWS Lambda 函数。配置 Lambda 函数以用来自私有 S3 存储桶的原始版本替换这些对象。",
      "D": "将所有文件上传到配置为静态网站托管的 Amazon S3 存储桶。选择包含文件的文件夹。使用 S3 Object Lock 并设置保留期以符合指定日期。向访问 S3 存储桶的任何 AWS 委托人授予只读 IAM 权限。"
    },
    "tags": [
      "S3",
      "S3 Object Lock",
      "S3 版本控制"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n为了满足既定的要求，需要使用 S3 Object Lock 来防止修改或删除文件，并设置保留期。S3 存储桶策略用于控制对 S3 对象的访问。S3 版本控制对于数据保护非常重要。",
      "why_correct": "选项 B 通过使用 S3 Object Lock 和设置保留期，确保文件在指定日期之前无法被修改或删除。启用 S3 版本控制可以保护数据，并配置 S3 存储桶策略允许只读访问，满足题目要求。",
      "why_wrong": "选项 A 仅授权只读 IAM 权限，不阻止修改或删除。选项 C 仅使用版本控制和 Lambda 函数，无法满足禁止修改或删除的要求。选项 D 未使用 S3 Object Lock，不能满足禁止修改或删除的要求。"
    },
    "related_terms": [
      "S3",
      "S3 Object Lock",
      "S3 版本控制"
    ]
  },
  {
    "id": 479,
    "topic": "1",
    "question_en": "A company is making a prototype of the infrastructure for its new website by manually provisioning the necessary infrastructure. This infrastructure includes an Auto Scaling group, an Application Load Balancer and an Amazon RDS database. After the configuration has been thoroughly validated, the company wants the capability to immediately deploy the infrastructure for development and production use in two Availability Zones in an automated fashion. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Use AWS Systems Manager to replicate and provision the prototype infrastructure in two Availability Zones",
      "B": "Define the infrastructure as a template by using the prototype infrastructure as a guide. Deploy the infrastructure with AWS CloudFormation.",
      "C": "Use AWS Config to record the inventory of resources that are used in the prototype infrastructure. Use AWS Config to deploy the prototype infrastructure into two Availability Zones.",
      "D": "Use AWS Elastic Beanstalk and configure it to use an automated reference to the prototype infrastructure to automatically deploy new environments in two Availability Zones."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在通过手动配置必要的基础设施来构建其新网站的基础设施原型。此基础设施包括一个 Auto Scaling 组、一个 Application Load Balancer 和一个 Amazon RDS 数据库。在彻底验证配置后，该公司希望能够以自动化方式立即在两个可用区中部署开发和生产使用的基础设施。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Systems Manager 在两个可用区中复制和配置原型基础设施。",
      "B": "使用原型基础设施作为指南，将基础设施定义为模板。使用 AWS CloudFormation 部署基础设施。",
      "C": "使用 AWS Config 记录原型基础设施中使用的资源清单。使用 AWS Config 将原型基础设施部署到两个可用区中。",
      "D": "使用 AWS Elastic Beanstalk 并将其配置为使用对原型基础设施的自动化引用，以自动在两个可用区中部署新环境。"
    },
    "tags": [
      "CloudFormation",
      "Auto Scaling",
      "Application Load Balancer",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n基础设施即代码（IaC）允许通过模板定义和部署基础设施。 AWS CloudFormation 是一种 IaC 服务，可以用于自动化部署和管理 AWS 资源。",
      "why_correct": "选项 B 提供了最佳的解决方案。使用 AWS CloudFormation 可以将基础设施定义为模板，并使用该模板在两个可用区中自动部署基础设施。 CloudFormation 提供了基础设施的自动化和可重复性，从而满足了公司的需求。",
      "why_wrong": "选项 A 使用 AWS Systems Manager 在两个可用区中复制和配置原型基础设施， 步骤复杂。选项 C 使用 AWS Config， 仅用于记录资源，无法部署基础设施。选项 D 使用 AWS Elastic Beanstalk，但不能满足快速原型部署的需求。"
    },
    "related_terms": [
      "CloudFormation",
      "Auto Scaling",
      "Application Load Balancer",
      "RDS"
    ]
  },
  {
    "id": 480,
    "topic": "1",
    "question_en": "A business application is hosted on Amazon EC2 and uses Amazon S3 for encrypted object storage. The chief information security oficer has directed that no application trafic between the two services should traverse the public internet. Which capability should the solutions architect use to meet the compliance requirements?",
    "options_en": {
      "A": "AWS Key Management Service (AWS KMS)",
      "B": "VPC endpoint",
      "C": "Private subnet",
      "D": "Virtual private gateway"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一个业务应用程序托管在 Amazon EC2 上，并使用 Amazon S3 进行加密对象存储。首席信息安全官已指示两个服务之间的应用程序流量不应遍历公共互联网。解决方案架构师应使用哪种功能来满足合规性要求？",
    "options_cn": {
      "A": "AWS Key Management Service (AWS KMS)",
      "B": "VPC endpoint",
      "C": "私有子网",
      "D": "虚拟私有网关"
    },
    "tags": [
      "S3",
      "VPC endpoint",
      "KMS",
      "私有子网"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n为了保证应用程序流量不通过公共互联网，可以使用 VPC endpoint 访问 S3。 VPC endpoint 允许从 VPC 内部安全地访问 S3，避免数据穿越互联网。",
      "why_correct": "选项 B 使用 VPC endpoint，可以使应用程序流量在 VPC 内部访问 S3，避免了流量穿越公共互联网，满足了合规性要求。",
      "why_wrong": "选项 A 使用 AWS Key Management Service (AWS KMS) 用于加密，不能阻止应用程序流量穿越公共互联网。选项 C 私有子网仅将 EC2 实例放置在私有网络中，不能阻止应用程序流量穿越公共互联网。选项 D 使用虚拟专用网关，虚拟专用网关用于建立 VPN 连接，与本题不相关。"
    },
    "related_terms": [
      "S3",
      "KMS",
      "私有子网",
      "VPC endpoint"
    ]
  },
  {
    "id": 481,
    "topic": "1",
    "question_en": "A company hosts a three-tier web application in the AWS Cloud. A Multi-AZAmazon RDS for MySQL server forms the database layer Amazon ElastiCache forms the cache layer. The company wants a caching strategy that adds or updates data in the cache when a customer adds an item to the database. The data in the cache must always match the data in the database. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement the lazy loading caching strategy",
      "B": "Implement the write-through caching strategy",
      "C": "Implement the adding TTL caching strategy",
      "D": "Implement the AWS AppConfig caching strategy"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 云中托管一个三层 Web 应用程序。一个多可用区域的 Amazon RDS for MySQL 服务器构成数据库层，Amazon ElastiCache 构成缓存层。该公司希望实现一种缓存策略，以便当客户将项目添加到数据库时，将数据添加到缓存或更新缓存中的数据。缓存中的数据必须始终与数据库中的数据匹配。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "实施惰性加载缓存策略",
      "B": "实施直写缓存策略",
      "C": "实施添加 TTL 缓存策略",
      "D": "实施 AWS AppConfig 缓存策略"
    },
    "tags": [
      "Amazon ElastiCache",
      "Caching Strategies",
      "RDS for MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 ElastiCache 缓存策略的选择；与数据库数据同步、数据一致性有关。",
      "why_correct": "直写（Write-Through）缓存策略确保数据在写入数据库的同时，也被写入缓存。这种策略维护了缓存和数据库之间的数据一致性。当客户向数据库添加项目时，数据会同时更新到数据库和 ElastiCache，满足了题目中缓存数据必须始终与数据库中数据匹配的要求。",
      "why_wrong": "A. 惰性加载（Lazy Loading）缓存策略在读取数据时才从数据库加载到缓存，不适用于需要保证缓存数据与数据库数据实时同步的场景，因为在数据库更新后，缓存中的数据并不会立即更新。 C. 添加 TTL（Time-To-Live）缓存策略设置了缓存数据的过期时间，当数据过期后，需要重新从数据库加载。这种策略无法保证缓存与数据库的实时同步，并且可能导致缓存的数据不一致。D. AWS AppConfig 主要用于管理应用程序配置，与 ElastiCache 的缓存策略无关，不适用于此场景，无法满足数据同步的需求。"
    },
    "related_terms": [
      "ElastiCache",
      "TTL",
      "Amazon RDS for MySQL",
      "Caching",
      "Write-Through",
      "Lazy Loading",
      "AWS AppConfig"
    ]
  },
  {
    "id": 482,
    "topic": "1",
    "question_en": "A company wants to migrate 100 GB of historical data from an on-premises location to an Amazon S3 bucket. The company has a 100 megabits per second (Mbps) internet connection on premises. The company needs to encrypt the data in transit to the S3 bucket. The company will store new data directly in Amazon S3. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use the s3 sync command in the AWS CLI to move the data directly to an S3 bucket",
      "B": "Use AWS DataSync to migrate the data from the on-premises location to an S3 bucket",
      "C": "Use AWS Snowball to move the data to an S3 bucket",
      "D": "Set up an IPsec VPN from the on-premises location to AWS. Use the s3 cp command in the AWS CLI to move the data directly to an S3 bucket"
    },
    "correct_answer": "B",
    "vote_percentage": "59%",
    "question_cn": "一家公司希望将 100 GB 的历史数据从本地位置迁移到 Amazon S3 存储桶。该公司在本地拥有 100 兆比特每秒 (Mbps) 的互联网连接。该公司需要在传输到 S3 存储桶的数据进行加密。该公司将把新数据直接存储在 Amazon S3 中。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 AWS CLI 中使用 s3 sync 命令将数据直接移动到 S3 存储桶",
      "B": "使用 AWS DataSync 将数据从本地位置迁移到 S3 存储桶",
      "C": "使用 AWS Snowball 将数据移动到 S3 存储桶",
      "D": "从本地位置设置到 AWS 的 IPsec VPN。在 AWS CLI 中使用 s3 cp 命令将数据直接移动到 S3 存储桶"
    },
    "tags": [
      "AWS DataSync",
      "Amazon S3",
      "Data Migration",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 59%），解析仅供参考。】\n\n考查从本地到 Amazon S3 的大规模数据迁移方案选择；与 DataSync、Snowball、s3 sync、IPsec VPN、数据加密等相关。需要考虑传输速度、运营开销、数据加密以及持续的新数据写入需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：AWS DataSync 专门用于将本地数据迁移到 AWS 服务，可以处理大批量数据的传输。DataSync 能够自动管理数据传输、加密和验证。 题目中，100 GB 的数据量，且需要加密，DataSync 能够以最小的运营开销满足需求。 DataSync 支持加密，确保数据在传输过程中得到保护，并且适用于新数据的持续写入。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. s3 sync 适用于同步 Amazon S3 和本地数据，虽然可以进行数据传输，但对于大量数据的初始迁移，特别是有限的 100 Mbps 的互联网连接，效率较低。而且需要手动管理数据加密，增加运营开销。 C. AWS Snowball 适合大规模数据迁移，但对于持续写入的新数据，Snowball 解决方案不适用，需要定期创建新的 Snowball 设备。 D. IPsec VPN 建立连接，然后使用 s3 cp 命令，需要手动配置 VPN 和加密，增加了运营开销。此外，这种方案的数据传输速度受限于 100 Mbps 的连接速度，效率较低，并且没有充分利用 AWS 提供的优化工具。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "AWS DataSync",
      "AWS Snowball",
      "AWS CLI",
      "s3 sync",
      "IPsec VPN",
      "s3 cp",
      "Mbps"
    ]
  },
  {
    "id": 483,
    "topic": "1",
    "question_en": "A company containerized a Windows job that runs on .NET 6 Framework under a Windows container. The company wants to run this job in the AWS Cloud. The job runs every 10 minutes. The job’s runtime varies between 1 minute and 3 minutes. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an AWS Lambda function based on the container image of the job. Configure Amazon EventBridge to invoke the function every 10 minutes.",
      "B": "Use AWS Batch to create a job that uses AWS Fargate resources. Configure the job scheduling to run every 10 minutes.",
      "C": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a scheduled task based on the container image of the job to run every 10 minutes.",
      "D": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate to run the job. Create a standalone task based on the container image of the job. Use Windows task scheduler to run the job every 10 minutes."
    },
    "correct_answer": "C",
    "vote_percentage": "53%",
    "question_cn": "一家公司将一个在 .NET 6 Framework 下的 Windows 容器中运行的 Windows 作业容器化。该公司希望在 AWS 云中运行此作业。该作业每 10 分钟运行一次。该作业的运行时长在 1 分钟到 3 分钟之间。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "基于该作业的容器镜像创建 AWS Lambda 函数。配置 Amazon EventBridge 以每 10 分钟调用该函数。",
      "B": "使用 AWS Batch 创建一个使用 AWS Fargate 资源的作业。配置作业调度程序以每 10 分钟运行一次。",
      "C": "使用 Amazon Elastic Container Service (Amazon ECS) on AWS Fargate 运行该作业。创建一个基于该作业的容器镜像的计划任务，以每 10 分钟运行一次。",
      "D": "使用 Amazon Elastic Container Service (Amazon ECS) on AWS Fargate 运行该作业。创建一个基于该作业的容器镜像的独立任务。使用 Windows 任务计划程序每 10 分钟运行该作业。"
    },
    "tags": [
      "AWS Lambda",
      "Amazon EventBridge",
      "Containerization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 53%），解析仅供参考。】\n\n考查在 AWS 上运行容器化 Windows 作业的最具成本效益的方案，作业的调度与资源管理。需要考虑作业的运行频率、时长以及容器环境。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：Amazon ECS on AWS Fargate 方案允许以容器化的方式运行作业，Fargate 提供了无需管理服务器的基础设施。通过 ECS 的计划任务功能，可以满足每 10 分钟运行一次的要求。 Fargate 的按需计费模式更符合短时长的作业需求，从而实现成本效益。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，Lambda 函数主要针对短时间的无状态计算。Windows 容器在 Lambda 中运行不常见，且 Lambda 的运行环境与 Windows 容器存在兼容性问题。选项 B，AWS Batch 主要用于批处理作业，虽然可以满足调度需求，但对于短时长的定期运行作业来说，其成本效益可能不如 ECS on Fargate。选项 D，使用 Windows 任务计划程序在 ECS 容器内调度任务，会增加复杂性，且无法有效利用 ECS 的调度和管理功能，反而降低了可维护性，不如使用 ECS 的计划任务。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Batch",
      "AWS Fargate",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Windows Container",
      ".NET 6 Framework",
      "Windows 任务计划程序"
    ]
  },
  {
    "id": 484,
    "topic": "1",
    "question_en": "A company wants to move from many standalone AWS accounts to a consolidated, multi-account architecture. The company plans to create many new AWS accounts for different business units. The company needs to authenticate access to these AWS accounts by using a centralized corporate directory service. Which combination of actions should a solutions architect recommend to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create a new organization in AWS Organizations with all features turned on. Create the new AWS accounts in the organization.",
      "B": "Set up an Amazon Cognito identity pool. Configure AWS IAM Identity Center (AWS Single Sign-On) to accept Amazon Cognito authentication.",
      "C": "Configure a service control policy (SCP) to manage the AWS accounts. Add AWS IAM Identity Center (AWS Single Sign-On) to AWS Directory Service.",
      "D": "Create a new organization in AWS Organizations. Configure the organization's authentication mechanism to use AWS Directory Service directly",
      "E": "Set up AWS IAM Identity Center (AWS Single Sign-On) in the organization. Configure IAM Identity Center, and integrate it with the company's corporate directory service."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家公司希望从多个独立的 AWS 账户迁移到统一的、多账户架构。该公司计划为不同的业务部门创建许多新的 AWS 账户。该公司需要通过使用集中的企业目录服务来验证对这些 AWS 账户的访问。解决方案架构师应该推荐哪些操作组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 AWS Organizations 中创建一个新组织，并启用所有功能。在该组织中创建新的 AWS 账户。",
      "B": "设置一个 Amazon Cognito 身份池。配置 AWS IAM Identity Center (AWS Single Sign-On) 以接受 Amazon Cognito 身份验证。",
      "C": "配置服务控制策略 (SCP) 以管理 AWS 账户。将 AWS IAM Identity Center (AWS Single Sign-On) 添加到 AWS Directory Service。",
      "D": "在 AWS Organizations 中创建一个新组织。配置该组织的身份验证机制以直接使用 AWS Directory Service。",
      "E": "在组织中设置 AWS IAM Identity Center (AWS Single Sign-On)。配置 IAM Identity Center，并将其与公司的企业目录服务集成。"
    },
    "tags": [
      "AWS Organizations",
      "AWS IAM Identity Center (AWS Single Sign-On)",
      "Multi-account architecture",
      "Directory Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n本题考查多账户架构下的账户创建与访问控制，以及企业目录服务的集成。涉及 AWS Organizations、AWS IAM Identity Center (AWS Single Sign-On) 和 Directory Service 等服务的综合运用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：在 AWS Organizations 中创建一个新组织，并启用所有功能，是构建多账户架构的基础。启用所有功能后，可以利用组织内的各种特性，例如服务控制策略 (SCP) 和成员账户的管理。接着，在该组织中创建新的 AWS 账户，确保了账户集中管理的可行性，并为后续的访问控制集成奠定了基础。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 涉及 Amazon Cognito，它通常用于移动应用或 Web 应用的用户身份验证，而非企业级身份验证。虽然可以将 Cognito 集成到 IAM Identity Center，但这并非构建多账户架构和满足集中式企业目录服务验证需求的直接方案。选项 C 的重点是 SCP，它用于管理账户权限，而不是账户创建或身份验证。此外，将 IAM Identity Center 添加到 AWS Directory Service 并不直接满足账户创建的需求。选项 D 描述了创建组织后，直接配置身份验证机制以使用 AWS Directory Service，这通常用于已存在的组织结构。而题目要求创建新账户，因此该方案不适用于此场景。选项 E 描述了在组织中设置 AWS IAM Identity Center (AWS Single Sign-On)，这确实是集中身份验证的关键步骤。但是，单独依靠此方案无法创建新的 AWS 账户，它需要与 Organizations 结合使用，因此无法单独满足要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Organizations",
      "Amazon Cognito",
      "IAM",
      "SCP",
      "AWS Directory Service",
      "AWS IAM Identity Center (AWS Single Sign-On)"
    ]
  },
  {
    "id": 485,
    "topic": "1",
    "question_en": "A company is looking for a solution that can store video archives in AWS from old news footage. The company needs to minimize costs and will rarely need to restore these files. When the files are needed, they must be available in a maximum of five minutes. What is the MOST cost-effective solution?",
    "options_en": {
      "A": "Store the video archives in Amazon S3 Glacier and use Expedited retrievals.",
      "B": "Store the video archives in Amazon S3 Glacier and use Standard retrievals.",
      "C": "Store the video archives in Amazon S3 Standard-Infrequent Access (S3 Standard-IA).",
      "D": "Store the video archives in Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "一家公司正在寻找一个解决方案，可以将旧新闻片段的视频存档存储在 AWS 中。该公司需要最大限度地降低成本，并且很少需要恢复这些文件。当需要这些文件时，它们必须在最多五分钟内可用。什么是最具成本效益的解决方案？",
    "options_cn": {
      "A": "将视频存档存储在 Amazon S3 Glacier 中并使用 Expedited retrievals。",
      "B": "将视频存档存储在 Amazon S3 Glacier 中并使用 Standard retrievals。",
      "C": "将视频存档存储在 Amazon S3 Standard-Infrequent Access (S3 Standard-IA) 中。",
      "D": "将视频存档存储在 Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA) 中。"
    },
    "tags": [
      "Amazon S3",
      "S3 Standard-IA",
      "Amazon S3 Glacier",
      "S3 Expedited retrievals",
      "S3 Standard retrievals",
      "S3 One Zone-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n本题考查了 Amazon S3 的存储类别选择，以及如何在满足恢复时间要求的前提下最大限度地降低存储成本。",
      "why_correct": "Amazon S3 Glacier 专为数据存档而设计，成本最低。Expedited retrievals 允许在几分钟内恢复数据，满足了题目的五分钟恢复时间要求。因此，将视频存档存储在 S3 Glacier 中并使用 Expedited retrievals 是最具成本效益的解决方案。",
      "why_wrong": "选项 B 使用 Standard retrievals 的恢复时间较长，可能无法满足五分钟的恢复时间要求。选项 C 和 D 虽然恢复时间较快，但成本高于 S3 Glacier，并且不符合题目中“最大限度地降低成本”的要求，因此不适用。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon S3 Glacier",
      "Expedited retrievals",
      "Standard retrievals",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    ]
  },
  {
    "id": 486,
    "topic": "1",
    "question_en": "A company is building a three-tier application on AWS. The presentation tier will serve a static website The logic tier is a containerized application. This application will store data in a relational database. The company wants to simplify deployment and to reduce operational costs. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon S3 to host static content. Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.",
      "B": "Use Amazon CloudFront to host static content. Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database.",
      "C": "Use Amazon S3 to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate for compute power. Use a managed Amazon RDS cluster for the database.",
      "D": "Use Amazon EC2 Reserved Instances to host static content. Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 for compute power. Use a managed Amazon RDS cluster for the database."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上构建一个三层应用程序。表示层将服务于一个静态网站。逻辑层是一个容器化应用程序。此应用程序会将数据存储在关系数据库中。该公司希望简化部署并降低运营成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 托管静态内容。使用 Amazon Elastic Container Service (Amazon ECS) 与 AWS Fargate 获取计算能力。 使用托管的 Amazon RDS 集群作为数据库。",
      "B": "使用 Amazon CloudFront 托管静态内容。使用 Amazon Elastic Container Service (Amazon ECS) 与 Amazon EC2 获取计算能力。使用托管的 Amazon RDS 集群作为数据库。",
      "C": "使用 Amazon S3 托管静态内容。使用 Amazon Elastic Kubernetes Service (Amazon EKS) 与 AWS Fargate 获取计算能力。使用托管的 Amazon RDS 集群作为数据库。",
      "D": "使用 Amazon EC2 Reserved Instances 托管静态内容。使用 Amazon Elastic Kubernetes Service (Amazon EKS) 与 Amazon EC2 获取计算能力。使用托管的 Amazon RDS 集群作为数据库。"
    },
    "tags": [
      "Amazon S3",
      "Amazon ECS",
      "AWS Fargate",
      "Amazon RDS",
      "Amazon CloudFront",
      "Amazon EKS",
      "Amazon EC2",
      "EC2 Reserved Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查构建三层应用程序的部署方案，并要求简化部署和降低运营成本。这涉及到静态网站托管、容器化应用程序的计算资源选择、数据库的选择。同时考察对不同 AWS 计算服务的理解和成本效益的比较。",
      "why_correct": "Amazon S3 托管静态网站内容是最经济高效且易于管理的方案，省去了服务器维护的成本。AWS Fargate 提供无服务器的容器计算能力，无需管理底层服务器，进一步简化了部署和运维，降低了成本。托管的 Amazon RDS 集群提供了可靠、易于管理的数据库服务，减少了数据库管理负担。这三者结合满足了题目的要求：简化部署并降低运营成本。",
      "why_wrong": "选项 B 错误在于，虽然 Amazon CloudFront 可以用于托管静态网站，但不如 Amazon S3 简单、直接。更重要的是，使用 Amazon EC2 作为计算资源，需要用户管理服务器的启动、配置、维护，这增加了运营成本和复杂性，与题目要求的简化部署和降低成本相悖。选项 C 错误在于，虽然 AWS Fargate 在 EKS 中也能使用，但 EKS 本身比 ECS 的管理复杂性要高，涉及 Kubernetes 集群的配置和维护，这增加了运营成本。选项 D 错误在于，使用 Amazon EC2 Reserved Instances 托管静态网站成本效率不如 Amazon S3；而且 Amazon EC2 也需要用户管理服务器，而不是无服务器架构。该方案同样增加了运营成本和复杂性，且 EKS 与 EC2 组合的复杂性也更高，不符合题目要求。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon ECS",
      "AWS Fargate",
      "Amazon RDS",
      "Amazon CloudFront",
      "Amazon EKS",
      "Amazon EC2",
      "EC2 Reserved Instances"
    ]
  },
  {
    "id": 487,
    "topic": "1",
    "question_en": "A company seeks a storage solution for its application. The solution must be highly available and scalable. The solution also must function as a file system be mountable by multiple Linux instances in AWS and on premises through native protocols, and have no minimum size requirements. The company has set up a Site-to-Site VPN for access from its on-premises network to its VPC. Which storage solution meets these requirements?",
    "options_en": {
      "A": "Amazon FSx Multi-AZ deployments",
      "B": "Amazon Elastic Block Store (Amazon EBS) Multi-Attach volumes",
      "C": "Amazon Elastic File System (Amazon EFS) with multiple mount targets",
      "D": "Amazon Elastic File System (Amazon EFS) with a single mount target and multiple access points"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为其应用程序寻找存储解决方案。该解决方案必须具有高可用性和可扩展性。该解决方案还必须充当文件系统，可以通过原生协议由 AWS 和本地的多个 Linux 实例挂载，并且没有最小大小要求。该公司已经设置了站点到站点 VPN，以便从其本地网络访问其 VPC。哪种存储解决方案满足这些要求？",
    "options_cn": {
      "A": "Amazon FSx 多可用区部署",
      "B": "Amazon Elastic Block Store (Amazon EBS) 多重连接卷",
      "C": "具有多个挂载目标的 Amazon Elastic File System (Amazon EFS)",
      "D": "具有单个挂载目标和多个访问点的 Amazon Elastic File System (Amazon EFS)"
    },
    "tags": [
      "Amazon EFS",
      "High Availability",
      "Scalability",
      "File System",
      "Mount Target",
      "Site-to-Site VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察存储解决方案的选型，特别是针对高可用性、可扩展性、文件系统挂载以及本地访问等需求的考量。涉及 Amazon EFS、Amazon FSx、Amazon EBS 等服务的特性对比。",
      "why_correct": "Amazon EFS 提供了高可用性和可扩展性的网络文件系统服务。它允许创建多个挂载目标，方便从多个 AWS 和本地 Linux 实例挂载，满足题目中文件系统和多实例挂载的需求。通过站点到站点 VPN，本地实例可以访问EFS，并且 EFS 本身没有最小大小的限制，完全符合题目的所有要求。",
      "why_wrong": "A. Amazon FSx 多可用区部署虽然提供了高可用性，但其原生协议并非所有 Linux 实例都能直接挂载，且部署的复杂度和成本通常高于 EFS。B. Amazon EBS 的多重连接卷，虽然能实现多实例访问，但它不是一个文件系统，而是块存储，无法满足作为文件系统的需求，并且 EBS 的可扩展性不如 EFS 灵活。D.  Amazon EFS 允许通过访问点访问，但单个挂载目标限制了并发访问的效率和灵活性，并且无法满足从多个 Linux 实例挂载的需求，这与题目的要求不符。"
    },
    "related_terms": [
      "Amazon EFS",
      "Amazon FSx",
      "Amazon EBS",
      "Linux",
      "Site-to-Site VPN",
      "VPC",
      "High Availability",
      "Scalability",
      "Mount Target",
      "Access Point"
    ]
  },
  {
    "id": 488,
    "topic": "1",
    "question_en": "A 4-year-old media company is using the AWS Organizations all features feature set to organize its AWS accounts. According to the company's finance team, the billing information on the member accounts must not be accessible to anyone, including the root user of the member accounts. Which solution will meet these requirements?",
    "options_en": {
      "A": "Add all finance team users to an IAM group. Attach an AWS managed policy named Billing to the group.",
      "B": "Attach an identity-based policy to deny access to the billing information to all users, including the root user.",
      "C": "Create a service control policy (SCP) to deny access to the billing information. Attach the SCP to the root organizational unit (OU).",
      "D": "Convert from the Organizations all features feature set to the Organizations consolidated billing feature set."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家成立 4 年的媒体公司正在使用 AWS Organizations 的所有功能特性集来组织其 AWS 账户。根据该公司的财务团队，成员账户上的账单信息不得被任何人访问，包括成员账户的根用户。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将所有财务团队用户添加到 IAM 组。将名为 Billing 的 AWS 托管策略附加到该组。",
      "B": "附加一个基于身份的策略，以拒绝所有用户（包括根用户）访问账单信息。",
      "C": "创建一个服务控制策略 (SCP) 以拒绝访问账单信息。将 SCP 附加到根组织单元 (OU)。",
      "D": "从 Organizations 的所有功能特性集转换为 Organizations 的合并账单功能特性集。"
    },
    "tags": [
      "AWS Organizations",
      "Service Control Policies (SCP)",
      "IAM",
      "Billing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS Organizations 及其服务控制策略（SCP）来限制对账单信息的访问。与 IAM 策略、权限边界、以及 Organizations 的功能集相关。",
      "why_correct": "创建一个 SCP 以拒绝访问账单信息是满足要求的最佳解决方案。SCP 允许集中控制组织内所有账户的权限，包括根用户，从而确保账单信息不被访问。将 SCP 附加到根组织单元 (OU) 将影响 OU 内的所有账户和子 OU，确保在整个组织范围内强制执行此策略。",
      "why_wrong": "A 选项中，将财务团队用户添加到 IAM 组并附加 Billing 的 AWS 托管策略，无法满足要求。IAM 策略仅限于 IAM 用户和角色，无法阻止根用户访问账单信息。B 选项中，基于身份的策略无法阻止根用户访问账单信息，因为根用户不受 IAM 策略约束，并且会覆盖用户级别的策略。D 选项中，从 Organizations 的所有功能特性集转换为 Organizations 的合并账单功能特性集，与限制访问账单信息无关，它主要是用于集中管理账单，并不能满足题目中阻止访问账单的需求。"
    },
    "related_terms": [
      "AWS Organizations",
      "IAM",
      "Service Control Policies (SCP)",
      "IAM Group",
      "IAM Policy",
      "Billing",
      "Organizations"
    ]
  },
  {
    "id": 489,
    "topic": "1",
    "question_en": "An ecommerce company runs an application in the AWS Cloud that is integrated with an on-premises warehouse solution. The company uses Amazon Simple Notification Service (Amazon SNS) to send order messages to an on-premises HTTPS endpoint so the warehouse application can process the orders. The local data center team has detected that some of the order messages were not received. A solutions architect needs to retain messages that are not delivered and analyze the messages for up to 14 days. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Configure an Amazon SNS dead letter queue that has an Amazon Kinesis Data Stream target with a retention period of 14 days.",
      "B": "Add an Amazon Simple Queue Service (Amazon SQS) queue with a retention period of 14 days between the application and Amazon SNS.",
      "C": "Configure an Amazon SNS dead letter queue that has an Amazon Simple Queue Service (Amazon SQS) target with a retention period of 14 days.",
      "D": "Configure an Amazon SNS dead letter queue that has an Amazon DynamoDB target with a TTL attribute set for a retention period of 14 days."
    },
    "correct_answer": "C",
    "vote_percentage": "71%",
    "question_cn": "一家电子商务公司在 AWS Cloud 中运行一个应用程序，该应用程序与本地仓库解决方案集成。该公司使用 Amazon Simple Notification Service (Amazon SNS) 将订单消息发送到本地 HTTPS 终端节点，以便仓库应用程序可以处理订单。本地数据中心团队检测到某些订单消息未被接收。解决方案架构师需要保留未交付的消息，并分析这些消息长达 14 天。哪种解决方案以最少的开发工作量满足这些要求？",
    "options_cn": {
      "A": "配置一个 Amazon SNS 死信队列，该队列具有一个 Amazon Kinesis Data Stream 目标，保留期为 14 天。",
      "B": "在应用程序和 Amazon SNS 之间添加一个 Amazon Simple Queue Service (Amazon SQS) 队列，保留期为 14 天。",
      "C": "配置一个 Amazon SNS 死信队列，该队列具有一个 Amazon Simple Queue Service (Amazon SQS) 目标，保留期为 14 天。",
      "D": "配置一个 Amazon SNS 死信队列，该队列具有一个 Amazon DynamoDB 目标，TTL 属性设置为保留期为 14 天。"
    },
    "tags": [
      "Amazon SNS",
      "Amazon SQS",
      "Dead-letter queue",
      "Message retention"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 71%），解析仅供参考。】\n\n考查 Amazon SNS 的死信队列（Dead-letter queue，DLQ）配置以及与 Amazon SQS 的集成，以及消息保留时长。与消息传递、错误处理和事件驱动架构相关。",
      "why_correct": "配置一个 Amazon SNS 死信队列，该队列的目标是 Amazon SQS 队列，并设置 SQS 队列的消息保留期为 14 天。当 SNS 无法将消息传递到订阅终端节点时，它会将消息发送到 DLQ。使用 SQS 作为 DLQ 可以保证消息的可靠性和持久性，因为 SQS 提供了消息保留功能。这种方案无需额外的应用程序开发工作，即可满足消息保留和分析的需求。",
      "why_wrong": "A 选项，Kinesis Data Streams 主要用于处理实时数据流，而不是直接作为 SNS 的 DLQ 目标。将 Kinesis Data Streams 用于此目的需要额外的复杂性和开发工作，例如需要创建消费者应用程序从 Kinesis 读取消息。B 选项，在应用程序和 SNS 之间添加 SQS 队列并不能解决消息未被接收的问题，而是用于解耦应用与 SNS 之间的消息传递。虽然 SQS 可以提供消息保留，但它并没有解决原问题中 SNS 无法传递消息的情况。D 选项，将 DynamoDB 作为 SNS 的 DLQ 目标不合适。虽然 DynamoDB 可以使用 TTL（Time to Live）属性来删除旧数据，但它不是为消息队列设计的，并且 DynamoDB 的设计目的也不是为了消息的可靠存储和检索。此外，DynamoDB 的使用还会增加开发复杂性和成本。"
    },
    "related_terms": [
      "Amazon SNS",
      "Amazon SQS",
      "DynamoDB",
      "HTTPS",
      "Dead-letter queue",
      "Amazon Kinesis Data Stream"
    ]
  },
  {
    "id": 490,
    "topic": "1",
    "question_en": "A gaming company uses Amazon DynamoDB to store user information such as geographic location, player data, and leaderboards. The company needs to configure continuous backups to an Amazon S3 bucket with a minimal amount of coding. The backups must not affect availability of the application and must not affect the read capacity units (RCUs) that are defined for the table. Which solution meets these requirements?",
    "options_en": {
      "A": "Use an Amazon EMR cluster. Create an Apache Hive job to back up the data to Amazon S3.",
      "B": "Export the data directly from DynamoDB to Amazon S3 with continuous backups. Turn on point-in-time recovery for the table.",
      "C": "Configure Amazon DynamoDB Streams. Create an AWS Lambda function to consume the stream and export the data to an Amazon S3 bucket.",
      "D": "Create an AWS Lambda function to export the data from the database tables to Amazon S3 on a regular basis. Turn on point-in-time recovery for the table."
    },
    "correct_answer": "B",
    "vote_percentage": "87%",
    "question_cn": "一家游戏公司使用 Amazon DynamoDB 存储用户信息，例如地理位置、玩家数据和排行榜。该公司需要配置连续备份到 Amazon S3 存储桶，并且只需最少的编码。备份不得影响应用程序的可用性，也不得影响为表定义的读取容量单元 (RCU)。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EMR 集群。创建 Apache Hive 作业将数据备份到 Amazon S3。",
      "B": "通过连续备份直接将数据从 DynamoDB 导出到 Amazon S3。为表打开时间点恢复。",
      "C": "配置 Amazon DynamoDB Streams。创建一个 AWS Lambda 函数来使用流并将数据导出到 Amazon S3 存储桶。",
      "D": "创建一个 AWS Lambda 函数，用于定期间隔将数据从数据库表导出到 Amazon S3。为表打开时间点恢复。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon S3",
      "DynamoDB Backup and Restore",
      "Point-in-time recovery",
      "AWS Lambda",
      "Amazon EMR",
      "Apache Hive",
      "DynamoDB Streams"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 87%），解析仅供参考。】\n\n本题考查 DynamoDB 的备份与恢复方案，以及对应用程序可用性和读取容量单元（RCU）的影响。与 DynamoDB 备份、S3 存储、Lambda 函数、EMR 集群以及 DynamoDB Streams 等服务相关。",
      "why_correct": "通过连续备份直接将数据从 DynamoDB 导出到 Amazon S3 是满足要求的最佳解决方案。DynamoDB 的连续备份功能在后台以增量方式进行备份，不会影响应用程序的可用性或表的读取容量单元（RCU）。启用时间点恢复（PITR）可以实现更细粒度的恢复，进一步增强数据保护。此方法提供了 DynamoDB 的原生备份和恢复能力，易于配置且维护成本较低。",
      "why_wrong": "选项 A，使用 Amazon EMR 集群和 Apache Hive 作业，需要手动编写和管理 EMR 集群，增加了复杂性，且 EMR 作业的运行可能会对性能产生影响，并增加额外的成本。选项 C，配置 DynamoDB Streams，虽然可以捕获表的变化，但需要编写 Lambda 函数来处理流数据并将其导出到 S3，增加了开发和维护的复杂性。虽然 Streams 方案的延迟可能较低，但它不直接提供备份功能，而是需要自定义实现备份逻辑。选项 D，创建 Lambda 函数定期导出数据，需要自行管理导出的频率和逻辑，实现成本高，且可能影响数据库性能，特别是当数据量较大时。虽然启用了 PITR，但这种方案本质上是一种自定义备份，不如 DynamoDB 内置的连续备份功能可靠和高效。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon S3",
      "DynamoDB",
      "S3",
      "AWS Lambda",
      "EMR",
      "Apache Hive",
      "DynamoDB Streams",
      "RCU",
      "PITR"
    ]
  },
  {
    "id": 491,
    "topic": "1",
    "question_en": "A solutions architect is designing an asynchronous application to process credit card data validation requests for a bank. The application must be secure and be able to process each request at least once. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS Key Management Service (SSE-KMS) for encryption. Add the kms:Decrypt permission for the Lambda execution role.",
      "B": "Use AWS Lambda event source mapping. Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use SQS managed encryption keys (SSE-SQS) for encryption. Add the encryption key invocation permission for the Lambda function.",
      "C": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) FIFO queues as the event source. Use AWS KMS keys (SSE-KMS). Add the kms:Decrypt permission for the Lambda execution role.",
      "D": "Use the AWS Lambda event source mapping. Set Amazon Simple Queue Service (Amazon SQS) standard queues as the event source. Use AWS KMS keys (SSE-KMS) for encryption. Add the encryption key invocation permission for the Lambda function."
    },
    "correct_answer": "A",
    "vote_percentage": "69%",
    "question_cn": "一位解决方案架构师正在设计一个异步应用程序，以处理银行的信用卡数据验证请求。该应用程序必须安全且能够至少处理每个请求一次。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Lambda 事件源映射。将 Amazon Simple Queue Service (Amazon SQS) 标准队列设置为事件源。使用 AWS Key Management Service (SSE-KMS) 进行加密。为 Lambda 执行角色添加 kms:Decrypt 权限。",
      "B": "使用 AWS Lambda 事件源映射。使用 Amazon Simple Queue Service (Amazon SQS) FIFO 队列作为事件源。使用 SQS 托管加密密钥 (SSE-SQS) 进行加密。为 Lambda 函数添加加密密钥调用权限。",
      "C": "使用 AWS Lambda 事件源映射。将 Amazon Simple Queue Service (Amazon SQS) FIFO 队列设置为事件源。使用 AWS KMS 密钥 (SSE-KMS)。为 Lambda 执行角色添加 kms:Decrypt 权限。",
      "D": "使用 AWS Lambda 事件源映射。将 Amazon Simple Queue Service (Amazon SQS) 标准队列设置为事件源。使用 AWS KMS 密钥 (SSE-KMS) 进行加密。为 Lambda 函数添加加密密钥调用权限。"
    },
    "tags": [
      "Amazon SQS",
      "Lambda",
      "AWS KMS",
      "SSE-KMS",
      "Lambda Event Source Mapping"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 69%），解析仅供参考。】\n\n考查基于 SQS 和 Lambda 构建异步应用程序，以及加密的最佳实践。涉及 SQS 队列类型选择、加密方式的选择，以及 Lambda 函数访问加密密钥的权限配置。与安全性、成本效益、以及至少处理一次消息（at-least-once）的保证相关。",
      "why_correct": "选项 A 采用了标准 SQS 队列，结合 Lambda 事件源映射，满足了异步处理的需求。使用 SSE-KMS 进行加密，并为 Lambda 执行角色授予 kms:Decrypt 权限，确保了数据的安全性和访问控制。标准队列虽然不保证消息顺序，但结合 Lambda 事件源映射，可以很好地处理并发请求，并实现较高的成本效益。",
      "why_wrong": "选项 B 使用了 FIFO 队列，虽然保证了消息顺序，但成本通常高于标准队列。使用 SSE-SQS 加密，虽然配置更简单，但控制能力不如 SSE-KMS，且与 KMS 集成度较低。为 Lambda 函数添加加密密钥调用权限是不必要的，因为 Lambda 函数通过执行角色访问 KMS 密钥，而不是直接调用。选项 C 使用 FIFO 队列，与选项 B 类似，成本较高。虽然使用了 SSE-KMS，但使用 FIFO 队列增加了复杂性和成本。选项 D 使用了标准队列，但为 Lambda 函数添加了不必要的加密密钥调用权限，而非使用 Lambda 执行角色访问 KMS 密钥，这增加了安全风险，并违背了最小权限原则。"
    },
    "related_terms": [
      "Amazon SQS",
      "SQS",
      "Lambda",
      "AWS KMS",
      "SSE-KMS",
      "FIFO queue",
      "Standard queue",
      "Lambda Event Source Mapping",
      "SSE-SQS",
      "kms:Decrypt"
    ]
  },
  {
    "id": 492,
    "topic": "1",
    "question_en": "A company has multiple AWS accounts for development work. Some staff consistently use oversized Amazon EC2 instances, which causes the company to exceed the yearly budget for the development accounts. The company wants to centrally restrict the creation of AWS resources in these accounts. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Develop AWS Systems Manager templates that use an approved EC2 creation process. Use the approved Systems Manager templates to provision EC2 instances.",
      "B": "Use AWS Organizations to organize the accounts into organizational units (OUs). Define and attach a service control policy (SCP) to control the usage of EC2 instance types.",
      "C": "Configure an Amazon EventBridge rule that invokes an AWS Lambda function when an EC2 instance is created. Stop disallowed EC2 instance types.",
      "D": "Set up AWS Service Catalog products for the staff to create the allowed EC2 instance types. Ensure that staff can deploy EC2 instances only by using the Service Catalog products."
    },
    "correct_answer": "B",
    "vote_percentage": "94%",
    "question_cn": "一家公司为开发工作设置了多个 AWS 账户。一些员工持续使用过大的 Amazon EC2 实例，导致公司超出了开发账户的年度预算。该公司希望集中限制在这些账户中创建 AWS 资源。哪种解决方案以最少的开发工作量满足这些要求？",
    "options_cn": {
      "A": "开发 AWS Systems Manager 模板，使用经过批准的 EC2 创建流程。使用经过批准的 Systems Manager 模板来配置 EC2 实例。",
      "B": "使用 AWS Organizations 将账户组织到组织单元 (OU) 中。定义并附加一个服务控制策略 (SCP) 来控制 EC2 实例类型的使用。",
      "C": "配置一个 Amazon EventBridge 规则，该规则在创建 EC2 实例时调用一个 AWS Lambda 函数。停止不允许的 EC2 实例类型。",
      "D": "为员工设置 AWS Service Catalog 产品，以便他们创建允许的 EC2 实例类型。确保员工只能通过使用 Service Catalog 产品来部署 EC2 实例。"
    },
    "tags": [
      "AWS Organizations",
      "Service Control Policies (SCP)",
      "Amazon EC2",
      "AWS Budget"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 94%），解析仅供参考。】\n\n考查通过 SCP 控制 EC2 实例类型，以限制资源使用，从而控制成本。这与 AWS Organizations 的 OU 结构、预算管理相关。",
      "why_correct": "使用 AWS Organizations 和 SCP 是一个集中管理、限制 AWS 资源创建的有效方法。首先，将开发账户组织到组织单元 (OU) 中。然后，定义一个服务控制策略 (SCP)，该策略明确允许或拒绝特定的 EC2 实例类型。SCP 应用于 OU 后，OU 中的所有账户都将受到该策略的约束，从而限制了员工可以创建的 EC2 实例类型。这种方法集中、易于管理，并且不需要编写自定义代码。",
      "why_wrong": "A 方案需要开发 Systems Manager 模板，这增加了开发工作量，违背了题目中“最少开发工作量”的要求。C 方案需要配置 EventBridge 规则和 Lambda 函数，也需要额外的开发和维护工作，且 Lambda 函数需要处理停止实例的逻辑，增加了复杂性。D 方案需要为员工设置 AWS Service Catalog 产品，这同样需要额外的配置和管理工作，相比 SCP 方案，工作量更大，且不如 SCP 直接、简洁。"
    },
    "related_terms": [
      "AWS Organizations",
      "SCP",
      "EC2",
      "Systems Manager",
      "EventBridge",
      "Lambda",
      "Service Catalog"
    ]
  },
  {
    "id": 493,
    "topic": "1",
    "question_en": "A company wants to use artificial intelligence (AI) to determine the quality of its customer service calls. The company currently manages calls in four different languages, including English. The company will offer new languages in the future. The company does not have the resources to regularly maintain machine learning (ML) models. The company needs to create written sentiment analysis reports from the customer service call recordings. The customer service call recording text must be translated into English. Which combination of steps will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Use Amazon Comprehend to translate the audio recordings into English.",
      "B": "Use Amazon Lex to create the written sentiment analysis reports.",
      "C": "Use Amazon Polly to convert the audio recordings into text.",
      "D": "Use Amazon Transcribe to convert the audio recordings in any language into text",
      "E": "Use Amazon Translate to translate text in any language to English",
      "F": "Use Amazon Comprehend to create the sentiment analysis reports."
    },
    "correct_answer": "DEF",
    "vote_percentage": "",
    "question_cn": "一家公司希望使用人工智能 (AI) 来确定其客户服务电话的质量。该公司目前管理四种不同语言的通话，包括英语。该公司将来会提供新语言。该公司没有资源定期维护机器学习 (ML) 模型。该公司需要从客户服务通话录音中创建书面情绪分析报告。客户服务通话录音文本必须翻译成英语。哪种步骤组合将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "使用 Amazon Comprehend 将音频录音翻译成英语。",
      "B": "使用 Amazon Lex 创建书面情绪分析报告。",
      "C": "使用 Amazon Polly 将音频录音转换为文本。",
      "D": "使用 Amazon Transcribe 将任何语言的音频录音转换为文本。",
      "E": "使用 Amazon Translate 将任何语言的文本翻译成英语。",
      "F": "使用 Amazon Comprehend 创建情绪分析报告。"
    },
    "tags": [
      "Amazon Transcribe",
      "Amazon Translate",
      "Amazon Comprehend",
      "Amazon Polly",
      "Amazon Lex"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DEF（社区 —），解析仅供参考。】\n\n考察使用 AWS 服务构建客户服务通话质量分析流程，包括语音转文本、翻译、情绪分析等，并关注自动化的模型维护需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 DEF。理由简述：选项 D 正确：Amazon Transcribe 将任意语言音频转为文本。选项 E 正确：Amazon Translate 将任意语言文本翻译成英语。选项 F 正确：Amazon Comprehend 基于文本创建情绪分析报告，且无需维护 ML 模型。流程为 D（转写）→ E（翻译）→ F（情绪报告），满足多语言录音到英文书面情绪报告的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 错误，Comprehend 不负责音频翻译。B 错误，Lex 用于对话机器人，非书面报告。C 错误，Polly 是文本转语音，与需求相反。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Transcribe",
      "Amazon Translate",
      "Amazon Comprehend",
      "Amazon Polly",
      "Amazon Lex",
      "ML",
      "AI"
    ]
  },
  {
    "id": 494,
    "topic": "1",
    "question_en": "A company uses Amazon EC2 instances to host its internal systems. As part of a deployment operation, an administrator tries to use the AWS CLI to terminate an EC2 instance. However, the administrator receives a 403 (Access Denied) error message. The administrator is using an IAM role that has the following IAM policy attached: What is the cause of the unsuccessful request?",
    "question_image": "/data/images/494.png",
    "options_en": {
      "A": "The EC2 instance has a resource-based policy with a Deny statement.",
      "B": "The principal has not been specified in the policy statement.",
      "C": "The \"Action\" field does not grant the actions that are required to terminate the EC2 instance.",
      "D": "The request to terminate the EC2 instance does not originate from the CIDR blocks 192.0.2.0/24 or 203.0.113.0/24."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon EC2 实例来托管其内部系统。作为部署操作的一部分，管理员尝试使用 AWS CLI 来终止一个 EC2 实例。但是，管理员收到 403 (访问被拒绝) 错误消息。管理员正在使用附加了以下 IAM 策略的 IAM 角色：导致请求不成功的原因是什么？",
    "options_cn": {
      "A": "EC2 实例有一个带有 Deny 语句的基于资源的策略。",
      "B": "策略语句中未指定主体。",
      "C": "\"Action\" 字段未授予终止 EC2 实例所需的权限。",
      "D": "终止 EC2 实例的请求并非源自 CIDR 块 192.0.2.0/24 或 203.0.113.0/24。"
    },
    "tags": [
      "IAM",
      "EC2",
      "IAM Policy",
      "EC2 Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n答案解析:\n\n根据提供的身份访问管理(IAM)策略和收到的错误消息,终止EC2实例请求未成功的原因是:\n\nD. 终止EC2实例的请求并非来自CIDR块 192.0.2.0/24或203.0.113.0/24。\n\n以下是解释:\n1. 该策略包含一个基于源IP地址限制访问的条件:\n\"Condition\":{\n    \"IpAddress\":{\n        \"aws:SourceIp\":[\n            \"192.0.2.0/24\",\n            \"203.0.113.0/24\"\n        ]\n    }\n}\n2. 此条件意味着只有当请求来自这两个CIDR块内的IP地址时,策略中指定的操作(包括\"ec2:TerminateInstances\")才被允许。\n3. 管理员收到了403(访问被拒绝)错误,这表明请求因权限不足而被拒绝。\n4. 鉴于该策略允许必要的操作(\"ec2:TerminateInstances\")且资源被正确指定(\"*\"允许访问所有EC2实例),访问被拒绝的最可能原因是管理员的请求来自指定CIDR块之外的IP地址。\n\n要解决此问题,管理员应:\n1. 核实其当前IP地址。\n2. 如有必要,更新IAM策略以包含其当前IP地址或涵盖其所在位置的更广泛范围。\n3. 或者,他们可以从属于允许的CIDR块的网络进行连接。\n\n请记住,在修改IAM策略时始终遵循最小权限原则,并定期审查和更新网络访问控制以维护安全性。",
      "why_correct": "D. 终止EC2实例的请求并非来自CIDR块 192.0.2.0/24或203.0.113.0/24。策略的 Condition 仅允许来自这两个 CIDR 的请求执行 ec2:TerminateInstances,请求来自其他 IP 会导致 403。",
      "why_wrong": "A/B/C 与题意不符: 策略已授予 ec2:TerminateInstances 且资源为 *,403 最可能由源 IP 不在允许的 CIDR 内导致。"
    },
    "related_terms": [
      "EC2",
      "IAM",
      "AWS CLI",
      "CIDR",
      "IAM Role",
      "IAM Policy",
      "403",
      "EC2 Instance"
    ]
  },
  {
    "id": 495,
    "topic": "1",
    "question_en": "A company is conducting an internal audit. The company wants to ensure that the data in an Amazon S3 bucket that is associated with the company’s AWS Lake Formation data lake does not contain sensitive customer or employee data. The company wants to discover personally identifiable information (PII) or financial information, including passport numbers and credit card numbers. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure AWS Audit Manager on the account. Select the Payment Card Industry Data Security Standards (PCI DSS) for auditing.",
      "B": "Configure Amazon S3 Inventory on the S3 bucket Configure Amazon Athena to query the inventory.",
      "C": "Configure Amazon Macie to run a data discovery job that uses managed identifiers for the required data types.",
      "D": "Use Amazon S3 Select to run a report across the S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在进行内部审计。该公司希望确保与公司 AWS Lake Formation 数据湖关联的 Amazon S3 存储桶中的数据不包含敏感的客户或员工数据。该公司希望发现个人身份信息 (PII) 或财务信息，包括护照号码和信用卡号码。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在账户上配置 AWS Audit Manager。选择支付卡行业数据安全标准 (PCI DSS) 进行审计。",
      "B": "在 S3 存储桶上配置 Amazon S3 Inventory。配置 Amazon Athena 来查询 Inventory。",
      "C": "配置 Amazon Macie 运行数据发现作业，该作业使用托管标识符来查找所需的数据类型。",
      "D": "使用 Amazon S3 Select 运行 S3 存储桶的报告。"
    },
    "tags": [
      "Amazon Macie",
      "Amazon S3",
      "Data Security",
      "PII",
      "Audit"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查利用 Amazon Macie 进行数据敏感信息检测，并与 AWS 数据湖安全、数据审计等概念相关。需要理解 Macie 的功能和应用场景，并对比其他选项在数据敏感信息检测方面的不足。",
      "why_correct": "Amazon Macie 是一种数据安全服务，它使用机器学习来自动发现、分类和保护 AWS 中的敏感数据，比如 PII 和财务信息。通过配置数据发现作业，Macie 能够扫描 S3 存储桶中的对象，识别包含指定数据类型的对象，例如护照号码、信用卡号等。这种方式可以满足题目中对敏感数据检测的要求。",
      "why_wrong": "A 选项，AWS Audit Manager 主要用于审计合规性，虽然可以基于 PCI DSS 进行审计，但其核心功能是评估控制措施是否符合合规标准，而不是直接扫描 S3 存储桶中的数据来查找 PII。B 选项，Amazon S3 Inventory 仅提供 S3 存储桶的对象清单，不具备数据内容扫描和敏感信息识别的功能；虽然可以使用 Amazon Athena 查询 Inventory，但这只是对 S3 对象的元数据进行分析，无法直接检测数据内容。D 选项，Amazon S3 Select 允许使用 SQL 语句查询 S3 对象的内容，但它需要用户预先知道要查找的具体数据模式或关键词，且功能不如 Amazon Macie 强大，无法满足对多种敏感数据类型的自动识别，且操作相对复杂。"
    },
    "related_terms": [
      "Amazon S3",
      "PII",
      "PCI DSS",
      "Amazon Athena",
      "Amazon Macie",
      "S3 Select",
      "Audit Manager",
      "S3 Inventory"
    ]
  },
  {
    "id": 496,
    "topic": "1",
    "question_en": "A company uses on-premises servers to host its applications. The company is running out of storage capacity. The applications use both block storage and NFS storage. The company needs a high-performing solution that supports local caching without re-architecting its existing applications. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Mount Amazon S3 as a file system to the on-premises servers.",
      "B": "Deploy an AWS Storage Gateway file gateway to replace NFS storage.",
      "C": "Deploy AWS Snowball Edge to provision NFS mounts to on-premises servers.",
      "D": "Deploy an AWS Storage Gateway volume gateway to replace the block storag",
      "E": "E. Deploy Amazon Elastic File System (Amazon EFS) volumes and mount them to on-premises servers."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用本地服务器来托管其应用程序。该公司即将用完存储容量。这些应用程序同时使用块存储和 NFS 存储。该公司需要一个高性能的解决方案，支持本地缓存，而无需重新架构其现有应用程序。解决方案架构师应采取哪些行动组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将 Amazon S3 挂载为本地服务器的文件系统。",
      "B": "部署 AWS Storage Gateway 文件网关以替换 NFS 存储。",
      "C": "部署 AWS Snowball Edge 以向本地服务器提供 NFS 挂载。",
      "D": "部署 AWS Storage Gateway 卷网关以替换块存储。",
      "E": "部署 Amazon Elastic File System (Amazon EFS) 卷并将它们挂载到本地服务器。"
    },
    "tags": [
      "Storage Gateway",
      "NFS",
      "Amazon S3",
      "Snowball Edge",
      "Amazon EFS",
      "块存储"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n该题考查了如何优化存储方案，满足高性能需求。Storage Gateway 提供混合云存储，包括文件网关和卷网关。EFS 和 Snowball Edge 都可以作为本地文件系统使用，但 EFS 更适合云原生应用，Snowball Edge 适合离线迁移。",
      "why_correct": "AWS Storage Gateway 文件网关可以替换 NFS 存储，提供本地缓存，以提高性能。该方案同时满足了高性能和现有应用程序架构的兼容性要求。",
      "why_wrong": "A 选项，S3 挂载为本地文件系统性能较差，不适合高性能需求。C 选项，Snowball Edge 适合数据导入导出，不适合持续 NFS 挂载。D 选项，卷网关用于替换块存储，不满足 NFS 需求。E 选项，EFS 卷需要本地服务器进行修改，不能满足题目中的‘无需重新架构其现有应用程序’的要求。"
    },
    "related_terms": [
      "AWS Storage Gateway",
      "NFS",
      "Amazon S3",
      "Snowball Edge",
      "Amazon EFS",
      "块存储"
    ]
  },
  {
    "id": 497,
    "topic": "1",
    "question_en": "A company has a service that reads and writes large amounts of data from an Amazon S3 bucket in the same AWS Region. The service is deployed on Amazon EC2 instances within the private subnet of a VPC. The service communicates with Amazon S3 over a NAT gateway in the public subnet. However, the company wants a solution that will reduce the data output costs. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Provision a dedicated EC2 NAT instance in the public subnet. Configure the route table for the private subnet to use the elastic network interface of this instance as the destination for all S3 trafic.",
      "B": "Provision a dedicated EC2 NAT instance in the private subnet. Configure the route table for the public subnet to use the elastic network interface of this instance as the destination for all S3 trafic.",
      "C": "Provision a VPC gateway endpoint. Configure the route table for the private subnet to use the gateway endpoint as the route for all S3 trafic.",
      "D": "Provision a second NAT gateway. Configure the route table for the private subnet to use this NAT gateway as the destination for all S3 trafic."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一项服务，该服务从同一 AWS 区域的 Amazon S3 存储桶读取和写入大量数据。该服务部署在 VPC 私有子网中的 Amazon EC2 实例上。该服务通过公共子网中的 NAT 网关与 Amazon S3 通信。但是，该公司希望找到一个能够降低数据输出成本的解决方案。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在公共子网中预置一个专用的 EC2 NAT 实例。配置私有子网的路由表，以使用此实例的弹性网络接口作为所有 S3 流量的目的地。",
      "B": "在私有子网中预置一个专用的 EC2 NAT 实例。配置公共子网的路由表，以使用此实例的弹性网络接口作为所有 S3 流量的目的地。",
      "C": "预置一个 VPC 网关终端节点。配置私有子网的路由表，以使用该网关终端节点作为所有 S3 流量的路由。",
      "D": "预置第二个 NAT 网关。配置私有子网的路由表，以使用此 NAT 网关作为所有 S3 流量的目的地。"
    },
    "tags": [
      "Amazon S3",
      "VPC",
      "NAT Gateway",
      "VPC 网关终端节点"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了如何降低 S3 的数据传输成本。NAT 网关用于私有子网访问互联网，但会产生数据传输费用。VPC 终端节点提供更经济的方式访问 S3。",
      "why_correct": "VPC 网关终端节点允许私有子网通过 AWS 内部网络访问 S3，避免了 NAT 网关的数据传输费用，是最具成本效益的解决方案。",
      "why_wrong": "A 选项，NAT 实例涉及 EC2 实例的运行成本，不如 VPC 终端节点。B 选项，NAT 实例部署在私有子网没有意义。D 选项，增加 NAT 网关会增加成本。"
    },
    "related_terms": [
      "Amazon S3",
      "VPC",
      "NAT Gateway",
      "VPC 网关终端节点"
    ]
  },
  {
    "id": 498,
    "topic": "1",
    "question_en": "A company uses Amazon S3 to store high-resolution pictures in an S3 bucket. To minimize application changes, the company stores the pictures as the latest version of an S3 object. The company needs to retain only the two most recent versions of the pictures. The company wants to reduce costs. The company has identified the S3 bucket as a large expense. Which solution will reduce the S3 costs with the LEAST operational overhead?",
    "options_en": {
      "A": "Use S3 Lifecycle to delete expired object versions and retain the two most recent versions.",
      "B": "Use an AWS Lambda function to check for older versions and delete all but the two most recent versions.",
      "C": "Use S3 Batch Operations to delete noncurrent object versions and retain only the two most recent versions.",
      "D": "Deactivate versioning on the S3 bucket and retain the two most recent versions."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon S3 在 S3 存储桶中存储高分辨率图片。为了最大限度地减少应用程序的更改，该公司将图片存储为 S3 对象的最新版本。该公司只需要保留图片的两个最新版本。该公司希望降低成本。该公司已确定 S3 存储桶是一项巨大的开支。哪种解决方案将以最少的运营开销来降低 S3 成本？",
    "options_cn": {
      "A": "使用 S3 Lifecycle 删除过期的对象版本并保留两个最新版本。",
      "B": "使用 AWS Lambda 函数检查较旧的版本并删除除两个最新版本之外的所有版本。",
      "C": "使用 S3 Batch Operations 删除非当前对象版本，并且只保留两个最新版本。",
      "D": "停用 S3 存储桶的版本控制并保留两个最新版本。"
    },
    "tags": [
      "Amazon S3",
      "S3 Lifecycle",
      "S3 Batch Operations",
      "版本控制"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n该题考察 S3 存储成本优化方案。 S3 版本控制增加了存储成本。S3 生命周期规则可以自动删除旧版本，S3 Batch Operations 可以批量操作对象。",
      "why_correct": "使用 S3 Lifecycle 可以自动删除过期的对象版本，保留最新版本，降低存储成本，并且无需人工干预。",
      "why_wrong": "B 选项，使用 Lambda 删除对象版本涉及代码开发和维护成本，开销较大。C 选项，使用 S3 Batch Operations 需要额外的配置，不如 S3 Lifecycle 简洁。D 选项，停用版本控制会永久删除旧版本，不符合题目要求。"
    },
    "related_terms": [
      "Amazon S3",
      "版本控制",
      "S3 Lifecycle",
      "S3 Batch Operations"
    ]
  },
  {
    "id": 499,
    "topic": "1",
    "question_en": "A company needs to minimize the cost of its 1 Gbps AWS Direct Connect connection. The company's average connection utilization is less than 10%. A solutions architect must recommend a solution that will reduce the cost without compromising security. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up a new 1 Gbps Direct Connect connection. Share the connection with another AWS account.",
      "B": "Set up a new 200 Mbps Direct Connect connection in the AWS Management Console.",
      "C": "Contact an AWS Direct Connect Partner to order a 1 Gbps connection. Share the connection with another AWS account.",
      "D": "Contact an AWS Direct Connect Partner to order a 200 Mbps hosted connection for an existing AWS account."
    },
    "correct_answer": "D",
    "vote_percentage": "83%",
    "question_cn": "一家公司需要最大限度地降低其 1 Gbps 的 AWS Direct Connect 连接成本。该公司的平均连接利用率低于 10%。一位解决方案架构师必须推荐一个在不损害安全性的前提下降低成本的解决方案。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置一个新的 1 Gbps 的 Direct Connect 连接。与其他 AWS 账户共享此连接。",
      "B": "在 AWS 管理控制台中设置一个新的 200 Mbps 的 Direct Connect 连接。",
      "C": "联系 AWS Direct Connect 合作伙伴订购 1 Gbps 的连接。与其他 AWS 账户共享此连接。",
      "D": "联系 AWS Direct Connect 合作伙伴为现有 AWS 账户订购 200 Mbps 的托管连接。"
    },
    "tags": [
      "AWS Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 83%），解析仅供参考。】\n\n考查如何通过 AWS Direct Connect 优化成本，并满足带宽利用率较低的需求。",
      "why_correct": "选项 D 提供了最经济高效的解决方案。通过联系 AWS Direct Connect 合作伙伴订购 200 Mbps 的托管连接，可以匹配较低的平均连接利用率。托管连接可以帮助客户有效降低成本，同时保持必要的安全性。",
      "why_wrong": "选项 A 错误，因为共享 1 Gbps 的 Direct Connect 连接虽然可以分摊成本，但没有考虑到低利用率，且 1 Gbps 的连接本身成本较高。选项 B 错误，直接设置 200 Mbps 的 Direct Connect 连接，虽然在带宽上匹配了需求，但直接从 AWS 设置连接，成本高于托管连接。选项 C 错误，虽然共享 1 Gbps 连接可以分摊成本，但没有充分利用低利用率的优势，且没有说明是托管连接，成本也可能较高，也不如托管 200Mbps 连接经济。"
    },
    "related_terms": [
      "AWS Direct Connect",
      "1 Gbps",
      "200 Mbps",
      "AWS Management Console",
      "AWS account",
      "Managed Connection"
    ]
  },
  {
    "id": 500,
    "topic": "1",
    "question_en": "A company has multiple Windows file servers on premises. The company wants to migrate and consolidate its files into an Amazon FSx for Windows File Server file system. File permissions must be preserved to ensure that access rights do not change. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Deploy AWS DataSync agents on premises. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system.",
      "B": "Copy the shares on each file server into Amazon S3 buckets by using the AWS CLI. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.",
      "C": "Remove the drives from each file server. Ship the drives to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system.",
      "D": "Order an AWS Snowcone devic",
      "E": "Connect the device to the on-premises network. Launch AWS DataSync agents on the device. Schedule DataSync tasks to transfer the data to the FSx for Windows File Server file system. E. Order an AWS Snowball Edge Storage Optimized device. Connect the device to the on-premises network. Copy data to the device by using the AWS CLI. Ship the device back to AWS for import into Amazon S3. Schedule AWS DataSync tasks to transfer the data to the FSx for Windows File Server file system."
    },
    "correct_answer": "AD",
    "vote_percentage": "95%",
    "question_cn": "一家公司在本地有多个 Windows 文件服务器。该公司希望将其文件迁移并整合到 Amazon FSx for Windows File Server 文件系统中。必须保留文件权限以确保访问权限不发生变化。哪些解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在本地部署 AWS DataSync 代理。安排 DataSync 任务将数据传输到 FSx for Windows File Server 文件系统。",
      "B": "使用 AWS CLI 将每个文件服务器上的共享复制到 Amazon S3 存储桶中。安排 AWS DataSync 任务将数据传输到 FSx for Windows File Server 文件系统。",
      "C": "从每个文件服务器中移除驱动器。将驱动器运送到 AWS 以导入到 Amazon S3 中。安排 AWS DataSync 任务将数据传输到 FSx for Windows File Server 文件系统。",
      "D": "订购 AWS Snowcone 设备。将该设备连接到本地网络。在该设备上启动 AWS DataSync 代理。安排 DataSync 任务将数据传输到 FSx for Windows File Server 文件系统。",
      "E": "订购 AWS Snowball Edge 存储优化设备。将该设备连接到本地网络。使用 AWS CLI 将数据复制到该设备。将该设备寄回 AWS 以导入到 Amazon S3 中。安排 AWS DataSync 任务将数据传输到 FSx for Windows File Server 文件系统。"
    },
    "tags": [
      "AWS DataSync",
      "Amazon FSx for Windows File Server",
      "AWS CLI",
      "Amazon S3",
      "AWS Snowcone",
      "AWS Snowball Edge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 95%），解析仅供参考。】\n\n该题考查 Windows 文件服务器迁移到 FSx 的方案。DataSync 能够保留文件权限，Snowball Edge 和 Snowcone 是离线数据传输工具，AWS CLI 用于 S3 上传。",
      "why_correct": "DataSync 代理可以直接将文件服务器的数据传输到 FSx for Windows File Server 文件系统，并保留文件权限。",
      "why_wrong": "B 选项，使用 AWS CLI 复制文件到 S3 涉及额外的步骤，且不如 DataSync 效率高。C 选项，移除驱动器并邮寄到 AWS 的方式过于复杂。D 选项，Snowcone 设备适用于边缘计算场景，不如 DataSync 方案简便。E 选项，Snowball Edge 方案同样过于复杂。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon FSx for Windows File Server",
      "AWS CLI",
      "Amazon S3",
      "AWS Snowcone",
      "AWS Snowball Edge"
    ]
  },
  {
    "id": 501,
    "topic": "1",
    "question_en": "A company wants to ingest customer payment data into the company's data lake in Amazon S3. The company receives payment data every minute on average. The company wants to analyze the payment data in real time. Then the company wants to ingest the data into the data lake. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Use Amazon Kinesis Data Streams to ingest data. Use AWS Lambda to analyze the data in real time.",
      "B": "Use AWS Glue to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.",
      "C": "Use Amazon Kinesis Data Firehose to ingest data. Use Amazon Kinesis Data Analytics to analyze the data in real time.",
      "D": "Use Amazon API Gateway to ingest data. Use AWS Lambda to analyze the data in real time."
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一家公司希望将客户付款数据摄取到公司在 Amazon S3 中的数据湖中。该公司平均每分钟接收一次付款数据。该公司希望实时分析付款数据，然后将数据摄取到数据湖中。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Streams 摄取数据。使用 AWS Lambda 实时分析数据。",
      "B": "使用 AWS Glue 摄取数据。使用 Amazon Kinesis Data Analytics 实时分析数据。",
      "C": "使用 Amazon Kinesis Data Firehose 摄取数据。使用 Amazon Kinesis Data Analytics 实时分析数据。",
      "D": "使用 Amazon API Gateway 摄取数据。使用 AWS Lambda 实时分析数据。"
    },
    "tags": [
      "Amazon Kinesis Data Streams",
      "AWS Lambda",
      "AWS Glue",
      "Amazon Kinesis Data Analytics",
      "Amazon Kinesis Data Firehose",
      "Amazon API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n考查如何以最高运营效率摄取、实时分析和存储客户付款数据到 Amazon S3 数据湖的方案选择。",
      "why_correct": "Amazon Kinesis Data Firehose 能够将数据以近乎实时的速度摄取到 Amazon S3 中。结合 Amazon Kinesis Data Analytics，可以对数据进行实时分析。Kinesis Data Firehose 简化了数据摄取流程，减少了维护成本，提高了运营效率。",
      "why_wrong": "选项 A 使用 Kinesis Data Streams 摄取数据，需要自行管理流并编写 Lambda 函数处理数据，增加了运营复杂性。选项 B 使用 AWS Glue 摄取数据，Glue 主要用于 ETL 任务，不适合实时数据摄取。选项 D 使用 API Gateway 接收数据，API Gateway 主要用于构建和发布 API，不适用于大量数据摄取，且需要 Lambda 函数处理，效率较低。"
    },
    "related_terms": [
      "Amazon S3",
      "Kinesis Data Firehose",
      "Kinesis Data Analytics",
      "Kinesis Data Streams",
      "AWS Lambda",
      "AWS Glue",
      "Amazon API Gateway"
    ]
  },
  {
    "id": 502,
    "topic": "1",
    "question_en": "A company runs a website that uses a content management system (CMS) on Amazon EC2. The CMS runs on a single EC2 instance and uses an Amazon Aurora MySQL Multi-AZ DB instance for the data tier. Website images are stored on an Amazon Elastic Block Store (Amazon EBS) volume that is mounted inside the EC2 instance. Which combination of actions should a solutions architect take to improve the performance and resilience of the website? (Choose two.)",
    "options_en": {
      "A": "Move the website images into an Amazon S3 bucket that is mounted on every EC2 instance",
      "B": "Share the website images by using an NFS share from the primary EC2 instance. Mount this share on the other EC2 instances.",
      "C": "Move the website images onto an Amazon Elastic File System (Amazon EFS) file system that is mounted on every EC2 instance.",
      "D": "Create an Amazon Machine Image (AMI) from the existing EC2 instanc",
      "E": "Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an accelerator in AWS Global Accelerator for the website E. Create an Amazon Machine Image (AMI) from the existing EC2 instance. Use the AMI to provision new instances behind an Application Load Balancer as part of an Auto Scaling group. Configure the Auto Scaling group to maintain a minimum of two instances. Configure an Amazon CloudFront distribution for the website."
    },
    "correct_answer": "CE",
    "vote_percentage": "70%",
    "question_cn": "一家公司在其 Amazon EC2 上运行一个使用内容管理系统 (CMS) 的网站。该 CMS 在单个 EC2 实例上运行，并使用 Amazon Aurora MySQL 多可用区数据库实例作为数据层。网站图片存储在安装在 EC2 实例内的 Amazon Elastic Block Store (Amazon EBS) 卷上。解决方案架构师应采取哪些组合操作来提高网站的性能和弹性？（选择两个。）",
    "options_cn": {
      "A": "将网站图片移动到安装在每个 EC2 实例上的 Amazon S3 存储桶中。",
      "B": "通过使用来自主 EC2 实例的 NFS 共享来共享网站图片。在其他 EC2 实例上挂载此共享。",
      "C": "将网站图片移动到安装在每个 EC2 实例上的 Amazon Elastic File System (Amazon EFS) 文件系统中。",
      "D": "从现有的 EC2 实例创建 Amazon Machine Image (AMI)。使用 AMI 在 Application Load Balancer 后面预置新实例，作为 Auto Scaling 组的一部分。配置 Auto Scaling 组以保持至少两个实例。为网站配置 AWS Global Accelerator 中的加速器。",
      "E": "从现有的 EC2 实例创建 Amazon Machine Image (AMI)。使用 AMI 在 Application Load Balancer 后面预置新实例，作为 Auto Scaling 组的一部分。配置 Auto Scaling 组以保持至少两个实例。为网站配置 Amazon CloudFront 分发。"
    },
    "tags": [
      "Amazon EC2",
      "CMS",
      "Amazon Aurora MySQL",
      "Amazon EBS",
      "Amazon S3",
      "Amazon EFS",
      "Application Load Balancer",
      "Auto Scaling",
      "AWS Global Accelerator",
      "Amazon CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 70%），解析仅供参考。】\n\n考察了提高 EC2 网站性能和弹性的常用架构模式，包括图片存储、负载均衡、自动伸缩和内容分发网络（CDN）的使用。",
      "why_correct": "选项 C：Amazon EFS 提供了可扩展的、共享的文件系统，多个 EC2 实例可以同时访问和共享图片资源，从而提高网站的弹性和性能。\n选项 E：使用 AMI、Application Load Balancer 和 Auto Scaling Group 可以实现弹性伸缩，增加网站的可用性。CloudFront 作为 CDN 缓存图片等静态资源，从而加速网站内容分发，减轻源服务器压力。",
      "why_wrong": "选项 A：将图片移动到每个 EC2 实例上的 S3 存储桶，会导致每个实例都需要维护自己的图片副本，不便于管理和更新，且未能实现共享文件系统，故不适用。\n选项 B：使用 NFS 共享文件系统，单点故障风险高，且性能瓶颈可能在主 EC2 实例上，无法有效提高性能和弹性。\n选项 D：AWS Global Accelerator 主要用于加速全球范围内的流量，而 CloudFront 更适合用于内容分发。使用 Global Accelerator 对静态内容加速效果不如 CloudFront，且会增加额外费用，所以此方案不优于使用 CloudFront。"
    },
    "related_terms": [
      "Amazon EC2",
      "content management system (CMS)",
      "Amazon Aurora MySQL",
      "Multi-AZ",
      "Amazon EBS",
      "Amazon S3",
      "Amazon EFS",
      "Amazon Machine Image (AMI)",
      "Application Load Balancer",
      "Auto Scaling group",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "NFS"
    ]
  },
  {
    "id": 503,
    "topic": "1",
    "question_en": "A company runs an infrastructure monitoring service. The company is building a new feature that will enable the service to monitor data in customer AWS accounts. The new feature will call AWS APIs in customer accounts to describe Amazon EC2 instances and read Amazon CloudWatch metrics. What should the company do to obtain access to customer accounts in the MOST secure way?",
    "options_en": {
      "A": "Ensure that the customers create an IAM role in their account with read-only EC2 and CloudWatch permissions and a trust policy to the company’s account.",
      "B": "Create a serverless API that implements a token vending machine to provide temporary AWS credentials for a role with read-only EC2 and CloudWatch permissions.",
      "C": "Ensure that the customers create an IAM user in their account with read-only EC2 and CloudWatch permissions. Encrypt and store customer access and secret keys in a secrets management system.",
      "D": "Ensure that the customers create an Amazon Cognito user in their account to use an IAM role with read-only EC2 and CloudWatch permissions. Encrypt and store the Amazon Cognito user and password in a secrets management system."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司运营着一个基础设施监控服务。该公司正在构建一个新功能，该功能将使该服务能够监控客户 AWS 账户中的数据。新功能将调用客户账户中的 AWS API 来描述 Amazon EC2 实例并读取 Amazon CloudWatch 指标。为了以最安全的方式获取对客户账户的访问权限，该公司应该怎么做？",
    "options_cn": {
      "A": "确保客户在其账户中创建一个 IAM 角色，该角色具有只读 EC2 和 CloudWatch 权限，并向该公司的账户授予信任策略。",
      "B": "创建一个无服务器 API，该 API 实现一个令牌发放机，以提供具有只读 EC2 和 CloudWatch 权限的角色的临时 AWS 凭证。",
      "C": "确保客户在其账户中创建一个具有只读 EC2 和 CloudWatch 权限的 IAM 用户。加密并将客户的访问密钥和秘密密钥存储在秘密管理系统中。",
      "D": "确保客户在其账户中创建一个 Amazon Cognito 用户，以使用具有只读 EC2 和 CloudWatch 权限的 IAM 角色。加密并将 Amazon Cognito 用户和密码存储在秘密管理系统中。"
    },
    "tags": [
      "IAM",
      "Amazon EC2",
      "Amazon CloudWatch",
      "Amazon Cognito"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n该题考查安全获取客户账户访问权限的方法。使用 IAM 角色和信任策略是推荐做法。",
      "why_correct": "确保客户在其账户中创建一个 IAM 角色，该角色具有只读 EC2 和 CloudWatch 权限，并向该公司的账户授予信任策略，是最安全的方法。",
      "why_wrong": "B 选项，创建令牌发放机增加了复杂性和安全风险。C 选项，存储客户的访问密钥和秘密密钥不安全。D 选项，Amazon Cognito 增加了复杂性，且存储客户访问密钥不安全。"
    },
    "related_terms": [
      "IAM",
      "Amazon EC2",
      "Amazon CloudWatch",
      "Amazon Cognito"
    ]
  },
  {
    "id": 504,
    "topic": "1",
    "question_en": "A company needs to connect several VPCs in the us-east-1 Region that span hundreds of AWS accounts. The company's networking team has its own AWS account to manage the cloud network. What is the MOST operationally eficient solution to connect the VPCs?",
    "options_en": {
      "A": "Set up VPC peering connections between each VPC. Update each associated subnet’s route table",
      "B": "Configure a NAT gateway and an internet gateway in each VPC to connect each VPC through the internet",
      "C": "Create an AWS Transit Gateway in the networking team’s AWS account. Configure static routes from each VPC.",
      "D": "Deploy VPN gateways in each VPC. Create a transit VPC in the networking team’s AWS account to connect to each VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要在 us-east-1 区域连接数百个 AWS 账户中的多个 VPC。该公司的网络团队有自己的 AWS 账户来管理云网络。哪种解决方案在运营上最有效，可以连接这些 VPC？",
    "options_cn": {
      "A": "在每个 VPC 之间设置 VPC 对等连接。更新每个关联子网的路由表。",
      "B": "在每个 VPC 中配置 NAT Gateway 和 Internet Gateway，通过互联网连接每个 VPC。",
      "C": "在网络团队的 AWS 账户中创建一个 AWS Transit Gateway。从每个 VPC 配置静态路由。",
      "D": "在每个 VPC 中部署 VPN 网关。在网络团队的 AWS 账户中创建一个 Transit VPC 来连接每个 VPC。"
    },
    "tags": [
      "VPC 对等连接",
      "AWS Transit Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n该题考查多个 VPC 的连接方案。Transit Gateway 可以简化 VPC 之间的网络连接。",
      "why_correct": "在网络团队的 AWS 账户中创建一个 AWS Transit Gateway，可以简化多个 VPC 的连接，具有更高的运营效率。",
      "why_wrong": "A 选项，VPC 对等连接需要为每个 VPC 单独设置，扩展性差。B 选项，每个 VPC 中配置 NAT Gateway 和 Internet Gateway 增加了复杂性和成本。D 选项，使用 VPN 网关也增加了复杂性。"
    },
    "related_terms": [
      "VPC 对等连接",
      "AWS Transit Gateway"
    ]
  },
  {
    "id": 505,
    "topic": "1",
    "question_en": "A company has Amazon EC2 instances that run nightly batch jobs to process data. The EC2 instances run in an Auto Scaling group that uses On-Demand billing. If a job fails on one instance, another instance will reprocess the job. The batch jobs run between 12:00 AM and 06:00 AM local time every day. Which solution will provide EC2 instances to meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Purchase a 1-year Savings Plan for Amazon EC2 that covers the instance family of the Auto Scaling group that the batch job uses.",
      "B": "Purchase a 1-year Reserved Instance for the specific instance type and operating system of the instances in the Auto Scaling group that the batch job uses.",
      "C": "Create a new launch template for the Auto Scaling group. Set the instances to Spot Instances. Set a policy to scale out based on CPU usage.",
      "D": "Create a new launch template for the Auto Scaling group. Increase the instance size. Set a policy to scale out based on CPU usage."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有 Amazon EC2 实例，这些实例运行夜间批处理作业来处理数据。EC2 实例在 Auto Scaling 组中运行，该组使用按需计费。如果一个实例上的作业失败，另一个实例将重新处理该作业。批处理作业在每天当地时间凌晨 12:00 到凌晨 06:00 之间运行。哪种解决方案将以最具成本效益的方式提供 EC2 实例以满足这些要求？",
    "options_cn": {
      "A": "为 Amazon EC2 购买 1 年期 Savings Plan，涵盖批处理作业使用的 Auto Scaling 组的实例系列。",
      "B": "为 Auto Scaling 组中批处理作业使用的实例的特定实例类型和操作系统购买 1 年期 Reserved Instance。",
      "C": "为 Auto Scaling 组创建一个新的启动模板。将实例设置为 Spot Instances。设置一个基于 CPU 使用率进行扩展的策略。",
      "D": "为 Auto Scaling 组创建一个新的启动模板。增加实例大小。设置一个基于 CPU 使用率进行扩展的策略。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Spot Instances",
      "Savings Plan",
      "Reserved Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n该题考查 EC2 成本优化。Spot Instances 通常比按需实例更便宜，但可能中断。Savings Plan 和 Reserved Instance 提供了不同类型的折扣。",
      "why_correct": "使用 Spot Instances 可以最大限度地降低成本，且 Auto Scaling 组能够处理实例中断的情况。基于 CPU 使用率进行扩展，可以动态地调整实例数量。",
      "why_wrong": "A 选项，1 年期 Savings Plan 无法完全保证成本效益，且与 Spot Instances 相比，成本更高。B 选项，Reserved Instance 需要预先承诺，灵活性较差，成本也相对较高。D 选项，增加实例大小无法完全保证成本效益。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Spot Instances",
      "Savings Plan",
      "Reserved Instance"
    ]
  },
  {
    "id": 506,
    "topic": "1",
    "question_en": "A social media company is building a feature for its website. The feature will give users the ability to upload photos. The company expects significant increases in demand during large events and must ensure that the website can handle the upload trafic from users. Which solution meets these requirements with the MOST scalability?",
    "options_en": {
      "A": "Upload files from the user's browser to the application servers. Transfer the files to an Amazon S3 bucket.",
      "B": "Provision an AWS Storage Gateway file gateway. Upload files directly from the user's browser to the file gateway.",
      "C": "Generate Amazon S3 presigned URLs in the application. Upload files directly from the user's browser into an S3 bucket.",
      "D": "Provision an Amazon Elastic File System (Amazon EFS) file system. Upload files directly from the user's browser to the file system."
    },
    "correct_answer": "C",
    "vote_percentage": "92%",
    "question_cn": "一家社交媒体公司正在为其网站构建一项功能。该功能将允许用户上传照片。该公司预计在大型活动期间需求量会显着增加，并且必须确保该网站能够处理来自用户的上传流量。哪种解决方案以最高的扩展能力满足这些要求？",
    "options_cn": {
      "A": "从用户的浏览器将文件上传到应用程序服务器。将文件传输到 Amazon S3 存储桶。",
      "B": "配置一个 AWS Storage Gateway 文件网关。直接从用户的浏览器将文件上传到文件网关。",
      "C": "在应用程序中生成 Amazon S3 预签名 URL。直接从用户的浏览器将文件上传到 S3 存储桶。",
      "D": "配置一个 Amazon Elastic File System (Amazon EFS) 文件系统。直接从用户的浏览器将文件上传到文件系统。"
    },
    "tags": [
      "Amazon S3",
      "预签名 URL",
      "AWS Storage Gateway",
      "Amazon EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 92%），解析仅供参考。】\n\n本题考察上传文件的方案，并关注可扩展性。S3 预签名 URL 允许直接从客户端上传文件，可以实现更好的扩展性。",
      "why_correct": "在应用程序中生成 S3 预签名 URL，可以直接从用户的浏览器上传文件到 S3 存储桶，减少了应用程序服务器的负载，从而提高了扩展能力。",
      "why_wrong": "A 选项，将文件上传到应用程序服务器会增加应用程序服务器的负载，限制了扩展能力。B 选项，Storage Gateway 文件网关不适合用户上传。D 选项，EFS 不适合直接从浏览器上传文件。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Storage Gateway",
      "Amazon EFS",
      "预签名 URL"
    ]
  },
  {
    "id": 507,
    "topic": "1",
    "question_en": "A company has a web application for travel ticketing. The application is based on a database that runs in a single data center in North America. The company wants to expand the application to serve a global user base. The company needs to deploy the application to multiple AWS Regions. Average latency must be less than 1 second on updates to the reservation database. The company wants to have separate deployments of its web platform across multiple Regions. However, the company must maintain a single primary reservation database that is globally consistent. Which solution should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Convert the application to use Amazon DynamoDB. Use a global table for the center reservation table. Use the correct Regional endpoint in each Regional deployment.",
      "B": "Migrate the database to an Amazon Aurora MySQL database. Deploy Aurora Read Replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database.",
      "C": "Migrate the database to an Amazon RDS for MySQL database. Deploy MySQL read replicas in each Region. Use the correct Regional endpoint in each Regional deployment for access to the database.",
      "D": "Migrate the application to an Amazon Aurora Serverless database. Deploy instances of the database to each Region. Use the correct Regional endpoint in each Regional deployment to access the database. Use AWS Lambda functions to process event streams in each Region to synchronize the databases."
    },
    "correct_answer": "A",
    "vote_percentage": "58%",
    "question_cn": "一家公司拥有一款用于旅行票务的 Web 应用程序。 该应用程序基于一个数据库，该数据库在北美的一个数据中心运行。 公司希望扩展该应用程序以服务于全球用户群。 公司需要将应用程序部署到多个 AWS 区域。 预订数据库的更新平均延迟必须小于 1 秒。 公司希望在多个区域中拥有其 Web 平台的单独部署。 但是，公司必须维护一个全局一致的单一主要预订数据库。 解决方案架构师应推荐哪种解决方案来满足这些要求？",
    "options_cn": {
      "A": "将应用程序转换为使用 Amazon DynamoDB。对中心预订表使用全局表。 在每个区域部署中使用正确的区域端点。",
      "B": "将数据库迁移到 Amazon Aurora MySQL 数据库。在每个区域部署 Aurora 副本。 在每个区域部署中使用正确的区域端点来访问数据库。",
      "C": "将数据库迁移到 Amazon RDS for MySQL 数据库。在每个区域部署 MySQL 副本。 在每个区域部署中使用正确的区域端点来访问数据库。",
      "D": "将应用程序迁移到 Amazon Aurora Serverless 数据库。 将数据库的实例部署到每个区域。 使用每个区域中的正确区域端点来访问数据库。 使用 AWS Lambda 函数处理每个区域中的事件流以同步数据库。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon Aurora MySQL",
      "Amazon RDS for MySQL",
      "Amazon Aurora Serverless"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 58%），解析仅供参考。】\n\n考查了跨区域数据库部署的方案选择，并需要考虑数据一致性和低延迟的要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：Amazon DynamoDB 全局表能够提供跨多个 AWS 区域的完全托管、多区域、多活动数据库，实现数据的自动复制。通过使用 DynamoDB 全局表，可以满足全局一致性及低于 1 秒的平均延迟要求。每个区域部署使用对应的区域端点，可以确保应用程序的本地化访问。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 和 C 都使用了 Aurora MySQL 和 RDS for MySQL，这两种数据库在跨区域复制时，虽然能通过副本提供就近读取，但写操作需要同步到主数据库，难以满足 1 秒内的平均延迟要求，且管理全局一致性比较复杂。选项 D 使用 Aurora Serverless，虽然可以自动扩展，但没有全局一致性的保证，并且需要使用 Lambda 函数来同步数据库，增加了复杂性和延迟。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "global table",
      "Amazon Aurora MySQL",
      "Amazon RDS for MySQL",
      "MySQL",
      "Aurora Serverless",
      "AWS Lambda"
    ]
  },
  {
    "id": 508,
    "topic": "1",
    "question_en": "A company has migrated multiple Microsoft Windows Server workloads to Amazon EC2 instances that run in the us-west-1 Region. The company manually backs up the workloads to create an image as needed. In the event of a natural disaster in the us-west-1 Region, the company wants to recover workloads quickly in the us-west-2 Region. The company wants no more than 24 hours of data loss on the EC2 instances. The company also wants to automate any backups of the EC2 instances. Which solutions will meet these requirements with the LEAST administrative effort? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags. Schedule the backup to run twice daily. Copy the image on demand.",
      "B": "Create an Amazon EC2-backed Amazon Machine Image (AMI) lifecycle policy to create a backup based on tags. Schedule the backup to run twice daily. Configure the copy to the us-west-2 Region.",
      "C": "Create backup vaults in us-west-1 and in us-west-2 by using AWS Backup. Create a backup plan for the EC2 instances based on tag values. Create an AWS Lambda function to run as a scheduled job to copy the backup data to us-west-2.",
      "D": "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the EC2 instances based on tag values. Define the destination for the copy as us-west-2. Specify the backup schedule to run twice daily",
      "E": "Create a backup vault by using AWS Backup. Use AWS Backup to create a backup plan for the EC2 instances based on tag values. Specify the backup schedule to run twice daily. Copy on demand to us-west-2."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一家公司已将多个 Microsoft Windows Server 工作负载迁移到在 us-west-1 区域中运行的 Amazon EC2 实例。该公司会根据需要手动备份工作负载以创建映像。如果 us-west-1 区域发生自然灾害，该公司希望快速恢复 us-west-2 区域中的工作负载。该公司希望 EC2 实例上的数据丢失不超过 24 小时。该公司还希望自动执行 EC2 实例的任何备份。哪些解决方案将以最少的管理工作满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建一个 Amazon EC2 支持的 Amazon Machine Image (AMI) 生命周期策略，以根据标签创建备份。将备份计划为每天运行两次。按需复制映像。",
      "B": "创建一个 Amazon EC2 支持的 Amazon Machine Image (AMI) 生命周期策略，以根据标签创建备份。将备份计划为每天运行两次。配置复制到 us-west-2 区域。",
      "C": "通过使用 AWS Backup 在 us-west-1 和 us-west-2 中创建备份库。创建一个基于标签值的 EC2 实例的备份计划。创建一个 AWS Lambda 函数，以作为计划作业运行，以将备份数据复制到 us-west-2。",
      "D": "通过使用 AWS Backup 创建一个备份库。使用 AWS Backup 为 EC2 实例创建一个基于标签值的备份计划。将复制的目标定义为 us-west-2。指定备份计划为每天运行两次。",
      "E": "通过使用 AWS Backup 创建一个备份库。使用 AWS Backup 为 EC2 实例创建一个基于标签值的备份计划。指定备份计划为每天运行两次。按需复制到 us-west-2。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon Machine Image (AMI)",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n考查了在不同 AWS 区域间备份 EC2 实例的解决方案，以及如何满足 RTO 和备份自动化需求。",
      "why_correct": "选项 B 使用 Amazon EC2 支持的 AMI 生命周期策略，通过标签创建备份，并配置复制到 us-west-2 区域，满足了备份自动化和跨区域恢复的需求，且能够满足数据丢失不超过 24 小时的 RTO 要求。选项 D 使用 AWS Backup，通过标签创建备份计划，并将复制的目标定义为 us-west-2，并设置每日两次的备份频率，同样满足了备份自动化、跨区域恢复和 RTO 的要求。",
      "why_wrong": "选项 A 缺少复制到 us-west-2 的配置，无法实现跨区域恢复。选项 C 使用 AWS Backup，但需要创建一个 Lambda 函数来复制备份数据，增加了管理复杂性，且 RTO 依赖于 Lambda 函数的执行。选项 E 使用 AWS Backup，但按需复制备份到 us-west-2，不能满足自动化的要求，并且在灾难发生时，需要手动触发复制，增加了恢复的时间，可能不满足 24 小时的 RTO 要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "AMI",
      "us-west-1",
      "us-west-2",
      "AWS Backup",
      "Lambda"
    ]
  },
  {
    "id": 509,
    "topic": "1",
    "question_en": "A company operates a two-tier application for image processing. The application uses two Availability Zones, each with one public subnet and one private subnet. An Application Load Balancer (ALB) for the web tier uses the public subnets. Amazon EC2 instances for the application tier use the private subnets. Users report that the application is running more slowly than expected. A security audit of the web server log files shows that the application is receiving millions of illegitimate requests from a small number of IP addresses. A solutions architect needs to resolve the immediate performance problem while the company investigates a more permanent solution. What should the solutions architect recommend to meet this requirement?",
    "options_en": {
      "A": "Modify the inbound security group for the web tier. Add a deny rule for the IP addresses that are consuming resources.",
      "B": "Modify the network ACL for the web tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources.",
      "C": "Modify the inbound security group for the application tier. Add a deny rule for the IP addresses that are consuming resources.",
      "D": "Modify the network ACL for the application tier subnets. Add an inbound deny rule for the IP addresses that are consuming resources."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家公司运营一个用于图像处理的两层应用程序。该应用程序使用两个可用区，每个可用区都有一个公有子网和一个私有子网。Web 层使用的 Application Load Balancer (ALB) 使用公有子网。应用程序层使用的 Amazon EC2 实例使用私有子网。用户报告说应用程序的运行速度比预期的要慢。对 Web 服务器日志文件的安全审计显示，该应用程序正在接收来自少量 IP 地址的数百万个非法请求。解决方案架构师需要解决当前的性能问题，同时公司调查一个更永久的解决方案。解决方案架构师应该推荐什么来满足此要求？",
    "options_cn": {
      "A": "修改 Web 层的入站安全组。添加一个拒绝规则，用于消耗资源的 IP 地址。",
      "B": "修改 Web 层子网的网络 ACL。添加一个拒绝规则，用于消耗资源的 IP 地址。",
      "C": "修改应用程序层的入站安全组。添加一个拒绝规则，用于消耗资源的 IP 地址。",
      "D": "修改应用程序层子网的网络 ACL。添加一个拒绝规则，用于消耗资源的 IP 地址。"
    },
    "tags": [
      "Application Load Balancer",
      "安全组",
      "网络 ACL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n本题考察解决性能问题的方案。网络 ACL 和安全组可以用来控制流量，安全组用于实例级别，网络 ACL 用于子网级别。",
      "why_correct": "修改 Web 层子网的网络 ACL，添加一个拒绝规则用于消耗资源的 IP 地址，可以阻止恶意请求，从而解决性能问题。",
      "why_wrong": "A 选项，安全组控制流量是实例级别的，无法影响整个子网。C 选项，应用程序层安全组无法阻止 Web 层收到的恶意请求。D 选项，应用程序层子网的网络 ACL 无法阻止 Web 层收到的恶意请求。"
    },
    "related_terms": [
      "Application Load Balancer",
      "安全组",
      "网络 ACL"
    ]
  },
  {
    "id": 510,
    "topic": "1",
    "question_en": "A global marketing company has applications that run in the ap-southeast-2 Region and the eu-west-1 Region. Applications that run in a VPC in eu-west-1 need to communicate securely with databases that run in a VPC in ap-southeast-2. Which network design will meet these requirements?",
    "options_en": {
      "A": "Create a VPC peering connection between the eu-west-1 VPC and the ap-southeast-2 VPC. Create an inbound rule in the eu-west-1 application security group that allows trafic from the database server IP addresses in the ap-southeast-2 security group.",
      "B": "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPC. Update the subnet route tables. Create an inbound rule in the ap-southeast-2 database security group that references the security group ID of the application servers in eu-west-1.",
      "C": "Configure a VPC peering connection between the ap-southeast-2 VPC and the eu-west-1 VPUpdate the subnet route tables. Create an inbound rule in the ap-southeast-2 database security group that allows trafic from the eu-west-1 application server IP addresses.",
      "D": "Create a transit gateway with a peering attachment between the eu-west-1 VPC and the ap-southeast-2 VPC. After the transit gateways are properly peered and routing is configured, create an inbound rule in the database security group that references the security group ID of the application servers in eu-west-1."
    },
    "correct_answer": "C",
    "vote_percentage": "87%",
    "question_cn": "一家全球营销公司在其 ap-southeast-2 区域和 eu-west-1 区域中运行应用程序。需要在 eu-west-1 中的 VPC 中运行的应用程序安全地与在 ap-southeast-2 中的 VPC 中运行的数据库通信。哪种网络设计将满足这些要求？",
    "options_cn": {
      "A": "在 eu-west-1 VPC 和 ap-southeast-2 VPC 之间创建 VPC 对等连接。在 eu-west-1 应用程序安全组中创建一个入站规则，该规则允许来自 ap-southeast-2 安全组中的数据库服务器 IP 地址的流量。",
      "B": "在 ap-southeast-2 VPC 和 eu-west-1 VPC 之间配置 VPC 对等连接。更新子网路由表。在 ap-southeast-2 数据库安全组中创建一个入站规则，该规则引用 eu-west-1 中应用程序服务器的安全组 ID。",
      "C": "在 ap-southeast-2 VPC 和 eu-west-1 VPC 之间配置 VPC 对等连接。更新子网路由表。在 ap-southeast-2 数据库安全组中创建一个入站规则，该规则允许来自 eu-west-1 应用程序服务器 IP 地址的流量。",
      "D": "在 eu-west-1 VPC 和 ap-southeast-2 VPC 之间创建一个具有对等附件的 Transit Gateway。在正确对等 Transit Gateway 并配置路由后，在数据库安全组中创建一个入站规则，该规则引用 eu-west-1 中应用程序服务器的安全组 ID。"
    },
    "tags": [
      "VPC 对等连接",
      "Transit Gateway",
      "安全组"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 87%），解析仅供参考。】\n\n考察了跨区域 VPC 之间数据库的安全通信配置，以及安全组和路由表的正确设置。",
      "why_correct": "选项 C 提供了正确的跨区域 VPC 连接和安全组配置。通过在 ap-southeast-2 VPC 和 eu-west-1 VPC 之间建立 VPC 对等连接，确保了网络互通。更新子网路由表保证了流量能够正确路由。在数据库安全组中允许来自 eu-west-1 应用程序服务器 IP 地址的流量，保证了应用程序能够访问数据库。",
      "why_wrong": "选项 A 错误在于安全组配置不正确，需要引用安全组 ID 而不是 IP 地址，且流量方向不匹配。选项 B 错误在于流量方向错误，需要在 ap-southeast-2 数据库安全组中允许来自 eu-west-1 应用程序的安全组 ID，而不是在 ap-southeast-2 创建入站规则。选项 D 错误在于使用了 Transit Gateway，虽然可以实现跨区域连接，但该选项描述的安全组配置依然错误，需要使用安全组 ID 而不是 IP 地址，且该配置方式比选项 C 更为复杂，不符合题目最简解决方案的要求。"
    },
    "related_terms": [
      "VPC",
      "ap-southeast-2",
      "eu-west-1",
      "VPC Peering Connection",
      "Security Group",
      "IP address",
      "Subnet Route Table",
      "Transit Gateway"
    ]
  },
  {
    "id": 511,
    "topic": "1",
    "question_en": "A company is developing software that uses a PostgreSQL database schema. The company needs to configure multiple development environments and databases for the company's developers. On average, each development environment is used for half of the 8-hour workday. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure each development environment with its own Amazon Aurora PostgreSQL database",
      "B": "Configure each development environment with its own Amazon RDS for PostgreSQL Single-AZ DB instances",
      "C": "Configure each development environment with its own Amazon Aurora On-Demand PostgreSQL-Compatible database",
      "D": "Configure each development environment with its own Amazon S3 bucket by using Amazon S3 Object Select"
    },
    "correct_answer": "C",
    "vote_percentage": "59%",
    "question_cn": "一家公司正在开发使用 PostgreSQL 数据库模式的软件。该公司需要为公司的开发人员配置多个开发环境和数据库。平均而言，每个开发环境使用 8 小时工作日的 50%。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用其自己的 Amazon Aurora PostgreSQL 数据库配置每个开发环境",
      "B": "使用其自己的 Amazon RDS for PostgreSQL Single-AZ DB 实例配置每个开发环境",
      "C": "使用其自己的 Amazon Aurora On-Demand PostgreSQL 兼容数据库配置每个开发环境",
      "D": "使用其自己的 Amazon S3 存储桶（通过使用 Amazon S3 Object Select）配置每个开发环境"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 59%），解析仅供参考。】\n\n考察如何在开发环境中以最具成本效益的方式配置 PostgreSQL 数据库。重点关注 Aurora On-Demand 的适用性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：Amazon Aurora On-Demand PostgreSQL 兼容数据库允许开发人员按需使用数据库，并且无需预先配置数据库实例。由于开发环境仅在 50% 的时间内使用，On-Demand 模式可以最大程度地降低成本，避免为未使用的资源付费。这种模式非常适合间歇性工作负载。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，Amazon Aurora PostgreSQL，虽然提供了高可用性和性能，但如果为每个开发环境都配置独立的 Aurora 数据库，成本会非常高。选项 B，Amazon RDS for PostgreSQL Single-AZ DB 实例，成本同样会超过 On-Demand 模式，且无法根据需求灵活伸缩。选项 D，使用 Amazon S3 存储桶，这种方案用于存储数据，而不是数据库。它不适合直接运行 PostgreSQL 数据库，与题目要求不符。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "PostgreSQL",
      "Amazon Aurora PostgreSQL",
      "Amazon RDS for PostgreSQL",
      "Single-AZ DB",
      "Amazon Aurora On-Demand PostgreSQL",
      "Amazon S3",
      "Amazon S3 Object Select"
    ]
  },
  {
    "id": 512,
    "topic": "1",
    "question_en": "A company uses AWS Organizations with resources tagged by account. The company also uses AWS Backup to back up its AWS infrastructure resources. The company needs to back up all AWS resources. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Config to identify all untagged resources. Tag the identified resources programmatically. Use tags in the backup plan.",
      "B": "Use AWS Config to identify all resources that are not running. Add those resources to the backup vault.",
      "C": "Require all AWS account owners to review their resources to identify the resources that need to be backed up.",
      "D": "Use Amazon Inspector to identify all noncompliant resources."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS Organizations，其资源按账户进行标记。该公司还使用 AWS Backup 来备份其 AWS 基础设施资源。该公司需要备份所有 AWS 资源。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Config 识别所有未标记的资源。以编程方式标记已识别的资源。在备份计划中使用标签。",
      "B": "使用 AWS Config 识别所有未运行的资源。将这些资源添加到备份库。",
      "C": "要求所有 AWS 账户所有者审查其资源以识别需要备份的资源。",
      "D": "使用 Amazon Inspector 识别所有不合规的资源。"
    },
    "tags": [
      "AWS Backup",
      "AWS Config",
      "AWS Organizations",
      "Tagging"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考察使用 AWS Backup 进行跨账户备份的场景下，如何通过标签实现资源的自动备份；涉及 AWS Config 的合规性评估、标签策略以及与 AWS Backup 的集成。此外，也考察了运维成本的概念。",
      "why_correct": "使用 AWS Config 可以发现未标记的资源，通过编程方式对这些资源进行标记，确保所有资源都应用了标签。在 AWS Backup 备份计划中使用标签，可以自动将所有带有指定标签的资源纳入备份范围。这种方法自动化程度高，降低了运营开销，并保证了备份覆盖的全面性。",
      "why_wrong": "选项 B 错误，AWS Config 用于合规性检查，而题目要求是备份所有资源，两者目标不同。将未运行的资源加入备份库并不能保证所有资源都被备份，且无法满足按需备份的要求。选项 C 错误，依靠账户所有者审查资源并标记会导致人为错误，备份范围不确定，无法保证备份的完整性，且增加了运营成本。选项 D 错误，Amazon Inspector 用于漏洞和安全检查，与备份需求无关，无法满足备份所有资源的需求。"
    },
    "related_terms": [
      "AWS Backup",
      "AWS Config",
      "AWS Organizations",
      "Amazon Inspector",
      "Tagging"
    ]
  },
  {
    "id": 513,
    "topic": "1",
    "question_en": "A social media company wants to allow its users to upload images in an application that is hosted in the AWS Cloud. The company needs a solution that automatically resizes the images so that the images can be displayed on multiple device types. The application experiences unpredictable trafic patterns throughout the day. The company is seeking a highly available solution that maximizes scalability. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a static website hosted in Amazon S3 that invokes AWS Lambda functions to resize the images and store the images in an Amazon S3 bucket.",
      "B": "Create a static website hosted in Amazon CloudFront that invokes AWS Step Functions to resize the images and store the images in an Amazon RDS database.",
      "C": "Create a dynamic website hosted on a web server that runs on an Amazon EC2 instance. Configure a process that runs on the EC2 instance to resize the images and store the images in an Amazon S3 bucket.",
      "D": "Create a dynamic website hosted on an automatically scaling Amazon Elastic Container Service (Amazon ECS) cluster that creates a resize job in Amazon Simple Queue Service (Amazon SQS). Set up an image-resizing program that runs on an Amazon EC2 instance to process the resize jobs."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家社交媒体公司希望允许其用户在其托管在 AWS 云中的应用程序中上传图像。该公司需要一个解决方案，该解决方案可以自动调整图像大小，以便可以在多种设备类型上显示图像。该应用程序在一天中会经历不可预测的流量模式。该公司正在寻找一个高度可用的解决方案，该解决方案可以最大限度地提高可扩展性。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个托管在 Amazon S3 中的静态网站，该网站调用 AWS Lambda 函数来调整图像大小并将图像存储在 Amazon S3 存储桶中。",
      "B": "创建一个托管在 Amazon CloudFront 中的静态网站，该网站调用 AWS Step Functions 来调整图像大小并将图像存储在 Amazon RDS 数据库中。",
      "C": "创建一个托管在在 Amazon EC2 实例上运行的 Web 服务器上的动态网站。配置一个在 EC2 实例上运行的进程以调整图像大小并将图像存储在 Amazon S3 存储桶中。",
      "D": "创建一个托管在自动缩放的 Amazon Elastic Container Service (Amazon ECS) 集群上的动态网站，该集群在 Amazon Simple Queue Service (Amazon SQS) 中创建一个调整大小的作业。设置一个在 Amazon EC2 实例上运行的图像调整大小程序以处理调整大小的作业。"
    },
    "tags": [
      "Amazon S3",
      "AWS Lambda",
      "Image resizing",
      "High Availability",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查构建高可用、可扩展的图片上传和处理架构；与 S3 静态网站托管、Lambda 函数、EC2、ECS、SQS 和 Step Functions 的选型、以及它们在不同场景下的适用性相关。",
      "why_correct": "选项 A 描述的方案能够满足需求。首先，使用 Amazon S3 托管静态网站，便于实现高可用性和可扩展性。当用户上传图片时，S3 触发 AWS Lambda 函数。Lambda 函数负责调整图片大小，然后将调整后的图片存储回 S3 存储桶。这种架构天然具有高可用性和弹性扩展能力，能够应对不可预测的流量模式，且 Lambda 按需运行，成本效益高。",
      "why_wrong": "选项 B 错误，因为它将图片存储在 Amazon RDS 数据库中。数据库存储不适用于存储大量的图片，并且会增加存储成本和访问延迟。此外，虽然 CloudFront 可提供 CDN 功能，但使用 Step Functions 处理图片大小调整会增加复杂性，对于简单场景来说，Lambda 更为合适。选项 C 错误，因为它使用 EC2 实例托管 Web 服务器，需要手动管理服务器的可用性和扩展性，难以满足高可用性和弹性扩展的需求。另外，在 EC2 实例上运行调整大小的进程，也会增加维护和管理的复杂性。选项 D 错误，它引入了 ECS 和 SQS，增加了架构的复杂性，且 ECS 实例上的图片调整程序需要自行管理，增加了复杂度。虽然 ECS 和 SQS 在某些场景下具有优势，但对于本题这种相对简单的图片处理需求，S3、Lambda 的组合更简洁、更高效。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Lambda",
      "Amazon CloudFront",
      "AWS Step Functions",
      "Amazon RDS",
      "Amazon EC2",
      "Amazon ECS",
      "Amazon SQS"
    ]
  },
  {
    "id": 514,
    "topic": "1",
    "question_en": "A company is running a microservices application on Amazon EC2 instances. The company wants to migrate the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for scalability. The company must configure the Amazon EKS control plane with endpoint private access set to true and endpoint public access set to false to maintain security compliance. The company must also put the data plane in private subnets. However, the company has received error notifications because the node cannot join the cluster. Which solution will allow the node to join the cluster?",
    "options_en": {
      "A": "Grant the required permission in AWS Identity and Access Management (IAM) to the AmazonEKSNodeRole IAM role.",
      "B": "Create interface VPC endpoints to allow nodes to access the control plane.",
      "C": "Recreate nodes in the public subnet. Restrict security groups for EC2 nodes.",
      "D": "Allow outbound trafic in the security group of the nodes."
    },
    "correct_answer": "B",
    "vote_percentage": "53%",
    "question_cn": "一家公司正在 Amazon EC2 实例上运行微服务应用程序。该公司希望将应用程序迁移到 Amazon Elastic Kubernetes Service (Amazon EKS) 集群以实现可扩展性。该公司必须将 Amazon EKS 控制平面配置为将 endpoint private access 设置为 true，并将 endpoint public access 设置为 false，以保持安全合规性。该公司还必须将数据平面置于私有子网中。但是，该公司收到了错误通知，因为节点无法加入集群。哪个解决方案将允许节点加入集群？",
    "options_cn": {
      "A": "在 AWS Identity and Access Management (IAM) 中向 AmazonEKSNodeRole IAM 角色授予所需的权限。",
      "B": "创建接口 VPC endpoint 以允许节点访问控制平面。",
      "C": "在公共子网中重新创建节点。限制 EC2 节点的安全组。",
      "D": "允许节点安全组中的出站流量。"
    },
    "tags": [
      "Amazon EKS",
      "VPC Endpoint",
      "IAM",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 53%），解析仅供参考。】\n\n本题考查 EKS 集群的私有访问配置和节点加入集群的常见问题及解决方案，以及 VPC Endpoint 的使用。 涉及到 EKS 控制平面和数据平面的分离，以及网络连通性。与 IAM 权限、安全组配置等相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：创建接口 VPC Endpoint 允许节点访问控制平面是正确的解决方案。由于 EKS 控制平面配置为私有访问，且数据平面位于私有子网中，节点无法直接通过公网访问控制平面。接口 VPC Endpoint 提供了从 VPC 到 EKS 控制平面的私有连接，允许节点在不通过 Internet 的情况下与控制平面通信，从而完成加入集群的操作。 VPC Endpoint 使用 AWS PrivateLink 技术，保证了私有连接的安全性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，在 IAM 中向 AmazonEKSNodeRole IAM 角色授予所需的权限，虽然是 EKS 节点所需的配置，但无法解决节点无法连接到控制平面的问题。权限配置正确是节点加入集群的前提，但不是根本原因。选项 C，在公共子网中重新创建节点，违背了题目中“将数据平面置于私有子网中”的要求，不符合安全合规性要求。同时，即使节点在公共子网，如果没有正确的网络连接，依然无法加入集群。选项 D，允许节点安全组中的出站流量，这本身没有错，但并不能解决节点无法访问控制平面的问题，而仅开放安全组的流量是不够的。节点仍需要能够访问 EKS 控制平面的 endpoint 才能加入集群。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EKS",
      "EC2",
      "IAM",
      "AWS PrivateLink",
      "VPC Endpoint"
    ]
  },
  {
    "id": 515,
    "topic": "1",
    "question_en": "A company is migrating an on-premises application to AWS. The company wants to use Amazon Redshift as a solution. Which use cases are suitable for Amazon Redshift in this scenario? (Choose three.)",
    "options_en": {
      "A": "Supporting data APIs to access data with traditional, containerized, and event-driven applications",
      "B": "Supporting client-side and server-side encryption",
      "C": "Building analytics workloads during specified hours and when the application is not active",
      "D": "Caching data to reduce the pressure on the backend database",
      "E": "Scaling globally to support petabytes of data and tens of millions of requests per minute",
      "F": "Creating a secondary replica of the cluster by using the AWS Management Console"
    },
    "correct_answer": "BCE",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其本地应用程序迁移到 AWS。该公司希望使用 Amazon Redshift 作为解决方案。在此场景中，哪些用例适合 Amazon Redshift？（选择三个。）",
    "options_cn": {
      "A": "支持数据 API 以访问传统、容器化和事件驱动应用程序的数据",
      "B": "支持客户端和服务器端加密",
      "C": "在指定时间段和应用程序不活动时构建分析工作负载",
      "D": "缓存数据以减轻后端数据库的压力",
      "E": "在全球范围内扩展以支持 PB 级数据和每分钟数千万个请求",
      "F": "使用 AWS 管理控制台创建集群的次要副本"
    },
    "tags": [
      "Amazon Redshift",
      "Encryption",
      "Data Warehousing",
      "Database"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BCE（社区 —），解析仅供参考。】\n\n本题考查 Amazon Redshift 的适用场景，涉及数据仓库、加密、高可用、以及与其它数据库技术的对比。重点在于理解 Redshift 的核心功能和设计目标。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BCE。理由简述：Amazon Redshift 支持客户端和服务器端加密，通过使用 AWS KMS（Key Management Service）加密数据，从而满足安全性需求。这有助于保护静态数据和传输中的数据，符合数据安全合规要求。Redshift 提供了多种加密选项，能够满足不同的安全策略需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. Amazon Redshift 主要用于数据仓库和分析工作负载，不支持数据 API 以访问传统、容器化和事件驱动应用程序的数据；这类需求更适合使用像 Amazon RDS 或 NoSQL 数据库，它们可以提供更灵活的数据访问方式。\nC. Amazon Redshift 主要用于分析工作负载，而非构建分析工作负载。在指定时间段和应用程序不活动时构建分析工作负载并非其主要功能，分析工作负载通常是持续运行的。这类需求可以考虑使用 AWS Glue 或 Amazon EMR 等服务。\nD. Amazon Redshift 不是缓存数据库。Redshift 是数据仓库，缓存数据以减轻后端数据库的压力是数据库缓存技术的功能，例如 Amazon ElastiCache 更适合这个场景。\nE. Amazon Redshift 虽然可以扩展以支持 PB 级数据，但它并不是专门为每分钟数千万个请求而设计的。这类需求更适合使用针对高吞吐量和并发设计的数据库，例如 Amazon DynamoDB。\nF. Amazon Redshift 提供了集群的副本，但其目的是为了高可用性和容错，而不是通过 AWS 管理控制台创建次要副本。创建副本通常通过快照或跨区域复制来实现。AWS 管理控制台主要用于管理集群，而不是创建副本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Redshift",
      "AWS KMS",
      "Amazon RDS",
      "NoSQL",
      "AWS Glue",
      "Amazon EMR",
      "Amazon ElastiCache",
      "Amazon DynamoDB"
    ]
  },
  {
    "id": 516,
    "topic": "1",
    "question_en": "A company provides an API interface to customers so the customers can retrieve their financial information. Еhe company expects a larger number of requests during peak usage times of the year. The company requires the API to respond consistently with low latency to ensure customer satisfaction. The company needs to provide a compute host for the API. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use an Application Load Balancer and Amazon Elastic Container Service (Amazon ECS).",
      "B": "Use Amazon API Gateway and AWS Lambda functions with provisioned concurrency.",
      "C": "Use an Application Load Balancer and an Amazon Elastic Kubernetes Service (Amazon EKS) cluster.",
      "D": "Use Amazon API Gateway and AWS Lambda functions with reserved concurrency."
    },
    "correct_answer": "B",
    "vote_percentage": "74%",
    "question_cn": "一家公司为客户提供 API 接口，以便客户可以检索其财务信息。该公司预计在一年中的高峰使用时段会有大量请求。该公司要求 API 始终以低延迟响应，以确保客户满意度。该公司需要为 API 提供计算主机。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Application Load Balancer 和 Amazon Elastic Container Service (Amazon ECS)。",
      "B": "使用 Amazon API Gateway 和带有预置并发的 AWS Lambda 函数。",
      "C": "使用 Application Load Balancer 和 Amazon Elastic Kubernetes Service (Amazon EKS) 集群。",
      "D": "使用 Amazon API Gateway 和带有保留并发的 AWS Lambda 函数。"
    },
    "tags": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Lambda Provisioned Concurrency",
      "API Low Latency"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 74%），解析仅供参考。】\n\n考查如何设计一个低延迟、高可用的 API 解决方案；与 API 网关、无服务器计算服务、并发控制的选型相关。",
      "why_correct": "Amazon API Gateway 提供了高性能的 API 端点，与 AWS Lambda 函数结合使用，可以按需扩展，满足高峰期的请求。预置并发 (Provisioned Concurrency) 保证了 Lambda 函数实例的快速启动，从而降低延迟，满足低延迟要求。这种架构具有极低的运营开销，因为 API Gateway 和 Lambda 都是全托管服务。",
      "why_wrong": "A. Amazon ECS 涉及到容器管理，相比 Lambda 函数，运维开销更大。虽然 Application Load Balancer 可以提供负载均衡，但 ECS 的部署和扩展不如 Lambda 灵活，且初始化时间更长，无法满足低延迟需求。\nC. Amazon EKS 涉及到 Kubernetes 集群的管理，运维开销比 ECS 更大。虽然 EKS 提供了容器编排能力，但其复杂性不适用于低延迟 API 的场景。EKS 的启动时间和资源管理也可能导致延迟问题。\nD. 保留并发 (Reserved Concurrency) 确保 Lambda 函数在特定数量的并发请求下不会被限制。虽然保留并发可以提高 API 的可用性，但其主要目的是防止函数被耗尽，而不是像预置并发那样主动初始化函数实例来缩短延迟。预置并发可以更有效地满足低延迟要求，而保留并发更侧重于资源分配和防止函数过载。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Application Load Balancer",
      "Amazon ECS",
      "Amazon EKS",
      "Lambda Provisioned Concurrency",
      "Lambda Reserved Concurrency"
    ]
  },
  {
    "id": 517,
    "topic": "1",
    "question_en": "A company wants to send all AWS Systems Manager Session Manager logs to an Amazon S3 bucket for archival purposes. Which solution will meet this requirement with the MOST operational eficiency?",
    "options_en": {
      "A": "Enable S3 logging in the Systems Manager console. Choose an S3 bucket to send the session data to.",
      "B": "Install the Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Export the logs to an S3 bucket from the group for archival purposes.",
      "C": "Create a Systems Manager document to upload all server logs to a central S3 bucket. Use Amazon EventBridge to run the Systems Manager document against all servers that are in the account daily.",
      "D": "Install an Amazon CloudWatch agent. Push all logs to a CloudWatch log group. Create a CloudWatch logs subscription that pushes any incoming log events to an Amazon Kinesis Data Firehose delivery stream. Set Amazon S3 as the destination."
    },
    "correct_answer": "A",
    "vote_percentage": "92%",
    "question_cn": "一家公司希望将所有 AWS Systems Manager Session Manager 日志发送到 Amazon S3 存储桶以进行存档。哪种解决方案将以最高的运营效率满足此要求？",
    "options_cn": {
      "A": "在 Systems Manager 控制台中打开 S3 日志记录。选择一个 S3 存储桶以将会话数据发送到该存储桶。",
      "B": "安装 Amazon CloudWatch 代理。将所有日志推送到 CloudWatch 日志组。从该组导出日志到 S3 存储桶以进行存档。",
      "C": "创建一个 Systems Manager 文档，将所有服务器日志上传到中央 S3 存储桶。使用 Amazon EventBridge 每天对帐户中的所有服务器运行 Systems Manager 文档。",
      "D": "安装 Amazon CloudWatch 代理。将所有日志推送到 CloudWatch 日志组。创建一个 CloudWatch 日志订阅，将所有传入的日志事件推送到 Amazon Kinesis Data Firehose 传送流。设置 Amazon S3 作为目的地。"
    },
    "tags": [
      "AWS Systems Manager",
      "Session Manager",
      "Amazon S3",
      "Amazon CloudWatch",
      "Amazon CloudWatch Logs",
      "Amazon Kinesis Data Firehose",
      "CloudWatch Logs subscription",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 92%），解析仅供参考。】\n\n考查如何将 AWS Systems Manager Session Manager 的日志存档到 Amazon S3，并关注运营效率。",
      "why_correct": "选项 A 提供最直接的解决方案，无需额外配置。在 Systems Manager 控制台中启用 S3 日志记录，并指定 S3 存储桶，即可自动将 Session Manager 日志发送到 S3，实现日志存档，运营效率最高。",
      "why_wrong": "选项 B 需要安装和配置 Amazon CloudWatch 代理，并从 CloudWatch Logs 导出到 S3，增加了配置的复杂性和管理成本。选项 C 依赖于 Systems Manager 文档和 EventBridge 调度，这增加了配置的复杂性，并且可能导致延迟。选项 D 需要安装 CloudWatch 代理，配置 CloudWatch Logs、Kinesis Data Firehose 和 S3，配置最为复杂，涉及多个组件的交互，运营效率最低。"
    },
    "related_terms": [
      "AWS Systems Manager Session Manager",
      "Amazon S3",
      "Systems Manager",
      "Amazon CloudWatch",
      "CloudWatch Logs",
      "Amazon EventBridge",
      "Systems Manager Document",
      "Amazon Kinesis Data Firehose",
      "CloudWatch Logs subscription"
    ]
  },
  {
    "id": 518,
    "topic": "1",
    "question_en": "An application uses an Amazon RDS MySQL DB instance. The RDS database is becoming low on disk space. A solutions architect wants to increase the disk space without downtime. Which solution meets these requirements with the LEAST amount of effort?",
    "options_en": {
      "A": "Enable storage autoscaling in RDS",
      "B": "Increase the RDS database instance size",
      "C": "Change the RDS database instance storage type to Provisioned IOPS",
      "D": "Back up the RDS database, increase the storage capacity, restore the database, and stop the previous instance"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一个应用程序使用 Amazon RDS MySQL 数据库实例。RDS 数据库的磁盘空间不足。一个解决方案架构师希望在不停机的情况下增加磁盘空间。哪个解决方案以最少的精力满足这些要求？",
    "options_cn": {
      "A": "在 RDS 中启用存储自动扩展。",
      "B": "增加 RDS 数据库实例的大小。",
      "C": "将 RDS 数据库实例存储类型更改为 Provisioned IOPS。",
      "D": "备份 RDS 数据库，增加存储容量，恢复数据库，并停止之前的实例。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Storage Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查 RDS 数据库的存储管理，以及在不中断服务的情况下增加存储空间的方法。需要考虑 RDS 的存储自动扩展、实例大小调整、存储类型变更以及备份恢复等方案的适用性，特别是针对不停机和最小工作量这两个关键需求。",
      "why_correct": "启用 RDS 存储自动扩展是满足需求的最佳方案。当数据库存储空间使用达到预定义阈值时，存储自动扩展会自动增加存储容量，无需停机或手动干预。这符合题目中“不停机”和“最少精力”的要求，且 RDS 会自动处理存储的增加和管理。",
      "why_wrong": "选项 B 增加 RDS 数据库实例的大小，会导致停机维护，不符合题目中“不停机”的要求。选项 C 将 RDS 数据库实例存储类型更改为 Provisioned IOPS，虽然可能改善性能，但并不能直接解决磁盘空间不足的问题，而且也涉及实例的修改。选项 D 备份 RDS 数据库、增加存储容量并恢复，会导致停机，并且需要手动操作，工作量较大，不符合“最少精力”的要求。此外，恢复数据库所需的时间也会影响可用性，而存储自动扩展则避免了这些问题。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS",
      "MySQL",
      "Provisioned IOPS",
      "RDS Storage Auto Scaling",
      "Backup",
      "Restore"
    ]
  },
  {
    "id": 519,
    "topic": "1",
    "question_en": "A consulting company provides professional services to customers worldwide. The company provides solutions and tools for customers to expedite gathering and analyzing data on AWS. The company needs to centrally manage and deploy a common set of solutions and tools for customers to use for self-service purposes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create AWS CloudFormation templates for the customers.",
      "B": "Create AWS Service Catalog products for the customers.",
      "C": "Create AWS Systems Manager templates for the customers.",
      "D": "Create AWS Config items for the customers."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家咨询公司为全球客户提供专业服务。该公司为客户提供解决方案和工具，以加速在 AWS 上收集和分析数据。该公司需要集中管理和部署一套通用的解决方案和工具，供客户用于自助服务。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为客户创建 AWS CloudFormation 模板。",
      "B": "为客户创建 AWS Service Catalog 产品。",
      "C": "为客户创建 AWS Systems Manager 模板。",
      "D": "为客户创建 AWS Config 项目。"
    },
    "tags": [
      "AWS Service Catalog"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 AWS Service Catalog 的应用场景；与 AWS CloudFormation, AWS Systems Manager, 和 AWS Config 的用途对比相关。",
      "why_correct": "AWS Service Catalog 允许组织创建、管理和分发在其 AWS 环境中使用的产品。这些产品可以包括计算、存储、数据库或软件，它们被打包成模板，使用户能够轻松地自助服务。在本案例中，咨询公司需要集中管理和部署通用的解决方案和工具，供客户自助服务，这正是 Service Catalog 的核心功能，满足了题目中的所有要求。",
      "why_wrong": "选项 A，AWS CloudFormation 主要用于基础设施即代码 (IaC)，用于定义和部署 AWS 资源。虽然 CloudFormation 模板可以被 Service Catalog 用作产品蓝图，但它本身并不提供集中管理和客户自助服务的功能，无法满足题目的需求。选项 C，AWS Systems Manager 主要用于运维管理，如配置管理、自动化和补丁管理等，无法满足集中管理和自助服务的需求。选项 D，AWS Config 用于资源配置合规性审计、记录和变更追踪，与客户自助服务和解决方案部署的场景无关。"
    },
    "related_terms": [
      "AWS Service Catalog",
      "AWS CloudFormation",
      "AWS Systems Manager",
      "AWS Config"
    ]
  },
  {
    "id": 520,
    "topic": "1",
    "question_en": "A company is designing a new web application that will run on Amazon EC2 Instances. The application will use Amazon DynamoDB for backend data storage. The application trafic will be unpredictable. The company expects that the application read and write throughput to the database will be moderate to high. The company needs to scale in response to application trafic. Which DynamoDB table configuration will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard table class. Set DynamoDB auto scaling to a maximum defined capacity.",
      "B": "Configure DynamoDB in on-demand mode by using the DynamoDB Standard table class.",
      "C": "Configure DynamoDB with provisioned read and write by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class. Set DynamoDB auto scaling to a maximum defined capacity.",
      "D": "Configure DynamoDB in on-demand mode by using the DynamoDB Standard Infrequent Access (DynamoDB Standard-IA) table class."
    },
    "correct_answer": "B",
    "vote_percentage": "63%",
    "question_cn": "一家公司正在设计一个将在 Amazon EC2 实例上运行的新 Web 应用程序。 该应用程序将使用 Amazon DynamoDB 作为后端数据存储。 应用程序的流量将是不可预测的。 公司预计对数据库的应用程序读写吞吐量将从中等到高。 公司需要根据应用程序流量进行扩展。 哪种 DynamoDB 表配置将最经济高效地满足这些要求？",
    "options_cn": {
      "A": "使用 DynamoDB 标准表类配置 DynamoDB，并使用预置的读写。 将 DynamoDB 自动伸缩设置为最大定义容量。",
      "B": "使用 DynamoDB 标准表类以按需模式配置 DynamoDB。",
      "C": "使用 DynamoDB 标准不频繁访问 (DynamoDB Standard-IA) 表类配置 DynamoDB，并使用预置的读写。 将 DynamoDB 自动伸缩设置为最大定义容量。",
      "D": "使用 DynamoDB 标准不频繁访问 (DynamoDB Standard-IA) 表类以按需模式配置 DynamoDB。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB On-Demand",
      "DynamoDB Auto Scaling",
      "DynamoDB Standard-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 63%），解析仅供参考。】\n\n考查 DynamoDB 的配置和扩展策略，特别是按需模式（On-Demand）和预置模式（Provisioned）的选择，以及与 Auto Scaling 的关系。与成本优化、可扩展性相关。",
      "why_correct": "选项 B 使用 DynamoDB 标准表类的按需模式。按需模式非常适合流量不可预测的应用程序，它会自动根据应用程序的流量需求进行扩展，无需预先规划容量。这避免了为未使用的容量付费，并能够应对突发流量，从而实现最经济高效的解决方案。",
      "why_wrong": "选项 A 错误，因为预置读写容量需要提前规划，对于流量不可预测的应用程序，预置容量可能无法满足峰值需求，或者预置过多导致成本浪费。虽然 DynamoDB Auto Scaling 可以帮助调整，但不如按需模式灵活。选项 C 错误，因为 DynamoDB Standard-IA 适用于访问频率较低的数据，但本题场景是应用程序读写吞吐量从中等到高，不适用。选项 D 错误，虽然 Standard-IA 也采用按需模式，但其主要针对访问频率较低的数据存储，无法满足高频读写的需求。使用 Standard-IA 会导致性能下降，并且可能不适用于此处的使用场景。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "EC2",
      "DynamoDB On-Demand",
      "DynamoDB Standard",
      "DynamoDB Auto Scaling",
      "DynamoDB Standard-IA"
    ]
  },
  {
    "id": 521,
    "topic": "1",
    "question_en": "A retail company has several businesses. The IT team for each business manages its own AWS account. Each team account is part of an organization in AWS Organizations. Each team monitors its product inventory levels in an Amazon DynamoDB table in the team's own AWS account. The company is deploying a central inventory reporting application into a shared AWS account. The application must be able to read items from all the teams' DynamoDB tables. Which authentication option will meet these requirements MOST securely?",
    "options_en": {
      "A": "Integrate DynamoDB with AWS Secrets Manager in the inventory application account. Configure the application to use the correct secret from Secrets Manager to authenticate and read the DynamoDB table. Schedule secret rotation for every 30 days.",
      "B": "In every business account, create an IAM user that has programmatic access. Configure the application to use the correct IAM user access key ID and secret access key to authenticate and read the DynamoDB table. Manually rotate IAM access keys every 30 days.",
      "C": "In every business account, create an IAM role named BU_ROLE with a policy that gives the role access to the DynamoDB table and a trust policy to trust a specific role in the inventory application account. In the inventory account, create a role named APP_ROLE that allows access to the STS AssumeRole API operation. Configure the application to use APP_ROLE and assume the crossaccount role BU_ROLE to read the DynamoDB table.",
      "D": "Integrate DynamoDB with AWS Certificate Manager (ACM). Generate identity certificates to authenticate DynamoDB. Configure the application to use the correct certificate to authenticate and read the DynamoDB table."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家零售公司拥有多家业务。每个业务的 IT 团队都管理自己的 AWS 账户。每个团队账户都是 AWS Organizations 中组织的一部分。每个团队都会在其团队自己的 AWS 账户中的 Amazon DynamoDB 表中监控其产品库存水平。公司正在将一个中央库存报告应用程序部署到共享的 AWS 账户中。该应用程序必须能够从所有团队的 DynamoDB 表中读取项目。哪种身份验证选项最安全地满足这些要求？",
    "options_cn": {
      "A": "将 DynamoDB 与库存应用程序账户中的 AWS Secrets Manager 集成。配置应用程序使用来自 Secrets Manager 的正确密钥进行身份验证并读取 DynamoDB 表。安排每 30 天轮换一次密钥。",
      "B": "在每个业务账户中，创建一个具有编程访问权限的 IAM 用户。配置应用程序使用正确的 IAM 用户访问密钥 ID 和秘密访问密钥进行身份验证并读取 DynamoDB 表。手动轮换 IAM 访问密钥，每 30 天一次。",
      "C": "在每个业务账户中，创建一个名为 BU_ROLE 的 IAM 角色，该角色具有访问 DynamoDB 表的策略和信任策略，以信任库存应用程序账户中的特定角色。在库存账户中，创建一个名为 APP_ROLE 的角色，该角色允许访问 STS AssumeRole API 操作。配置应用程序使用 APP_ROLE 并代入跨账户角色 BU_ROLE 以读取 DynamoDB 表。",
      "D": "将 DynamoDB 与 AWS Certificate Manager (ACM) 集成。生成身份证书以对 DynamoDB 进行身份验证。配置应用程序使用正确的证书进行身份验证并读取 DynamoDB 表。"
    },
    "tags": [
      "IAM",
      "DynamoDB",
      "STS",
      "AWS Organizations",
      "Cross-account access"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查跨账户访问 DynamoDB 的安全身份验证方案，涉及 IAM 角色、STS AssumeRole 操作以及组织结构。需要考虑安全性、管理成本和访问权限的控制。",
      "why_correct": "选项 C 采用 IAM 角色和跨账户角色代入（AssumeRole）的方式，是实现跨账户安全访问 DynamoDB 的最佳实践。每个业务账户创建一个 IAM 角色（BU_ROLE），赋予其访问 DynamoDB 表的权限；库存应用程序账户创建一个 IAM 角色（APP_ROLE），允许它代入（Assume） BU_ROLE。这利用了 IAM 角色提供的临时凭证，避免了长期存在的访问密钥。这种方法符合最小权限原则，并易于管理和审计。",
      "why_wrong": "选项 A 使用 AWS Secrets Manager 管理密钥。虽然 Secret Manager 提供了密钥轮换功能，但这种方案将密钥暴露在库存应用程序账户中，需要对密钥进行存储和管理，增加了安全风险，且维护成本较高。选项 B 使用 IAM 用户，并需要手动轮换访问密钥，这增加了管理负担，并且密钥管理不安全，存在密钥泄露的风险。选项 D 使用 AWS Certificate Manager (ACM)。ACM 主要用于管理 SSL/TLS 证书，而不是用于 DynamoDB 的身份验证。DynamoDB 不直接支持证书身份验证，因此该选项不可行。"
    },
    "related_terms": [
      "IAM",
      "DynamoDB",
      "STS",
      "AWS Organizations",
      "Secrets Manager",
      "ACM",
      "AssumeRole",
      "IAM User",
      "IAM Role"
    ]
  },
  {
    "id": 522,
    "topic": "1",
    "question_en": "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS). The company's workload is not consistent throughout the day. The company wants Amazon EKS to scale in and out according to the workload. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Use an AWS Lambda function to resize the EKS cluster.",
      "B": "Use the Kubernetes Metrics Server to activate horizontal pod autoscaling.",
      "C": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
      "D": "Use Amazon API Gateway and connect it to Amazon EKS",
      "E": "Use AWS App Mesh to observe network activity."
    },
    "correct_answer": "BC",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon Elastic Kubernetes Service (Amazon EKS) 运行容器应用程序。该公司的负载在一天中并不一致。该公司希望 Amazon EKS 根据工作负载进行横向扩展和缩减。以下哪种步骤组合将以最小的运营开销满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "使用 AWS Lambda 函数调整 EKS 集群的大小。",
      "B": "使用 Kubernetes Metrics Server 激活水平 Pod 自动伸缩。",
      "C": "使用 Kubernetes Cluster Autoscaler 管理集群中的节点数量。",
      "D": "使用 Amazon API Gateway 并将其连接到 Amazon EKS。",
      "E": "使用 AWS App Mesh 观察网络活动。"
    },
    "tags": [
      "Amazon EKS",
      "Kubernetes",
      "Horizontal Pod Autoscaling (HPA)",
      "Kubernetes Cluster Autoscaler",
      "Kubernetes Metrics Server"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 100%），解析仅供参考。】\n\n考查 Amazon EKS 的弹性伸缩配置；与 Kubernetes 原生组件的整合、以及负载均衡的配置相关。",
      "why_correct": "使用 Kubernetes Metrics Server 激活水平 Pod 自动伸缩（Horizontal Pod Autoscaling, HPA）是实现 EKS 集群 Pod 自动伸缩的关键步骤。Metrics Server 负责收集 Pod 的 CPU 和内存等资源使用情况指标，HPA 根据这些指标来决定是否横向扩展或缩减 Pod 数量，从而响应负载变化，满足了根据负载自动调整的需求。",
      "why_wrong": "选项 A，使用 AWS Lambda 函数调整 EKS 集群的大小，这种方案较为复杂，需要自定义逻辑，增加了运营开销，不符合题干中“最小的运营开销”的要求。选项 C，使用 Kubernetes Cluster Autoscaler 管理集群中的节点数量，这负责集群节点（Node）的自动伸缩，与 Pod 级别的自动伸缩（HPA）互补，是需要结合使用的，但它不能单独满足“根据工作负载横向扩展和缩减”的要求。选项 D，使用 Amazon API Gateway 并将其连接到 Amazon EKS，API Gateway 主要用于管理 API 流量，与集群 Pod 的自动伸缩无关，无法满足题目要求。选项 E，使用 AWS App Mesh 观察网络活动，App Mesh 用于服务网格管理，侧重于服务间的通信和监控，与负载自动伸缩无关，无法解决负载动态变化的问题。"
    },
    "related_terms": [
      "Amazon EKS",
      "Kubernetes",
      "AWS Lambda",
      "Amazon API Gateway",
      "Horizontal Pod Autoscaling (HPA)",
      "Kubernetes Metrics Server",
      "Kubernetes Cluster Autoscaler",
      "AWS App Mesh"
    ]
  },
  {
    "id": 523,
    "topic": "1",
    "question_en": "A company runs a microservice-based serverless web application. The application must be able to retrieve data from multiple Amazon DynamoDB tables A solutions architect needs to give the application the ability to retrieve the data with no impact on the baseline performance of the application. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "AWS AppSync pipeline resolvers",
      "B": "Amazon CloudFront with Lambda@Edge functions",
      "C": "Edge-optimized Amazon API Gateway with AWS Lambda functions",
      "D": "Amazon Athena Federated Query with a DynamoDB connector"
    },
    "correct_answer": "D",
    "vote_percentage": "49%",
    "question_cn": "一家公司运营一个基于微服务的无服务器 Web 应用程序。 该应用程序必须能够从多个 Amazon DynamoDB 表中检索数据。 一个解决方案架构师需要为该应用程序提供检索数据的能力，且不对应用程序的基线性能造成影响。 哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "AWS AppSync pipeline resolvers",
      "B": "带有 Lambda@Edge 函数的 Amazon CloudFront",
      "C": "带有 AWS Lambda 函数的 Edge-optimized Amazon API Gateway",
      "D": "带有 DynamoDB 连接器的 Amazon Athena Federated Query"
    },
    "tags": [
      "AWS AppSync",
      "DynamoDB",
      "Lambda",
      "Amazon CloudFront",
      "Amazon API Gateway",
      "Amazon Athena",
      "Federated Query",
      "Lambda@Edge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 49%），解析仅供参考。】\n\n考察使用 Athena Federated Query 查询 DynamoDB 表，以及在满足性能要求的同时，提高运营效率的方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：Amazon Athena Federated Query 允许使用 SQL 查询多个数据源，包括 DynamoDB 表，无需改变应用程序的基线性能。 DynamoDB 连接器使得 Athena 可以访问 DynamoDB 表中的数据。这种解决方案无需维护任何服务器，并支持通过 SQL 进行数据检索，具有高度的运营效率。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. AWS AppSync pipeline resolvers 主要用于 GraphQL API 的数据处理，虽然可以与 DynamoDB 交互，但其复杂性更高，运营维护成本也更高。 B. 带有 Lambda@Edge 函数的 Amazon CloudFront 主要用于缓存和内容分发，不直接用于从 DynamoDB 表中检索数据。 C. 带有 AWS Lambda 函数的 Edge-optimized Amazon API Gateway 虽然可以访问 DynamoDB，但 API Gateway 的设计更偏向于 API 管理，而非直接的批量数据查询，且 Lambda 函数的调用开销会影响性能和效率。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "AWS Lambda",
      "Amazon Athena Federated Query",
      "AWS AppSync",
      "Amazon CloudFront",
      "API Gateway",
      "Lambda@Edge",
      "DynamoDB Connector"
    ]
  },
  {
    "id": 524,
    "topic": "1",
    "question_en": "A company wants to analyze and troubleshoot Access Denied errors and Unauthorized errors that are related to IAM permissions. The company has AWS CloudTrail turned on. Which solution will meet these requirements with the LEAST effort?",
    "options_en": {
      "A": "Use AWS Glue and write custom scripts to query CloudTrail logs for the errors.",
      "B": "Use AWS Batch and write custom scripts to query CloudTrail logs for the errors.",
      "C": "Search CloudTrail logs with Amazon Athena queries to identify the errors.",
      "D": "Search CloudTrail logs with Amazon QuickSight. Create a dashboard to identify the errors."
    },
    "correct_answer": "C",
    "vote_percentage": "68%",
    "question_cn": "一家公司希望分析和排除与 IAM 权限相关的“访问被拒绝”和“未授权”错误。该公司已启用 AWS CloudTrail。哪种解决方案以最少的精力满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Glue 并编写自定义脚本来查询 CloudTrail 日志以查找这些错误。",
      "B": "使用 AWS Batch 并编写自定义脚本来查询 CloudTrail 日志以查找这些错误。",
      "C": "使用 Amazon Athena 查询搜索 CloudTrail 日志以识别这些错误。",
      "D": "使用 Amazon QuickSight 搜索 CloudTrail 日志。创建一个仪表板以识别这些错误。"
    },
    "tags": [
      "AWS CloudTrail",
      "Amazon Athena",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 68%），解析仅供参考。】\n\n考查利用 CloudTrail 日志分析 IAM 权限问题，涉及不同分析工具的选择与对比，重点关注效率和工作量。",
      "why_correct": "Amazon Athena 是一种交互式查询服务，可以使用标准 SQL 查询直接分析 CloudTrail 日志。Athena 无需任何管理工作，即开即用，可以快速查询和分析 CloudTrail 日志，从而识别“访问被拒绝”和“未授权”错误。它以最小的精力满足需求，适合这种快速诊断 IAM 权限问题的场景。",
      "why_wrong": "A. AWS Glue 需要编写自定义脚本，涉及到数据转换和脚本维护，增加了工作量，不符合最小精力的要求。B. AWS Batch 也是运行自定义脚本的平台，需要创建作业和管理基础设施，比 Athena 更复杂，工作量更大，也不符合要求。D. Amazon QuickSight 虽然可以用于数据可视化，但首先需要将 CloudTrail 日志导入数据源，并且 QuickSight 本身不是用于直接查询日志的工具，需要先做 ETL，不适合快速分析，增加了不必要的工作量。"
    },
    "related_terms": [
      "CloudTrail",
      "IAM",
      "Amazon Athena",
      "AWS Glue",
      "AWS Batch",
      "Amazon QuickSight"
    ]
  },
  {
    "id": 525,
    "topic": "1",
    "question_en": "A company wants to add its existing AWS usage cost to its operation cost dashboard. A solutions architect needs to recommend a solution that will give the company access to its usage cost programmatically. The company must be able to access cost data for the current year and forecast costs for the next 12 months. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Access usage cost-related data by using the AWS Cost Explorer API with pagination.",
      "B": "Access usage cost-related data by using downloadable AWS Cost Explorer report .csv files.",
      "C": "Configure AWS Budgets actions to send usage cost data to the company through FTP.",
      "D": "Create AWS Budgets reports for usage cost data. Send the data to the company through SMTP."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其现有的 AWS 使用成本添加到其运营成本仪表板中。一位解决方案架构师需要推荐一个解决方案，该解决方案将使该公司能够以编程方式访问其使用成本。该公司必须能够访问当年的成本数据并预测未来 12 个月的成本。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用带有分页功能的 AWS Cost Explorer API 访问与使用成本相关的数据。",
      "B": "使用可下载的 AWS Cost Explorer 报告 .csv 文件访问与使用成本相关的数据。",
      "C": "配置 AWS Budgets 操作，通过 FTP 将使用成本数据发送给公司。",
      "D": "创建 AWS Budgets 报告以获取使用成本数据。通过 SMTP 将数据发送给公司。"
    },
    "tags": [
      "AWS Budgets",
      "Cost Explorer",
      "SMTP",
      "AWS Cost Management"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查以编程方式访问 AWS 成本数据并进行预测的解决方案。侧重于以最少运营开销满足需求。",
      "why_correct": "选项 A 提供了通过 API 以编程方式访问 AWS Cost Explorer 的功能，能够获取历史成本数据。Cost Explorer API 支持分页，方便处理大量数据。此外，Cost Explorer 具备预测功能，可以满足预测未来12个月成本的需求，且运营开销相对较低。",
      "why_wrong": "选项 B 涉及手动下载 CSV 文件，不符合以编程方式访问数据的要求，且无法自动化成本预测。选项 C 使用 AWS Budgets 操作，通过 FTP 发送数据，这并非访问成本数据的标准方式，且 FTP 协议维护成本较高。选项 D 虽然使用 Budgets 报告，但通过 SMTP 发送不便于编程访问和大规模预测，且功能不够完善。"
    },
    "related_terms": [
      "AWS Cost Explorer API",
      "AWS Cost Explorer",
      ".csv",
      "AWS Budgets",
      "FTP",
      "SMTP"
    ]
  },
  {
    "id": 526,
    "topic": "1",
    "question_en": "A solutions architect is reviewing the resilience of an application. The solutions architect notices that a database administrator recently failed over the application's Amazon Aurora PostgreSQL database writer instance as part of a scaling exercise. The failover resulted in 3 minutes of downtime for the application. Which solution will reduce the downtime for scaling exercises with the LEAST operational overhead?",
    "options_en": {
      "A": "Create more Aurora PostgreSQL read replicas in the cluster to handle the load during failover.",
      "B": "Set up a secondary Aurora PostgreSQL cluster in the same AWS Region. During failover, update the application to use the secondary cluster's writer endpoint.",
      "C": "Create an Amazon ElastiCache for Memcached cluster to handle the load during failover.",
      "D": "Set up an Amazon RDS proxy for the database. Update the application to use the proxy endpoint."
    },
    "correct_answer": "D",
    "vote_percentage": "72%",
    "question_cn": "一位解决方案架构师正在审查应用程序的弹性。该解决方案架构师注意到数据库管理员最近作为扩展练习的一部分，故障转移了应用程序的 Amazon Aurora PostgreSQL 数据库写入器实例。故障转移导致应用程序停机 3 分钟。哪种解决方案将以最少的运营开销来减少扩展练习的停机时间？",
    "options_cn": {
      "A": "在集群中创建更多 Aurora PostgreSQL 只读副本以处理故障转移期间的负载。",
      "B": "在同一 AWS 区域设置一个辅助 Aurora PostgreSQL 集群。在故障转移期间，更新应用程序以使用辅助集群的写入器端点。",
      "C": "创建一个 Amazon ElastiCache for Memcached 集群来处理故障转移期间的负载。",
      "D": "为数据库设置一个 Amazon RDS 代理。更新应用程序以使用代理端点。"
    },
    "tags": [
      "Aurora PostgreSQL",
      "RDS",
      "ElastiCache",
      "RDS Proxy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 72%），解析仅供参考。】\n\n此题考察 Aurora 数据库的容错和停机时间优化。Aurora 数据库故障转移时间通常较短，但仍需优化。RDS Proxy 是数据库代理，可以减少故障转移时间，从而优化应用程序的可用性。",
      "why_correct": "RDS Proxy 提供数据库连接池和故障转移支持，可以减少故障转移对应用程序的影响。",
      "why_wrong": "A 选项创建只读副本无法减少写入器实例的故障转移时间。B 选项涉及跨区域部署，操作复杂且成本较高，没有充分利用 RDS Proxy。C 选项 ElastiCache for Memcached 主要用于缓存，不能解决数据库故障转移问题。"
    },
    "related_terms": [
      "Aurora PostgreSQL",
      "Amazon Aurora",
      "RDS Proxy",
      "RDS",
      "ElastiCache for Memcached"
    ]
  },
  {
    "id": 527,
    "topic": "1",
    "question_en": "A company has a regional subscription-based streaming service that runs in a single AWS Region. The architecture consists of web servers and application servers on Amazon EC2 instances. The EC2 instances are in Auto Scaling groups behind Elastic Load Balancers. The architecture includes an Amazon Aurora global database cluster that extends across multiple Availability Zones. The company wants to expand globally and to ensure that its application has minimal downtime. Which solution will provide the MOST fault tolerance?",
    "options_en": {
      "A": "Extend the Auto Scaling groups for the web tier and the application tier to deploy instances in Availability Zones in a second Region. Use an Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.",
      "B": "Deploy the web tier and the application tier to a second Region. Add an Aurora PostgreSQL cross-Region Aurora Replica in the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed.",
      "C": "Deploy the web tier and the application tier to a second Region. Create an Aurora PostgreSQL database in the second Region. Use AWS Database Migration Service (AWS DMS) to replicate the primary database to the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region.",
      "D": "Deploy the web tier and the application tier to a second Region. Use an Amazon Aurora global database to deploy the database in the primary Region and the second Region. Use Amazon Route 53 health checks with a failover routing policy to the second Region. Promote the secondary to primary as needed."
    },
    "correct_answer": "D",
    "vote_percentage": "94%",
    "question_cn": "一家公司拥有一个基于区域的订阅流媒体服务，该服务在一个 AWS 区域中运行。该架构包括 Amazon EC2 实例上的 Web 服务器和应用程序服务器。EC2 实例位于 Elastic Load Balancer 之后的 Auto Scaling 组中。该架构包括一个跨多个可用区的 Amazon Aurora 全局数据库集群。该公司希望进行全球扩展，并确保其应用程序停机时间最短。哪种解决方案将提供最大的容错能力？",
    "options_cn": {
      "A": "扩展 Web 层和应用程序层的 Auto Scaling 组，以在第二个区域的可用区中部署实例。使用 Aurora 全局数据库在主区域和第二个区域中部署数据库。使用 Amazon Route 53 运行状况检查和故障转移路由策略到第二个区域。",
      "B": "将 Web 层和应用程序层部署到第二个区域。在第二个区域中添加 Aurora PostgreSQL 跨区域 Aurora 副本。使用 Amazon Route 53 运行状况检查和故障转移路由策略到第二个区域。根据需要将辅助数据库提升为主数据库。",
      "C": "将 Web 层和应用程序层部署到第二个区域。在第二个区域中创建一个 Aurora PostgreSQL 数据库。使用 AWS Database Migration Service (AWS DMS) 将主数据库复制到第二个区域。使用 Amazon Route 53 运行状况检查和故障转移路由策略到第二个区域。",
      "D": "将 Web 层和应用程序层部署到第二个区域。使用 Amazon Aurora 全局数据库在主区域和第二个区域中部署数据库。使用 Amazon Route 53 运行状况检查和故障转移路由策略到第二个区域。根据需要将辅助数据库提升为主数据库。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "Aurora Global Database",
      "Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 94%），解析仅供参考。】\n\n考查在多区域环境下，如何设计高容错的流媒体服务架构，重点关注数据库的跨区域部署和 Route 53 的故障转移策略。",
      "why_correct": "选项 D 提供了最佳的容错能力。它使用 Aurora Global Database，确保数据在两个区域之间同步。结合 Route 53 的运行状况检查和故障转移策略，可以在主区域故障时快速切换到备用区域，从而实现最小停机时间。",
      "why_wrong": "选项 A 同样使用 Aurora Global Database，但仅部署了 Web 层和应用程序层，可能导致跨区域的数据读取延迟。选项 B 使用 Aurora 跨区域副本，但提升为新的主数据库可能需要一定的恢复时间，影响停机时间。选项 C 使用 AWS DMS 进行数据库复制，复制过程可能存在延迟，故障切换时数据一致性难以保证，并且 Aurora PostgreSQL 无法使用 Global Database。"
    },
    "related_terms": [
      "Amazon EC2",
      "Elastic Load Balancer",
      "Auto Scaling group",
      "Amazon Aurora",
      "Amazon Aurora Global Database",
      "Amazon Route 53",
      "Amazon Aurora PostgreSQL",
      "AWS Database Migration Service (AWS DMS)"
    ]
  },
  {
    "id": 528,
    "topic": "1",
    "question_en": "A data analytics company wants to migrate its batch processing system to AWS. The company receives thousands of small data files periodically during the day through FTP. An on-premises batch job processes the data files overnight. However, the batch job takes hours to finish running. The company wants the AWS solution to process incoming data files as soon as possible with minimal changes to the FTP clients that send the files. The solution must delete the incoming data files after the files have been processed successfully. Processing for each file needs to take 3-8 minutes. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Use an Amazon EC2 instance that runs an FTP server to store incoming files as objects in Amazon S3 Glacier Flexible Retrieval. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the objects nightly from S3 Glacier Flexible Retrieval. Delete the objects after the job has processed the objects.",
      "B": "Use an Amazon EC2 instance that runs an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use Amazon EventBridge rules to invoke the job to process the files nightly from the EBS volume. Delete the files after the job has processed the files.",
      "C": "Use AWS Transfer Family to create an FTP server to store incoming files on an Amazon Elastic Block Store (Amazon EBS) volume. Configure a job queue in AWS Batch. Use an Amazon S3 event notification when each file arrives to invoke the job in AWS Batch. Delete the files after the job has processed the files.",
      "D": "Use AWS Transfer Family to create an FTP server to store incoming files in Amazon S3 Standard. Create an AWS Lambda function to process the files and to delete the files after they are processed. Use an S3 event notification to invoke the Lambda function when the files arrive."
    },
    "correct_answer": "D",
    "vote_percentage": "93%",
    "question_cn": "一家数据分析公司希望将其批处理系统迁移到 AWS。该公司每天通过 FTP 定期间隔接收数千个小数据文件。一个本地批处理作业在夜间处理数据文件。但是，批处理作业需要数小时才能完成运行。该公司希望 AWS 解决方案尽快处理传入数据文件，同时尽可能减少对发送文件的 FTP 客户端的更改。该解决方案必须在文件成功处理后删除传入数据文件。每个文件的处理需要 3-8 分钟。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "使用运行 FTP 服务器的 Amazon EC2 实例将传入文件作为对象存储在 Amazon S3 Glacier Flexible Retrieval 中。在 AWS Batch 中配置一个作业队列。使用 Amazon EventBridge 规则每天晚上从 S3 Glacier Flexible Retrieval 中调用作业来处理这些对象。在作业处理完对象后删除这些对象。",
      "B": "使用运行 FTP 服务器的 Amazon EC2 实例将传入文件存储在 Amazon Elastic Block Store (Amazon EBS) 卷上。在 AWS Batch 中配置一个作业队列。使用 Amazon EventBridge 规则每天晚上从 EBS 卷调用作业来处理这些文件。在作业处理完文件后删除这些文件。",
      "C": "使用 AWS Transfer Family 创建一个 FTP 服务器，将传入文件存储在 Amazon Elastic Block Store (Amazon EBS) 卷上。在 AWS Batch 中配置一个作业队列。当每个文件到达时，使用 Amazon S3 事件通知来调用 AWS Batch 中的作业。在作业处理完文件后删除这些文件。",
      "D": "使用 AWS Transfer Family 创建一个 FTP 服务器，将传入文件存储在 Amazon S3 Standard 中。创建一个 AWS Lambda 函数来处理文件，并在处理完文件后删除这些文件。使用 S3 事件通知在文件到达时调用 Lambda 函数。"
    },
    "tags": [
      "FTP",
      "S3",
      "AWS Batch",
      "EventBridge",
      "AWS Transfer Family"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 93%），解析仅供参考。】\n\n考察使用 AWS 服务构建批处理系统，处理通过 FTP 接收的、存储在 S3 中的小数据文件，并根据题目需求选择最具运营效率的解决方案。",
      "why_correct": "选项 D 提供了最直接、高效的解决方案。使用 AWS Transfer Family 作为 FTP 服务器，将文件上传到 S3 Standard。当文件上传到 S3 后，通过 S3 事件通知触发 Lambda 函数。Lambda 函数可以并行处理每个文件，并在处理完成后删除文件，满足了题目对处理速度和文件删除的需求，且对 FTP 客户端的改动最小。",
      "why_wrong": "选项 A 和 B 均将文件存储在 EBS 上，EBS 的设计不适合存储频繁访问和处理的大量小文件，且增加了 EC2 实例的运维复杂性。此外，A 使用 S3 Glacier Flexible Retrieval 存储，其检索时间较长，不符合快速处理文件的需求。选项 C 也使用 EBS，并使用了 S3 事件通知来触发 AWS Batch 作业，但 AWS Batch 的启动开销和处理时间相对较长，不如 Lambda 效率高。"
    },
    "related_terms": [
      "AWS Transfer Family",
      "FTP",
      "Amazon S3 Standard",
      "AWS Lambda",
      "S3 Event Notification",
      "Amazon EC2",
      "Amazon S3 Glacier Flexible Retrieval",
      "AWS Batch",
      "Amazon EventBridge",
      "Amazon Elastic Block Store (Amazon EBS)"
    ]
  },
  {
    "id": 529,
    "topic": "1",
    "question_en": "A company is migrating its workloads to AWS. The company has transactional and sensitive data in its databases. The company wants to use AWS Cloud solutions to increase security and reduce operational overhead for the databases. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the databases to Amazon EC2. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.",
      "B": "Migrate the databases to Amazon RDS Configure encryption at rest.",
      "C": "Migrate the data to Amazon S3 Use Amazon Macie for data security and protection",
      "D": "Migrate the database to Amazon RDS. Use Amazon CloudWatch Logs for data security and protection."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其工作负载迁移到 AWS。该公司在其数据库中拥有事务性和敏感数据。该公司希望使用 AWS 云解决方案来提高安全性并减少数据库的运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon EC2。使用 AWS Key Management Service (AWS KMS) 的 AWS 托管密钥进行加密。",
      "B": "将数据库迁移到 Amazon RDS。配置静态加密。",
      "C": "将数据迁移到 Amazon S3。使用 Amazon Macie 进行数据安全和保护",
      "D": "将数据库迁移到 Amazon RDS。使用 Amazon CloudWatch Logs 进行数据安全和保护。"
    },
    "tags": [
      "EC2",
      "RDS",
      "S3",
      "KMS",
      "Macie",
      "CloudWatch Logs"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查了在 AWS 上迁移数据库时，如何通过加密和降低运营开销来提高安全性。重点考察了 RDS 的加密能力。",
      "why_correct": "Amazon RDS 提供了静态加密功能，使用户能够加密 RDS 数据库实例的存储卷。这满足了题目中对安全性的要求。RDS 作为托管服务，可以降低数据库的运营开销，例如备份和维护。",
      "why_wrong": "选项 A，将数据库迁移到 EC2 虽然可以控制，但增加了运营开销，需要手动管理数据库。选项 C，将数据库迁移到 S3 不适用，因为 S3 主要用于对象存储，而非数据库。选项 D，CloudWatch Logs 主要用于监控和日志记录，不能满足静态加密需求，无法直接提高数据库安全性。"
    },
    "related_terms": [
      "Amazon RDS",
      "AWS KMS",
      "Amazon EC2",
      "Amazon S3",
      "Amazon Macie",
      "Amazon CloudWatch Logs",
      "encryption",
      "database"
    ]
  },
  {
    "id": 530,
    "topic": "1",
    "question_en": "A company has an online gaming application that has TCP and UDP multiplayer gaming capabilities. The company uses Amazon Route 53 to point the application trafic to multiple Network Load Balancers (NLBs) in different AWS Regions. The company needs to improve application performance and decrease latency for the online game in preparation for user growth. Which solution will meet these requirements?",
    "options_en": {
      "A": "Add an Amazon CloudFront distribution in front of the NLBs. Increase the Cache-Control max-age parameter.",
      "B": "Replace the NLBs with Application Load Balancers (ALBs). Configure Route 53 to use latency-based routing.",
      "C": "Add AWS Global Accelerator in front of the NLBs. Configure a Global Accelerator endpoint to use the correct listener ports.",
      "D": "Add an Amazon API Gateway endpoint behind the NLBs. Enable API caching. Override method caching for the different stages."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个具有 TCP 和 UDP 多人游戏功能的在线游戏应用程序。该公司使用 Amazon Route 53 将应用程序流量指向不同 AWS 区域中的多个 Network Load Balancer (NLB)。该公司需要提高应用程序性能并减少在线游戏的延迟，为用户增长做好准备。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 NLB 前面添加 Amazon CloudFront 分发。增加 Cache-Control max-age 参数。",
      "B": "用 Application Load Balancer (ALB) 替换 NLB。配置 Route 53 以使用基于延迟的路由。",
      "C": "在 NLB 前面添加 AWS Global Accelerator。配置 Global Accelerator 终端节点以使用正确的监听器端口。",
      "D": "在 NLB 后面添加 Amazon API Gateway 终端节点。启用 API 缓存。覆盖不同阶段的方法缓存。"
    },
    "tags": [
      "Route 53",
      "NLB",
      "CloudFront",
      "Global Accelerator",
      "API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS Global Accelerator 优化网络性能，并降低游戏应用程序延迟。强调了其对 TCP 和 UDP 流量的支持。",
      "why_correct": "AWS Global Accelerator 能够通过将用户的流量引导到最佳 AWS 边缘站点来提高应用程序性能，从而减少延迟。它支持 TCP 和 UDP 协议，非常适合多人游戏应用程序的需求。正确配置 Global Accelerator 的终端节点以使用正确的监听器端口，确保流量能够正确路由到后端的 NLB。",
      "why_wrong": "选项 A，虽然 CloudFront 可以缓存静态内容，但它不直接支持 TCP 和 UDP 流量，因此无法优化多人游戏的实时通信。选项 B，Application Load Balancer (ALB) 主要用于 HTTP/HTTPS 流量，不适用于 TCP 和 UDP 游戏流量；基于延迟的路由也无法有效降低用户的游戏延迟。选项 D，API Gateway 主要用于构建和管理 API，不适合直接处理 TCP 和 UDP 游戏流量，启用 API 缓存对游戏性能提升有限。"
    },
    "related_terms": [
      "Amazon Route 53",
      "Network Load Balancer (NLB)",
      "AWS Global Accelerator",
      "Application Load Balancer (ALB)",
      "Amazon CloudFront",
      "API Gateway",
      "TCP",
      "UDP",
      "Cache-Control max-age"
    ]
  },
  {
    "id": 531,
    "topic": "1",
    "question_en": "A company needs to integrate with a third-party data feed. The data feed sends a webhook to notify an external service when new data is ready for consumption. A developer wrote an AWS Lambda function to retrieve data when the company receives a webhook callback. The developer must make the Lambda function available for the third party to call. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create a function URL for the Lambda function. Provide the Lambda function URL to the third party for the webhook.",
      "B": "Deploy an Application Load Balancer (ALB) in front of the Lambda function. Provide the ALB URL to the third party for the webhook.",
      "C": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Attach the topic to the Lambda function. Provide the public hostname of the SNS topic to the third party for the webhook.",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Attach the queue to the Lambda function. Provide the public hostname of the SQS queue to the third party for the webhook."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要与第三方数据源集成。当新数据准备好供使用时，数据源会发送一个 webhook 来通知外部服务。开发人员编写了一个 AWS Lambda 函数，以便在公司收到 webhook 回调时检索数据。开发人员必须使 Lambda 函数可供第三方调用。哪个解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "为 Lambda 函数创建一个函数 URL。将 Lambda 函数 URL 提供给第三方用于 webhook。",
      "B": "在 Lambda 函数前面部署一个 Application Load Balancer (ALB)。将 ALB URL 提供给第三方用于 webhook。",
      "C": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题。将该主题附加到 Lambda 函数。将 SNS 主题的公共主机名提供给第三方用于 webhook。",
      "D": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。将该队列附加到 Lambda 函数。将 SQS 队列的公共主机名提供给第三方用于 webhook。"
    },
    "tags": [
      "Lambda",
      "Application Load Balancer (ALB)",
      "SNS",
      "SQS",
      "API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查如何配置 Lambda 函数，使其能够接收来自第三方数据源的 webhook 调用，并实现最高的运营效率。",
      "why_correct": "Lambda 函数 URL 提供了一种简单且运营高效的方式来暴露 Lambda 函数作为 HTTPS 终端节点。通过函数 URL，开发人员可以快速创建一个公开可访问的 URL，第三方数据源可以使用该 URL 将 webhook 发送到 Lambda 函数。这种方法无需配置额外的 AWS 服务，因此运营开销最低。",
      "why_wrong": "选项 B 引入了 Application Load Balancer (ALB)，增加了配置和管理的复杂性，对于一个简单的 webhook 接收场景来说是多余的。选项 C 和 D 分别使用了 SNS 和 SQS，这两种服务都不适合直接接收来自第三方的 webhook 请求，它们更适用于异步处理和解耦。使用 SNS 或 SQS 会增加架构的复杂性，并且需要额外的配置来处理 webhook 的接收和转发。"
    },
    "related_terms": [
      "Lambda function",
      "webhook",
      "Function URL",
      "Application Load Balancer (ALB)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Queue Service (Amazon SQS)"
    ]
  },
  {
    "id": 532,
    "topic": "1",
    "question_en": "A company has a workload in an AWS Region. Customers connect to and access the workload by using an Amazon API Gateway REST API. The company uses Amazon Route 53 as its DNS provider. The company wants to provide individual and secure URLs for all customers. Which combination of steps will meet these requirements with the MOST operational eficiency? (Choose three.)",
    "options_en": {
      "A": "Register the required domain in a registrar. Create a wildcard custom domain name in a Route 53 hosted zone and record in the zone that points to the API Gateway endpoint.",
      "B": "Request a wildcard certificate that matches the domains in AWS Certificate Manager (ACM) in a different Region.",
      "C": "Create hosted zones for each customer as required in Route 53. Create zone records that point to the API Gateway endpoint.",
      "D": "Request a wildcard certificate that matches the custom domain name in AWS Certificate Manager (ACM) in the same Region",
      "E": "Create multiple API endpoints for each customer in API Gateway",
      "F": "Create a custom domain name in API Gateway for the REST API. Import the certificate from AWS Certificate Manager (ACM)."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 区域中有一个工作负载。客户使用 Amazon API Gateway REST API 连接并访问该工作负载。该公司使用 Amazon Route 53 作为其 DNS 提供商。该公司希望为所有客户提供单独且安全的 URL。哪种步骤组合将以最高的运营效率满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "在注册商处注册所需的域名。在 Route 53 托管区域中创建一个通配符自定义域名，并在该区域中创建指向 API Gateway 终端节点的记录。",
      "B": "在另一个区域的 AWS Certificate Manager (ACM) 中请求与这些域匹配的通配符证书。",
      "C": "根据需要在 Route 53 中为每个客户创建托管区域。创建指向 API Gateway 终端节点的区域记录。",
      "D": "在同一区域的 AWS Certificate Manager (ACM) 中请求与自定义域名匹配的通配符证书。",
      "E": "在 API Gateway 中为每个客户创建多个 API 终端节点。",
      "F": "在 API Gateway 中为 REST API 创建一个自定义域名。从 AWS Certificate Manager (ACM) 导入证书。"
    },
    "tags": [
      "Route 53",
      "API Gateway",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察 API Gateway 的自定义域名配置。需要为每个客户提供独立的、安全的 URL，需要配置域名和 HTTPS 证书。最佳实践是使用通配符证书。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：C 选项在 Route 53 中为每个客户创建托管区域。为每个客户提供定制的 DNS 记录，指向 API Gateway 终端节点，实现每个客户拥有独立 URL 的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项创建通配符域名，无法给每个客户提供独立的 URL。B 选项，ACM 需要在同一区域配置证书。D 选项 同 B。E 选项，API Gateway 的终端节点配置不满足题意。F 选项 API Gateway 的自定义域名无法满足多客户需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Route 53",
      "API Gateway",
      "ACM"
    ]
  },
  {
    "id": 533,
    "topic": "1",
    "question_en": "A company stores data in Amazon S3. According to regulations, the data must not contain personally identifiable information (PII). The company recently discovered that S3 buckets have some objects that contain PII. The company needs to automatically detect PII in S3 buckets and to notify the company’s security team. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData event type from Macie findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
      "B": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Notification Service (Amazon SNS) notification to the security team.",
      "C": "Use Amazon Macie. Create an Amazon EventBridge rule to filter the SensitiveData:S3Object/Personal event type from Macie findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team.",
      "D": "Use Amazon GuardDuty. Create an Amazon EventBridge rule to filter the CRITICAL event type from GuardDuty findings and to send an Amazon Simple Queue Service (Amazon SQS) notification to the security team."
    },
    "correct_answer": "A",
    "vote_percentage": "83%",
    "question_cn": "一家公司将数据存储在 Amazon S3 中。根据法规，数据不得包含个人身份信息 (PII)。该公司最近发现 S3 存储桶中存在包含 PII 的对象。该公司需要自动检测 S3 存储桶中的 PII，并通知公司的安全团队。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Macie。创建一个 Amazon EventBridge 规则，以从 Macie 调查结果中筛选 SensitiveData 事件类型，并向安全团队发送 Amazon Simple Notification Service (Amazon SNS) 通知。",
      "B": "使用 Amazon GuardDuty。创建一个 Amazon EventBridge 规则，以从 GuardDuty 调查结果中筛选 CRITICAL 事件类型，并向安全团队发送 Amazon Simple Notification Service (Amazon SNS) 通知。",
      "C": "使用 Amazon Macie。创建一个 Amazon EventBridge 规则，以从 Macie 调查结果中筛选 SensitiveData:S3Object/Personal 事件类型，并向安全团队发送 Amazon Simple Queue Service (Amazon SQS) 通知。",
      "D": "使用 Amazon GuardDuty。创建一个 Amazon EventBridge 规则，以从 GuardDuty 调查结果中筛选 CRITICAL 事件类型，并向安全团队发送 Amazon Simple Queue Service (Amazon SQS) 通知。"
    },
    "tags": [
      "Macie",
      "EventBridge",
      "SNS",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 83%），解析仅供参考。】\n\n考察使用 Amazon Macie 检测 S3 存储桶中的 PII，并通过 Amazon SNS 发送通知。重点在于 Macie 的数据敏感度检测和 EventBridge 事件处理。",
      "why_correct": "Amazon Macie 专门用于发现和保护 S3 存储桶中的敏感数据，包括 PII。通过配置 EventBridge 规则，筛选 Macie 的调查结果，特别是 SensitiveData 事件，可以触发通知。使用 Amazon SNS 可以方便地将警报发送给安全团队，满足自动检测和通知的需求。",
      "why_wrong": "选项 B 错误，因为 Amazon GuardDuty 主要用于威胁检测，而非数据敏感度分析，不具备直接检测 PII 的能力。选项 C 错误，虽然使用了 Macie，但错误地使用 SQS 队列进行通知，通常 SQS 用于解耦和异步处理，而 SNS 更适合即时通知。选项 D 错误，因为使用了 GuardDuty 并且使用 SQS 通知，与题干需求不符。"
    },
    "related_terms": [
      "Amazon S3",
      "PII",
      "Amazon Macie",
      "Amazon EventBridge",
      "SensitiveData",
      "Amazon SNS",
      "Amazon GuardDuty",
      "CRITICAL",
      "Amazon SQS",
      "S3Object/Personal"
    ]
  },
  {
    "id": 534,
    "topic": "1",
    "question_en": "A company wants to build a logging solution for its multiple AWS accounts. The company currently stores the logs from all accounts in a centralized account. The company has created an Amazon S3 bucket in the centralized account to store the VPC fiow logs and AWS CloudTrail logs. All logs must be highly available for 30 days for frequent analysis, retained for an additional 60 days for backup purposes, and deleted 90 days after creation. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Transition objects to the S3 Standard storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
      "B": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
      "C": "Transition objects to the S3 Glacier Flexible Retrieval storage class 30 days after creation. Write an expiration action that directs Amazon S3 to delete objects after 90 days.",
      "D": "Transition objects to the S3 One Zone-Infrequent Access (S3 One Zone-IA) storage class 30 days after creation. Move all objects to the S3 Glacier Flexible Retrieval storage class after 90 days. Write an expiration action that directs Amazon S3 to delete objects after 90 days."
    },
    "correct_answer": "C",
    "vote_percentage": "62%",
    "question_cn": "一家公司希望为其多个 AWS 账户构建一个日志记录解决方案。该公司目前将所有账户的日志存储在一个集中账户中。该公司已在集中账户中创建了一个 Amazon S3 存储桶，用于存储 VPC 流日志和 AWS CloudTrail 日志。所有日志必须在创建后 30 天内高度可用，以供频繁分析，额外保留 60 天用于备份，并在创建后 90 天删除。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在创建 30 天后，将对象过渡到 S3 标准存储类。编写一个过期操作，指示 Amazon S3 在 90 天后删除对象。",
      "B": "在创建 30 天后，将对象过渡到 S3 Standard-IA 存储类。在 90 天后，将所有对象移动到 S3 Glacier Flexible Retrieval 存储类。编写一个过期操作，指示 Amazon S3 在 90 天后删除对象。",
      "C": "在创建 30 天后，将对象过渡到 S3 Glacier Flexible Retrieval 存储类。编写一个过期操作，指示 Amazon S3 在 90 天后删除对象。",
      "D": "在创建 30 天后，将对象过渡到 S3 One Zone-IA 存储类。在 90 天后，将所有对象移动到 S3 Glacier Flexible Retrieval 存储类。编写一个过期操作，指示 Amazon S3 在 90 天后删除对象。"
    },
    "tags": [
      "S3",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 One Zone-IA",
      "S3 Glacier",
      "Lifecycle Configuration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 62%），解析仅供参考。】\n\n考查了如何使用 Amazon S3 生命周期策略和存储类来管理日志的存储成本和可用性。",
      "why_correct": "选项 C 提供了最具成本效益的解决方案。在创建 30 天后，将对象迁移到 S3 Glacier Flexible Retrieval 存储类，满足了备份需求。S3 Glacier Flexible Retrieval 存储类适用于不经常访问的数据，可以满足备份需求。编写一个过期操作，在 90 天后删除对象，满足了删除需求。",
      "why_wrong": "选项 A 的问题在于，S3 标准存储类并不适合备份，而且成本较高。选项 B 的问题在于，使用 S3 Standard-IA 存储类虽然价格比 S3 标准存储类低，但仍高于 Glacier，而且过渡到 Glacier 需要额外成本。选项 D 的问题在于，S3 One Zone-IA 存储类的可用性不如其他 S3 存储类，而且在 90 天后再次迁移到 Glacier 会增加成本。"
    },
    "related_terms": [
      "Amazon S3",
      "VPC Flow Logs",
      "AWS CloudTrail",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier Flexible Retrieval",
      "S3 One Zone-IA"
    ]
  },
  {
    "id": 535,
    "topic": "1",
    "question_en": "A company is building an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for its workloads. All secrets that are stored in Amazon EKS must be encrypted in the Kubernetes etcd key-value store. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a new AWS Key Management Service (AWS KMS) key. Use AWS Secrets Manager to manage, rotate, and store all secrets in Amazon EKS.",
      "B": "Create a new AWS Key Management Service (AWS KMS) key. Enable Amazon EKS KMS secrets encryption on the Amazon EKS cluster.",
      "C": "Create the Amazon EKS cluster with default options. Use the Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) driver as an add-on.",
      "D": "Create a new AWS Key Management Service (AWS KMS) key with the alias/aws/ebs alias. Enable default Amazon Elastic Block Store (Amazon EBS) volume encryption for the account."
    },
    "correct_answer": "B",
    "vote_percentage": "95%",
    "question_cn": "一家公司正在为其工作负载构建 Amazon Elastic Kubernetes Service (Amazon EKS) 集群。存储在 Amazon EKS 中的所有密钥都必须在 Kubernetes etcd 键值存储中加密。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 AWS Key Management Service (AWS KMS) 密钥。使用 AWS Secrets Manager 在 Amazon EKS 中管理、轮换和存储所有密钥。",
      "B": "创建一个新的 AWS Key Management Service (AWS KMS) 密钥。在 Amazon EKS 集群上打开 Amazon EKS KMS 密钥加密。",
      "C": "使用默认选项创建 Amazon EKS 集群。将 Amazon Elastic Block Store (Amazon EBS) 容器存储接口 (CSI) 驱动程序用作附加组件。",
      "D": "创建一个新的 AWS Key Management Service (AWS KMS) 密钥，别名为/aws/ebs。为账户打开默认的 Amazon Elastic Block Store (Amazon EBS) 卷加密。"
    },
    "tags": [
      "EKS",
      "KMS",
      "Secrets Manager",
      "EBS",
      "CSI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 95%），解析仅供参考。】\n\n本题考察了 Amazon EKS 中 Kubernetes etcd 密钥的加密方法，以及如何通过 KMS 密钥实现加密。",
      "why_correct": "选项 B 正确，因为它直接符合题意。通过在 Amazon EKS 集群上启用 KMS 密钥加密，所有存储在 etcd 中的密钥都将使用指定的 KMS 密钥进行加密，满足了题目对密钥加密的要求。",
      "why_wrong": "选项 A 错误，虽然 AWS Secrets Manager 可以管理和轮换密钥，但它不是 EKS etcd 密钥的直接加密解决方案，且 Secrets Manager 存储的秘密本身也需要加密；选项 C 错误，Amazon EBS CSI 驱动程序与 EKS etcd 密钥加密无关，主要用于提供持久性存储；选项 D 错误，虽然 EBS 卷加密与 KMS 密钥相关，但它加密的是 EBS 卷本身，而非 etcd 中的密钥，且/aws/ebs 别名是 EBS 默认的 KMS 密钥别名，通常用于加密 EBS 卷，与 EKS etcd 密钥加密无关。"
    },
    "related_terms": [
      "Amazon EKS",
      "Kubernetes",
      "etcd",
      "AWS KMS",
      "AWS Secrets Manager",
      "Amazon EBS",
      "CSI"
    ]
  },
  {
    "id": 536,
    "topic": "1",
    "question_en": "A company wants to provide data scientists with near real-time read-only access to the company's production Amazon RDS for PostgreSQL database. The database is currently configured as a Single-AZ database. The data scientists use complex queries that will not affect the production database. The company needs a solution that is highly available. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Scale the existing production database in a maintenance window to provide enough power for the data scientists.",
      "B": "Change the setup from a Single-AZ to a Multi-AZ instance deployment with a larger secondary standby instance. Provide the data scientists access to the secondary instance.",
      "C": "Change the setup from a Single-AZ to a Multi-AZ instance deployment. Provide two additional read replicas for the data scientists.",
      "D": "Change the setup from a Single-AZ to a Multi-AZ cluster deployment with two readable standby instances. Provide read endpoints to the data scientists."
    },
    "correct_answer": "D",
    "vote_percentage": "82%",
    "question_cn": "一家公司希望为数据科学家提供对公司生产 Amazon RDS for PostgreSQL 数据库的近乎实时的只读访问权限。该数据库目前配置为单可用区数据库。数据科学家使用复杂的查询，这些查询不会影响生产数据库。该公司需要一个高可用性的解决方案。哪种解决方案能最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在维护窗口期间扩展现有的生产数据库，为数据科学家提供足够的算力。",
      "B": "将设置从单可用区更改为多可用区实例部署，并使用一个更大的辅助备用实例。为数据科学家提供对辅助实例的访问权限。",
      "C": "将设置从单可用区更改为多可用区实例部署。为数据科学家提供两个额外的只读副本。",
      "D": "将设置从单可用区更改为多可用区集群部署，并提供两个可读备用实例。为数据科学家提供读取端点。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "Read Replicas"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 82%），解析仅供参考。】\n\n考查 Amazon RDS for PostgreSQL 的高可用性、只读副本配置以及成本效益的解决方案。",
      "why_correct": "选项 D 提供了最具成本效益的解决方案。通过将单可用区实例迁移到多可用区集群，并启用两个可读备用实例，可以为数据科学家提供近乎实时的只读访问权限。这种配置兼顾了高可用性和性能，同时利用了只读副本减少了对生产数据库的影响。",
      "why_wrong": "选项 A 不满足高可用性需求，且扩展现有的生产数据库无法解决只读访问的需求。选项 B 虽然提供了高可用性，但仅使用一个辅助备用实例可能无法满足数据科学家复杂查询的性能需求，且成本较高。选项 C 虽然提供了高可用性，但只读副本数量有限，可能无法满足数据科学家复杂的查询负载，也无法实现成本效益最大化。"
    },
    "related_terms": [
      "Amazon RDS for PostgreSQL",
      "Single-AZ",
      "Multi-AZ",
      "Read Replica",
      "Reader Endpoint"
    ]
  },
  {
    "id": 537,
    "topic": "1",
    "question_en": "A company runs a three-tier web application in the AWS Cloud that operates across three Availability Zones. The application architecture has an Application Load Balancer, an Amazon EC2 web server that hosts user session states, and a MySQL database that runs on an EC2 instance. The company expects sudden increases in application trafic. The company wants to be able to scale to meet future application capacity demands and to ensure high availability across all three Availability Zones. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
      "B": "Migrate the MySQL database to Amazon RDS for MySQL with a Multi-AZ DB cluster deployment. Use Amazon ElastiCache for Memcached with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
      "C": "Migrate the MySQL database to Amazon DynamoDB Use DynamoDB Accelerator (DAX) to cache reads. Store the session data in DynamoDB. Migrate the web server to an Auto Scaling group that is in three Availability Zones.",
      "D": "Migrate the MySQL database to Amazon RDS for MySQL in a single Availability Zone. Use Amazon ElastiCache for Redis with high availability to store session data and to cache reads. Migrate the web server to an Auto Scaling group that is in three Availability Zones."
    },
    "correct_answer": "A",
    "vote_percentage": "79%",
    "question_cn": "一家公司在 AWS 云中运行一个三层 Web 应用程序，该应用程序跨越三个可用区。该应用程序架构包含一个 Application Load Balancer、一个托管用户会话状态的 Amazon EC2 Web 服务器和一个在 EC2 实例上运行的 MySQL 数据库。该公司预计应用程序流量会突然增加。该公司希望能够扩展以满足未来的应用程序容量需求，并确保跨所有三个可用区的高可用性。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 MySQL 数据库迁移到 Amazon RDS for MySQL，并进行多可用区数据库集群部署。使用具有高可用性的 Amazon ElastiCache for Redis 来存储会话数据并缓存读取操作。将 Web 服务器迁移到位于三个可用区中的 Auto Scaling 组。",
      "B": "将 MySQL 数据库迁移到 Amazon RDS for MySQL，并进行多可用区数据库集群部署。使用具有高可用性的 Amazon ElastiCache for Memcached 来存储会话数据并缓存读取操作。将 Web 服务器迁移到位于三个可用区中的 Auto Scaling 组。",
      "C": "将 MySQL 数据库迁移到 Amazon DynamoDB。使用 DynamoDB Accelerator (DAX) 来缓存读取操作。将会话数据存储在 DynamoDB 中。将 Web 服务器迁移到位于三个可用区中的 Auto Scaling 组。",
      "D": "将 MySQL 数据库迁移到单个可用区中的 Amazon RDS for MySQL。使用具有高可用性的 Amazon ElastiCache for Redis 来存储会话数据并缓存读取操作。将 Web 服务器迁移到位于三个可用区中的 Auto Scaling 组。"
    },
    "tags": [
      "EC2",
      "MySQL",
      "RDS",
      "ElastiCache",
      "DynamoDB",
      "DAX",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 79%），解析仅供参考。】\n\n考查了如何设计具有高可用性、可扩展性的三层 Web 应用程序架构，包括数据库、缓存和 Web 服务器的部署策略。",
      "why_correct": "将 MySQL 数据库迁移到 Amazon RDS for MySQL，并配置多可用区部署，可以实现数据库的高可用性和容错能力。使用 Amazon ElastiCache for Redis 存储会话数据和缓存读取操作，可以提高性能并减轻数据库压力。将 Web 服务器部署在 Auto Scaling 组中，横跨三个可用区，可以实现自动伸缩和高可用性。",
      "why_wrong": "B 选项与 A 选项主要区别在于使用 Memcached 而不是 Redis，Memcached 并不像 Redis 那样提供内置的数据持久化和主从复制功能，在可用性和数据安全性方面不如 Redis。C 选项将 MySQL 数据库迁移到 DynamoDB 是不恰当的，因为 MySQL 是关系型数据库，而 DynamoDB 是 NoSQL 数据库，不适用于所有关系型数据库的场景。D 选项将 RDS for MySQL 部署在单个可用区中，这将导致单点故障，无法满足题目中对高可用性的需求。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon EC2",
      "MySQL",
      "Amazon RDS for MySQL",
      "Multi-AZ",
      "Amazon ElastiCache for Redis",
      "Auto Scaling",
      "Amazon ElastiCache for Memcached",
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)"
    ]
  },
  {
    "id": 538,
    "topic": "1",
    "question_en": "A global video streaming company uses Amazon CloudFront as a content distribution network (CDN). The company wants to roll out content in a phased manner across multiple countries. The company needs to ensure that viewers who are outside the countries to which the company rolls out content are not able to view the content. Which solution will meet these requirements?",
    "options_en": {
      "A": "Add geographic restrictions to the content in CloudFront by using an allow list. Set up a custom error message.",
      "B": "Set up a new URL tor restricted content. Authorize access by using a signed URL and cookies. Set up a custom error message.",
      "C": "Encrypt the data for the content that the company distributes. Set up a custom error message.",
      "D": "Create a new URL for restricted content. Set up a time-restricted access policy for signed URLs."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家全球视频流公司使用 Amazon CloudFront 作为内容分发网络 (CDN)。该公司希望分阶段向多个国家/地区推出内容。该公司需要确保位于该公司未推出内容的国家/地区的观看者无法观看该内容。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过使用允许列表，在 CloudFront 中为内容添加地理限制。设置自定义错误消息。",
      "B": "为受限内容设置一个新 URL。通过使用签名 URL 和 Cookie 授权访问。设置自定义错误消息。",
      "C": "加密公司分发的内容数据。设置自定义错误消息。",
      "D": "为受限内容创建一个新 URL。为签名 URL 设置时间限制访问策略。"
    },
    "tags": [
      "CloudFront",
      "地理限制",
      "签名 URL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察 CloudFront 的地理限制。  可以使用地理限制来控制内容的分发，并且使用签名 URL 来保护内容。",
      "why_correct": "A 选项 允许通过地理限制配置，来实现内容访问的控制。",
      "why_wrong": "B 选项使用签名 URL 和 cookie 授权，实现控制。C 选项  加密内容，实现控制。D 选项使用签名 URL，可以实现访问控制，不满足题意。"
    },
    "related_terms": [
      "CloudFront",
      "地理限制",
      "签名 URL"
    ]
  },
  {
    "id": 539,
    "topic": "1",
    "question_en": "A company wants to use the AWS Cloud to improve its on-premises disaster recovery (DR) configuration. The company's core production business application uses Microsoft SQL Server Standard, which runs on a virtual machine (VM). The application has a recovery point objective (RPO) of 30 seconds or fewer and a recovery time objective (RTO) of 60 minutes. The DR solution needs to minimize costs wherever possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure a multi-site active/active setup between the on-premises server and AWS by using Microsoft SQL Server Enterprise with Always On availability groups.",
      "B": "Configure a warm standby Amazon RDS for SQL Server database on AWS. Configure AWS Database Migration Service (AWS DMS) to use change data capture (CDC).",
      "C": "Use AWS Elastic Disaster Recovery configured to replicate disk changes to AWS as a pilot light.",
      "D": "Use third-party backup software to capture backups every night. Store a secondary set of backups in Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "58%",
    "question_cn": "一家公司希望使用 AWS Cloud 来改善其本地灾难恢复 (DR) 配置。该公司的核心生产业务应用程序使用 Microsoft SQL Server Standard，该应用程序运行在虚拟机 (VM) 上。该应用程序的恢复点目标 (RPO) 为 30 秒或更短，恢复时间目标 (RTO) 为 60 分钟。DR 解决方案需要尽可能降低成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过使用 Microsoft SQL Server Enterprise 和 Always On 可用性组，在本地服务器和 AWS 之间配置多站点 active/active 设置。",
      "B": "在 AWS 上配置一个 warm standby Amazon RDS for SQL Server 数据库。配置 AWS Database Migration Service (AWS DMS) 以使用更改数据捕获 (CDC)。",
      "C": "使用 AWS Elastic Disaster Recovery，配置其将磁盘更改复制到 AWS 作为 pilot light。",
      "D": "使用第三方备份软件每晚捕获备份。将第二组备份存储在 Amazon S3 中。"
    },
    "tags": [
      "DR",
      "Microsoft SQL Server",
      "RDS",
      "AWS Elastic Disaster Recovery",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 58%），解析仅供参考。】\n\n考察在 AWS 上实现 Microsoft SQL Server 的灾难恢复方案，并考虑 RPO、RTO 和成本因素。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：warm standby 方案可以在满足较低 RTO 和 RPO 需求的同时，兼顾成本效益。使用 Amazon RDS for SQL Server 作为备用数据库，通过 AWS DMS 的 CDC (Change Data Capture) 功能实现近乎实时的同步，满足 RPO 要求。Warm standby 模式比 cold standby 提供更快的恢复速度，同时比 active/active 模式成本更低，更符合题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项配置 active/active 设置会增加成本，不符合题目中“尽可能降低成本”的要求。C 选项使用 AWS Elastic Disaster Recovery 的 pilot light 方案， RPO 和 RTO 可能无法满足题目要求，并且配置复杂程度相对更高。D 选项备份方案 RPO 往往无法满足 30 秒的要求，RTO 也可能无法满足 60 分钟的要求，并且恢复速度较慢。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Cloud",
      "Disaster Recovery (DR)",
      "Microsoft SQL Server Standard",
      "Virtual Machine (VM)",
      "Recovery Point Objective (RPO)",
      "Recovery Time Objective (RTO)",
      "Microsoft SQL Server Enterprise",
      "Always On Availability Groups",
      "AWS",
      "Amazon RDS for SQL Server",
      "AWS Database Migration Service (AWS DMS)",
      "Change Data Capture (CDC)",
      "AWS Elastic Disaster Recovery",
      "Amazon S3"
    ]
  },
  {
    "id": 540,
    "topic": "1",
    "question_en": "A company has an on-premises server that uses an Oracle database to process and store customer information. The company wants to use an AWS database service to achieve higher availability and to improve application performance. The company also wants to ofioad reporting from its primary database system. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Use AWS Database Migration Service (AWS DMS) to create an Amazon RDS DB instance in multiple AWS Regions. Point the reporting functions toward a separate DB instance from the primary DB instance.",
      "B": "Use Amazon RDS in a Single-AZ deployment to create an Oracle database. Create a read replica in the same zone as the primary DB instance. Direct the reporting functions to the read replica.",
      "C": "Use Amazon RDS deployed in a Multi-AZ cluster deployment to create an Oracle database. Direct the reporting functions to use the reader instance in the cluster deployment.",
      "D": "Use Amazon RDS deployed in a Multi-AZ instance deployment to create an Amazon Aurora database. Direct the reporting functions to the reader instances."
    },
    "correct_answer": "D",
    "vote_percentage": "67%",
    "question_cn": "一家公司拥有一台本地服务器，该服务器使用 Oracle 数据库来处理和存储客户信息。该公司希望使用 AWS 数据库服务来实现更高的可用性并提高应用程序性能。该公司还希望将报表功能从其主要数据库系统卸载。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Database Migration Service (AWS DMS) 在多个 AWS 区域中创建 Amazon RDS 数据库实例。将报表功能指向与主数据库实例不同的数据库实例。",
      "B": "在单可用区部署中使用 Amazon RDS 创建 Oracle 数据库。在与主数据库实例相同的可用区中创建只读副本。将报表功能定向到只读副本。",
      "C": "在多可用区集群部署中使用 Amazon RDS 创建 Oracle 数据库。将报表功能定向到集群部署中的读取器实例。",
      "D": "在多可用区实例部署中使用 Amazon RDS 创建 Amazon Aurora 数据库。将报表功能定向到读取器实例。"
    },
    "tags": [
      "RDS",
      "Oracle",
      "Read Replicas",
      "Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 67%），解析仅供参考。】\n\n此题考察数据库的可用性、性能和报表功能。  使用只读副本或者 Aurora 都可以实现报表功能卸载。",
      "why_correct": "D 选项使用 Aurora 数据库，可以利用读取器实例卸载报表功能，满足了题意。",
      "why_wrong": "A 选项  DMS 迁移涉及数据迁移，流程复杂。B 选项单可用区，不能满足可用性需求。C 选项使用只读副本的可用性较低。"
    },
    "related_terms": [
      "RDS",
      "Oracle",
      "Aurora",
      "Read Replicas"
    ]
  },
  {
    "id": 541,
    "topic": "1",
    "question_en": "A company wants to build a web application on AWS. Client access requests to the website are not predictable and can be idle for a long time. Only customers who have paid a subscription fee can have the ability to sign in and use the web application. Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options_en": {
      "A": "Create an AWS Lambda function to retrieve user information from Amazon DynamoDB. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.",
      "B": "Create an Amazon Elastic Container Service (Amazon ECS) service behind an Application Load Balancer to retrieve user information from Amazon RDS. Create an Amazon API Gateway endpoint to accept RESTful APIs. Send the API calls to the Lambda function.",
      "C": "Create an Amazon Cognito user pool to authenticate users.",
      "D": "Create an Amazon Cognito identity pool to authenticate users",
      "E": "Use AWS Amplify to serve the frontend web content with HTML, CSS, and JS. Use an integrated Amazon CloudFront configuration",
      "F": "Use Amazon S3 static web hosting with PHP, CSS, and JS. Use Amazon CloudFront to serve the frontend web content."
    },
    "correct_answer": "ACE",
    "vote_percentage": "",
    "question_cn": "一家公司希望在 AWS 上构建一个 Web 应用程序。客户端对网站的访问请求是不可预测的，并且可以闲置很长时间。只有支付了订阅费的客户才能登录并使用 Web 应用程序。哪些步骤组合将以最具成本效益的方式满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，用于从 Amazon DynamoDB 检索用户信息。创建一个 Amazon API Gateway 端点以接受 RESTful API。将 API 调用发送到 Lambda 函数。",
      "B": "创建一个位于 Application Load Balancer 之后的 Amazon Elastic Container Service (Amazon ECS) 服务，用于从 Amazon RDS 检索用户信息。创建一个 Amazon API Gateway 端点以接受 RESTful API。将 API 调用发送到 Lambda 函数。",
      "C": "创建一个 Amazon Cognito 用户池来验证用户。",
      "D": "创建一个 Amazon Cognito 身份池来验证用户。",
      "E": "使用 AWS Amplify 通过 HTML、CSS 和 JS 提供前端 Web 内容。使用集成的 Amazon CloudFront 配置。",
      "F": "使用 Amazon S3 静态 Web 托管，其中包含 PHP、CSS 和 JS。使用 Amazon CloudFront 提供前端 Web 内容。"
    },
    "tags": [
      "Lambda",
      "API Gateway",
      "DynamoDB",
      "ECS",
      "RDS",
      "Cognito",
      "Amplify",
      "CloudFront",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACE（社区 —），解析仅供参考。】\n\n该题考察了 Web 应用程序的成本优化方案。A 选项使用了 Serverless 架构，包括 Lambda 和 API Gateway，以及 DynamoDB 存储用户信息，可以满足按需使用和成本效益的需求。E 选项使用 Amplify 搭建前端，可以方便地部署和管理前端内容，并结合 CloudFront 进行加速。C 选项 Cognito 用户池提供身份验证。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACE。理由简述：选项 A 组合使用 Lambda、API Gateway 和 DynamoDB，可以满足网站登录验证和按需访问的需求，并具有成本效益。E 选项使用 Amplify 提供前端，利用 CloudFront 加速内容分发。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 使用 ECS 和 RDS，增加了运维成本，并且在闲置时间没有优势。选项 C 和 D 仅提供用户身份验证，无法满足 Web 应用程序的完整需求。选项 F 使用 S3 静态托管，需要手动维护 PHP、CSS 和 JS，增加了复杂性，且未实现登录功能。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Lambda",
      "API Gateway",
      "DynamoDB",
      "ECS",
      "RDS",
      "Cognito",
      "CloudFront",
      "S3",
      "Web 应用程序",
      "Amplify",
      "RESTful API",
      "HTML",
      "CSS",
      "JS"
    ]
  },
  {
    "id": 542,
    "topic": "1",
    "question_en": "A media company uses an Amazon CloudFront distribution to deliver content over the internet. The company wants only premium customers to have access to the media streams and file content. The company stores all content in an Amazon S3 bucket. The company also delivers content on demand to customers for a specific purpose, such as movie rentals or music downloads. Which solution will meet these requirements?",
    "options_en": {
      "A": "Generate and provide S3 signed cookies to premium customers.",
      "B": "Generate and provide CloudFront signed URLs to premium customers.",
      "C": "Use origin access control (OAC) to limit the access of non-premium customers.",
      "D": "Generate and activate field-level encryption to block non-premium customers."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家媒体公司使用 Amazon CloudFront 分发来通过互联网提供内容。该公司只希望高级客户才能访问媒体流和文件内容。该公司将所有内容存储在 Amazon S3 存储桶中。该公司还根据特定目的（如电影租赁或音乐下载）按需向客户提供内容。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "生成 S3 签名 Cookie 并提供给高级客户。",
      "B": "生成 CloudFront 签名 URL 并提供给高级客户。",
      "C": "使用源访问控制 (OAC) 来限制非高级客户的访问。",
      "D": "生成并激活字段级加密以阻止非高级客户。"
    },
    "tags": [
      "CloudFront",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查如何使用 CloudFront 和 S3 实现受限内容访问。签名 URL 和签名 Cookie 允许对 S3 上的私有内容进行受限访问。CloudFront 签名 URL 提供了比签名 Cookie 更简单的方案，可以控制访问权限。源访问控制 (OAC) 用于控制对 S3 桶的访问，通常与 CloudFront 结合使用，并不能直接控制用户访问权限。",
      "why_correct": "选项 B 使用 CloudFront 签名 URL 来限制对 S3 中内容的访问，符合需求。",
      "why_wrong": "选项 A 使用 S3 签名 Cookie 提供了另一种访问控制的方式，但不如 CloudFront 签名 URL 灵活。选项 C 使用源访问控制 (OAC) 无法直接限制用户访问。选项 D 字段级加密与用户访问控制无关。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "S3 签名 Cookie",
      "CloudFront 签名 URL",
      "OAC",
      "字段级加密"
    ]
  },
  {
    "id": 543,
    "topic": "1",
    "question_en": "A company runs Amazon EC2 instances in multiple AWS accounts that are individually bled. The company recently purchased a Savings Pian. Because of changes in the company’s business requirements, the company has decommissioned a large number of EC2 instances. The company wants to use its Savings Plan discounts on its other AWS accounts. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "From the AWS Account Management Console of the management account, turn on discount sharing from the billing preferences section.",
      "B": "From the AWS Account Management Console of the account that purchased the existing Savings Plan, turn on discount sharing from the billing preferences section. Include all accounts.",
      "C": "From the AWS Organizations management account, use AWS Resource Access Manager (AWS RAM) to share the Savings Plan with other accounts.",
      "D": "Create an organization in AWS Organizations in a new payer account. Invite the other AWS accounts to join the organization from the management account",
      "E": "Create an organization in AWS Organizations in the existing AWS account with the existing EC2 instances and Savings Plan. Invite the other AWS accounts to join the organization from the management account."
    },
    "correct_answer": "AE",
    "vote_percentage": "40%",
    "question_cn": "一家公司在多个单独计费的 AWS 账户中运行 Amazon EC2 实例。该公司最近购买了 Savings Plan。由于公司业务需求的变化，该公司已停用了大量 EC2 实例。该公司希望在其其他 AWS 账户上使用其 Savings Plan 折扣。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "从管理账户的 AWS 账户管理控制台中，从计费偏好部分打开折扣共享。",
      "B": "从购买现有 Savings Plan 的账户的 AWS 账户管理控制台中，从计费偏好部分打开折扣共享。包括所有账户。",
      "C": "从 AWS Organizations 管理账户中，使用 AWS Resource Access Manager (AWS RAM) 与其他账户共享 Savings Plan。",
      "D": "在新付款人账户的 AWS Organizations 中创建一个组织。从管理账户邀请其他 AWS 账户加入该组织。",
      "E": "在包含现有 EC2 实例和 Savings Plan 的现有 AWS 账户的 AWS Organizations 中创建一个组织。从管理账户邀请其他 AWS 账户加入该组织。"
    },
    "tags": [
      "Savings Plan",
      "Organizations",
      "RAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 40%），解析仅供参考。】\n\n此题考查如何共享 Savings Plans。Savings Plans 可以在同一账户内使用。跨账户共享需要账户间的配置，可以使用 Organizations 的功能来实现，减少开销。AWS Resource Access Manager (AWS RAM) 用于跨账户共享资源。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：选项 A 从管理账户中打开折扣共享，可以实现 Savings Plan 的共享。选项 C 使用 AWS RAM 也可以实现 Savings Plan 的共享。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 在购买 Savings Plan 的账户中打开折扣共享是错误的，无法实现跨账户共享。选项 D 和 E 描述了组织账户的创建，但与 Savings Plan 的共享无关。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS RAM",
      "Savings Plan",
      "折扣共享",
      "管理账户"
    ]
  },
  {
    "id": 544,
    "topic": "1",
    "question_en": "A retail company uses a regional Amazon API Gateway API for its public REST APIs. The API Gateway endpoint is a custom domain name that points to an Amazon Route 53 alias record. A solutions architect needs to create a solution that has minimal effects on customers and minimal data loss to release the new version of APIs. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a canary release deployment stage for API Gateway. Deploy the latest API version. Point an appropriate percentage of trafic to the canary stage. After API verification, promote the canary stage to the production stage.",
      "B": "Create a new API Gateway endpoint with a new version of the API in OpenAPI YAML file format. Use the import-to-update operation in merge mode into the API in API Gateway. Deploy the new version of the API to the production stage.",
      "C": "Create a new API Gateway endpoint with a new version of the API in OpenAPI JSON file format. Use the import-to-update operation in overwrite mode into the API in API Gateway. Deploy the new version of the API to the production stage.",
      "D": "Create a new API Gateway endpoint with new versions of the API definitions. Create a custom domain name for the new API Gateway API. Point the Route 53 alias record to the new API Gateway API custom domain name."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家零售公司使用区域 Amazon API Gateway API 来处理其公共 REST API。API Gateway 终端节点是一个自定义域名，指向 Amazon Route 53 别名记录。解决方案架构师需要创建一个对客户影响最小且数据丢失最小的解决方案来发布新版本的 API。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 API Gateway 创建一个 canary 发布部署阶段。部署最新的 API 版本。将适当比例的流量指向 canary 阶段。在 API 验证后，将 canary 阶段升级到生产阶段。",
      "B": "使用 OpenAPI YAML 文件格式创建包含新版本 API 的新 API Gateway 终端节点。在 API Gateway 中使用 import-to-update 操作以合并模式导入 API。将新版本的 API 部署到生产阶段。",
      "C": "使用 OpenAPI JSON 文件格式创建包含新版本 API 的新 API Gateway 终端节点。在 API Gateway 中使用 import-to-update 操作以覆盖模式导入 API。将新版本的 API 部署到生产阶段。",
      "D": "创建一个包含新版本 API 定义的新 API Gateway 终端节点。为新的 API Gateway API 创建自定义域名。将 Route 53 别名记录指向新的 API Gateway API 自定义域名。"
    },
    "tags": [
      "API Gateway",
      "Route 53",
      "canary release"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查 API Gateway 的新版本发布策略。Canary 发布可以逐步将流量引向新版本，从而降低风险。使用 canary 部署阶段，可以最小化对客户的影响，并减少数据丢失的风险。",
      "why_correct": "选项 A 使用 API Gateway 的 canary 发布阶段，可以最小化影响和数据丢失。",
      "why_wrong": "选项 B 导入方式无法进行流量控制。选项 C 覆盖模式会直接覆盖 API，可能导致中断。选项 D 需要创建新的 API，并需要更新 DNS 记录，影响较大。"
    },
    "related_terms": [
      "API Gateway",
      "Route 53",
      "canary release",
      "OpenAPI",
      "域名",
      "自定义域名"
    ]
  },
  {
    "id": 545,
    "topic": "1",
    "question_en": "A company wants to direct its users to a backup static error page if the company's primary website is unavailable. The primary website's DNS records are hosted in Amazon Route 53. The domain is pointing to an Application Load Balancer (ALB). The company needs a solution that minimizes changes and infrastructure overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Update the Route 53 records to use a latency routing policy. Add a static error page that is hosted in an Amazon S3 bucket to the records so that the trafic is sent to the most responsive endpoints.",
      "B": "Set up a Route 53 active-passive failover configuration. Direct trafic to a static error page that is hosted in an Amazon S3 bucket when Route 53 health checks determine that the ALB endpoint is unhealthy.",
      "C": "Set up a Route 53 active-active configuration with the ALB and an Amazon EC2 instance that hosts a static error page as endpoints. Configure Route 53 to send requests to the instance only if the health checks fail for the ALB.",
      "D": "Update the Route 53 records to use a multivalue answer routing policy. Create a health check. Direct trafic to the website if the health check passes. Direct trafic to a static error page that is hosted in Amazon S3 if the health check does not pass."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家公司希望在其主要网站不可用时，将用户定向到备份静态错误页面。该公司的主要网站的 DNS 记录托管在 Amazon Route 53 中。该域名指向 Application Load Balancer (ALB)。该公司需要一个可以最大限度地减少更改和基础设施开销的解决方案。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "更新 Route 53 记录以使用延迟路由策略。将托管在 Amazon S3 存储桶中的静态错误页面添加到记录中，以便将流量发送到响应速度最快的端点。",
      "B": "设置 Route 53 主动-被动故障转移配置。当 Route 53 运行状况检查确定 ALB 端点运行状况不佳时，将流量定向到托管在 Amazon S3 存储桶中的静态错误页面。",
      "C": "使用 ALB 和托管静态错误页面的 Amazon EC2 实例作为端点，设置 Route 53 主动-主动配置。配置 Route 53，仅在 ALB 的运行状况检查失败时，才将请求发送到该实例。",
      "D": "更新 Route 53 记录以使用多值答案路由策略。创建一个运行状况检查。如果运行状况检查通过，则将流量定向到网站。如果运行状况检查未通过，则将流量定向到托管在 Amazon S3 中的静态错误页面。"
    },
    "tags": [
      "Route 53",
      "ALB",
      "S3",
      "故障转移"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n本题考察 DNS 故障转移的配置。Route 53 提供了故障转移的配置，当主站点不可用时，将流量导向备份站点。主动-被动故障转移可以自动检测主站点的健康状况，并切换到备份站点。",
      "why_correct": "选项 B 设置 Route 53 主动-被动故障转移，当 ALB 不可用时，将流量导向 S3 的静态页面，满足要求。",
      "why_wrong": "选项 A 使用延迟路由，无法实现故障转移。选项 C 配置为主动-主动，不符合题意。选项 D 使用多值答案路由，无法实现故障转移。"
    },
    "related_terms": [
      "Route 53",
      "ALB",
      "S3",
      "故障转移",
      "运行状况检查"
    ]
  },
  {
    "id": 546,
    "topic": "1",
    "question_en": "A recent analysis of a company's IT expenses highlights the need to reduce backup costs. The company's chief information oficer wants to simplify the on-premises backup infrastructure and reduce costs by eliminating the use of physical backup tapes. The company must preserve the existing investment in the on-premises backup applications and workfiows. What should a solutions architect recommend?",
    "options_en": {
      "A": "Set up AWS Storage Gateway to connect with the backup applications using the NFS interface.",
      "B": "Set up an Amazon EFS file system that connects with the backup applications using the NFS interface.",
      "C": "Set up an Amazon EFS file system that connects with the backup applications using the iSCSI interface.",
      "D": "Set up AWS Storage Gateway to connect with the backup applications using the iSCSI-virtual tape library (VTL) interface."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "最近对一家公司的 IT 支出的分析强调需要降低备份成本。公司首席信息官希望简化本地备份基础设施，并通过取消使用物理备份磁带来降低成本。公司必须保留对本地备份应用程序和工作流程的现有投资。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "设置 AWS Storage Gateway，使用 NFS 接口与备份应用程序连接。",
      "B": "设置 Amazon EFS 文件系统，使用 NFS 接口与备份应用程序连接。",
      "C": "设置 Amazon EFS 文件系统，使用 iSCSI 接口与备份应用程序连接。",
      "D": "设置 AWS Storage Gateway，使用 iSCSI-虚拟磁带库 (VTL) 接口与备份应用程序连接。"
    },
    "tags": [
      "Storage Gateway",
      "VTL",
      "iSCSI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查本地备份到云的方案。Storage Gateway 提供了与本地备份应用程序的连接。VTL (Virtual Tape Library) 接口模拟磁带库，方便将本地备份迁移到云端。",
      "why_correct": "选项 D 使用 Storage Gateway 的 VTL 接口，可以兼容本地备份应用程序，并简化备份基础设施。",
      "why_wrong": "选项 A 和 B 使用 NFS 接口，无法实现磁带库的模拟。选项 C 使用 iSCSI 接口，但不是 VTL，无法直接模拟磁带库。"
    },
    "related_terms": [
      "Storage Gateway",
      "VTL",
      "iSCSI",
      "NFS",
      "备份",
      "本地备份"
    ]
  },
  {
    "id": 547,
    "topic": "1",
    "question_en": "A company has data collection sensors at different locations. The data collection sensors stream a high volume of data to the company. The company wants to design a platform on AWS to ingest and process high-volume streaming data. The solution must be scalable and support data collection in near real time. The company must store the data in Amazon S3 for future reporting. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon Kinesis Data Firehose to deliver streaming data to Amazon S3.",
      "B": "Use AWS Glue to deliver streaming data to Amazon S3.",
      "C": "Use AWS Lambda to deliver streaming data and store the data to Amazon S3.",
      "D": "Use AWS Database Migration Service (AWS DMS) to deliver streaming data to Amazon S3."
    },
    "correct_answer": "A",
    "vote_percentage": "84%",
    "question_cn": "一家公司在不同地点设有数据收集传感器。这些数据收集传感器向公司流式传输大量数据。该公司希望在 AWS 上设计一个平台来摄取和处理大容量流数据。该解决方案必须具有可扩展性，并支持近实时的数据收集。该公司必须将数据存储在 Amazon S3 中，以供将来报告。哪种解决方案以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Firehose 将流数据传输到 Amazon S3。",
      "B": "使用 AWS Glue 将流数据传输到 Amazon S3。",
      "C": "使用 AWS Lambda 传输流数据并将数据存储到 Amazon S3。",
      "D": "使用 AWS Database Migration Service (AWS DMS) 将流数据传输到 Amazon S3。"
    },
    "tags": [
      "Kinesis Data Firehose",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 84%），解析仅供参考。】\n\n本题考察流数据处理方案。Kinesis Data Firehose 能够将流数据直接写入 S3，并提供近实时的数据处理，满足可扩展性和低运营开销的要求。",
      "why_correct": "选项 A 使用 Kinesis Data Firehose 将流数据传输到 S3，满足低运营开销和近实时需求。",
      "why_wrong": "选项 B 使用 AWS Glue，需要额外的配置。选项 C 使用 Lambda 增加了复杂性。选项 D 使用 DMS 无法处理流数据。"
    },
    "related_terms": [
      "Kinesis Data Firehose",
      "S3",
      "AWS Glue",
      "AWS Lambda",
      "AWS DMS",
      "流数据"
    ]
  },
  {
    "id": 548,
    "topic": "1",
    "question_en": "A company has separate AWS accounts for its finance, data analytics, and development departments. Because of costs and security concerns, the company wants to control which services each AWS account can use. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Systems Manager templates to control which AWS services each department can use.",
      "B": "Create organization units (OUs) for each department in AWS Organizations. Attach service control policies (SCPs) to the OUs.",
      "C": "Use AWS CloudFormation to automatically provision only the AWS services that each department can use.",
      "D": "Set up a list of products in AWS Service Catalog in the AWS accounts to manage and control the usage of specific AWS services."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司为其财务、数据分析和开发部门分别设置了 AWS 账户。由于成本和安全方面的考虑，公司希望控制每个 AWS 账户可以使用哪些服务。哪种解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Systems Manager 模板来控制每个部门可以使用哪些 AWS 服务。",
      "B": "在 AWS Organizations 中为每个部门创建组织单位（OU）。将服务控制策略（SCP）附加到 OU。",
      "C": "使用 AWS CloudFormation 自动配置每个部门可以使用的 AWS 服务。",
      "D": "在 AWS 账户中设置 AWS Service Catalog 中的产品列表，以管理和控制特定 AWS 服务的使用。"
    },
    "tags": [
      "Organizations",
      "SCP",
      "OU",
      "Service Catalog"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n本题考察如何控制账户的服务使用。使用 Organizations 中的服务控制策略 (SCP) 可以在账户级别控制服务的使用。通过组织单位 (OU) 可以对多个账户应用相同的 SCP。",
      "why_correct": "选项 B 使用 Organizations 和 SCP，可以控制账户可以使用哪些服务，并减少运营开销。",
      "why_wrong": "选项 A 使用 Systems Manager 模板无法控制服务的使用。选项 C 使用 CloudFormation 配置，过于复杂。选项 D 使用 Service Catalog，增加了管理开销，并且并非最佳实践。"
    },
    "related_terms": [
      "SCP",
      "OU",
      "AWS Systems Manager",
      "CloudFormation",
      "Organizations",
      "Service Catalog"
    ]
  },
  {
    "id": 549,
    "topic": "1",
    "question_en": "A company has created a multi-tier application for its ecommerce website. The website uses an Application Load Balancer that resides in the public subnets, a web tier in the public subnets, and a MySQL cluster hosted on Amazon EC2 instances in the private subnets. The MySQL database needs to retrieve product catalog and pricing information that is hosted on the internet by a third-party provider. A solutions architect must devise a strategy that maximizes security without increasing operational overhead. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Deploy a NAT instance in the VPC. Route all the internet-based trafic through the NAT instance.",
      "B": "Deploy a NAT gateway in the public subnets. Modify the private subnet route table to direct all internet-bound trafic to the NAT gateway.",
      "C": "Configure an internet gateway and attach it to the VPModify the private subnet route table to direct internet-bound trafic to the internet gateway.",
      "D": "Configure a virtual private gateway and attach it to the VPC. Modify the private subnet route table to direct internet-bound trafic to the virtual private gateway."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司为其电子商务网站创建了一个多层应用程序。该网站使用位于公共子网中的 Application Load Balancer，公共子网中的 Web 层以及托管在私有子网中的 Amazon EC2 实例上的 MySQL 集群。MySQL 数据库需要检索由第三方提供商托管在互联网上的产品目录和定价信息。解决方案架构师必须设计一种策略，该策略可在不增加运营开销的情况下最大程度地提高安全性。解决方案架构师应如何满足这些要求？",
    "options_cn": {
      "A": "在 VPC 中部署 NAT 实例。将所有基于互联网的流量路由到 NAT 实例。",
      "B": "在公共子网中部署 NAT 网关。修改私有子网路由表，将所有互联网流量导向 NAT 网关。",
      "C": "配置互联网网关并将其附加到 VPC。修改私有子网路由表，将互联网流量导向互联网网关。",
      "D": "配置虚拟私有网关并将其附加到 VPC。修改私有子网路由表，将互联网流量导向虚拟私有网关。"
    },
    "tags": [
      "VPC",
      "NAT Gateway",
      "NAT Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察私有子网访问互联网的方案。NAT 网关是更推荐的方案，相比于 NAT 实例，更容易管理，并且具有高可用性。在私有子网中使用 NAT 网关，可以使私有子网中的资源访问互联网，而无需公网 IP。",
      "why_correct": "选项 B 在公共子网中部署 NAT 网关，可以使私有子网中的资源访问互联网，并且安全性高。",
      "why_wrong": "选项 A 使用 NAT 实例，运维成本高。选项 C 和 D 配置互联网网关和虚拟专用网关，均不符合题意。"
    },
    "related_terms": [
      "VPC",
      "NAT Gateway",
      "互联网网关",
      "私有子网",
      "NAT Instance",
      "公有子网"
    ]
  },
  {
    "id": 550,
    "topic": "1",
    "question_en": "A company is using AWS Key Management Service (AWS KMS) keys to encrypt AWS Lambda environment variables. A solutions architect needs to ensure that the required permissions are in place to decrypt and use the environment variables. Which steps must the solutions architect take to implement the correct permissions? (Choose two.)",
    "options_en": {
      "A": "Add AWS KMS permissions in the Lambda resource policy.",
      "B": "Add AWS KMS permissions in the Lambda execution role.",
      "C": "Add AWS KMS permissions in the Lambda function policy.",
      "D": "Allow the Lambda execution role in the AWS KMS key policy",
      "E": "Allow the Lambda resource policy in the AWS KMS key policy."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 AWS Key Management Service (AWS KMS) 密钥来加密 AWS Lambda 环境变量。一位解决方案架构师需要确保具备解密和使用环境变量的必要权限。解决方案架构师必须采取哪些步骤才能实施正确的权限？（选择两项。）",
    "options_cn": {
      "A": "在 Lambda 资源策略中添加 AWS KMS 权限。",
      "B": "在 Lambda 执行角色中添加 AWS KMS 权限。",
      "C": "在 Lambda 函数策略中添加 AWS KMS 权限。",
      "D": "在 AWS KMS 密钥策略中允许 Lambda 执行角色。",
      "E": "在 AWS KMS 密钥策略中允许 Lambda 资源策略。"
    },
    "tags": [
      "KMS",
      "Lambda",
      "IAM",
      "环境变量"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n本题考察如何使用 KMS 加密 Lambda 环境变量。Lambda 函数需要 IAM 权限来访问 KMS 密钥。需要将 KMS 权限添加到 Lambda 执行角色中，以及在 KMS 密钥策略中允许 Lambda 执行角色。",
      "why_correct": "选项 B 在 Lambda 执行角色中添加 KMS 权限，可以使 Lambda 函数访问 KMS 密钥，解密环境变量。选项 D 在 KMS 密钥策略中允许 Lambda 执行角色访问 KMS 密钥。",
      "why_wrong": "选项 A 和 C 无法实现解密环境变量。选项 E 在 KMS 密钥策略中允许 Lambda 资源策略是错误的。"
    },
    "related_terms": [
      "KMS",
      "Lambda",
      "IAM",
      "资源策略",
      "环境变量",
      "密钥策略",
      "执行角色"
    ]
  },
  {
    "id": 551,
    "topic": "1",
    "question_en": "A company has a financial application that produces reports. The reports average 50 KB in size and are stored in Amazon S3. The reports are frequently accessed during the first week after production and must be stored for several years. The reports must be retrievable within 6 hours. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier after 7 days.",
      "B": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) after 7 days.",
      "C": "Use S3 Intelligent-Tiering. Configure S3 Intelligent-Tiering to transition the reports to S3 Standard-Infrequent Access (S3 Standard-IA) and S3 Glacier.",
      "D": "Use S3 Standard. Use an S3 Lifecycle rule to transition the reports to S3 Glacier Deep Archive after 7 days."
    },
    "correct_answer": "A",
    "vote_percentage": "65%",
    "question_cn": "一家公司有一个生成报告的金融应用程序。报告平均大小为 50 KB，并存储在 Amazon S3 中。报告在生成后的第一周内会被频繁访问，并且必须存储数年。报告必须在 6 小时内可检索。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "使用 S3 Standard。使用 S3 生命周期规则在 7 天后将报告转换为 S3 Glacier。",
      "B": "使用 S3 Standard。使用 S3 生命周期规则在 7 天后将报告转换为 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "C": "使用 S3 Intelligent-Tiering。配置 S3 Intelligent-Tiering 将报告转换为 S3 Standard-Infrequent Access (S3 Standard-IA) 和 S3 Glacier。",
      "D": "使用 S3 Standard。使用 S3 生命周期规则在 7 天后将报告转换为 S3 Glacier Deep Archive。"
    },
    "tags": [
      "S3",
      "生命周期规则",
      "存储类别"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 65%），解析仅供参考。】\n\n考查 S3 存储类与生命周期规则的结合使用，以及根据访问频率和数据持久性需求选择合适的存储方案。",
      "why_correct": "S3 Standard 适用于频繁访问的数据。使用 S3 生命周期规则在 7 天后将报告迁移到 S3 Glacier，可以满足长期存储需求且降低成本。S3 Glacier 提供了低成本的存储，虽然检索时间较长，但符合题目要求的 6 小时内可检索的需求。",
      "why_wrong": "选项 B 错误，S3 Standard-IA 虽然成本较低，但检索时间较短，不适用于长期存储。选项 C 错误，S3 Intelligent-Tiering 无法直接配置转换为 S3 Glacier，且在 S3 Intelligent-Tiering 中配置到 S3 Standard-IA 也无法满足长期存储的需求。选项 D 错误，S3 Glacier Deep Archive 的检索时间更长，可能不满足 6 小时内可检索的要求，并且成本与 S3 Glacier 相差不大。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard",
      "S3 Glacier",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 Intelligent-Tiering",
      "S3 Glacier Deep Archive",
      "S3 lifecycle rules"
    ]
  },
  {
    "id": 552,
    "topic": "1",
    "question_en": "A company needs to optimize the cost of its Amazon EC2 instances. The company also needs to change the type and family of its EC2 instances every 2-3 months. What should the company do to meet these requirements?",
    "options_en": {
      "A": "Purchase Partial Upfront Reserved Instances for a 3-year term.",
      "B": "Purchase a No Upfront Compute Savings Plan for a 1-year term.",
      "C": "Purchase All Upfront Reserved Instances for a 1-year term.",
      "D": "Purchase an All Upfront EC2 Instance Savings Plan for a 1-year term."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要优化其 Amazon EC2 实例的成本。该公司还需要每 2-3 个月更改其 EC2 实例的类型和系列。该公司应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "为期 3 年，购买部分预付费的预留实例。",
      "B": "为期 1 年，购买无预付计算节省计划。",
      "C": "为期 1 年，购买全部预付费的预留实例。",
      "D": "为期 1 年，购买全部预付费的 EC2 实例节省计划。"
    },
    "tags": [
      "EC2",
      "节省计划",
      "预留实例"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了如何在满足灵活变更实例类型和系列需求的前提下，优化 Amazon EC2 实例的成本。重点在于选择适合短期、频繁变更的成本优化方案。",
      "why_correct": "无预付计算节省计划（Compute Savings Plans）提供了最大的灵活性，允许客户在 EC2、Fargate 和 Lambda 之间灵活切换。这种方案没有对实例类型或系列的要求，非常适合需要定期更改 EC2 实例配置的公司。购买期限为 1 年，与题目中 2-3 个月变更实例的需求相符。",
      "why_wrong": "选项 A 预留实例（Reserved Instances）虽然提供成本优化，但对于实例类型和系列有严格的要求，不适合需要频繁变更的公司。选项 C 全部预付费的预留实例与 A 类似，灵活性差。选项 D EC2 实例节省计划（EC2 Instance Savings Plans）虽然可以提供成本优化，但不如 Compute Savings Plans 灵活，无法同时适用于 EC2、Fargate 和 Lambda 之间的灵活切换，且也对实例类型和系列有一定限制。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 instances",
      "Reserved Instances",
      "Compute Savings Plans",
      "EC2 Instance Savings Plans",
      "Fargate",
      "Lambda"
    ]
  },
  {
    "id": 553,
    "topic": "1",
    "question_en": "A solutions architect needs to review a company's Amazon S3 buckets to discover personally identifiable information (PII). The company stores the PII data in the us-east-1 Region and us-west-2 Region. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure Amazon Macie in each Region. Create a job to analyze the data that is in Amazon S3.",
      "B": "Configure AWS Security Hub for all Regions. Create an AWS Config rule to analyze the data that is in Amazon S3.",
      "C": "Configure Amazon Inspector to analyze the data that is in Amazon S3.",
      "D": "Configure Amazon GuardDuty to analyze the data that is in Amazon S3."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要审查一家公司的 Amazon S3 存储桶以发现个人身份信息 (PII)。该公司将 PII 数据存储在 us-east-1 区域和 us-west-2 区域。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在每个区域配置 Amazon Macie。创建一个作业来分析 Amazon S3 中的数据。",
      "B": "为所有区域配置 AWS Security Hub。创建一个 AWS Config 规则来分析 Amazon S3 中的数据。",
      "C": "配置 Amazon Inspector 来分析 Amazon S3 中的数据。",
      "D": "配置 Amazon GuardDuty 来分析 Amazon S3 中的数据。"
    },
    "tags": [
      "Macie",
      "S3",
      "PII"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考察如何发现 S3 存储桶中的 PII 数据。Amazon Macie 提供了自动发现和分类 PII 数据的能力。部署 Macie 可以扫描存储桶并发现敏感信息。",
      "why_correct": "选项 A 使用 Amazon Macie 扫描 S3 存储桶，可以识别 PII 数据。",
      "why_wrong": "选项 B 使用 AWS Security Hub 和 AWS Config，无法直接发现 PII 数据。选项 C 和 D，Amazon Inspector 和 GuardDuty，不具备 PII 数据发现的功能。"
    },
    "related_terms": [
      "Macie",
      "S3",
      "PII",
      "AWS Security Hub",
      "AWS Config",
      "Amazon Inspector",
      "GuardDuty"
    ]
  },
  {
    "id": 554,
    "topic": "1",
    "question_en": "A company's SAP application has a backend SQL Server database in an on-premises environment. The company wants to migrate its on- premises application and database server to AWS. The company needs an instance type that meets the high demands of its SAP database. On-premises performance data shows that both the SAP application and the database have high memory utilization. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use the compute optimized instance family for the application. Use the memory optimized instance family for the database.",
      "B": "Use the storage optimized instance family for both the application and the database.",
      "C": "Use the memory optimized instance family for both the application and the database.",
      "D": "Use the high performance computing (HPC) optimized instance family for the application. Use the memory optimized instance family for the database."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的 SAP 应用程序在本地环境中有一个后端 SQL Server 数据库。该公司希望将其本地应用程序和数据库服务器迁移到 AWS。该公司需要一种实例类型来满足其 SAP 数据库的高需求。本地性能数据显示，SAP 应用程序和数据库都具有高内存利用率。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为应用程序使用计算优化实例系列。为数据库使用内存优化实例系列。",
      "B": "为应用程序和数据库都使用存储优化设备实例系列。",
      "C": "为应用程序和数据库都使用内存优化实例系列。",
      "D": "为应用程序使用高性能计算 (HPC) 优化实例系列。为数据库使用内存优化实例系列。"
    },
    "tags": [
      "EC2",
      "SAP",
      "内存优化",
      "计算优化"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察 SAP 应用程序的实例选择。SAP 应用程序和数据库需要高内存。内存优化实例系列适合于内存密集型工作负载。",
      "why_correct": "选项 C 为应用程序和数据库都使用内存优化实例系列，可以满足高内存需求。",
      "why_wrong": "选项 A 使用计算优化实例系列，不满足数据库的内存需求。选项 B 使用存储优化实例，不满足数据库的内存需求。选项 D 使用 HPC 优化，不符合数据库内存需求。"
    },
    "related_terms": [
      "EC2",
      "SAP",
      "内存优化",
      "实例类型",
      "计算优化"
    ]
  },
  {
    "id": 555,
    "topic": "1",
    "question_en": "A company runs an application in a VPC with public and private subnets. The VPC extends across multiple Availability Zones. The application runs on Amazon EC2 instances in private subnets. The application uses an Amazon Simple Queue Service (Amazon SQS) queue. A solutions architect needs to design a secure solution to establish a connection between the EC2 instances and the SQS queue. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows trafic from the EC2 instances that are in the private subnets.",
      "B": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach to the interface endpoint a VPC endpoint policy that allows access from the EC2 instances that are in the private subnets.",
      "C": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the public subnets. Attach an Amazon SQS access policy to the interface VPC endpoint that allows requests from only a specified VPC endpoint.",
      "D": "Implement a gateway endpoint for Amazon SQS. Add a NAT gateway to the private subnets. Attach an IAM role to the EC2 instances that allows access to the SQS queue."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在具有公有子网和私有子网的 VPC 中运行应用程序。VPC 跨多个可用区。该应用程序在私有子网中的 Amazon EC2 实例上运行。该应用程序使用 Amazon Simple Queue Service (Amazon SQS) 队列。解决方案架构师需要设计一个安全解决方案，以在 EC2 实例和 SQS 队列之间建立连接。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Amazon SQS 实施一个接口 VPC endpoint。配置该 endpoint 以使用私有子网。为该 endpoint 添加一个安全组，该安全组具有一个入站访问规则，允许来自私有子网中的 EC2 实例的流量。",
      "B": "为 Amazon SQS 实施一个接口 VPC endpoint。配置该 endpoint 以使用公有子网。将一个 VPC endpoint 策略附加到接口 endpoint，该策略允许来自私有子网中的 EC2 实例的访问。",
      "C": "为 Amazon SQS 实施一个接口 VPC endpoint。配置该 endpoint 以使用公有子网。将 Amazon SQS 访问策略附加到接口 VPC endpoint，该策略仅允许来自指定 VPC endpoint 的请求。",
      "D": "为 Amazon SQS 实施一个网关 endpoint。将一个 NAT Gateway 添加到私有子网。将一个 IAM 角色附加到 EC2 实例，该角色允许访问 SQS 队列。"
    },
    "tags": [
      "VPC",
      "SQS",
      "接口 VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考察如何在 EC2 实例和 SQS 队列之间建立安全连接。接口 VPC endpoint 提供了一种安全的方式，可以在 VPC 中访问 SQS。 必须在接口 endpoint 上配置安全组，以允许来自 EC2 实例的流量。",
      "why_correct": "选项 A 为 SQS 实施接口 VPC endpoint，配置 endpoint 使用私有子网，并配置安全组，满足安全连接需求。",
      "why_wrong": "选项 B 配置 endpoint 使用公有子网，不符合安全最佳实践。选项 C 配置 endpoint 使用公有子网，不符合安全最佳实践。选项 D 使用 NAT 网关，会增加复杂性，也并不安全。"
    },
    "related_terms": [
      "VPC",
      "SQS",
      "安全组",
      "NAT Gateway",
      "接口 VPC endpoint"
    ]
  },
  {
    "id": 556,
    "topic": "1",
    "question_en": "A solutions architect is using an AWS CloudFormation template to deploy a three-tier web application. The web application consists of a web tier and an application tier that stores and retrieves user data in Amazon DynamoDB tables. The web and application tiers are hosted on Amazon EC2 instances, and the database tier is not publicly accessible. The application EC2 instances need to access the DynamoDB tables without exposing API credentials in the template. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an IAM role to read the DynamoDB tables. Associate the role with the application instances by referencing an instance profile.",
      "B": "Create an IAM role that has the required permissions to read and write from the DynamoDB tables. Add the role to the EC2 instance profile, and associate the instance profile with the application instances.",
      "C": "Use the parameter section in the AWS CloudFormation template to have the user input access and secret keys from an already-created IAM user that has the required permissions to read and write from the DynamoDB tables.",
      "D": "Create an IAM user in the AWS CloudFormation template that has the required permissions to read and write from the DynamoDB tables. Use the GetAtt function to retrieve the access and secret keys, and pass them to the application instances through the user data."
    },
    "correct_answer": "B",
    "vote_percentage": "87%",
    "question_cn": "一位解决方案架构师正在使用 AWS CloudFormation 模板来部署一个三层 Web 应用程序。该 Web 应用程序包含一个 Web 层和一个应用程序层，该层将用户数据存储在 Amazon DynamoDB 表中并从中检索用户数据。 Web 层和应用程序层托管在 Amazon EC2 实例上，数据库层不可公开访问。应用程序 EC2 实例需要访问 DynamoDB 表，而无需在模板中暴露 API 凭证。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 IAM 角色来读取 DynamoDB 表。通过引用实例配置文件将该角色与应用程序实例关联。",
      "B": "创建一个 IAM 角色，该角色具有从 DynamoDB 表中读取和写入所需的权限。将该角色添加到 EC2 实例配置文件，并将实例配置文件与应用程序实例关联。",
      "C": "使用 AWS CloudFormation 模板中的参数部分，让用户输入来自已创建的 IAM 用户（该用户具有从 DynamoDB 表中读取和写入所需的权限）的访问密钥和密钥。 ",
      "D": "在 AWS CloudFormation 模板中创建一个 IAM 用户，该用户具有从 DynamoDB 表中读取和写入所需的权限。使用 GetAtt 函数检索访问密钥和密钥，并通过用户数据将它们传递给应用程序实例。"
    },
    "tags": [
      "IAM",
      "DynamoDB",
      "EC2",
      "CloudFormation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 87%），解析仅供参考。】\n\n此题考察了如何在 EC2 实例上安全地访问 DynamoDB。正确的做法是使用 IAM 角色，将权限与 EC2 实例关联，避免在代码中硬编码凭证。这提高了安全性，并简化了凭证的管理。实例配置文件将 IAM 角色与 EC2 实例关联。",
      "why_correct": "选项 B 提供了最安全的解决方案。它创建了一个具有读取和写入 DynamoDB 权限的 IAM 角色，并将该角色分配给 EC2 实例配置文件。实例配置文件在 EC2 实例启动时，将 IAM 角色分配给实例，使得应用程序可以访问 DynamoDB 而无需嵌入凭证。",
      "why_wrong": "选项 A 仅具有读取权限，无法满足写入需求。选项 C 不安全，因为需要在 CloudFormation 模板中暴露 API 凭证。选项 D 同样不安全，通过用户数据传递凭证也存在风险。"
    },
    "related_terms": [
      "IAM",
      "DynamoDB",
      "EC2",
      "CloudFormation",
      "API",
      "GetAtt",
      "EC2 instance profile",
      "User Data"
    ]
  },
  {
    "id": 557,
    "topic": "1",
    "question_en": "A solutions architect manages an analytics application. The application stores large amounts of semistructured data in an Amazon S3 bucket. The solutions architect wants to use parallel data processing to process the data more quickly. The solutions architect also wants to use information that is stored in an Amazon Redshift database to enrich the data. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Athena to process the S3 data. Use AWS Glue with the Amazon Redshift data to enrich the S3 data.",
      "B": "Use Amazon EMR to process the S3 data. Use Amazon EMR with the Amazon Redshift data to enrich the S3 data.",
      "C": "Use Amazon EMR to process the S3 data. Use Amazon Kinesis Data Streams to move the S3 data into Amazon Redshift so that the data can be enriched.",
      "D": "Use AWS Glue to process the S3 data. Use AWS Lake Formation with the Amazon Redshift data to enrich the S3 data."
    },
    "correct_answer": "B",
    "vote_percentage": "71%",
    "question_cn": "一位解决方案架构师管理一个分析应用程序。该应用程序将大量半结构化数据存储在 Amazon S3 存储桶中。解决方案架构师希望使用并行数据处理来更快地处理数据。解决方案架构师还希望使用存储在 Amazon Redshift 数据库中的信息来丰富数据。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Athena 处理 S3 数据。使用 AWS Glue 和 Amazon Redshift 数据来丰富 S3 数据。",
      "B": "使用 Amazon EMR 处理 S3 数据。使用 Amazon EMR 和 Amazon Redshift 数据来丰富 S3 数据。",
      "C": "使用 Amazon EMR 处理 S3 数据。使用 Amazon Kinesis Data Streams 将 S3 数据移动到 Amazon Redshift，以便可以丰富数据。",
      "D": "使用 AWS Glue 处理 S3 数据。使用 AWS Lake Formation 和 Amazon Redshift 数据来丰富 S3 数据。"
    },
    "tags": [
      "S3",
      "Redshift",
      "EMR",
      "Athena",
      "Glue",
      "Lake Formation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 71%），解析仅供参考。】\n\n考查如何使用并行数据处理加速 S3 数据的处理，并结合 Amazon Redshift 中的数据进行数据丰富。",
      "why_correct": "Amazon EMR 提供了可扩展的并行处理能力，非常适合处理存储在 S3 中的大量半结构化数据。 EMR 能够直接访问 S3 数据，并且可以通过配置在 EMR 集群中运行的作业来访问 Redshift，从而实现数据丰富。",
      "why_wrong": "选项 A 使用 Athena 处理数据，Athena 适合交互式查询，不适合大规模并行数据处理。选项 C 使用 Kinesis Data Streams 导入 Redshift，增加了复杂性，且 Kinesis 更适合实时数据流处理，而非批量数据的丰富。选项 D 使用 AWS Glue 处理数据，AWS Glue 虽然可以处理数据，但它与 EMR 相比，处理大规模数据的效率可能较低，并且其数据丰富流程的复杂性可能不及 EMR。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon Redshift",
      "Amazon EMR",
      "Amazon Athena",
      "AWS Glue",
      "Kinesis Data Streams",
      "AWS Lake Formation"
    ]
  },
  {
    "id": 558,
    "topic": "1",
    "question_en": "A company has two VPCs that are located in the us-west-2 Region within the same AWS account. The company needs to allow network trafic between these VPCs. Approximately 500 GB of data transfer will occur between the VPCs each month. What is the MOST cost-effective solution to connect these VPCs?",
    "options_en": {
      "A": "Implement AWS Transit Gateway to connect the VPCs. Update the route tables of each VPC to use the transit gateway for inter-VPC communication.",
      "B": "Implement an AWS Site-to-Site VPN tunnel between the VPCs. Update the route tables of each VPC to use the VPN tunnel for inter-VPC communication.",
      "C": "Set up a VPC peering connection between the VPCs. Update the route tables of each VPC to use the VPC peering connection for inter- VPC communication.",
      "D": "Set up a 1 GB AWS Direct Connect connection between the VPCs. Update the route tables of each VPC to use the Direct Connect connection for inter-VPC communication."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在同一 AWS 账户中的 us-west-2 区域中拥有两个 VPC。该公司需要允许这些 VPC 之间的网络流量。每个月将发生大约 500 GB 的数据传输。连接这些 VPC 的最具成本效益的解决方案是什么？",
    "options_cn": {
      "A": "实施 AWS Transit Gateway 以连接 VPC。更新每个 VPC 的路由表以使用 Transit Gateway 进行 VPC 间通信。",
      "B": "在 VPC 之间实施 AWS Site-to-Site VPN 隧道。更新每个 VPC 的路由表以使用 VPN 隧道进行 VPC 间通信。",
      "C": "在 VPC 之间设置 VPC 对等连接。更新每个 VPC 的路由表以使用 VPC 对等连接进行 VPC 间通信。",
      "D": "在 VPC 之间设置 1 GB 的 AWS Direct Connect 连接。更新每个 VPC 的路由表以使用 Direct Connect 连接进行 VPC 间通信。"
    },
    "tags": [
      "VPC",
      "Transit Gateway",
      "Site-to-Site VPN",
      "VPC Peering",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察了 VPC 间网络连接的成本效益。VPC 对等连接通常是数据传输量较小情况下的最经济选择。当数据传输量较大时，其他的选择会更划算。",
      "why_correct": "选项 C 使用 VPC 对等连接，这是在两个 VPC 之间建立连接的最具成本效益的方式，尤其是对于每月 500 GB 的数据传输。",
      "why_wrong": "选项 A 使用 AWS Transit Gateway，虽然可以连接 VPC，但成本较高，不适合小数据量场景。选项 B 使用 Site-to-Site VPN，成本也高于 VPC 对等连接。选项 D 使用 AWS Direct Connect，其最低连接速度为 1 Gbps，且有额外费用，不适合小数据量场景。"
    },
    "related_terms": [
      "VPC",
      "Transit Gateway",
      "Site-to-Site VPN",
      "VPC Peering",
      "Direct Connect",
      "VPC Interconnection"
    ]
  },
  {
    "id": 559,
    "topic": "1",
    "question_en": "A company hosts multiple applications on AWS for different product lines. The applications use different compute resources, including Amazon EC2 instances and Application Load Balancers. The applications run in different AWS accounts under the same organization in AWS Organizations across multiple AWS Regions. Teams for each product line have tagged each compute resource in the individual accounts. The company wants more details about the cost for each product line from the consolidated billing feature in Organizations. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Select a specific AWS generated tag in the AWS Billing console.",
      "B": "Select a specific user-defined tag in the AWS Billing console.",
      "C": "Select a specific user-defined tag in the AWS Resource Groups console.",
      "D": "Activate the selected tag from each AWS account",
      "E": "Activate the selected tag from the Organizations management account."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 上为不同的产品线托管多个应用程序。这些应用程序使用不同的计算资源，包括 Amazon EC2 实例和 Application Load Balancer。这些应用程序在 AWS Organizations 中的不同 AWS 账户中运行，跨越多个 AWS 区域。每个产品线的团队已经在各个账户中标记了每个计算资源。该公司希望从 Organizations 的合并账单功能中获取关于每个产品线的更多成本详细信息。哪些步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 AWS 账单控制台中选择一个特定的 AWS 生成的标签。",
      "B": "在 AWS 账单控制台中选择一个特定的用户自定义标签。",
      "C": "在 AWS Resource Groups 控制台中选择一个特定的用户自定义标签。",
      "D": "从每个 AWS 账户中激活所选标签。",
      "E": "从 Organizations 管理账户中激活所选标签。"
    },
    "tags": [
      "Organizations",
      "Cost Allocation Tags"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n此题考察了在 AWS Organizations 中进行成本追踪。使用成本分配标签可以更精细地了解不同产品线的成本。用户自定义标签可以提供更大的灵活性，便于组织按需进行成本分析。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：选项 B 是正确的，因为可以通过在账单控制台中选择用户自定义标签来查看成本。组织可以通过用户自定义标签来细分成本，从而能够更好地分析成本。选项 D 是正确的，需要在每个账户中激活标签。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，AWS 生成的标签无法自定义。选项 C 使用 Resource Groups 控制台无法用于成本分析。选项 E，从组织管理账户激活标签将无法在所有账户中都可用。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Budgets",
      "Organizations",
      "Cost Allocation Tags"
    ]
  },
  {
    "id": 560,
    "topic": "1",
    "question_en": "A company's solutions architect is designing an AWS multi-account solution that uses AWS Organizations. The solutions architect has organized the company's accounts into organizational units (OUs). The solutions architect needs a solution that will identify any changes to the OU hierarchy. The solution also needs to notify the company's operations team of any changes. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Provision the AWS accounts by using AWS Control Tower. Use account drift notifications to identify the changes to the OU hierarchy.",
      "B": "Provision the AWS accounts by using AWS Control Tower. Use AWS Config aggregated rules to identify the changes to the OU hierarchy.",
      "C": "Use AWS Service Catalog to create accounts in Organizations. Use an AWS CloudTrail organization trail to identify the changes to the OU hierarchy.",
      "D": "Use AWS CloudFormation templates to create accounts in Organizations. Use the drift detection operation on a stack to identify the changes to the OU hierarchy."
    },
    "correct_answer": "A",
    "vote_percentage": "79%",
    "question_cn": "一家公司的解决方案架构师正在设计一个使用 AWS Organizations 的 AWS 多账户解决方案。解决方案架构师已将公司的账户组织到组织单元 (OU) 中。解决方案架构师需要一个解决方案来识别 OU 层次结构的任何更改。该解决方案还需要将任何更改通知给公司的运营团队。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Control Tower 配置 AWS 账户。使用账户漂移通知来识别 OU 层次结构的更改。",
      "B": "使用 AWS Control Tower 配置 AWS 账户。使用 AWS Config 聚合规则来识别 OU 层次结构的更改。",
      "C": "使用 AWS Service Catalog 在 Organizations 中创建账户。使用 AWS CloudTrail 组织跟踪来识别 OU 层次结构的更改。",
      "D": "使用 AWS CloudFormation 模板在 Organizations 中创建账户。对堆栈使用漂移检测操作来识别 OU 层次结构的更改。"
    },
    "tags": [
      "Organizations",
      "Control Tower",
      "Account Drift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 79%），解析仅供参考。】\n\n此题考察如何在 AWS Organizations 中检测 OU 层次结构的更改。 AWS Control Tower 提供了账户管理和合规性框架，结合账户漂移通知可以检测配置更改。这种方案开销较小，更易于管理。",
      "why_correct": "选项 A 使用 AWS Control Tower 和账户漂移通知，可以以最少的运营开销来识别和通知 OU 层次结构的更改。",
      "why_wrong": "选项 B 使用 AWS Control Tower，但使用了 AWS Config 聚合规则，这并非检测 OU 层次结构更改的最直接方式。选项 C 使用 AWS Service Catalog 和 AWS CloudTrail，开销较大且实现相对复杂。选项 D 使用 CloudFormation 和堆栈漂移检测，没有利用 Control Tower 的优势，且实现比较复杂。"
    },
    "related_terms": [
      "OU",
      "Organizations",
      "Control Tower",
      "Account Drift"
    ]
  },
  {
    "id": 561,
    "topic": "1",
    "question_en": "A company's website handles millions of requests each day, and the number of requests continues to increase. A solutions architect needs to improve the response time of the web application. The solutions architect determines that the application needs to decrease latency when retrieving product details from the Amazon DynamoDB table. Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Set up a DynamoDB Accelerator (DAX) cluster. Route all read requests through DAX.",
      "B": "Set up Amazon ElastiCache for Redis between the DynamoDB table and the web application. Route all read requests through Redis.",
      "C": "Set up Amazon ElastiCache for Memcached between the DynamoDB table and the web application. Route all read requests through Memcached.",
      "D": "Set up Amazon DynamoDB Streams on the table, and have AWS Lambda read from the table and populate Amazon ElastiCache. Route all read requests through ElastiCache."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司的网站每天处理数百万个请求，而且请求数量持续增加。一位解决方案架构师需要提高 Web 应用程序的响应时间。解决方案架构师确定应用程序需要在从 Amazon DynamoDB 表检索产品详细信息时减少延迟。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "设置 DynamoDB Accelerator (DAX) 集群。将所有读取请求路由通过 DAX。",
      "B": "在 DynamoDB 表和 Web 应用程序之间设置 Amazon ElastiCache for Redis。将所有读取请求路由通过 Redis。",
      "C": "在 DynamoDB 表和 Web 应用程序之间设置 Amazon ElastiCache for Memcached。将所有读取请求路由通过 Memcached。",
      "D": "在表上设置 Amazon DynamoDB Streams，并让 AWS Lambda 从表中读取数据并填充 Amazon ElastiCache。将所有读取请求路由通过 ElastiCache。"
    },
    "tags": [
      "DynamoDB",
      "DAX",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察了如何提高 DynamoDB 应用程序的性能。DAX 是 DynamoDB 的缓存，专门针对 DynamoDB 的读取进行优化，能有效降低延迟。其对现有代码改动较小，方便部署。",
      "why_correct": "选项 A 提供了最直接的解决方案，使用 DAX 缓存 DynamoDB 数据，减少延迟。DAX 是 DynamoDB 专用缓存，可以降低读取延迟。",
      "why_wrong": "选项 B 使用 ElastiCache for Redis，虽然可以缓存数据，但需要修改应用程序代码以使用 Redis。选项 C 使用 ElastiCache for Memcached，同样需要修改应用程序代码，且与 DynamoDB 结合不如 DAX 优化。选项 D 使用 DynamoDB Streams 和 ElastiCache，增加了复杂性。"
    },
    "related_terms": [
      "DynamoDB",
      "DAX",
      "ElastiCache",
      "Redis",
      "DynamoDB Streams",
      "Memcached"
    ]
  },
  {
    "id": 562,
    "topic": "1",
    "question_en": "A solutions architect needs to ensure that API calls to Amazon DynamoDB from Amazon EC2 instances in a VPC do not travel across the internet. Which combination of steps should the solutions architect take to meet this requirement? (Choose two.)",
    "options_en": {
      "A": "Create a route table entry for the endpoint.",
      "B": "Create a gateway endpoint for DynamoDB.",
      "C": "Create an interface endpoint for Amazon EC2.",
      "D": "Create an elastic network interface for the endpoint in each of the subnets of the VPC",
      "E": "Create a security group entry in the endpoint's security group to provide access."
    },
    "correct_answer": "AB",
    "vote_percentage": "69%",
    "question_cn": "一位解决方案架构师需要确保来自 VPC 中 Amazon EC2 实例的对 Amazon DynamoDB 的 API 调用不会跨互联网传输。解决方案架构师应采取哪些组合步骤来满足此要求？（选择两项。）",
    "options_cn": {
      "A": "为终端节点创建路由表条目。",
      "B": "为 DynamoDB 创建一个网关终端节点。",
      "C": "为 Amazon EC2 创建一个接口终端节点。",
      "D": "在 VPC 的每个子网中为终端节点创建一个弹性网络接口。",
      "E": "在终端节点的安全组中创建一个安全组条目以提供访问权限。"
    },
    "tags": [
      "EC2",
      "DynamoDB",
      "VPC Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 69%），解析仅供参考。】\n\n此题考察了如何在 VPC 内安全地访问 DynamoDB，避免流量通过公网。VPC 终端节点提供了安全的私有访问 DynamoDB 的方式，通过终端节点，流量不会离开 VPC。",
      "why_correct": "选项 A 为终端节点创建路由表条目，确保流量通过终端节点路由。选项 B 创建 DynamoDB 网关终端节点，允许 VPC 中的 EC2 实例通过私有连接访问 DynamoDB，无需经过互联网。",
      "why_wrong": "选项 C，接口终端节点是用于访问其他服务的。选项 D 和 E，弹性网络接口和安全组在终端节点创建之后配置。"
    },
    "related_terms": [
      "EC2",
      "DynamoDB",
      "VPC Endpoint",
      "Route Table",
      "Security Group"
    ]
  },
  {
    "id": 555,
    "topic": "",
    "question_en": "Implement an interface VPC endpoint for Amazon SQS. Configure the endpoint to use the private subnets. Add to the endpoint a security group that has an inbound access rule that allows traffic from the EC2 instances that are in the private subnets.",
    "options_en": {},
    "correct_answer": "",
    "vote_percentage": "",
    "question_cn": "为 Amazon SQS 实施一个 VPC 终端节点。配置该终端节点以使用私有子网。为该终端节点添加一个安全组，该安全组包含一个入站访问规则，允许来自位于私有子网中的 EC2 实例的流量。",
    "options_cn": {
      "A": "创建一个 IAM 策略，允许 EC2 实例访问 SQS。将此策略附加到 EC2 实例的 IAM 角色。",
      "B": "创建一个 VPC 终端节点，并将它配置为使用私有子网。在终端节点的安全组中，添加一个允许来自私有子网中 EC2 实例的流量的入站规则。",
      "C": "创建一个 NAT Gateway，以便 EC2 实例可以通过它访问 SQS。在 NAT Gateway 的安全组中，添加一个允许来自 EC2 实例的流量的入站规则。",
      "D": "创建一个 Application Load Balancer，并将它配置为指向 SQS。在 ALB 的安全组中，添加一个允许来自 EC2 实例的流量的入站规则。"
    },
    "tags": [
      "SQS",
      "VPC Endpoint",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 （社区 —），解析仅供参考。】\n\n该问题考查了如何使用 VPC 终端节点安全地访问 SQS 服务。VPC 终端节点允许从 VPC 内部通过私有连接访问 SQS，避免流量流出 VPC，提高安全性。安全组则控制流量的入站和出站规则。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 该选项。理由简述：该解决方案描述了如何创建一个 VPC 终端节点来连接到 SQS，以及如何配置安全组允许来自私有子网中的 EC2 实例的流量。这确保了 EC2 实例能够通过私有连接安全地访问 SQS。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 仅提供了 IAM 策略，但没有说明如何通过私有连接访问 SQS。选项 C 使用了 NAT Gateway，这会使流量流出 VPC，与题目要求不符。选项 D 使用了 Application Load Balancer，这并非访问 SQS 的正确方式。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SQS",
      "EC2",
      "IAM",
      "VPC Endpoint",
      "Security Group"
    ]
  },
  {
    "id": 563,
    "topic": "1",
    "question_en": "A company runs its applications on both Amazon Elastic Kubernetes Service (Amazon EKS) clusters and on-premises Kubernetes clusters. The company wants to view all clusters and workloads from a central location. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon CloudWatch Container Insights to collect and group the cluster information.",
      "B": "Use Amazon EKS Connector to register and connect all Kubernetes clusters.",
      "C": "Use AWS Systems Manager to collect and view the cluster information.",
      "D": "Use Amazon EKS Anywhere as the primary cluster to view the other clusters with native Kubernetes commands."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家公司在其 Amazon Elastic Kubernetes Service (Amazon EKS) 集群和本地 Kubernetes 集群上运行其应用程序。该公司希望从一个中心位置查看所有集群和工作负载。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch Container Insights 收集并分组集群信息。",
      "B": "使用 Amazon EKS Connector 注册并连接所有 Kubernetes 集群。",
      "C": "使用 AWS Systems Manager 收集和查看集群信息。",
      "D": "使用 Amazon EKS Anywhere 作为主集群，使用原生 Kubernetes 命令查看其他集群。"
    },
    "tags": [
      "EKS",
      "CloudWatch",
      "EKS Connector"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n此题考察了如何从中心位置查看多个 Kubernetes 集群。Amazon EKS Connector 提供了注册和连接 Kubernetes 集群的功能，使得可以从 AWS 控制台统一管理和监控集群，降低运营复杂性。",
      "why_correct": "选项 B 使用 Amazon EKS Connector 注册并连接所有 Kubernetes 集群，可以从一个中心位置查看所有集群和工作负载。",
      "why_wrong": "选项 A 使用 CloudWatch Container Insights，主要用于监控单个集群的性能，不能用于跨集群的统一视图。选项 C 使用 AWS Systems Manager，无法提供对 Kubernetes 集群的深入监控。选项 D 使用 EKS Anywhere 和原生 Kubernetes 命令，增加了管理的复杂性。"
    },
    "related_terms": [
      "EKS",
      "CloudWatch",
      "EKS Connector",
      "Kubernetes"
    ]
  },
  {
    "id": 564,
    "topic": "1",
    "question_en": "A company is building an ecommerce application and needs to store sensitive customer information. The company needs to give customers the ability to complete purchase transactions on the website. The company also needs to ensure that sensitive customer data is protected, even from database administrators. Which solution meets these requirements?",
    "options_en": {
      "A": "Store sensitive data in an Amazon Elastic Block Store (Amazon EBS) volume. Use EBS encryption to encrypt the data. Use an IAM instance role to restrict access.",
      "B": "Store sensitive data in Amazon RDS for MySQL. Use AWS Key Management Service (AWS KMS) client-side encryption to encrypt the data.",
      "C": "Store sensitive data in Amazon S3. Use AWS Key Management Service (AWS KMS) server-side encryption to encrypt the data. Use S3 bucket policies to restrict access.",
      "D": "Store sensitive data in Amazon FSx for Windows Server. Mount the file share on application servers. Use Windows file permissions to restrict access."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在构建一个电子商务应用程序，需要存储敏感的客户信息。该公司需要让客户能够在网站上完成购买交易。该公司还需要确保敏感的客户数据受到保护，即使是数据库管理员也无法访问。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "将敏感数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。使用 EBS 加密来加密数据。使用 IAM 实例角色来限制访问。",
      "B": "将敏感数据存储在 Amazon RDS for MySQL 中。使用 AWS Key Management Service (AWS KMS) 客户端加密来加密数据。",
      "C": "将敏感数据存储在 Amazon S3 中。使用 AWS Key Management Service (AWS KMS) 服务器端加密来加密数据。使用 S3 存储桶策略来限制访问。",
      "D": "将敏感数据存储在 Amazon FSx for Windows Server 中。在应用程序服务器上挂载文件共享。使用 Windows 文件权限来限制访问。"
    },
    "tags": [
      "S3",
      "RDS",
      "KMS",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察了如何安全地存储敏感数据。结合加密、访问控制可以实现数据库管理员也无法访问敏感数据的需求。AWS KMS 提供了密钥管理和加密功能，可以满足要求。",
      "why_correct": "选项 B 将敏感数据存储在 Amazon RDS for MySQL 中，并使用 AWS KMS 客户端加密，即使数据库管理员也无法解密数据，满足了安全需求。",
      "why_wrong": "选项 A 使用 EBS 加密，但 EBS 卷的加密仅限于存储层，数据库管理员仍然可以访问数据。选项 C 使用 S3 和 KMS 服务器端加密，S3 适用于对象存储，不适用于结构化数据库。选项 D 使用 FSx for Windows Server 和 Windows 文件权限，不适用于敏感数据的存储需求。"
    },
    "related_terms": [
      "S3",
      "RDS",
      "KMS",
      "EBS",
      "FSx",
      "Security"
    ]
  },
  {
    "id": 565,
    "topic": "1",
    "question_en": "A company has an on-premises MySQL database that handles transactional data. The company is migrating the database to the AWS Cloud. The migrated database must maintain compatibility with the company's applications that use the database. The migrated database also must scale automatically during periods of increased demand. Which migration solution will meet these requirements?",
    "options_en": {
      "A": "Use native MySQL tools to migrate the database to Amazon RDS for MySQL. Configure elastic storage scaling.",
      "B": "Migrate the database to Amazon Redshift by using the mysqldump utility. Turn on Auto Scaling for the Amazon Redshift cluster.",
      "C": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon Aurora. Turn on Aurora Auto Scaling.",
      "D": "Use AWS Database Migration Service (AWS DMS) to migrate the database to Amazon DynamoDB. Configure an Auto Scaling policy."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一台处理事务数据的本地 MySQL 数据库。该公司正在将该数据库迁移到 AWS 云。迁移后的数据库必须与使用该数据库的公司的应用程序保持兼容性。迁移后的数据库还必须在需求增加期间自动扩展。哪个迁移解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用原生 MySQL 工具将数据库迁移到 Amazon RDS for MySQL。配置弹性存储扩展。",
      "B": "使用 mysqldump 实用程序将数据库迁移到 Amazon Redshift。为 Amazon Redshift 集群打开 Auto Scaling。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 将数据库迁移到 Amazon Aurora。打开 Aurora Auto Scaling。",
      "D": "使用 AWS Database Migration Service (AWS DMS) 将数据库迁移到 Amazon DynamoDB。配置 Auto Scaling 策略。"
    },
    "tags": [
      "MySQL",
      "Aurora",
      "DMS",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察了数据库迁移和可扩展性。AWS DMS 提供数据库迁移服务，可以迁移到 Aurora，Aurora 具有自动伸缩能力，以应对流量的增加。",
      "why_correct": "选项 C 使用 AWS Database Migration Service (AWS DMS) 将 MySQL 数据库迁移到 Amazon Aurora，并打开 Aurora Auto Scaling，可以满足自动扩展的要求。",
      "why_wrong": "选项 A 使用原生 MySQL 工具，无法实现自动伸缩。选项 B 迁移到 Amazon Redshift 不兼容 MySQL，不满足兼容性需求。选项 D 迁移到 DynamoDB 不兼容 MySQL，不满足兼容性需求。"
    },
    "related_terms": [
      "MySQL",
      "Aurora",
      "DMS",
      "Auto Scaling",
      "MySQL to Aurora",
      "Database Migration"
    ]
  },
  {
    "id": 566,
    "topic": "1",
    "question_en": "A company runs multiple Amazon EC2 Linux instances in a VPC across two Availability Zones. The instances host applications that use a hierarchical directory structure. The applications need to read and write rapidly and concurrently to shared storage. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an Amazon S3 bucket. Allow access from all the EC2 instances in the VPC.",
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system from each EC2 instance.",
      "C": "Create a file system on a Provisioned IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) volume. Attach the EBS volume to all the EC2 instances.",
      "D": "Create file systems on Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. Synchronize the EBS volumes across the different EC2 instances."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司在跨越两个可用区的 VPC 中运行多个 Amazon EC2 Linux 实例。这些实例托管使用分层目录结构的应用程序。应用程序需要快速且并发地读写共享存储。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon S3 存储桶。允许从 VPC 中的所有 EC2 实例进行访问。",
      "B": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统。从每个 EC2 实例挂载 EFS 文件系统。",
      "C": "在预置 IOPS SSD (io2) Amazon Elastic Block Store (Amazon EBS) 卷上创建文件系统。将 EBS 卷附加到所有 EC2 实例。",
      "D": "在附加到每个 EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 卷上创建文件系统。在不同的 EC2 实例之间同步 EBS 卷。"
    },
    "tags": [
      "S3",
      "EFS",
      "EC2",
      "Storage"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查共享文件系统在 EC2 实例间的应用，以及对读写性能和并发性的需求。",
      "why_correct": "Amazon EFS 是一种可扩展的、弹性的文件系统，可以供多个 EC2 实例并发访问。EFS 能够提供高吞吐量和低延迟，满足应用程序快速读写共享存储的需求。通过将 EFS 文件系统挂载到每个 EC2 实例，应用程序可以像访问本地文件系统一样访问共享数据。",
      "why_wrong": "A 选项，Amazon S3 是一个对象存储服务，不适合作为共享文件系统，无法支持并发的文件系统操作。C 选项，预置 IOPS EBS 卷虽然可以提供高性能，但无法同时挂载到多个 EC2 实例上，不能满足共享存储的需求。D 选项，在不同的 EC2 实例之间同步 EBS 卷会引入额外的复杂性和延迟，并且无法保证数据一致性，不适合快速并发的读写需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "VPC",
      "Amazon S3",
      "Amazon EFS",
      "Elastic File System (EFS)",
      "Elastic Block Store (EBS)",
      "io2",
      "Linux",
      "EC2 instance"
    ]
  },
  {
    "id": 567,
    "topic": "1",
    "question_en": "A solutions architect is designing a workload that will store hourly energy consumption by business tenants in a building. The sensors will feed a database through HTTP requests that will add up usage for each tenant. The solutions architect must use managed services when possible. The workload will receive more features in the future as the solutions architect adds independent components. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in an Amazon DynamoDB table.",
      "B": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon S3 bucket to store the processed data.",
      "C": "Use Amazon API Gateway with AWS Lambda functions to receive the data from the sensors, process the data, and store the data in a Microsoft SQL Server Express database on an Amazon EC2 instance.",
      "D": "Use an Elastic Load Balancer that is supported by an Auto Scaling group of Amazon EC2 instances to receive and process the data from the sensors. Use an Amazon Elastic File System (Amazon EFS) shared file system to store the processed data."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在设计一个工作负载，用于存储建筑物中业务租户的每小时能源消耗。传感器将通过 HTTP 请求将数据馈送到数据库，这些请求将累加每个租户的用量。解决方案架构师必须尽可能使用托管服务。随着解决方案架构师添加独立组件，工作负载将在未来接收更多功能。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon API Gateway 和 AWS Lambda 函数接收来自传感器的数据，处理数据，并将数据存储在 Amazon DynamoDB 表中。",
      "B": "使用由 Amazon EC2 实例的 Auto Scaling 组支持的 Elastic Load Balancer 接收和处理来自传感器的数据。使用 Amazon S3 存储桶存储已处理的数据。",
      "C": "使用 Amazon API Gateway 和 AWS Lambda 函数接收来自传感器的数据，处理数据，并将数据存储在 Amazon EC2 实例上的 Microsoft SQL Server Express 数据库中。",
      "D": "使用由 Amazon EC2 实例的 Auto Scaling 组支持的 Elastic Load Balancer 接收和处理来自传感器的数据。使用 Amazon Elastic File System (Amazon EFS) 共享文件系统存储已处理的数据。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "Architecture",
      "Serverless"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察了如何构建无服务器架构。AWS API Gateway 和 Lambda 函数结合 DynamoDB 可以快速构建可扩展的应用程序。这种方案也减少了运营开销。",
      "why_correct": "选项 A 使用 Amazon API Gateway 和 AWS Lambda 函数，将数据存储在 Amazon DynamoDB 表中，满足了尽可能使用托管服务，降低运营开销的需求，且易于扩展。",
      "why_wrong": "选项 B 使用 Elastic Load Balancer 和 EC2 实例，增加了管理和运维成本。选项 C 使用 Amazon API Gateway 和 Lambda 函数，将数据存储在 Amazon EC2 实例上的 Microsoft SQL Server Express 数据库中，增加了管理和运维成本。选项 D 使用 Elastic Load Balancer 和 EFS，同样增加了管理和运维成本。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "Serverless",
      "EBS",
      "ELB",
      "EFS",
      "Architecture"
    ]
  },
  {
    "id": 568,
    "topic": "1",
    "question_en": "A solutions architect is designing the storage architecture for a new web application used for storing and viewing engineering drawings. All application components will be deployed on the AWS infrastructure. The application design must support caching to minimize the amount of time that users wait for the engineering drawings to load. The application must be able to store petabytes of data. Which combination of storage and caching should the solutions architect use?",
    "options_en": {
      "A": "Amazon S3 with Amazon CloudFront",
      "B": "Amazon S3 Glacier with Amazon ElastiCache",
      "C": "Amazon Elastic Block Store (Amazon EBS) volumes with Amazon CloudFront",
      "D": "AWS Storage Gateway with Amazon ElastiCache"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为用于存储和查看工程图纸的新 Web 应用程序设计存储架构。所有应用程序组件都将部署在 AWS 基础设施上。应用程序设计必须支持缓存，以最大限度地减少用户等待工程图纸加载的时间。该应用程序必须能够存储 PB 级数据。 解决方案架构师应该使用哪种存储和缓存组合？",
    "options_cn": {
      "A": "Amazon S3 与 Amazon CloudFront",
      "B": "Amazon S3 Glacier 与 Amazon ElastiCache",
      "C": "Amazon Elastic Block Store (Amazon EBS) 卷与 Amazon CloudFront",
      "D": "AWS Storage Gateway 与 Amazon ElastiCache"
    },
    "tags": [
      "S3",
      "CloudFront",
      "Storage"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察了存储和缓存的组合。S3 提供了存储，CloudFront 提供了缓存。 这种组合能够满足 PB 级数据存储和缓存的需求。 这是在 AWS 上构建 Web 应用的最优解。",
      "why_correct": "选项 A 使用 Amazon S3 存储工程图纸，并使用 Amazon CloudFront 进行缓存，满足了存储和缓存的需求，且支持 PB 级数据。 S3 存储成本低廉, CloudFront 能够加速访问。",
      "why_wrong": "选项 B 使用 Glacier，Glacier 适用于不常访问的归档数据，不适用于快速读取。选项 C 使用 EBS 卷，不适合存储 PB 级数据。选项 D 使用 Storage Gateway 和 ElastiCache，Storage Gateway 用于混合云，不适合纯 AWS 架构。"
    },
    "related_terms": [
      "S3",
      "CloudFront",
      "Storage",
      "Caching"
    ]
  },
  {
    "id": 569,
    "topic": "1",
    "question_en": "An Amazon EventBridge rule targets a third-party API. The third-party API has not received any incoming trafic. A solutions architect needs to determine whether the rule conditions are being met and if the rule's target is being invoked. Which solution will meet these requirements?",
    "options_en": {
      "A": "Check for metrics in Amazon CloudWatch in the namespace for AWS/Events.",
      "B": "Review events in the Amazon Simple Queue Service (Amazon SQS) dead-letter queue.",
      "C": "Check for the events in Amazon CloudWatch Logs.",
      "D": "Check the trails in AWS CloudTrail for the EventBridge events."
    },
    "correct_answer": "A",
    "vote_percentage": "69%",
    "question_cn": "一个 Amazon EventBridge 规则指向一个第三方 API。该第三方 API 尚未收到任何传入流量。一个解决方案架构师需要确定是否满足规则条件以及是否调用了规则的目标。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 Amazon CloudWatch 的 AWS/Events 命名空间中检查指标。",
      "B": "查看 Amazon Simple Queue Service (Amazon SQS) 死信队列中的事件。",
      "C": "在 Amazon CloudWatch Logs 中检查事件。",
      "D": "检查 AWS CloudTrail 中 EventBridge 事件的跟踪记录。"
    },
    "tags": [
      "EventBridge",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 69%），解析仅供参考。】\n\n此题考察了如何监控 EventBridge 规则是否被触发。可以通过 CloudWatch 指标来确定规则的触发情况。",
      "why_correct": "选项 A 在 Amazon CloudWatch 的 AWS/Events 命名空间中检查指标，可以确定 EventBridge 规则是否匹配和是否调用了目标。",
      "why_wrong": "选项 B 检查 SQS 死信队列，无法确定规则是否被触发。选项 C 检查 CloudWatch Logs，不能直接反映 EventBridge 规则的触发情况。选项 D 检查 CloudTrail 中 EventBridge 事件的跟踪记录，提供了审计信息，但不能直接确定是否满足规则条件以及是否调用了目标。"
    },
    "related_terms": [
      "EventBridge",
      "CloudWatch",
      "CloudTrail",
      "SQS"
    ]
  },
  {
    "id": 570,
    "topic": "1",
    "question_en": "A company has a large workload that runs every Friday evening. The workload runs on Amazon EC2 instances that are in two Availability Zones in the us-east-1 Region. Normally, the company must run no more than two instances at all times. However, the company wants to scale up to six instances each Friday to handle a regularly repeating increased workload. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a reminder in Amazon EventBridge to scale the instances.",
      "B": "Create an Auto Scaling group that has a scheduled action.",
      "C": "Create an Auto Scaling group that uses manual scaling.",
      "D": "Create an Auto Scaling group that uses automatic scaling."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一项大型工作负载，该工作负载每周五晚上运行。 该工作负载在 us-east-1 区域的两个可用区中的 Amazon EC2 实例上运行。 通常情况下，该公司在任何时候都不能运行超过两个实例。 但是，该公司希望在每个周五扩展到六个实例，以处理定期重复增加的工作负载。 哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 Amazon EventBridge 中创建一个提醒，以扩展实例。",
      "B": "创建一个具有计划操作的 Auto Scaling 组。",
      "C": "创建一个使用手动扩展的 Auto Scaling 组。",
      "D": "创建一个使用自动扩展的 Auto Scaling 组。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察使用 Auto Scaling 组实现周期性实例数量伸缩，以及选择合适的伸缩策略来满足特定时间段的负载需求。",
      "why_correct": "选项 B 创建了一个具有计划操作的 Auto Scaling 组，能够满足每周五自动扩展至六个实例的需求。计划操作允许在指定时间修改 Auto Scaling 组的期望容量，从而实现周期性的伸缩行为。这种方法完全符合题目的需求，并且运维开销最低。",
      "why_wrong": "选项 A 使用 Amazon EventBridge 触发扩展操作，虽然可以实现周期性调度，但其操作对象需要依赖于其他服务，实现复杂，运维开销较高。选项 C 使用手动扩展的 Auto Scaling 组，不符合自动化的要求，需要手动干预，无法满足题目中周五自动伸缩的需求。选项 D 使用自动扩展的 Auto Scaling 组，虽然能够根据负载动态伸缩，但无法直接满足题目中固定时间、固定实例数量的伸缩需求，需要配合额外的配置来实现，实现相对复杂，且容易造成实例数量的不确定性，不符合题意。"
    },
    "related_terms": [
      "Amazon EC2",
      "us-east-1",
      "Amazon EventBridge",
      "Auto Scaling Group"
    ]
  },
  {
    "id": 571,
    "topic": "1",
    "question_en": "A company is creating a REST API. The company has strict requirements for the use of TLS. The company requires TLSv1.3 on the API endpoints. The company also requires a specific public third-party certificate authority (CA) to sign the TLS certificate. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use a local machine to create a certificate that is signed by the third-party CImport the certificate into AWS Certificate Manager (ACM). Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.",
      "B": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an HTTP API in Amazon API Gateway with a custom domain. Configure the custom domain to use the certificate.",
      "C": "Use AWS Certificate Manager (ACM) to create a certificate that is signed by the third-party CA. Import the certificate into AWS Certificate Manager (ACM). Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate.",
      "D": "Create a certificate in AWS Certificate Manager (ACM) that is signed by the third-party CA. Create an AWS Lambda function with a Lambda function URL. Configure the Lambda function URL to use the certificate."
    },
    "correct_answer": "A",
    "vote_percentage": "72%",
    "question_cn": "一家公司正在创建 REST API。该公司对 TLS 的使用有严格的要求。该公司要求在 API 终端节点上使用 TLSv1.3。该公司还要求使用特定的公共第三方证书颁发机构 (CA) 来签署 TLS 证书。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用本地机器创建由第三方 CA 签名的证书。将证书导入 AWS Certificate Manager (ACM)。在 Amazon API Gateway 中创建一个带有自定义域名的 HTTP API。配置自定义域名以使用该证书。",
      "B": "在 AWS Certificate Manager (ACM) 中创建由第三方 CA 签名的证书。在 Amazon API Gateway 中创建一个带有自定义域名的 HTTP API。配置自定义域名以使用该证书。",
      "C": "使用 AWS Certificate Manager (ACM) 创建由第三方 CA 签名的证书。将证书导入 AWS Certificate Manager (ACM)。创建一个带有 Lambda 函数 URL 的 AWS Lambda 函数。配置 Lambda 函数 URL 以使用该证书。",
      "D": "在 AWS Certificate Manager (ACM) 中创建由第三方 CA 签名的证书。创建一个带有 Lambda 函数 URL 的 AWS Lambda 函数。配置 Lambda 函数 URL 以使用该证书。"
    },
    "tags": [
      "Amazon API Gateway",
      "AWS Certificate Manager (ACM)",
      "TLS",
      "HTTP API",
      "Lambda",
      "Lambda Function URL",
      "TLSv1.3",
      "Custom Domain Name",
      "Certificate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 72%），解析仅供参考。】\n\n考查在 Amazon API Gateway 中配置 TLSv1.3 和使用自定义证书。与 ACM、自定义域名、HTTP API 以及证书导入流程相关。",
      "why_correct": "选项 A 满足所有要求。首先，需要使用第三方 CA 签名的证书，而 ACM 无法直接创建这种证书，因此需要在本地创建。然后，将证书导入 ACM。最后，在 API Gateway 中创建 HTTP API，并配置自定义域名，将该证书关联到 API 终端节点。API Gateway 支持配置 TLSv1.3，并且通过自定义域名可以使用导入的证书。",
      "why_wrong": {
        "B": "选项 B 无法满足要求，因为 ACM 本身不提供直接创建由第三方 CA 签名的证书的功能。ACM 仅支持请求由 AWS 托管的证书或导入已存在的证书。因此，创建第三方 CA 签名的证书需要在 ACM 之外完成。",
        "C": "选项 C 无法满足需求，因为 Lambda 函数 URL 不支持使用自定义域名和导入的证书。Lambda 函数 URL 主要用于直接访问 Lambda 函数，其安全配置选项有限，不包含自定义 TLS 配置。此外，即使 ACM 证书导入后，也无法在 Lambda 函数 URL 上使用。",
        "D": "选项 D 同样无法满足需求，因为 Lambda 函数 URL 缺乏配置自定义域名的能力，并且无法使用第三方 CA 签名的证书。Lambda 函数 URL 的设计目的并非用于提供自定义域名和高级 TLS 配置。"
      }
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Certificate Manager (ACM)",
      "TLS",
      "HTTP API",
      "Lambda",
      "CA",
      "TLSv1.3",
      "Lambda Function URL",
      "Custom Domain Name",
      "Certificate"
    ]
  },
  {
    "id": 572,
    "topic": "1",
    "question_en": "A company runs an application on AWS. The application receives inconsistent amounts of usage. The application uses AWS Direct Connect to connect to an on-premises MySQL-compatible database. The on-premises database consistently uses a minimum of 2 GiB of memory. The company wants to migrate the on-premises database to a managed AWS service. The company wants to use auto scaling capabilities to manage unexpected workload increases. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Provision an Amazon DynamoDB database with default read and write capacity settings.",
      "B": "Provision an Amazon Aurora database with a minimum capacity of 1 Aurora capacity unit (ACU).",
      "C": "Provision an Amazon Aurora Serverless v2 database with a minimum capacity of 1 Aurora capacity unit (ACU).",
      "D": "Provision an Amazon RDS for MySQL database with 2 GiB of memory."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行应用程序。该应用程序接收到的使用量不一致。该应用程序使用 AWS Direct Connect 连接到本地 MySQL 兼容数据库。本地数据库始终使用至少 2 GiB 的内存。该公司希望将本地数据库迁移到托管 AWS 服务。该公司希望使用自动伸缩功能来管理意外的工作负载增加。哪种解决方案以最小的管理开销满足这些要求？",
    "options_cn": {
      "A": "配置一个 Amazon DynamoDB 数据库，使用默认的读取和写入容量设置。",
      "B": "配置一个 Amazon Aurora 数据库，最小容量为 1 个 Aurora 容量单元 (ACU)。",
      "C": "配置一个 Amazon Aurora Serverless v2 数据库，最小容量为 1 个 Aurora 容量单元 (ACU)。",
      "D": "配置一个 Amazon RDS for MySQL 数据库，使用 2 GiB 的内存。"
    },
    "tags": [
      "Amazon Aurora",
      "Aurora Serverless v2",
      "AWS Direct Connect",
      "Amazon RDS",
      "Amazon DynamoDB",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查数据库迁移策略，以及针对可变负载的数据库选型，重点在于对不同 Aurora 版本的理解和选择。同时，也考察了与本地数据库、Direct Connect 的关联。",
      "why_correct": "Amazon Aurora Serverless v2 数据库是专为适应不规则、不可预测工作负载而设计的。它能够根据应用程序的实际使用情况自动扩展和缩减计算资源，无需预先配置容量。对于使用量不一致的应用程序，Serverless v2 提供了最大的灵活性和成本效益，同时满足了自动伸缩的需求。Aurora Serverless v2 可以根据需要自动扩展，无需手动管理数据库的容量，从而降低了管理开销。",
      "why_wrong": "A 选项，Amazon DynamoDB 是一种 NoSQL 数据库，不兼容 MySQL，无法直接替换现有的 MySQL 数据库。此外，DynamoDB 的使用场景更侧重于读写密集型应用，而非关系型数据库。B 选项，Amazon Aurora 数据库，虽然提供了性能和可扩展性，但若选择预置容量，需要手动配置容量并且会为未使用的容量付费。这与最小化管理开销和自动伸缩的需求不符。D 选项，Amazon RDS for MySQL 数据库需要预先配置 2 GiB 内存，并不能根据负载变化自动伸缩，无法满足题目中对弹性伸缩和管理开销的要求。"
    },
    "related_terms": [
      "Amazon Aurora",
      "Aurora Serverless v2",
      "MySQL",
      "Amazon DynamoDB",
      "AWS Direct Connect",
      "Amazon RDS for MySQL",
      "ACU",
      "GiB"
    ]
  },
  {
    "id": 573,
    "topic": "1",
    "question_en": "A company wants to use an event-driven programming model with AWS Lambda. The company wants to reduce startup latency for Lambda functions that run on Java 11. The company does not have strict latency requirements for the applications. The company wants to reduce cold starts and outlier latencies when a function scales up. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure Lambda provisioned concurrency.",
      "B": "Increase the timeout of the Lambda functions.",
      "C": "Increase the memory of the Lambda functions.",
      "D": "Configure Lambda SnapStart."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望使用基于事件的编程模型与 AWS Lambda。该公司希望减少在 Java 11 上运行的 Lambda 函数的启动延迟。该公司对应用程序的延迟没有严格的要求。该公司希望在函数扩展时减少冷启动和异常延迟。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置 Lambda 预置并发。",
      "B": "增加 Lambda 函数的超时时间。",
      "C": "增加 Lambda 函数的内存。",
      "D": "配置 Lambda SnapStart。"
    },
    "tags": [
      "AWS Lambda",
      "Lambda performance",
      "Lambda cold start"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查了降低 Lambda 函数冷启动延迟的解决方案，以及如何选择最具成本效益的方案。",
      "why_correct": "配置 Lambda SnapStart 可以在函数发布时创建快照，并将快照复用于后续函数调用，从而显著降低冷启动延迟。SnapStart 针对 Java 11 运行时进行了优化，能够满足对延迟不敏感的应用程序需求，并且在函数扩展时减少冷启动和异常延迟，是成本效益最高的方案。",
      "why_wrong": "配置 Lambda 预置并发会为函数预先初始化并发执行环境，虽然可以降低延迟，但会产生持续的成本，对于延迟不敏感的应用程序来说，成本较高。增加 Lambda 函数的超时时间并不能解决冷启动问题，反而可能增加调用失败的风险。增加 Lambda 函数的内存可能会略微改善性能，但对冷启动延迟的改善有限，且会增加成本，并非最具成本效益的方案。"
    },
    "related_terms": [
      "AWS Lambda",
      "Java 11",
      "Lambda SnapStart",
      "Lambda预置并发",
      "Cold Start",
      "Timeout"
    ]
  },
  {
    "id": 574,
    "topic": "1",
    "question_en": "A financial services company launched a new application that uses an Amazon RDS for MySQL database. The company uses the application to track stock market trends. The company needs to operate the application for only 2 hours at the end of each week. The company needs to optimize the cost of running the database. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Migrate the existing RDS for MySQL database to an Aurora Serverless v2 MySQL database cluster.",
      "B": "Migrate the existing RDS for MySQL database to an Aurora MySQL database cluster.",
      "C": "Migrate the existing RDS for MySQL database to an Amazon EC2 instance that runs MySQL. Purchase an instance reservation for the EC2 instance.",
      "D": "Migrate the existing RDS for MySQL database to an Amazon Elastic Container Service (Amazon ECS) cluster that uses MySQL container images to run tasks."
    },
    "correct_answer": "A",
    "vote_percentage": "88%",
    "question_cn": "一家金融服务公司推出了一款使用 Amazon RDS for MySQL 数据库的新应用程序。该公司使用该应用程序跟踪股票市场趋势。该公司每周只需要在周末运行该应用程序 2 小时。该公司需要优化运行数据库的成本。哪种解决方案可以最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "将现有的 RDS for MySQL 数据库迁移到 Aurora Serverless v2 MySQL 数据库集群。",
      "B": "将现有的 RDS for MySQL 数据库迁移到 Aurora MySQL 数据库集群。",
      "C": "将现有的 RDS for MySQL 数据库迁移到运行 MySQL 的 Amazon EC2 实例。购买 EC2 实例的预留。",
      "D": "将现有的 RDS for MySQL 数据库迁移到使用 MySQL 容器镜像运行任务的 Amazon Elastic Container Service (Amazon ECS) 集群。"
    },
    "tags": [
      "Amazon RDS",
      "Amazon Aurora",
      "Aurora Serverless",
      "Amazon EC2",
      "Amazon ECS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 88%），解析仅供参考。】\n\n考察成本优化，重点在于数据库的按需使用以及 Serverless 架构的适用性，需要结合 Aurora Serverless、EC2 预留实例和 ECS 容器实例的特点进行分析。",
      "why_correct": "Aurora Serverless v2 数据库集群非常适合这种使用模式。Aurora Serverless v2 能够根据应用程序的需求自动扩展和缩减计算资源，并且按秒付费。由于应用程序仅在周末运行 2 小时，Aurora Serverless v2 提供了最高的成本效益，因为它只在数据库使用时才收费，这与题目的需求完全匹配。",
      "why_wrong": "选项 B 错误，Aurora MySQL 集群虽然性能优异，但它不是 Serverless 架构，需要持续运行实例，即使在不使用时也会产生费用，不符合题目的成本优化需求。选项 C 错误，虽然 EC2 实例可以通过预留实例降低成本，但仍需要持续为实例付费，即便在应用程序未运行时也是如此。此外，管理 EC2 上的数据库需要手动管理数据库的维护、备份等，增加了运维负担。选项 D 错误，Amazon ECS 虽然适合运行容器化的应用程序，但需要持续运行 ECS 集群和数据库，即使在周末运行 2 小时的情况下，ECS 实例和数据库实例的成本依然较高，无法实现成本优化。"
    },
    "related_terms": [
      "Aurora Serverless v2",
      "Aurora MySQL",
      "Amazon EC2",
      "MySQL",
      "Amazon ECS",
      "Amazon RDS for MySQL"
    ]
  },
  {
    "id": 575,
    "topic": "1",
    "question_en": "A company deploys its applications on Amazon Elastic Kubernetes Service (Amazon EKS) behind an Application Load Balancer in an AWS Region. The application needs to store data in a PostgreSQL database engine. The company wants the data in the database to be highly available. The company also needs increased capacity for read workloads. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create an Amazon DynamoDB database table configured with global tables.",
      "B": "Create an Amazon RDS database with Multi-AZ deployments.",
      "C": "Create an Amazon RDS database with Multi-AZ DB cluster deployment.",
      "D": "Create an Amazon RDS database configured with cross-Region read replicas."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 AWS 区域的 Application Load Balancer 后面的 Amazon Elastic Kubernetes Service (Amazon EKS) 上部署其应用程序。该应用程序需要在 PostgreSQL 数据库引擎中存储数据。该公司希望数据库中的数据具有高可用性。该公司还需要增加读取工作负载的容量。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "创建一个配置了全局表的 Amazon DynamoDB 数据库表。",
      "B": "创建一个具有多可用区部署的 Amazon RDS 数据库。",
      "C": "创建一个具有多可用区数据库集群部署的 Amazon RDS 数据库。",
      "D": "创建一个配置了跨区域读取副本的 Amazon RDS 数据库。"
    },
    "tags": [
      "Amazon RDS",
      "Multi-AZ",
      "Read Replicas",
      "Amazon EKS",
      "PostgreSQL",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察了在 Amazon EKS 上运行的应用程序对数据库高可用性和读取扩展性的需求，以及 RDS 数据库的配置选项。",
      "why_correct": "选项 C 提供了最佳的解决方案。通过创建具有多可用区数据库集群部署的 Amazon RDS 数据库，可以实现数据库的高可用性，因为数据会在多个可用区之间复制。同时，RDS 数据库集群通常支持读取副本，可以轻松增加读取工作负载的容量，提升运营效率。",
      "why_wrong": "选项 A 错误，因为 DynamoDB 主要用于 NoSQL 数据，不适用于 PostgreSQL 数据库。选项 B 错误，虽然多可用区部署能提供高可用性，但无法直接增加读取工作负载的容量，需要进一步配置读取副本。选项 D 错误，跨区域读取副本会增加延迟，且运营效率低于 RDS 数据库集群提供的本地可用区内的读取副本机制，不一定能满足读取性能需求。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Application Load Balancer",
      "PostgreSQL",
      "Amazon RDS",
      "Multi-AZ deployment",
      "Read Replica",
      "Amazon DynamoDB",
      "Database cluster",
      "Multi-AZ database cluster"
    ]
  },
  {
    "id": 576,
    "topic": "1",
    "question_en": "A company is building a RESTful serverless web application on AWS by using Amazon API Gateway and AWS Lambda. The users of this web application will be geographically distributed, and the company wants to reduce the latency of API requests to these users. Which type of endpoint should a solutions architect use to meet these requirements?",
    "options_en": {
      "A": "Private endpoint",
      "B": "Regional endpoint",
      "C": "Interface VPC endpoint",
      "D": "Edge-optimized endpoint"
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上构建一个使用 Amazon API Gateway 和 AWS Lambda 的无服务器 RESTful Web 应用程序。 该 Web 应用程序的用户将分布在不同地理位置，并且公司希望减少这些用户 API 请求的延迟。 解决方案架构师应该使用哪种类型的端点来满足这些要求？",
    "options_cn": {
      "A": "私有端点",
      "B": "区域端点",
      "C": "接口 VPC 端点",
      "D": "边缘优化端点"
    },
    "tags": [
      "Amazon API Gateway",
      "API Gateway",
      "Lambda",
      "Edge-optimized endpoint",
      "Region endpoint",
      "Private endpoint",
      "Interface VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查 API Gateway 边缘优化端点的应用场景；与 API Gateway 各种端点的选型及延迟优化相关。",
      "why_correct": "边缘优化端点（Edge-optimized endpoints）是 API Gateway 提供的专门为全球用户设计的端点类型。它利用 Amazon CloudFront 作为内容分发网络（CDN），将 API 部署到全球多个 CloudFront 边缘站点，从而减少了用户与 API Gateway 之间的物理距离，显著降低了延迟。由于题中提到用户分布在不同地理位置，且需要减少 API 请求的延迟，边缘优化端点是最合适的解决方案。",
      "why_wrong": "私有端点（Private endpoints）用于在 VPC 内部访问 API Gateway，仅允许来自 VPC 内部的流量，无法满足全球用户的需求，与题目要求的减少延迟、服务全球用户相悖。区域端点（Region endpoints）将 API 部署到特定 AWS 区域，虽然可以提供较低的延迟，但仅限于该区域内的用户，无法有效优化全球用户的访问延迟。接口 VPC 端点（Interface VPC endpoints）允许通过 VPC 访问 API Gateway，但它主要用于 VPC 内部的连接，不具备全球加速的能力，同样无法满足全球用户的需求，且增加了配置复杂性。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "API Gateway",
      "Lambda",
      "CloudFront",
      "VPC"
    ]
  },
  {
    "id": 577,
    "topic": "1",
    "question_en": "A company uses an Amazon CloudFront distribution to serve content pages for its website. The company needs to ensure that clients use a TLS certificate when accessing the company's website. The company wants to automate the creation and renewal of the TLS certificates. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Use a CloudFront security policy to create a certificate.",
      "B": "Use a CloudFront origin access control (OAC) to create a certificate.",
      "C": "Use AWS Certificate Manager (ACM) to create a certificate. Use DNS validation for the domain.",
      "D": "Use AWS Certificate Manager (ACM) to create a certificate. Use email validation for the domain."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon CloudFront 分发来为其网站提供内容页面。该公司需要确保客户在使用 TLS 证书访问该公司网站时。该公司希望自动化 TLS 证书的创建和续订。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "使用 CloudFront 安全策略创建证书。",
      "B": "使用 CloudFront 源访问控制 (OAC) 创建证书。",
      "C": "使用 AWS Certificate Manager (ACM) 创建证书。对域使用 DNS 验证。",
      "D": "使用 AWS Certificate Manager (ACM) 创建证书。对域使用电子邮件验证。"
    },
    "tags": [
      "CloudFront",
      "ACM",
      "TLS",
      "DNS validation",
      "Email validation",
      "SSL/TLS certificates"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查 CloudFront 搭配 ACM 实现 TLS 证书的自动化创建和续订，以及 DNS 验证和邮件验证的区别。",
      "why_correct": "AWS Certificate Manager (ACM) 提供了免费的 SSL/TLS 证书，可用于 CloudFront 分发。通过 ACM，可以自动化证书的创建、续订和管理。对于域使用 DNS 验证，可以实现全自动的证书验证和续订流程，无需手动干预。",
      "why_wrong": "选项 A 错误，CloudFront 安全策略与证书创建无关，主要用于配置安全协议和加密套件。选项 B 错误，CloudFront 源访问控制（OAC）用于控制对源的访问，与 TLS 证书的创建无关。选项 D 错误，虽然 ACM 也可以通过电子邮件验证，但 DNS 验证更具自动化优势，减少了手动操作，并且在实践中更易于管理和自动化，因此运营效率相对较低。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "TLS",
      "ACM",
      "AWS Certificate Manager",
      "DNS",
      "CloudFront Security Policy",
      "CloudFront Origin Access Control (OAC)",
      "SSL"
    ]
  },
  {
    "id": 578,
    "topic": "1",
    "question_en": "A company deployed a serverless application that uses Amazon DynamoDB as a database layer. The application has experienced a large increase in users. The company wants to improve database response time from milliseconds to microseconds and to cache requests to the database. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use DynamoDB Accelerator (DAX).",
      "B": "Migrate the database to Amazon Redshift.",
      "C": "Migrate the database to Amazon RDS.",
      "D": "Use Amazon ElastiCache for Redis."
    },
    "correct_answer": "A",
    "vote_percentage": "88%",
    "question_cn": "一家公司部署了一个使用 Amazon DynamoDB 作为数据库层的无服务器应用程序。该应用程序的用户数量大幅增加。公司希望将数据库响应时间从毫秒缩短到微秒，并缓存对数据库的请求。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 DynamoDB Accelerator (DAX)。",
      "B": "将数据库迁移到 Amazon Redshift。",
      "C": "将数据库迁移到 Amazon RDS。",
      "D": "使用 Amazon ElastiCache for Redis。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Caching"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 88%），解析仅供参考。】\n\n本题考查 DynamoDB 的性能优化和缓存策略，以及不同数据库服务的适用场景。重点在于如何通过缓存来降低数据库延迟，并选择开销最小的方案。与 ElastiCache、Redshift、RDS 的选型与对比相关。",
      "why_correct": "DynamoDB Accelerator (DAX) 是一种完全兼容 DynamoDB 的缓存服务，可以显著减少读取操作的延迟，将响应时间从毫秒缩短到微秒级别。DAX 专门为 DynamoDB 设计，可以自动处理缓存的失效和数据同步，无需手动管理缓存基础设施。它与 DynamoDB 集成，部署简单，运维开销低，满足了题目的要求，即缩短响应时间并缓存请求，且运营开销最小。",
      "why_wrong": "B. 将数据库迁移到 Amazon Redshift：Amazon Redshift 是一种数据仓库服务，主要用于大规模数据分析，而非用于支持低延迟的在线事务处理 (OLTP) 应用，因此不适合用于解决数据库响应时间问题。C. 将数据库迁移到 Amazon RDS：Amazon RDS 是一种托管关系型数据库服务，例如 MySQL, PostgreSQL, Oracle 等，与 DynamoDB 相比，RDS 的设计目标和性能特点不同，迁移到 RDS 并不能直接解决低延迟和缓存的需求，且需要修改应用程序的数据库访问代码，增加了运营开销。D. 使用 Amazon ElastiCache for Redis：虽然 ElastiCache for Redis 可以用于缓存数据，以提高性能，但它需要手动管理缓存的同步和失效，并且需要修改应用程序代码来使用缓存，不如 DAX 方便，且 DAX 是专门为 DynamoDB 设计的缓存解决方案，开销更低，易于部署和管理。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon Redshift",
      "Amazon RDS",
      "Amazon ElastiCache for Redis",
      "Redis"
    ]
  },
  {
    "id": 579,
    "topic": "1",
    "question_en": "A company runs an application that uses Amazon RDS for PostgreSQL. The application receives trafic only on weekdays during business hours. The company wants to optimize costs and reduce operational overhead based on this usage. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use the Instance Scheduler on AWS to configure start and stop schedules.",
      "B": "Turn off automatic backups. Create weekly manual snapshots of the database.",
      "C": "Create a custom AWS Lambda function to start and stop the database based on minimum CPU utilization.",
      "D": "Purchase All Upfront reserved DB instances."
    },
    "correct_answer": "A",
    "vote_percentage": "95%",
    "question_cn": "一家公司运行一个使用 Amazon RDS for PostgreSQL 的应用程序。该应用程序仅在工作日的营业时间内接收流量。该公司希望根据此使用情况优化成本并减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS 上的 Instance Scheduler 配置启动和停止计划。",
      "B": "关闭自动备份。每周手动拍摄数据库快照。",
      "C": "创建一个自定义 AWS Lambda 函数，根据最低 CPU 利用率启动和停止数据库。",
      "D": "购买 All Upfront 预留数据库实例。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "AWS Lambda",
      "CPU Utilization",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 95%），解析仅供参考。】\n\n考查如何通过资源调度和计费优化，降低 Amazon RDS for PostgreSQL 数据库的成本和运营开销，以满足特定时间段的流量需求。",
      "why_correct": "使用 AWS Instance Scheduler 可以在指定的时间段自动启动和停止 RDS 数据库实例。这允许在非营业时间关闭数据库，从而节省计算和存储成本。这种方法满足了按需使用的要求，并降低了运营开销。",
      "why_wrong": "选项 B 关闭自动备份，虽然可以节省存储成本，但会增加数据丢失的风险，并且增加了手动操作的开销，这与减少运营开销的目标相悖。选项 C 使用 CPU 利用率来启动和停止数据库，可能无法准确反映流量情况，且创建和维护 Lambda 函数本身也增加了开销。选项 D 购买 All Upfront 预留实例，虽然可以降低整体成本，但如果数据库在非营业时间关闭，则无法充分利用已预留的实例，从而无法最大化成本优化。"
    },
    "related_terms": [
      "Amazon RDS for PostgreSQL",
      "AWS Instance Scheduler",
      "RDS",
      "CPU",
      "Lambda",
      "All Upfront",
      "Database snapshot"
    ]
  },
  {
    "id": 580,
    "topic": "1",
    "question_en": "A company uses locally attached storage to run a latency-sensitive application on premises. The company is using a lift and shift method to move the application to the AWS Cloud. The company does not want to change the application architecture. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for Lustre file system to run the application.",
      "B": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP2 volume to run the application.",
      "C": "Configure an Auto Scaling group with an Amazon EC2 instance. Use an Amazon FSx for OpenZFS file system to run the application.",
      "D": "Host the application on an Amazon EC2 instance. Use an Amazon Elastic Block Store (Amazon EBS) GP3 volume to run the application."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用本地连接的存储在其场所运行一个对延迟敏感的应用程序。该公司正在使用一种迁移方法将应用程序移动到 AWS 云。该公司不想更改应用程序架构。哪种解决方案可以最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EC2 实例配置一个 Auto Scaling 组。使用 Amazon FSx for Lustre 文件系统来运行该应用程序。",
      "B": "在 Amazon EC2 实例上托管应用程序。使用 Amazon Elastic Block Store (Amazon EBS) GP2 卷来运行该应用程序。",
      "C": "使用 Amazon EC2 实例配置一个 Auto Scaling 组。使用 Amazon FSx for OpenZFS 文件系统来运行该应用程序。",
      "D": "在 Amazon EC2 实例上托管应用程序。使用 Amazon Elastic Block Store (Amazon EBS) GP3 卷来运行该应用程序。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon EBS",
      "Amazon FSx",
      "Auto Scaling",
      "Cost Optimization",
      "Latency"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查在不改变应用程序架构的前提下，将本地延迟敏感型应用迁移至 AWS 云，并选择最具成本效益的存储方案。",
      "why_correct": "选项 D 提供了最具成本效益的解决方案。使用 Amazon EBS GP3 卷能满足对延迟敏感型应用的需求，并且在成本效益上优于其他选项。 GP3 卷提供了可独立配置的 IOPS 和吞吐量，有助于优化性能，且通常比 GP2 卷和 FSx 解决方案更经济。",
      "why_wrong": "选项 A 和 C 涉及 Amazon FSx 文件系统，虽然它们提供了高性能，但成本较高，不符合最具成本效益的要求。选项 A 使用 Lustre 文件系统，适用于需要高吞吐量的工作负载，但对延迟敏感型应用来说可能并非最佳选择。 选项 B 使用 GP2 卷，虽然满足延迟需求，但成本和性能不如 GP3 卷。由于需要 Auto Scaling，所以选项 B 并不能最有效满足需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon FSx for Lustre",
      "Amazon FSx for OpenZFS",
      "Amazon EBS",
      "GP2",
      "GP3",
      "IOPS",
      "吞吐量"
    ]
  },
  {
    "id": 581,
    "topic": "1",
    "question_en": "A company runs a stateful production application on Amazon EC2 instances. The application requires at least two EC2 instances to always be running. A solutions architect needs to design a highly available and fault-tolerant architecture for the application. The solutions architect creates an Auto Scaling group of EC2 instances. Which set of additional steps should the solutions architect take to meet these requirements?",
    "options_en": {
      "A": "Set the Auto Scaling group's minimum capacity to two. Deploy one On-Demand Instance in one Availability Zone and one On-Demand Instance in a second Availability Zone.",
      "B": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two On-Demand Instances in a second Availability Zone.",
      "C": "Set the Auto Scaling group's minimum capacity to two. Deploy four Spot Instances in one Availability Zone.",
      "D": "Set the Auto Scaling group's minimum capacity to four. Deploy two On-Demand Instances in one Availability Zone and two Spot Instances in a second Availability Zone."
    },
    "correct_answer": "B",
    "vote_percentage": "70%",
    "question_cn": "一家公司在 Amazon EC2 实例上运行一个有状态的生产应用程序。该应用程序需要至少两个 EC2 实例始终运行。一位解决方案架构师需要为该应用程序设计一个高可用性和容错的架构。该解决方案架构师创建了一个 EC2 实例的 Auto Scaling 组。解决方案架构师应该采取哪一组额外的步骤来满足这些要求？",
    "options_cn": {
      "A": "将 Auto Scaling 组的最小容量设置为 2。在一个可用区中部署一个按需实例，在第二个可用区中部署一个按需实例。",
      "B": "将 Auto Scaling 组的最小容量设置为 4。在一个可用区中部署两个按需实例，在第二个可用区中部署两个按需实例。",
      "C": "将 Auto Scaling 组的最小容量设置为 2。在一个可用区中部署四个 Spot 实例。",
      "D": "将 Auto Scaling 组的最小容量设置为 4。在一个可用区中部署两个按需实例，在第二个可用区中部署两个 Spot 实例。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "Availability Zone",
      "Spot Instances",
      "On-Demand Instances",
      "High Availability",
      "Fault Tolerance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 70%），解析仅供参考。】\n\n考察使用 Auto Scaling 组实现 EC2 实例的高可用性和容错能力，以及如何设置最小实例数量和跨可用区的部署。",
      "why_correct": "选项 B 满足了高可用性和容错性的需求。将 Auto Scaling 组的最小容量设置为 4，确保了应用程序至少有 4 个实例运行。在两个不同的可用区中部署实例，即使其中一个可用区发生故障，应用程序仍可在另一个可用区中继续运行。",
      "why_wrong": "选项 A 错误，因为仅部署了两个实例，无法满足应用程序需要至少两个实例始终运行的需求。选项 C 错误，因为仅在一个可用区中部署所有 Spot 实例，会降低可用性。选项 D 错误，虽然设置了 4 个实例，但使用了 Spot 实例，且没有说明 Spot 实例的部署策略，可能无法满足应用程序的持续运行需求；并且，虽然部署了 2 个可用区，但是，如果 Spot 实例因为竞价原因被回收，依然可能导致实例数量不足。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Availability Zone",
      "Spot Instance",
      "On-Demand Instance"
    ]
  },
  {
    "id": 582,
    "topic": "1",
    "question_en": "An ecommerce company uses Amazon Route 53 as its DNS provider. The company hosts its website on premises and in the AWS Cloud. The company's on-premises data center is near the us-west-1 Region. The company uses the eu-central-1 Region to host the website. The company wants to minimize load time for the website as much as possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up a geolocation routing policy. Send the trafic that is near us-west-1 to the on-premises data center. Send the trafic that is near eu-central-1 to eu-central-1.",
      "B": "Set up a simple routing policy that routes all trafic that is near eu-central-1 to eu-central-1 and routes all trafic that is near the on- premises datacenter to the on-premises data center.",
      "C": "Set up a latency routing policy. Associate the policy with us-west-1.",
      "D": "Set up a weighted routing policy. Split the trafic evenly between eu-central-1 and the on-premises data center."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家电子商务公司使用 Amazon Route 53 作为其 DNS 提供商。该公司在其本地和 AWS 云中托管其网站。该公司的本地数据中心靠近 us-west-1 区域。该公司使用 eu-central-1 区域来托管网站。该公司希望尽可能减少网站的加载时间。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置地理位置路由策略。将靠近 us-west-1 的流量发送到本地数据中心。将靠近 eu-central-1 的流量发送到 eu-central-1。",
      "B": "设置一个简单路由策略，将所有靠近 eu-central-1 的流量路由到 eu-central-1，并将所有靠近本地数据中心的流量路由到本地数据中心。",
      "C": "设置一个延迟路由策略。将该策略与 us-west-1 关联。",
      "D": "设置加权路由策略。在 eu-central-1 和本地数据中心之间平均分配流量。"
    },
    "tags": [
      "Amazon Route 53",
      "Routing Policies",
      "Geolocation Routing",
      "Latency-Based Routing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n本题考查 Route 53 的路由策略，特别是如何基于用户地理位置将流量导向最佳的资源。与地理位置路由（Geolocation Routing）、延迟路由（Latency-Based Routing）和加权路由（Weighted Routing）的选型和对比相关。",
      "why_correct": "选项 A 采用地理位置路由策略，可以基于用户的地理位置将流量路由到不同的资源。由于用户在 us-west-1 附近，则将流量发送到本地数据中心，而用户在 eu-central-1 附近，则发送到 eu-central-1 区域的网站。这样可以最小化网站加载时间，因为用户访问的是距离他们最近的服务器。",
      "why_wrong": "选项 B 采用简单路由策略，无法根据用户地理位置进行智能路由，所有流量都被导向特定资源，无法优化用户体验。选项 C 采用延迟路由策略，虽然可以基于延迟选择最近的资源，但只与一个区域关联，无法实现本地数据中心和 eu-central-1 之间的流量分发。选项 D 采用加权路由策略，在 eu-central-1 和本地数据中心之间平均分配流量，无法考虑用户的地理位置，无法优化用户的访问延迟，不符合题目需求。"
    },
    "related_terms": [
      "Amazon Route 53",
      "DNS",
      "Geolocation Routing",
      "Latency-Based Routing",
      "Weighted Routing",
      "us-west-1",
      "eu-central-1"
    ]
  },
  {
    "id": 583,
    "topic": "1",
    "question_en": "A company has 5 PB of archived data on physical tapes. The company needs to preserve the data on the tapes for another 10 years for compliance purposes. The company wants to migrate to AWS in the next 6 months. The data center that stores the tapes has a 1 Gbps uplink internet connectivity. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Read the data from the tapes on premises. Stage the data in a local NFS storage. Use AWS DataSync to migrate the data to Amazon S3 Glacier Flexible Retrieval.",
      "B": "Use an on-premises backup application to read the data from the tapes and to write directly to Amazon S3 Glacier Deep Archive.",
      "C": "Order multiple AWS Snowball devices that have Tape Gateway. Copy the physical tapes to virtual tapes in Snowball. Ship the Snowball devices to AWS. Create a lifecycle policy to move the tapes to Amazon S3 Glacier Deep Archive.",
      "D": "Configure an on-premises Tape Gateway. Create virtual tapes in the AWS Cloud. Use backup software to copy the physical tape to the virtual tape."
    },
    "correct_answer": "C",
    "vote_percentage": "97%",
    "question_cn": "一家公司拥有 5 PB 的已存档数据，这些数据存储在物理磁带上。该公司需要将磁带上的数据保留 10 年，以符合法规遵从性要求。该公司希望在接下来的 6 个月内迁移到 AWS。存储磁带的数据中心具有 1 Gbps 的上行互联网连接。哪种解决方案可以最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "从本地磁带中读取数据。将数据暂存在本地 NFS 存储中。使用 AWS DataSync 将数据迁移到 Amazon S3 Glacier Flexible Retrieval。",
      "B": "使用本地备份应用程序从磁带中读取数据，并直接写入 Amazon S3 Glacier Deep Archive。",
      "C": "订购多个具有 Tape Gateway 的 AWS Snowball 设备。将物理磁带复制到 Snowball 中的虚拟磁带。将 Snowball 设备运送到 AWS。创建生命周期策略以将磁带移动到 Amazon S3 Glacier Deep Archive。",
      "D": "配置本地 Tape Gateway。在 AWS 云中创建虚拟磁带。使用备份软件将物理磁带复制到虚拟磁带。"
    },
    "tags": [
      "AWS Snowball",
      "Tape Gateway",
      "Amazon S3 Glacier Deep Archive",
      "AWS DataSync",
      "Amazon S3 Glacier Flexible Retrieval"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 97%），解析仅供参考。】\n\n考查大体量数据的离线迁移方案选择，并考虑成本效益。涉及 AWS Snowball、Tape Gateway、S3 Glacier 存储类等服务的应用场景和优缺点对比。",
      "why_correct": "选项 C 提供了最具成本效益的解决方案，适用于大型数据迁移和长期数据归档的需求。 使用 AWS Snowball 设备可以实现数据的物理运输，绕过慢速的互联网连接限制。 将物理磁带数据复制到 Snowball 上的虚拟磁带中，再将 Snowball 设备运送到 AWS，最后通过生命周期策略将数据转移到 Amazon S3 Glacier Deep Archive，满足了法规遵从性要求，且 Glacier Deep Archive 提供了最低的存储成本。 这种方案既解决了数据量大、网络带宽限制的问题，又兼顾了长期存储的成本。",
      "why_wrong": "选项 A 的问题在于，使用 DataSync 迁移 5 PB 数据到 Glacier Flexible Retrieval 受到 1 Gbps 网络连接的限制，迁移时间过长，不符合 6 个月的迁移时间要求。此外，数据先存储在 NFS 存储中会增加额外成本。 选项 B 中，直接写入 Amazon S3 Glacier Deep Archive 可能会受到本地备份应用程序的性能限制，且同样受到网络带宽的限制。 选项 D 方案也依赖 1 Gbps 的网络连接，将数据通过 Tape Gateway 上传，同样会造成迁移时间过长的问题。此外，此方案没有考虑成本效益，Tape Gateway 本身及网络传输会产生费用。"
    },
    "related_terms": [
      "AWS Snowball",
      "AWS DataSync",
      "Amazon S3",
      "NFS",
      "Tape Gateway",
      "Amazon S3 Glacier Deep Archive",
      "Amazon S3 Glacier Flexible Retrieval",
      "1 Gbps"
    ]
  },
  {
    "id": 584,
    "topic": "1",
    "question_en": "A company is deploying an application that processes large quantities of data in parallel. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to prevent groups of nodes from sharing the same underlying hardware. Which networking solution meets these requirements?",
    "options_en": {
      "A": "Run the EC2 instances in a spread placement group.",
      "B": "Group the EC2 instances in separate accounts.",
      "C": "Configure the EC2 instances with dedicated tenancy.",
      "D": "Configure the EC2 instances with shared tenancy."
    },
    "correct_answer": "A",
    "vote_percentage": "82%",
    "question_cn": "一家公司正在部署一个应用程序，该应用程序并行处理大量数据。该公司计划使用 Amazon EC2 实例来处理工作负载。网络架构必须可配置，以防止节点组共享相同的底层硬件。哪种网络解决方案满足这些要求？",
    "options_cn": {
      "A": "在分散放置组中运行 EC2 实例。",
      "B": "将 EC2 实例分组到不同的账户中。",
      "C": "使用专有租用配置 EC2 实例。",
      "D": "使用共享租用配置 EC2 实例。"
    },
    "tags": [
      "Amazon EC2",
      "Placement Groups"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 82%），解析仅供参考。】\n\n考查 EC2 实例的网络架构配置，特别是关于实例放置的选项，以及它们如何影响底层硬件的隔离。与 Placement Groups 的类型、EC2 实例的租用模式相关。",
      "why_correct": "在分散放置组（Spread Placement Group）中运行 EC2 实例能够确保实例放置在不同的底层硬件上。这种方式能够最大限度地减少单个硬件故障对应用程序的影响，从而满足防止节点组共享相同底层硬件的需求。这提高了应用程序的可用性和弹性。",
      "why_wrong": "选项 B（将 EC2 实例分组到不同的账户中）并不能直接解决共享底层硬件的问题，它主要侧重于资源隔离和访问控制，与底层硬件隔离无关。选项 C（使用专有租用配置 EC2 实例）虽然提供硬件隔离，但隔离的粒度是物理服务器级别，而非实例级别的硬件，不一定能满足“节点组”的要求，而且价格较高。选项 D（使用共享租用配置 EC2 实例）不提供任何硬件隔离，实例可能与其它客户的实例共享相同的底层硬件，因此不符合题目要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "Placement Groups",
      "Spread Placement Group",
      "Dedicated tenancy",
      "Shared tenancy",
      "Instances"
    ]
  },
  {
    "id": 585,
    "topic": "1",
    "question_en": "A solutions architect is designing a disaster recovery (DR) strategy to provide Amazon EC2 capacity in a failover AWS Region. Business requirements state that the DR strategy must meet capacity in the failover Region. Which solution will meet these requirements?",
    "options_en": {
      "A": "Purchase On-Demand Instances in the failover Region.",
      "B": "Purchase an EC2 Savings Plan in the failover Region.",
      "C": "Purchase regional Reserved Instances in the failover Region.",
      "D": "Purchase a Capacity Reservation in the failover Region."
    },
    "correct_answer": "D",
    "vote_percentage": "94%",
    "question_cn": "一个解决方案架构师正在设计一个灾难恢复 (DR) 策略，以在故障转移 AWS 区域中提供 Amazon EC2 容量。 业务需求规定，DR 策略必须满足故障转移区域中的容量。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在故障转移区域购买按需实例。",
      "B": "在故障转移区域购买 EC2 储蓄计划。",
      "C": "在故障转移区域购买区域预留实例。",
      "D": "在故障转移区域购买容量预留。"
    },
    "tags": [
      "EC2",
      "DR",
      "CapacityReservation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 94%），解析仅供参考。】\n\n考查在故障转移区域中确保 EC2 容量的灾难恢复策略。",
      "why_correct": "容量预留 (Capacity Reservations) 允许在特定可用区中预留 EC2 实例的容量。当需要时，可以直接启动实例而无需担心容量不足的问题。这完全满足了题目的要求，确保了在故障转移区域的可用容量。",
      "why_wrong": "按需实例 (On-Demand Instances) 无法保证故障转移区域的可用容量，可能会因为资源不足导致故障转移失败。EC2 储蓄计划 (Savings Plans) 提供成本优化，但不能保证容量。区域预留实例 (Regional Reserved Instances) 提供成本优化和容量保障，但是实例类型和属性需要和预留实例匹配才能使用，灵活性不足，也不能确保所有类型的实例都有预留资源。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2",
      "Disaster Recovery (DR)",
      "AWS",
      "Capacity Reservations",
      "On-Demand Instances",
      "Savings Plans",
      "Regional Reserved Instances"
    ]
  },
  {
    "id": 586,
    "topic": "1",
    "question_en": "A company has five organizational units (OUs) as part of its organization in AWS Organizations. Each OU correlates to the five businesses that the company owns. The company's research and development (R&D) business is separating from the company and will need its own organization. A solutions architect creates a separate new management account for this purpose. What should the solutions architect do next in the new management account?",
    "options_en": {
      "A": "Have the R&D AWS account be part of both organizations during the transition.",
      "B": "Invite the R&D AWS account to be part of the new organization after the R&D AWS account has left the prior organization.",
      "C": "Create a new R&D AWS account in the new organization. Migrate resources from the prior R&D AWS account to the new R&D AWS account.",
      "D": "Have the R&D AWS account join the new organization. Make the new management account a member of the prior organization."
    },
    "correct_answer": "B",
    "vote_percentage": "87%",
    "question_cn": "一家公司在其 AWS Organizations 中有五个组织单元 (OU)。每个 OU 与该公司拥有的五个业务相关。该公司的研发 (R&D) 业务将从公司分离出来，并且需要自己的组织。一位解决方案架构师为此创建了一个单独的新的管理账户。解决方案架构师在新管理账户中接下来应该怎么做？",
    "options_cn": {
      "A": "在过渡期间，让研发 AWS 账户成为两个组织的一部分。",
      "B": "在研发 AWS 账户离开之前的组织之后，邀请研发 AWS 账户加入新的组织。",
      "C": "在新组织中创建一个新的研发 AWS 账户。将资源从之前的研发 AWS 账户迁移到新的研发 AWS 账户。",
      "D": "让研发 AWS 账户加入新的组织。将新的管理账户作为之前组织的成员。"
    },
    "tags": [
      "AWS Organizations",
      "Management Account"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 87%），解析仅供参考。】\n\n考察 AWS Organizations 中账户从一个组织转移到另一个组织的操作流程。重点在于账户迁移的正确顺序以及账户之间的关系。",
      "why_correct": "选项 B 描述了正确的账户迁移顺序。首先，在研发 AWS 账户离开之前的组织之后，确保账户资源不会受到影响。然后，邀请研发 AWS 账户加入新的组织，完成组织转移。这种方式确保了账户的连贯性和安全性。",
      "why_wrong": "选项 A 错误，因为 AWS 账户不能同时属于两个独立的组织。选项 C 错误，这涉及创建一个新的账户并迁移资源，会增加工作量并可能导致停机时间，而题目中未提及需要这样做。选项 D 错误，管理账户不能作为成员加入另一个组织，这违反了 AWS Organizations 的结构规则，会产生冲突。将新的管理账户作为之前组织的成员是错误的。"
    },
    "related_terms": [
      "AWS Organizations",
      "OU",
      "AWS Account",
      "R&D",
      "AWS"
    ]
  },
  {
    "id": 587,
    "topic": "1",
    "question_en": "A company is designing a solution to capture customer activity in different web applications to process analytics and make predictions. Customer activity in the web applications is unpredictable and can increase suddenly. The company requires a solution that integrates with other web applications. The solution must include an authorization step for security purposes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives in an Amazon Elastic File System (Amazon EFS) file system. Authorization is resolved at the GWLB.",
      "B": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis data stream that stores the information that the company receives in an Amazon S3 bucket. Use an AWS Lambda function to resolve authorization.",
      "C": "Configure an Amazon API Gateway endpoint in front of an Amazon Kinesis Data Firehose that stores the information that the company receives in an Amazon S3 bucket. Use an API Gateway Lambda authorizer to resolve authorization.",
      "D": "Configure a Gateway Load Balancer (GWLB) in front of an Amazon Elastic Container Service (Amazon ECS) container instance that stores the information that the company receives on an Amazon Elastic File System (Amazon EFS) file system. Use an AWS Lambda function to resolve authorization."
    },
    "correct_answer": "C",
    "vote_percentage": "84%",
    "question_cn": "一家公司正在设计一个解决方案，以捕获不同 Web 应用程序中的客户活动，用于处理分析和进行预测。 Web 应用程序中的客户活动是不可预测的，并且可能突然增加。该公司需要一个与其他 Web 应用程序集成的解决方案。该解决方案必须包括一个用于安全目的的授权步骤。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 Amazon Elastic Container Service (Amazon ECS) 容器实例前面配置一个 Gateway Load Balancer (GWLB)，该实例将公司接收到的信息存储在 Amazon Elastic File System (Amazon EFS) 文件系统中。授权在 GWLB 上解决。",
      "B": "在 Amazon Kinesis 数据流前面配置一个 Amazon API Gateway 终端节点，该数据流将公司接收到的信息存储在 Amazon S3 存储桶中。使用 AWS Lambda 函数来解决授权。",
      "C": "在 Amazon API Gateway 终端节点前面配置一个 Amazon Kinesis Data Firehose，该数据流将公司接收到的信息存储在 Amazon S3 存储桶中。使用 API Gateway Lambda 授权器来解决授权。",
      "D": "在 Amazon Elastic Container Service (Amazon ECS) 容器实例前面配置一个 Gateway Load Balancer (GWLB)，该实例将公司接收到的信息存储在 Amazon Elastic File System (Amazon EFS) 文件系统中。使用 AWS Lambda 函数来解决授权。"
    },
    "tags": [
      "API Gateway",
      "Kinesis",
      "ECS",
      "Authorization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 84%），解析仅供参考。】\n\n考查 Web 应用程序事件流处理架构，包括 Kinesis Data Firehose、API Gateway、Lambda 授权器，以及 S3 存储。同时，考查了这些服务的组合使用，以及安全授权的实现方式。",
      "why_correct": "Amazon API Gateway 充当 Web 应用程序的入口点，并可配置 Lambda 授权器进行安全授权。 Kinesis Data Firehose 可以将 API Gateway 接收的数据直接流式传输到 Amazon S3，满足存储需求。这种架构方案能够适应不可预测的流量波动，并且易于与其他 Web 应用程序集成。",
      "why_wrong": "选项 A 和 D 使用了 GWLB 和 ECS，它们更适合于负载均衡和容器化的应用程序，而非直接处理 Web 应用的事件流。 虽然两者都提到了授权，但其架构不适合 Web 应用程序事件流处理的场景。选项 B 使用 API Gateway 和 Kinesis Data Streams，虽然功能上可行，但 Kinesis Data Firehose 专为数据流式传输到 S3 而设计，更符合题目需求，且实现成本更低。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "Amazon Kinesis Data Firehose",
      "Amazon S3",
      "AWS Lambda",
      "API Gateway Lambda authorizer",
      "Amazon ECS",
      "Gateway Load Balancer (GWLB)",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon Kinesis Data Streams"
    ]
  },
  {
    "id": 588,
    "topic": "1",
    "question_en": "An ecommerce company wants a disaster recovery solution for its Amazon RDS DB instances that run Microsoft SQL Server Enterprise Edition. The company's current recovery point objective (RPO) and recovery time objective (RTO) are 24 hours. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a cross-Region read replica and promote the read replica to the primary instance.",
      "B": "Use AWS Database Migration Service (AWS DMS) to create RDS cross-Region replication.",
      "C": "Use cross-Region replication every 24 hours to copy native backups to an Amazon S3 bucket.",
      "D": "Copy automatic snapshots to another Region every 24 hours."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司希望为其运行 Microsoft SQL Server Enterprise Edition 的 Amazon RDS DB 实例提供灾难恢复解决方案。该公司当前的恢复点目标 (RPO) 和恢复时间目标 (RTO) 为 24 小时。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "创建跨区域只读副本并将只读副本提升为主实例。",
      "B": "使用 AWS Database Migration Service (AWS DMS) 创建 RDS 跨区域复制。",
      "C": "每 24 小时使用跨区域复制将本机备份复制到 Amazon S3 存储桶。",
      "D": "每 24 小时将自动快照复制到另一个区域。"
    },
    "tags": [
      "RDS",
      "DR",
      "DMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查 Amazon RDS 数据库实例的灾难恢复方案，并评估在特定 RPO 和 RTO 需求下，不同方案的成本效益。",
      "why_correct": "选项 D 提供了最具成本效益的解决方案。 RDS 自动快照是 RDS 数据库的内置功能，跨区域复制自动快照只需要存储成本，并且可以满足 24 小时的 RPO 和 RTO。手动创建快照复制到其他区域也可以，但相比自动快照，需要额外的管理工作。",
      "why_wrong": "选项 A，创建跨区域只读副本并在故障时提升为主实例，需要额外的 RDS 实例费用，成本较高。选项 B，使用 AWS DMS 创建 RDS 跨区域复制，RDS 跨区域复制不如自动快照成本效益高。选项 C，使用跨区域复制将本机备份复制到 Amazon S3，虽然可以满足 RPO 和 RTO 要求，但需要用户配置备份策略，维护备份的流程，增加了管理的复杂性，且成本相对较高。"
    },
    "related_terms": [
      "Amazon RDS",
      "Microsoft SQL Server",
      "RPO",
      "RTO",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon S3"
    ]
  },
  {
    "id": 589,
    "topic": "1",
    "question_en": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer that has sticky sessions enabled. The web server currently hosts the user session state. The company wants to ensure high availability and avoid user session state loss in the event of a web server outage. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an Amazon ElastiCache for Memcached instance to store the session data. Update the application to use ElastiCache for Memcached to store the session state.",
      "B": "Use Amazon ElastiCache for Redis to store the session state. Update the application to use ElastiCache for Redis to store the session state.",
      "C": "Use an AWS Storage Gateway cached volume to store session data. Update the application to use AWS Storage Gateway cached volume to store the session state.",
      "D": "Use Amazon RDS to store the session state. Update the application to use Amazon RDS to store the session state."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司在Application Load Balancer后面的Auto Scaling组中的Amazon EC2实例上运行一个Web应用程序，该负载均衡器已启用粘性会话。Web服务器当前托管用户会话状态。该公司希望确保高可用性，并在Web服务器发生故障时避免用户会话状态丢失。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用Amazon ElastiCache for Memcached实例存储会话数据。更新应用程序以使用ElastiCache for Memcached存储会话状态。",
      "B": "使用Amazon ElastiCache for Redis存储会话状态。更新应用程序以使用ElastiCache for Redis存储会话状态。",
      "C": "使用AWS Storage Gateway缓存卷存储会话数据。更新应用程序以使用AWS Storage Gateway缓存卷存储会话状态。",
      "D": "使用Amazon RDS存储会话状态。更新应用程序以使用Amazon RDS存储会话状态。"
    },
    "tags": [
      "EC2",
      "ALB",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n考查如何在Application Load Balancer (ALB) 后面，通过使用缓存服务保障 EC2 实例上 Web 应用程序的高可用性，并避免用户会话状态丢失。",
      "why_correct": "Amazon ElastiCache for Redis 是一个高度可用的内存中数据存储，可以用于存储用户会话状态。通过将应用程序配置为使用 ElastiCache for Redis 存储会话数据，可以确保在 Web 服务器故障时，会话状态不会丢失，同时满足高可用性的要求。Redis 提供了持久化选项，可以进一步保障数据的可靠性。",
      "why_wrong": "选项 A 使用 ElastiCache for Memcached，Memcached 虽然也是缓存服务，但相比 Redis，功能较弱，没有持久化机制，且 Redis 提供了更多高级特性，更适合用于存储会话状态。选项 C 使用 AWS Storage Gateway 缓存卷，Storage Gateway 主要用于连接本地存储和云存储，不适用于存储会话数据。选项 D 使用 Amazon RDS 存储会话状态，RDS 是关系型数据库服务，虽然可以存储数据，但读写速度不如内存型缓存，且数据库开销较大，不适合用于快速访问的用户会话数据存储，且性能瓶颈可能影响用户体验。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "Auto Scaling",
      "ElastiCache for Memcached",
      "ElastiCache for Redis",
      "AWS Storage Gateway",
      "Amazon RDS",
      "Web application",
      "Session State"
    ]
  },
  {
    "id": 590,
    "topic": "1",
    "question_en": "A company migrated a MySQL database from the company's on-premises data center to an Amazon RDS for MySQL DB instance. The company sized the RDS DB instance to meet the company's average daily workload. Once a month, the database performs slowly when the company runs queries for a report. The company wants to have the ability to run reports and maintain the performance of the daily workloads. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a read replica of the database. Direct the queries to the read replica.",
      "B": "Create a backup of the database. Restore the backup to another DB instance. Direct the queries to the new database.",
      "C": "Export the data to Amazon S3. Use Amazon Athena to query the S3 bucket.",
      "D": "Resize the DB instance to accommodate the additional workload."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司将 MySQL 数据库从公司的本地数据中心迁移到 Amazon RDS for MySQL DB 实例。该公司调整了 RDS DB 实例的大小以满足该公司的平均每日工作负载。每个月，当公司运行查询以获取报告时，数据库的运行速度会变慢。该公司希望能够运行报告并保持每日工作负载的性能。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建数据库的只读副本。将查询定向到只读副本。",
      "B": "创建数据库的备份。将备份还原到另一个 DB 实例。将查询定向到新数据库。",
      "C": "将数据导出到 Amazon S3。使用 Amazon Athena 查询 S3 存储桶。",
      "D": "调整 DB 实例的大小以适应额外的工作负载。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "Read Replica"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考察了 RDS 的性能优化和扩展。为了应对报告查询导致的数据库性能下降，可以使用只读副本来分担负载。",
      "why_correct": "创建数据库的只读副本并将查询定向到只读副本，可以分流读取流量，提高数据库性能，而不会影响日常工作负载。",
      "why_wrong": "B 选项，备份和恢复会增加停机时间，并且需要重新配置连接，影响日常工作负载。C 选项，使用 Amazon Athena 查询 S3 存储桶，数据需要先导出，并且 Athena 的性能可能不满足需求。D 选项，调整实例大小无法立即解决问题，并且增加了成本。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "Read Replica",
      "Amazon S3",
      "Amazon Athena"
    ]
  },
  {
    "id": 591,
    "topic": "1",
    "question_en": "A company runs a container application by using Amazon Elastic Kubernetes Service (Amazon EKS). The application includes microservices that manage customers and place orders. The company needs to route incoming requests to the appropriate microservices. Which solution will meet this requirement MOST cost-effectively?",
    "options_en": {
      "A": "Use the AWS Load Balancer Controller to provision a Network Load Balancer.",
      "B": "Use the AWS Load Balancer Controller to provision an Application Load Balancer.",
      "C": "Use an AWS Lambda function to connect the requests to Amazon EKS.",
      "D": "Use Amazon API Gateway to connect the requests to Amazon EKS."
    },
    "correct_answer": "B",
    "vote_percentage": "75%",
    "question_cn": "一家公司使用 Amazon Elastic Kubernetes Service (Amazon EKS) 运行容器应用程序。该应用程序包括管理客户和下订单的微服务。该公司需要将传入的请求路由到适当的微服务。哪种解决方案将以最具成本效益的方式满足此要求？",
    "options_cn": {
      "A": "使用 AWS Load Balancer Controller 配置 Network Load Balancer。",
      "B": "使用 AWS Load Balancer Controller 配置 Application Load Balancer。",
      "C": "使用 AWS Lambda 函数将请求连接到 Amazon EKS。",
      "D": "使用 Amazon API Gateway 将请求连接到 Amazon EKS。"
    },
    "tags": [
      "EKS",
      "Load Balancer",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 75%），解析仅供参考。】\n\n考查在 Amazon EKS 中以最具成本效益的方式进行请求路由的解决方案。主要考察对不同负载均衡器的选择和适用场景的理解。",
      "why_correct": "Application Load Balancer (ALB) 是七层负载均衡器，特别适合用于容器化应用程序，可以根据 HTTP/HTTPS 请求的内容进行路由。AWS Load Balancer Controller 可以在 EKS 集群中自动配置和管理 ALB，实现基于路径、主机名等规则的灵活路由。ALB 的按需付费模式使其具有成本效益。",
      "why_wrong": "Network Load Balancer (NLB) 是四层负载均衡器，虽然性能很高，但其基于 IP 地址的路由方式不如 ALB 灵活，且通常成本高于 ALB。AWS Lambda 函数和 Amazon API Gateway 虽然可以与 EKS 集群集成，但会增加额外的复杂性和成本，且 API Gateway 的成本结构可能并不具备成本效益，尤其对于大规模流量而言。"
    },
    "related_terms": [
      "Amazon EKS",
      "Application Load Balancer",
      "AWS Load Balancer Controller",
      "Network Load Balancer",
      "AWS Lambda",
      "Amazon API Gateway"
    ]
  },
  {
    "id": 592,
    "topic": "1",
    "question_en": "A company uses AWS and sells access to copyrighted images. The company’s global customer base needs to be able to access these images quickly. The company must deny access to users from specific countries. The company wants to minimize costs as much as possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon S3 to store the images. Turn on multi-factor authentication (MFA) and public bucket access. Provide customers with a link to the S3 bucket.",
      "B": "Use Amazon S3 to store the images. Create an IAM user for each customer. Add the users to a group that has permission to access the S3 bucket.",
      "C": "Use Amazon EC2 instances that are behind Application Load Balancers (ALBs) to store the images. Deploy the instances only in the countries the company services. Provide customers with links to the ALBs for their specific country's instances.",
      "D": "Use Amazon S3 to store the images. Use Amazon CloudFront to distribute the images with geographic restrictions. Provide a signed URL for each customer to access the data in CloudFront."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS 并出售对受版权保护的图像的访问权限。该公司的全球客户群需要能够快速访问这些图像。该公司必须拒绝来自特定国家/地区的用户的访问权限。该公司希望尽可能降低成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 存储图像。打开多重身份验证 (MFA) 和公共存储桶访问权限。为客户提供指向 S3 存储桶的链接。",
      "B": "使用 Amazon S3 存储图像。为每个客户创建一个 IAM 用户。将用户添加到有权访问 S3 存储桶的组。",
      "C": "使用位于 Application Load Balancer (ALB) 后的 Amazon EC2 实例存储图像。仅在公司服务的国家/地区部署实例。为客户提供指向其特定国家/地区的实例的 ALB 的链接。",
      "D": "使用 Amazon S3 存储图像。使用 Amazon CloudFront 分发带有地理限制的图像。为每个客户提供签名 URL 以访问 CloudFront 中的数据。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "Geo-restriction"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查通过 S3 存储静态内容，结合 CloudFront 的地理限制和签名 URL 来满足全球客户快速访问、限制特定国家/地区访问以及控制成本的需求。",
      "why_correct": "选项 D 提供了最佳解决方案。使用 Amazon S3 存储图像，CloudFront 作为 CDN 加速全球访问，并支持地理限制。通过提供签名 URL，可以控制对 CloudFront 内容的访问，满足了题目的所有要求，包括访问速度、访问控制和成本效益。",
      "why_wrong": "选项 A 错误，因为开启公共存储桶访问权限会允许任何人访问 S3 中的图像，无法实现访问控制，并且 MFA 与此场景关系不大。选项 B 错误，为每个客户创建 IAM 用户和访问 S3 的链接不适合大规模的客户访问，且成本较高。选项 C 错误，使用 EC2 实例存储图像，并仅部署在公司服务的国家/地区，无法满足全球客户的快速访问需求，而且维护成本较高，灵活性也较差。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "MFA",
      "IAM",
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "CDN",
      "geographic restrictions",
      "signed URL"
    ]
  },
  {
    "id": 593,
    "topic": "1",
    "question_en": "A solutions architect is designing a highly available Amazon ElastiCache for Redis based solution. The solutions architect needs to ensure that failures do not result in performance degradation or loss of data locally and within an AWS Region. The solution needs to provide high availability at the node level and at the Region level. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Multi-AZ Redis replication groups with shards that contain multiple nodes.",
      "B": "Use Redis shards that contain multiple nodes with Redis append only files (AOF) turned on.",
      "C": "Use a Multi-AZ Redis cluster with more than one read replica in the replication group.",
      "D": "Use Redis shards that contain multiple nodes with Auto Scaling turned on."
    },
    "correct_answer": "A",
    "vote_percentage": "75%",
    "question_cn": "一个解决方案架构师正在设计一个高可用性的基于 Amazon ElastiCache for Redis 的解决方案。解决方案架构师需要确保故障不会导致本地和 AWS 区域内出现性能下降或数据丢失。该解决方案需要在节点级别和区域级别提供高可用性。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用具有包含多个节点的 shard 的多可用区 Redis 复制组。",
      "B": "使用包含多个节点的 Redis shard，并打开 Redis 追加文件 (AOF)。",
      "C": "使用多可用区 Redis 集群，在复制组中有多个只读副本。",
      "D": "使用包含多个节点的 Redis shard，并打开 Auto Scaling。"
    },
    "tags": [
      "ElastiCache",
      "Redis",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 75%），解析仅供参考。】\n\n本题考察了 ElastiCache for Redis 的高可用性设计。需要在节点和区域级别提供高可用性，以避免单点故障和数据丢失。",
      "why_correct": "使用具有包含多个节点的 shard 的多可用区 Redis 复制组可以提供节点级别和区域级别的高可用性，防止数据丢失并确保性能。",
      "why_wrong": "B 选项，使用 AOF 可以在一定程度上实现数据恢复，但不能提供高可用性。C 选项，多可用区 Redis 集群，只读副本只能在副本级别提供保护，不满足区域级别的 HA。D 选项，Auto Scaling 不能提供数据保护，并且不满足区域级别 HA。"
    },
    "related_terms": [
      "ElastiCache",
      "Redis",
      "Multi-AZ",
      "High Availability"
    ]
  },
  {
    "id": 594,
    "topic": "1",
    "question_en": "A company plans to migrate to AWS and use Amazon EC2 On-Demand Instances for its application. During the migration testing phase, a technical team observes that the application takes a long time to launch and load memory to become fully productive. Which solution will reduce the launch time of the application during the next testing phase?",
    "options_en": {
      "A": "Launch two or more EC2 On-Demand Instances. Turn on auto scaling features and make the EC2 On-Demand Instances available during the next testing phase.",
      "B": "Launch EC2 Spot Instances to support the application and to scale the application so it is available during the next testing phase.",
      "C": "Launch the EC2 On-Demand Instances with hibernation turned on. Configure EC2 Auto Scaling warm pools during the next testing phase.",
      "D": "Launch EC2 On-Demand Instances with Capacity Reservations. Start additional EC2 instances during the next testing phase."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划迁移到 AWS，并为其应用程序使用 Amazon EC2 按需实例。在迁移测试阶段，技术团队观察到应用程序需要很长时间才能启动并加载内存以完全投入生产。哪个解决方案将在下一次测试阶段减少应用程序的启动时间？",
    "options_cn": {
      "A": "启动两个或更多 EC2 按需实例。打开自动缩放功能，并在下一次测试阶段提供 EC2 按需实例。",
      "B": "启动 EC2 Spot 实例以支持应用程序并扩展应用程序，以便在下一次测试阶段可用。",
      "C": "启动已启用休眠的 EC2 按需实例。在下一次测试阶段配置 EC2 Auto Scaling 温池。",
      "D": "启动带有容量预留的 EC2 按需实例。在下一次测试阶段启动额外的 EC2 实例。"
    },
    "tags": [
      "EC2",
      "Hibernate",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了 EC2 实例启动时间的优化。需要减少应用程序的启动时间。",
      "why_correct": "启动已启用休眠的 EC2 按需实例，可以保存实例状态，在下次测试时快速恢复。在下一次测试阶段配置 EC2 Auto Scaling 温池可以实现快速的启动和扩展。",
      "why_wrong": "A 选项，增加实例数量，并不能解决启动时间长的问题。B 选项，Spot 实例可能随时中断，不适合作为测试环境。D 选项，容量预留可以保证容量，但不能减少启动时间。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "Hibernate",
      "Spot Instances",
      "CapacityReservation"
    ]
  },
  {
    "id": 595,
    "topic": "1",
    "question_en": "A company's applications run on Amazon EC2 instances in Auto Scaling groups. The company notices that its applications experience sudden trafic increases on random days of the week. The company wants to maintain application performance during sudden trafic increases. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use manual scaling to change the size of the Auto Scaling group.",
      "B": "Use predictive scaling to change the size of the Auto Scaling group.",
      "C": "Use dynamic scaling to change the size of the Auto Scaling group.",
      "D": "Use schedule scaling to change the size of the Auto Scaling group."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的应用程序在 Auto Scaling 组中的 Amazon EC2 实例上运行。该公司注意到其应用程序在一周中的随机日子会遇到突然的流量增加。该公司希望在突然的流量增加期间保持应用程序的性能。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用手动扩展来更改 Auto Scaling 组的大小。",
      "B": "使用预测扩展来更改 Auto Scaling 组的大小。",
      "C": "使用动态扩展来更改 Auto Scaling 组的大小。",
      "D": "使用计划扩展来更改 Auto Scaling 组的大小。"
    },
    "tags": [
      "Auto Scaling",
      "Dynamic Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了 Auto Scaling 的配置，用于应对突发的流量增加，以保持应用程序的性能。",
      "why_correct": "使用动态扩展可以根据实时的指标（如 CPU 利用率或网络流量）自动调整 Auto Scaling 组的大小，从而以最具成本效益的方式满足流量变化的需求。",
      "why_wrong": "A 选项，手动扩展需要人工干预，无法及时响应流量变化。B 选项，预测扩展依赖于预测，可能会有误差。D 选项，计划扩展基于预定的时间表，无法应对突发的流量增加。"
    },
    "related_terms": [
      "Auto Scaling",
      "Dynamic Scaling",
      "Predictive Scaling",
      "Scheduled Scaling"
    ]
  },
  {
    "id": 596,
    "topic": "1",
    "question_en": "An ecommerce application uses a PostgreSQL database that runs on an Amazon EC2 instance. During a monthly sales event, database usage increases and causes database connection issues for the application. The trafic is unpredictable for subsequent monthly sales events, which impacts the sales forecast. The company needs to maintain performance when there is an unpredictable increase in trafic. Which solution resolves this issue in the MOST cost-effective way?",
    "options_en": {
      "A": "Migrate the PostgreSQL database to Amazon Aurora Serverless v2.",
      "B": "Enable auto scaling for the PostgreSQL database on the EC2 instance to accommodate increased usage.",
      "C": "Migrate the PostgreSQL database to Amazon RDS for PostgreSQL with a larger instance type.",
      "D": "Migrate the PostgreSQL database to Amazon Redshift to accommodate increased usage."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一个电子商务应用程序使用在 Amazon EC2 实例上运行的 PostgreSQL 数据库。在每月促销活动期间，数据库使用量增加，导致应用程序的数据库连接问题。后续每月促销活动的流量是不可预测的，这影响了销售预测。该公司需要在流量不可预测地增加时保持性能。哪种解决方案以最具成本效益的方式解决了这个问题？",
    "options_cn": {
      "A": "将 PostgreSQL 数据库迁移到 Amazon Aurora Serverless v2。",
      "B": "为 EC2 实例上的 PostgreSQL 数据库启用自动缩放以适应增加的使用量。",
      "C": "将 PostgreSQL 数据库迁移到 Amazon RDS for PostgreSQL，使用更大的实例类型。",
      "D": "将 PostgreSQL 数据库迁移到 Amazon Redshift 以适应增加的使用量。"
    },
    "tags": [
      "Aurora",
      "RDS",
      "PostgreSQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n考查在流量不可预测增加的情况下，如何以最具成本效益的方式提升数据库性能。",
      "why_correct": "Amazon Aurora Serverless v2 提供了按需扩展数据库容量的能力，非常适合流量不可预测的场景。它能够自动根据应用程序的需求扩展和缩减计算资源，无需预先规划容量，并按实际使用量付费，从而实现成本优化。",
      "why_wrong": "选项 B，为 EC2 实例启用自动缩放虽然可以应对流量增加，但无法快速响应流量的突增，需要手动调整。选项 C，迁移到 RDS for PostgreSQL 并使用更大的实例，虽然提升了性能，但无法动态扩展，对于不可预测的流量，容易造成资源浪费或性能不足。选项 D，迁移到 Amazon Redshift 用于数据仓库，并非为 OLTP 应用程序设计，不适合电子商务应用程序的数据库需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "PostgreSQL",
      "Amazon Aurora Serverless v2",
      "Amazon RDS for PostgreSQL",
      "Amazon Redshift",
      "OLTP"
    ]
  },
  {
    "id": 597,
    "topic": "1",
    "question_en": "A company hosts an internal serverless application on AWS by using Amazon API Gateway and AWS Lambda. The company’s employees report issues with high latency when they begin using the application each day. The company wants to reduce latency. Which solution will meet these requirements?",
    "options_en": {
      "A": "Increase the API Gateway throttling limit.",
      "B": "Set up a scheduled scaling to increase Lambda provisioned concurrency before employees begin to use the application each day.",
      "C": "Create an Amazon CloudWatch alarm to initiate a Lambda function as a target for the alarm at the beginning of each day.",
      "D": "Increase the Lambda function memory."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司通过使用 Amazon API Gateway 和 AWS Lambda 在 AWS 上托管内部无服务器应用程序。公司员工报告说，他们每天开始使用该应用程序时会遇到高延迟的问题。该公司希望减少延迟。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "增加 API Gateway 限制。 ",
      "B": "设置一个计划扩展，以便在员工每天开始使用该应用程序之前增加 Lambda 预置并发。 ",
      "C": "创建一个 Amazon CloudWatch 警报，以在每天开始时将一个 Lambda 函数作为警报的目标。 ",
      "D": "增加 Lambda 函数内存。 "
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "Performance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了解决 Lambda 函数高延迟的问题。需要寻找减少延迟的解决方案。",
      "why_correct": "设置 Lambda 函数的预置并发，可以减少冷启动的延迟，从而减少 API Gateway 的延迟。",
      "why_wrong": "A 选项，增加 API Gateway 限制，并不能解决延迟问题。C 选项，创建 CloudWatch 警报，无法解决根本原因，反而会增加开销。D 选项，增加 Lambda 函数内存，也无法根本解决延迟问题，且可能增加成本。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "CloudWatch",
      "Performance"
    ]
  },
  {
    "id": 598,
    "topic": "1",
    "question_en": "A research company uses on-premises devices to generate data for analysis. The company wants to use the AWS Cloud to analyze the data. The devices generate .csv files and support writing the data to an SMB file share. Company analysts must be able to use SQL commands to query the data. The analysts will run queries periodically throughout the day. Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)",
    "options_en": {
      "A": "Deploy an AWS Storage Gateway on premises in Amazon S3 File Gateway mode.",
      "B": "Deploy an AWS Storage Gateway on premises in Amazon FSx File Gateway made.",
      "C": "Set up an AWS Glue crawler to create a table based on the data that is in Amazon S3.",
      "D": "Set up an Amazon EMR cluster with EMR File System (EMRFS) to query the data that is in Amazon S3. Provide access to analysts",
      "E": "Set up an Amazon Redshift cluster to query the data that is in Amazon S3. Provide access to analysts",
      "F": "Setup Amazon Athena to query the data that is in Amazon S3. Provide access to analysts."
    },
    "correct_answer": "ACF",
    "vote_percentage": "",
    "question_cn": "一家研究公司使用本地设备生成用于分析的数据。该公司希望使用 AWS Cloud 来分析数据。这些设备生成 .csv 文件，并支持将数据写入 SMB 文件共享。公司分析师必须能够使用 SQL 命令查询数据。分析师将在一天中定期间隔运行查询。哪种步骤组合将以最具成本效益的方式满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "在本地以 Amazon S3 File Gateway 模式部署 AWS Storage Gateway。",
      "B": "在本地以 Amazon FSx File Gateway 模式部署 AWS Storage Gateway。",
      "C": "设置 AWS Glue 爬虫，基于 Amazon S3 中的数据创建表。",
      "D": "设置一个带有 EMR 文件系统 (EMRFS) 的 Amazon EMR 集群，以查询 Amazon S3 中的数据。为分析师提供访问权限。",
      "E": "设置一个 Amazon Redshift 集群，以查询 Amazon S3 中的数据。为分析师提供访问权限。",
      "F": "设置 Amazon Athena 以查询 Amazon S3 中的数据。为分析师提供访问权限。"
    },
    "tags": [
      "S3",
      "Storage Gateway",
      "Athena",
      "EMR",
      "Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACF（社区 —），解析仅供参考。】\n\n考察如何使用 AWS 服务，以最具成本效益的方式，满足将本地 SMB 文件共享中的 .csv 数据导入 AWS Cloud 进行 SQL 查询的需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACF。理由简述：选项 A 正确：在本地以 S3 File Gateway 模式部署 Storage Gateway，将 SMB 文件共享数据同步到 S3。选项 C 正确：AWS Glue 爬虫基于 S3 数据自动发现结构并创建表。选项 F 正确：Amazon Athena 按查询付费、无服务器，直接查询 S3 数据，满足分析师定期间隔 SQL 查询且成本低。A+C+F 组合（社区投票 96%）满足选三且最具成本效益。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，FSx File Gateway 用于访问 FSx for Windows File Server，不适用于本题 SMB 到 S3 的同步。选项 D 错误，EMR 集群成本与运维较高。选项 E 错误，Redshift 适合长期数据仓库，本题按需查询场景下 Athena 更省成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "Storage Gateway",
      "Athena",
      "EMR",
      "Redshift",
      "AWS Glue"
    ]
  },
  {
    "id": 599,
    "topic": "1",
    "question_en": "A company wants to use Amazon Elastic Container Service (Amazon ECS) clusters and Amazon RDS DB instances to build and run a payment processing application. The company will run the application in its on-premises data center for compliance purposes. A solutions architect wants to use AWS Outposts as part of the solution. The solutions architect is working with the company's operational team to build the application. Which activities are the responsibility of the company's operational team? (Choose three.)",
    "options_en": {
      "A": "Providing resilient power and network connectivity to the Outposts racks",
      "B": "Managing the virtualization hypervisor, storage systems, and the AWS services that run on Outposts",
      "C": "Physical security and access controls of the data center environment",
      "D": "Availability of the Outposts infrastructure including the power supplies, servers, and networking equipment within the Outposts racks",
      "E": "Physical maintenance of Outposts components",
      "F": "Providing extra capacity for Amazon ECS clusters to mitigate server failures and maintenance events"
    },
    "correct_answer": "ACE",
    "vote_percentage": "21%",
    "question_cn": "一家公司希望使用 Amazon Elastic Container Service (Amazon ECS) 集群和 Amazon RDS 数据库实例来构建和运行支付处理应用程序。出于合规性考虑，该公司将在其本地数据中心运行该应用程序。一位解决方案架构师希望使用 AWS Outposts 作为解决方案的一部分。该解决方案架构师正在与公司的运营团队合作构建该应用程序。以下哪些活动是该公司运营团队的责任？（选择三个。）",
    "options_cn": {
      "A": "为 Outposts 机架提供弹性的电源和网络连接",
      "B": "管理虚拟化管理程序、存储系统以及在 Outposts 上运行的 AWS 服务",
      "C": "数据中心环境的物理安全和访问控制",
      "D": "Outposts 基础设施的可用性，包括 Outposts 机架内的电源、服务器和网络设备",
      "E": "Outposts 组件的物理维护",
      "F": "为 Amazon ECS 集群提供额外的容量，以减轻服务器故障和维护事件"
    },
    "tags": [
      "ECS",
      "RDS",
      "Outposts"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACE（社区 21%），解析仅供参考。】\n\n本题考察了 AWS Outposts 的职责划分。需要确定哪些活动是公司运营团队的责任。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACE。理由简述：选项 A，为 Outposts 机架提供弹性的电源和网络连接。选项 C，数据中心环境的物理安全和访问控制。选项 E，Outposts 组件的物理维护，是公司运营团队的责任，AWS 负责 Outposts 的基础设施可用性 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项，管理虚拟化管理程序、存储系统以及在 Outposts 上运行的 AWS 服务，是 AWS 的责任。D 选项，Outposts 基础设施的可用性，包括 Outposts 机架内的电源、服务器和网络设备，是 AWS 的责任。F 选项，为 Amazon ECS 集群提供额外的容量，以减轻服务器故障和维护事件，是 AWS 的责任。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ECS",
      "RDS",
      "Outposts"
    ]
  },
  {
    "id": 600,
    "topic": "1",
    "question_en": "A company is planning to migrate a TCP-based application into the company's VPC. The application is publicly accessible on a nonstandard TCP port through a hardware appliance in the company's data center. This public endpoint can process up to 3 million requests per second with low latency. The company requires the same level of performance for the new public endpoint in AWS. What should a solutions architect recommend to meet this requirement?",
    "options_en": {
      "A": "Deploy a Network Load Balancer (NLB). Configure the NLB to be publicly accessible over the TCP port that the application requires.",
      "B": "Deploy an Application Load Balancer (ALB). Configure the ALB to be publicly accessible over the TCP port that the application requires.",
      "C": "Deploy an Amazon CloudFront distribution that listens on the TCP port that the application requires. Use an Application Load Balancer as the origin.",
      "D": "Deploy an Amazon API Gateway API that is configured with the TCP port that the application requires. Configure AWS Lambda functions with provisioned concurrency to process the requests."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划将其基于 TCP 的应用程序迁移到公司的 VPC 中。该应用程序通过公司数据中心中的硬件设备，在非标准 TCP 端口上公开访问。此公共端点每秒可以处理多达 300 万个请求，并且具有低延迟。该公司要求 AWS 中的新公共端点具有相同的性能水平。解决方案架构师应该推荐什么来满足此要求？",
    "options_cn": {
      "A": "部署一个 Network Load Balancer (NLB)。将 NLB 配置为通过应用程序所需的 TCP 端口进行公开访问。",
      "B": "部署一个 Application Load Balancer (ALB)。将 ALB 配置为通过应用程序所需的 TCP 端口进行公开访问。",
      "C": "部署一个 Amazon CloudFront 分发，该分发侦听应用程序所需的 TCP 端口。使用 Application Load Balancer 作为源。",
      "D": "部署一个 Amazon API Gateway API，该 API 配置有应用程序所需的 TCP 端口。配置具有预置并发的 AWS Lambda 函数来处理请求。"
    },
    "tags": [
      "NLB",
      "ALB",
      "CloudFront",
      "API Gateway",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察负载均衡器的选择。Network Load Balancer (NLB) 适用于需要处理 TCP/UDP 流量并需要超高性能和低延迟的场景，这与题干描述相符。Application Load Balancer (ALB) 适用于需要 HTTP/HTTPS 流量的场景，并提供更高级的功能，例如基于内容的路由。CloudFront 是内容分发网络，主要用于加速内容分发，并不适合直接处理 TCP 流量。API Gateway 适用于构建 API，但在此场景中不是最佳选择。",
      "why_correct": "NLB 能够处理 TCP 流量，并提供低延迟和高吞吐量，满足了题目中对性能的要求。",
      "why_wrong": "选项 B 中 ALB 适用于 HTTP/HTTPS，不适用于 TCP 端口。选项 C 中 CloudFront 无法监听 TCP 端口，且使用 ALB 作为源增加复杂性。选项 D 中 API Gateway 并非最佳选择，且 Lambda 函数处理 TCP 流量效率不高。"
    },
    "related_terms": [
      "Network Load Balancer",
      "NLB",
      "Application Load Balancer",
      "ALB",
      "Amazon CloudFront",
      "API Gateway",
      "AWS Lambda",
      "TCP"
    ]
  },
  {
    "id": 601,
    "topic": "1",
    "question_en": "A company runs its critical database on an Amazon RDS for PostgreSQL DB instance. The company wants to migrate to Amazon Aurora PostgreSQL with minimal downtime and data loss. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a DB snapshot of the RDS for PostgreSQL DB instance to populate a new Aurora PostgreSQL DB cluster.",
      "B": "Create an Aurora read replica of the RDS for PostgreSQL DB instance. Promote the Aurora read replicate to a new Aurora PostgreSQL DB cluster.",
      "C": "Use data import from Amazon S3 to migrate the database to an Aurora PostgreSQL DB cluster.",
      "D": "Use the pg_dump utility to back up the RDS for PostgreSQL database. Restore the backup to a new Aurora PostgreSQL DB cluster."
    },
    "correct_answer": "B",
    "vote_percentage": "85%",
    "question_cn": "一家公司在其 Amazon RDS for PostgreSQL 数据库实例上运行关键数据库。该公司希望以最小的停机时间和数据丢失迁移到 Amazon Aurora PostgreSQL。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建 RDS for PostgreSQL 数据库实例的数据库快照，以填充一个新的 Aurora PostgreSQL 数据库集群。",
      "B": "创建 RDS for PostgreSQL 数据库实例的 Aurora 副本。将 Aurora 副本提升到新的 Aurora PostgreSQL 数据库集群。",
      "C": "使用从 Amazon S3 导入的数据将数据库迁移到 Aurora PostgreSQL 数据库集群。",
      "D": "使用 pg_dump 实用程序备份 RDS for PostgreSQL 数据库。将备份恢复到新的 Aurora PostgreSQL 数据库集群。"
    },
    "tags": [
      "RDS",
      "Aurora PostgreSQL",
      "Snapshot"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 85%），解析仅供参考。】\n\n此题考查数据库迁移策略。Aurora PostgreSQL 兼容 PostgreSQL，且提供了比 RDS 更高的性能和可用性。使用 Aurora 副本迁移，停机时间最小，可以实现快速切换。数据库快照虽然也可以迁移数据，但不如 Aurora 副本高效。S3 导入和 pg_dump 备份恢复的方式，都涉及较长的停机时间。",
      "why_correct": "创建 Aurora 副本，然后将其提升为新的 Aurora PostgreSQL 数据库集群，可以最大限度地减少停机时间，并减少数据丢失。",
      "why_wrong": "选项 A 中创建快照需要进行数据复制，停机时间较长。选项 C 和 D 均涉及数据导入或备份恢复，停机时间更长，且操作复杂。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL",
      "RDS for PostgreSQL",
      "Snapshot",
      "pg_dump"
    ]
  },
  {
    "id": 602,
    "topic": "1",
    "question_en": "A company's infrastructure consists of hundreds of Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) storage. A solutions architect must ensure that every EC2 instance can be recovered after a disaster. What should the solutions architect do to meet this requirement with the LEAST amount of effort?",
    "options_en": {
      "A": "Take a snapshot of the EBS storage that is attached to each EC2 instance. Create an AWS CloudFormation template to launch new EC2 instances from the EBS storage.",
      "B": "Take a snapshot of the EBS storage that is attached to each EC2 instance. Use AWS Elastic Beanstalk to set the environment based on the EC2 template and attach the EBS storage.",
      "C": "Use AWS Backup to set up a backup plan for the entire group of EC2 instances. Use the AWS Backup API or the AWS CLI to speed up the restore process for multiple EC2 instances.",
      "D": "Create an AWS Lambda function to take a snapshot of the EBS storage that is attached to each EC2 instance and copy the Amazon Machine Images (AMIs). Create another Lambda function to perform the restores with the copied AMIs and attach the EBS storage."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的基础设施由数百个 Amazon EC2 实例组成，这些实例使用 Amazon Elastic Block Store (Amazon EBS) 存储。一位解决方案架构师必须确保在发生灾难后可以恢复每个 EC2 实例。为了以最少的精力满足此要求，解决方案架构师应该怎么做？",
    "options_cn": {
      "A": "拍摄附加到每个 EC2 实例的 EBS 存储的快照。创建一个 AWS CloudFormation 模板，以便从 EBS 存储启动新的 EC2 实例。",
      "B": "拍摄附加到每个 EC2 实例的 EBS 存储的快照。使用 AWS Elastic Beanstalk 根据 EC2 模板设置环境并附加 EBS 存储。",
      "C": "使用 AWS Backup 为整个 EC2 实例组设置备份计划。使用 AWS Backup API 或 AWS CLI 加快多个 EC2 实例的恢复过程。",
      "D": "创建一个 AWS Lambda 函数来拍摄附加到每个 EC2 实例的 EBS 存储的快照，并复制 Amazon Machine Images (AMIs)。创建另一个 Lambda 函数，使用复制的 AMI 执行恢复并附加 EBS 存储。"
    },
    "tags": [
      "EC2",
      "EBS",
      "Backup",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察灾难恢复策略。AWS Backup 提供了集中式的备份管理，可以备份 EC2 实例及其 EBS 卷，并提供恢复功能。EBS 快照是单个卷的备份，需要手动管理，且恢复过程复杂。Lambda 函数需要额外的开发和维护工作。",
      "why_correct": "AWS Backup 提供自动化备份和恢复功能，可以轻松地备份和恢复 EC2 实例，满足以最少精力满足要求的条件。",
      "why_wrong": "选项 A 和 B 中快照需要手动管理恢复流程，不够自动化。选项 D 中 Lambda 函数方案涉及复杂实现，增加了运营成本。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS Backup",
      "AWS CloudFormation",
      "AWS Lambda",
      "Amazon Machine Images (AMIs)"
    ]
  },
  {
    "id": 603,
    "topic": "1",
    "question_en": "A company recently migrated to the AWS Cloud. The company wants a serverless solution for large-scale parallel on-demand processing of a semistructured dataset. The data consists of logs, media files, sales transactions, and IoT sensor data that is stored in Amazon S3. The company wants the solution to process thousands of items in the dataset in parallel. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Use the AWS Step Functions Map state in Inline mode to process the data in parallel.",
      "B": "Use the AWS Step Functions Map state in Distributed mode to process the data in parallel.",
      "C": "Use AWS Glue to process the data in parallel.",
      "D": "Use several AWS Lambda functions to process the data in parallel."
    },
    "correct_answer": "B",
    "vote_percentage": "94%",
    "question_cn": "一家公司最近迁移到了 AWS 云。该公司希望使用无服务器解决方案，以大规模并行按需处理半结构化数据集。数据由日志、媒体文件、销售交易和 IoT 传感器数据组成，这些数据存储在 Amazon S3 中。该公司希望该解决方案能够并行处理数据集中数千个项目。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "在内联模式下使用 AWS Step Functions Map 状态并行处理数据。",
      "B": "在分布式模式下使用 AWS Step Functions Map 状态并行处理数据。",
      "C": "使用 AWS Glue 并行处理数据。",
      "D": "使用多个 AWS Lambda 函数并行处理数据。"
    },
    "tags": [
      "Step Functions",
      "S3",
      "Lambda",
      "Glue"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 94%），解析仅供参考。】\n\n此题考查大数据处理方案。无服务器方案中，Step Functions Map 状态能够并行处理大规模数据集。在分布式模式下，Step Functions Map 状态能够并行启动多个 Lambda 函数来处理数据，从而实现并行处理。AWS Glue 适用于 ETL 任务，但可能不如 Map 状态灵活。Lambda 函数方案需要手动管理并发，而 Step Functions Map 状态可以更好地管理。",
      "why_correct": "分布式模式的 Step Functions Map 状态能够并行处理数据集中数千个项目，提供最佳的运营效率。",
      "why_wrong": "选项 A 中内联模式的 Step Functions Map 状态的并行度有限。选项 C 中 AWS Glue 方案的并行处理能力有限，且不完全是无服务器。选项 D 中多个 Lambda 函数需要手动管理并发，增加了复杂性。"
    },
    "related_terms": [
      "S3",
      "Step Functions",
      "AWS Lambda",
      "AWS Glue"
    ]
  },
  {
    "id": 604,
    "topic": "1",
    "question_en": "A company will migrate 10 PB of data to Amazon S3 in 6 weeks. The current data center has a 500 Mbps uplink to the internet. Other on- premises applications share the uplink. The company can use 80% of the internet bandwidth for this one-time migration task. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure AWS DataSync to migrate the data to Amazon S3 and to automatically verify the data.",
      "B": "Use rsync to transfer the data directly to Amazon S3.",
      "C": "Use the AWS CLI and multiple copy processes to send the data directly to Amazon S3.",
      "D": "Order multiple AWS Snowball devices. Copy the data to the devices. Send the devices to AWS to copy the data to Amazon S3."
    },
    "correct_answer": "D",
    "vote_percentage": "94%",
    "question_cn": "一家公司将在 6 周内将 10 PB 的数据迁移到 Amazon S3。当前数据中心具有 500 Mbps 的上行链路到互联网。其他本地应用程序共享上行链路。该公司可以使用 80% 的互联网带宽来完成此一次性迁移任务。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 AWS DataSync 将数据迁移到 Amazon S3 并自动验证数据。",
      "B": "使用 rsync 将数据直接传输到 Amazon S3。",
      "C": "使用 AWS CLI 和多个复制进程将数据直接发送到 Amazon S3。",
      "D": "订购多个 AWS Snowball 设备。将数据复制到设备。将设备发送到 AWS 以将数据复制到 Amazon S3。"
    },
    "tags": [
      "DataSync",
      "S3",
      "Snowball"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 94%），解析仅供参考。】\n\n考察大规模数据迁移方案的选择，以及网络带宽限制下如何高效迁移大量数据到 Amazon S3。",
      "why_correct": "AWS Snowball 设备是针对大规模数据迁移设计的物理设备。题目中 10 PB 的数据量和有限的 500 Mbps 上行链路，使得直接通过网络传输数据变得缓慢。使用 Snowball，可以在本地将数据复制到设备，然后邮寄给 AWS，从而绕过网络带宽限制，实现快速迁移。",
      "why_wrong": "A 选项，AWS DataSync 适用于在线数据迁移，虽然可以自动验证数据，但受限于 500 Mbps 的上行链路，6 周内迁移 10 PB 数据不可行。B 选项，rsync 虽然是数据同步工具，但同样受限于网络带宽，不适合大规模数据迁移。C 选项，使用 AWS CLI 加上多个复制进程虽然可以并发上传，但本质上还是通过网络传输数据，受到带宽限制，无法满足时间要求。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS DataSync",
      "rsync",
      "AWS CLI",
      "AWS Snowball"
    ]
  },
  {
    "id": 605,
    "topic": "1",
    "question_en": "A company has several on-premises Internet Small Computer Systems Interface (ISCSI) network storage servers. The company wants to reduce the number of these servers by moving to the AWS Cloud. A solutions architect must provide low-latency access to frequently used data and reduce the dependency on on-premises servers with a minimal number of infrastructure changes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy an Amazon S3 File Gateway.",
      "B": "Deploy Amazon Elastic Block Store (Amazon EBS) storage with backups to Amazon S3.",
      "C": "Deploy an AWS Storage Gateway volume gateway that is configured with stored volumes.",
      "D": "Deploy an AWS Storage Gateway volume gateway that is configured with cached volumes."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有多个本地互联网小型计算机系统接口 (ISCSI) 网络存储服务器。该公司希望通过迁移到 AWS 云来减少这些服务器的数量。解决方案架构师必须提供对常用数据的低延迟访问，并减少对本地服务器的依赖，同时将基础设施更改降到最低。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "部署 Amazon S3 File Gateway。",
      "B": "部署 Amazon Elastic Block Store (Amazon EBS) 存储，并将备份到 Amazon S3。",
      "C": "部署一个 AWS Storage Gateway 卷网关，该网关配置有存储卷。",
      "D": "部署一个 AWS Storage Gateway 卷网关，该网关配置有缓存卷。"
    },
    "tags": [
      "S3 File Gateway",
      "EBS",
      "Storage Gateway",
      "ISCSI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查通过 AWS Storage Gateway 迁移本地 iSCSI 存储，并满足低延迟和最小化基础设施更改的需求。",
      "why_correct": "选项 D 部署了缓存卷的 AWS Storage Gateway 卷网关。缓存卷将常用的数据缓存在本地，从而提供低延迟访问。Storage Gateway 与 iSCSI 兼容，可以通过最小的基础设施更改来满足对本地服务器依赖的需求，同时将数据存储在 AWS 中。",
      "why_wrong": "选项 A 部署 Amazon S3 File Gateway，File Gateway 主要用于将文件对象存储在 S3 中，不直接支持 iSCSI 协议，不适用本地 iSCSI 存储迁移。选项 B 部署 Amazon EBS 存储，虽然可以提供低延迟，但迁移整个 EBS 存储需要大量基础设施更改，无法满足题目中对最小化基础设施更改的需求。选项 C 部署 Storage Gateway 卷网关，但未配置缓存卷，则不能利用本地缓存来提供低延迟访问，与题目对低延迟访问的需求相悖。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon EBS",
      "AWS Storage Gateway",
      "iSCSI",
      "File Gateway",
      "Volume Gateway",
      "Cache Volume"
    ]
  },
  {
    "id": 606,
    "topic": "1",
    "question_en": "A solutions architect is designing an application that will allow business users to upload objects to Amazon S3. The solution needs to maximize object durability. Objects also must be readily available at any time and for any length of time. Users will access objects frequently within the first 30 days after the objects are uploaded, but users are much less likely to access objects that are older than 30 days. Which solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Glacier after 30 days.",
      "B": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days.",
      "C": "Store all the objects in S3 Standard with an S3 Lifecycle rule to transition the objects to S3 One Zone-Infrequent Access (S3 One Zone- IA) after 30 days.",
      "D": "Store all the objects in S3 Intelligent-Tiering with an S3 Lifecycle rule to transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 30 days."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一位解决方案架构师正在设计一个应用程序，该应用程序将允许业务用户将对象上传到 Amazon S3。该解决方案需要最大化对象持久性。对象还必须随时可用，并且可以使用任何时长。用户在对象上传后的前 30 天内会频繁访问对象，但用户访问 30 天以上的对象的可能性要小得多。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "将所有对象存储在 S3 Standard 中，并使用 S3 生命周期规则在 30 天后将对象转换为 S3 Glacier。",
      "B": "将所有对象存储在 S3 Standard 中，并使用 S3 生命周期规则在 30 天后将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "C": "将所有对象存储在 S3 Standard 中，并使用 S3 生命周期规则在 30 天后将对象转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)。",
      "D": "使用 S3 Intelligent-Tiering 存储所有对象，并使用 S3 生命周期规则在 30 天后将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA)。"
    },
    "tags": [
      "S3",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier",
      "Lifecycle"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n此题考查 S3 存储类的选择。S3 Standard 提供高可用性和高性能，适用于频繁访问的数据。S3 Standard-IA 适用于不经常访问的数据，成本较低。S3 Glacier 适用于长期归档数据，成本最低。根据访问频率，选择合适的 S3 存储类。生命周期规则用于在不同存储类之间转换对象。",
      "why_correct": "S3 Standard-IA 适合于用户在前 30 天内频繁访问，30 天后访问频率较低的场景，具有成本效益。",
      "why_wrong": "选项 A 中 Glacier 适用于归档，不适用于频繁访问。选项 C 中 S3 One Zone-IA 可用性较低。选项 D 中使用 Intelligent-Tiering 增加了成本。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard-IA",
      "S3 Glacier",
      "S3 Intelligent-Tiering",
      "S3 Standard",
      "S3 Standard-Infrequent Access",
      "Lifecycle rules",
      "S3 One Zone-Infrequent Access"
    ]
  },
  {
    "id": 607,
    "topic": "1",
    "question_en": "A company has migrated a two-tier application from its on-premises data center to the AWS Cloud. The data tier is a Multi-AZ deployment of Amazon RDS for Oracle with 12 TB of General Purpose SSD Amazon Elastic Block Store (Amazon EBS) storage. The application is designed to process and store documents in the database as binary large objects (blobs) with an average document size of 6 MB. The database size has grown over time, reducing the performance and increasing the cost of storage. The company must improve the database performance and needs a solution that is highly available and resilient. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Reduce the RDS DB instance size. Increase the storage capacity to 24 TiB. Change the storage type to Magnetic.",
      "B": "Increase the RDS DB instance size. Increase the storage capacity to 24 TiChange the storage type to Provisioned IOPS.",
      "C": "Create an Amazon S3 bucket. Update the application to store documents in the S3 bucket. Store the object metadata in the existing database.",
      "D": "Create an Amazon DynamoDB table. Update the application to use DynamoDB. Use AWS Database Migration Service (AWS DMS) to migrate data from the Oracle database to DynamoDB."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司已将其两层应用程序从其本地数据中心迁移到 AWS 云。数据层是 Amazon RDS for Oracle 的多可用区部署，具有 12 TB 的通用 SSD Amazon Elastic Block Store (Amazon EBS) 存储。该应用程序设计为处理并将文档作为二进制大对象 (blob) 存储在数据库中，平均文档大小为 6 MB。随着时间的推移，数据库大小不断增长，降低了性能并增加了存储成本。公司必须提高数据库性能，并且需要一个高可用性和弹性的解决方案。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "减小 RDS 数据库实例大小。将存储容量增加到 24 TiB。将存储类型更改为 Magnetic。",
      "B": "增大 RDS 数据库实例大小。将存储容量增加到 24 TiB。将存储类型更改为 Provisioned IOPS。",
      "C": "创建一个 Amazon S3 存储桶。更新应用程序以将文档存储在 S3 存储桶中。将对象元数据存储在现有数据库中。",
      "D": "创建一个 Amazon DynamoDB 表。更新应用程序以使用 DynamoDB。使用 AWS Database Migration Service (AWS DMS) 将数据从 Oracle 数据库迁移到 DynamoDB。"
    },
    "tags": [
      "RDS",
      "Oracle",
      "S3",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查数据库优化和存储方案的选择。将文档存储在 S3 中，将元数据存储在数据库中，可以降低数据库存储成本和提高性能。使用 DynamoDB 替换 Oracle 数据库可能会涉及迁移成本，且需要调整应用程序代码。增加 RDS 实例存储空间，或调整存储类型，都无法完全解决数据库性能瓶颈。",
      "why_correct": "将文档存储在 S3 中，将文档元数据存储在 RDS 中，能够降低存储成本，提高数据库性能，满足题目要求。",
      "why_wrong": "选项 A 和 B 中增加 RDS 存储，无法解决数据库存储大量 blob 对象带来的性能问题。选项 D 中 DynamoDB 迁移涉及大量工作，成本较高。"
    },
    "related_terms": [
      "Amazon RDS for Oracle",
      "Amazon S3",
      "Amazon DynamoDB",
      "AWS Database Migration Service (AWS DMS)",
      "blob"
    ]
  },
  {
    "id": 608,
    "topic": "1",
    "question_en": "A company has an application that serves clients that are deployed in more than 20.000 retail storefront locations around the world. The application consists of backend web services that are exposed over HTTPS on port 443. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The retail locations communicate with the web application over the public internet. The company allows each retail location to register the IP address that the retail location has been allocated by its local ISP. The company's security team recommends to increase the security of the application endpoint by restricting access to only the IP addresses registered by the retail locations. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Associate an AWS WAF web ACL with the ALB. Use IP rule sets on the ALB to filter trafic. Update the IP addresses in the rule to include the registered IP addresses.",
      "B": "Deploy AWS Firewall Manager to manage the ALConfigure firewall rules to restrict trafic to the ALModify the firewall rules to include the registered IP addresses.",
      "C": "Store the IP addresses in an Amazon DynamoDB table. Configure an AWS Lambda authorization function on the ALB to validate that incoming requests are from the registered IP addresses.",
      "D": "Configure the network ACL on the subnet that contains the public interface of the ALB. Update the ingress rules on the network ACL with entries for each of the registered IP addresses."
    },
    "correct_answer": "A",
    "vote_percentage": "88%",
    "question_cn": "一家公司有一个应用程序，为部署在全球超过 20,000 个零售店地点的客户提供服务。该应用程序由后端 Web 服务组成，这些服务通过 HTTPS 在端口 443 上公开。该应用程序托管在 Application Load Balancer (ALB) 后面的 Amazon EC2 实例上。零售店通过公共互联网与 Web 应用程序通信。该公司允许每个零售店注册其本地 ISP 分配给该零售店的 IP 地址。该公司的安全团队建议通过仅限制对零售店注册的 IP 地址的访问来提高应用程序端点的安全性。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将 AWS WAF Web ACL 与 ALB 关联。在 ALB 上使用 IP 规则集来过滤流量。更新规则中的 IP 地址以包含已注册的 IP 地址。",
      "B": "部署 AWS Firewall Manager 以管理 ALB。配置防火墙规则以限制流量到 ALB。修改防火墙规则以包含已注册的 IP 地址。",
      "C": "将 IP 地址存储在 Amazon DynamoDB 表中。在 ALB 上配置 AWS Lambda 授权函数以验证传入请求是否来自已注册的 IP 地址。",
      "D": "在包含 ALB 公共接口的子网上配置网络 ACL。使用每个已注册 IP 地址的条目更新网络 ACL 上的入口规则。"
    },
    "tags": [
      "ALB",
      "WAF",
      "IP",
      "DynamoDB",
      "Lambda",
      "ACL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 88%），解析仅供参考。】\n\n本题考察 Web 应用程序的安全防护。AWS WAF 允许通过 IP 规则集限制对 Web 应用程序的访问。将 ALB 与 WAF Web ACL 关联，并配置 IP 规则，可以实现只允许注册 IP 地址访问。DynamoDB 和 Lambda 方案增加了复杂性。网络 ACL 限制了子网的访问，不够精细。",
      "why_correct": "使用 AWS WAF Web ACL 并配置 IP 规则，可以根据注册的 IP 地址限制访问，满足安全要求。",
      "why_wrong": "选项 B 中使用 Firewall Manager 可能涉及额外的配置。选项 C 中 Lambda 方案增加了复杂性。选项 D 中网络 ACL 限制了子网级别的访问，不够精细，无法实现针对注册 IP 地址的访问控制。"
    },
    "related_terms": [
      "Application Load Balancer",
      "ALB",
      "AWS WAF",
      "IP",
      "Amazon DynamoDB",
      "AWS Lambda",
      "Network ACL",
      "Firewall Manager"
    ]
  },
  {
    "id": 609,
    "topic": "1",
    "question_en": "A company is building a data analysis platform on AWS by using AWS Lake Formation. The platform will ingest data from different sources such as Amazon S3 and Amazon RDS. The company needs a secure solution to prevent access to portions of the data that contain sensitive information. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an IAM role that includes permissions to access Lake Formation tables.",
      "B": "Create data filters to implement row-level security and cell-level security.",
      "C": "Create an AWS Lambda function that removes sensitive information before Lake Formation ingests the data.",
      "D": "Create an AWS Lambda function that periodically queries and removes sensitive information from Lake Formation tables."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 AWS Lake Formation 在 AWS 上构建一个数据分析平台。该平台将从不同的源（例如 Amazon S3 和 Amazon RDS）摄取数据。该公司需要一个安全解决方案，以防止访问包含敏感信息的数据部分。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 IAM 角色，该角色包含访问 Lake Formation 表的权限。",
      "B": "创建数据过滤器以实施行级安全性和单元级安全性。",
      "C": "创建一个 AWS Lambda 函数，在 Lake Formation 摄取数据之前删除敏感信息。",
      "D": "创建一个 AWS Lambda 函数，该函数定期间隔查询并从 Lake Formation 表中删除敏感信息。"
    },
    "tags": [
      "Lake Formation",
      "IAM",
      "Lambda",
      "Data Filter"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 AWS Lake Formation 的安全特性，特别是行级和单元级安全性的实现方式，以及如何以最小的运营开销满足数据安全需求。",
      "why_correct": "选项 B 创建数据过滤器以实施行级安全性和单元级安全性，这正是 Lake Formation 提供的核心安全功能。数据过滤器允许根据用户或用户组的身份，限制对数据的访问，从而满足了保护敏感信息的安全需求。这种方法提供了精细的控制，并且易于管理。",
      "why_wrong": "选项 A 创建 IAM 角色，虽然可以控制访问权限，但它无法提供行级或单元级安全性，无法精细地控制对表中数据的访问。选项 C 和 D 涉及使用 Lambda 函数删除敏感信息，这增加了运营开销和复杂性，并且在数据被摄取到 Lake Formation 之后才进行处理，这不高效，且无法完全保证数据的安全性。"
    },
    "related_terms": [
      "AWS Lake Formation",
      "Amazon S3",
      "Amazon RDS",
      "IAM",
      "Lambda"
    ]
  },
  {
    "id": 610,
    "topic": "1",
    "question_en": "A company deploys Amazon EC2 instances that run in a VPC. The EC2 instances load source data into Amazon S3 buckets so that the data can be processed in the future. According to compliance laws, the data must not be transmitted over the public internet. Servers in the company's on-premises data center will consume the output from an application that runs on the EC2 instances. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy an interface VPC endpoint for Amazon EC2. Create an AWS Site-to-Site VPN connection between the company and the VPC.",
      "B": "Deploy a gateway VPC endpoint for Amazon S3. Set up an AWS Direct Connect connection between the on-premises network and the VPC.",
      "C": "Set up an AWS Transit Gateway connection from the VPC to the S3 buckets. Create an AWS Site-to-Site VPN connection between the company and the VPC.",
      "D": "Set up proxy EC2 instances that have routes to NAT gateways. Configure the proxy EC2 instances to fetch S3 data and feed the application instances."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家公司部署了在 VPC 中运行的 Amazon EC2 实例。EC2 实例将 源数据 加载到 Amazon S3 存储桶中，以便将来可以处理数据。根据合规性法规，数据不得通过公共互联网传输。该公司本地数据中心中的服务器将使用在 EC2 实例上运行的应用程序的输出。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Amazon EC2 部署接口 VPC endpoint。在公司和 VPC 之间创建 AWS Site-to-Site VPN 连接。",
      "B": "为 Amazon S3 部署网关 VPC endpoint。在本地网络和 VPC 之间设置 AWS Direct Connect 连接。",
      "C": "从 VPC 到 S3 存储桶设置 AWS Transit Gateway 连接。在公司和 VPC 之间创建 AWS Site-to-Site VPN 连接。",
      "D": "设置具有指向 NAT 网关的路由的代理 EC2 实例。配置代理 EC2 实例以获取 S3 数据并提供给应用程序实例。"
    },
    "tags": [
      "EC2",
      "S3",
      "VPC",
      "VPN",
      "Direct Connect",
      "Transit Gateway",
      "NAT Gateway",
      "VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n此题考察如何安全地访问 S3 数据。为了满足数据不出公网的要求，最佳实践是使用网关 VPC endpoint，这样 EC2 实例可以直接访问 S3，而无需通过公网。Direct Connect 连接提供私有网络连接，满足数据不出公网的要求。VPN 连接是备选项，但需要额外的配置。",
      "why_correct": "网关 VPC endpoint 提供了 EC2 实例访问 S3 的私有连接，且通过 Direct Connect 实现了本地网络和 VPC 之间的连接。",
      "why_wrong": "选项 A 中使用接口 VPC endpoint 不是最佳选择，且 VPN 连接增加了复杂性。选项 C 中 Transit Gateway 和 VPN 连接增加了复杂性。选项 D 中使用 NAT 网关，数据仍然经过公网，不满足数据不出公网的要求。"
    },
    "related_terms": [
      "EC2",
      "S3",
      "VPC",
      "VPN",
      "AWS Direct Connect",
      "AWS Transit Gateway",
      "NAT Gateway",
      "VPC endpoint"
    ]
  },
  {
    "id": 611,
    "topic": "1",
    "question_en": "A company has an application with a REST-based interface that allows data to be received in near-real time from a third-party vendor. Once received, the application processes and stores the data for further analysis. The application is running on Amazon EC2 instances. The third-party vendor has received many 503 Service Unavailable Errors when sending data to the application. When the data volume spikes, the compute capacity reaches its maximum limit and the application is unable to process all requests. Which design should a solutions architect recommend to provide a more scalable solution?",
    "options_en": {
      "A": "Use Amazon Kinesis Data Streams to ingest the data. Process the data using AWS Lambda functions.",
      "B": "Use Amazon API Gateway on top of the existing application. Create a usage plan with a quota limit for the third-party vendor.",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) to ingest the data. Put the EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
      "D": "Repackage the application as a container. Deploy the application using Amazon Elastic Container Service (Amazon ECS) using the EC2 launch type with an Auto Scaling group."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个基于 REST 的接口的应用程序，允许近乎实时地从第三方供应商接收数据。接收到数据后，应用程序会处理并存储数据以供进一步分析。该应用程序运行在 Amazon EC2 实例上。第三方供应商在向应用程序发送数据时收到了许多 503 Service Unavailable 错误。当数据量激增时，计算容量达到其最大限制，并且应用程序无法处理所有请求。解决方案架构师应该推荐哪种设计以提供更具可扩展性的解决方案？",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Streams 摄取数据。使用 AWS Lambda 函数处理数据。",
      "B": "在现有应用程序之上使用 Amazon API Gateway。为第三方供应商创建一个具有配额限制的使用计划。",
      "C": "使用 Amazon Simple Notification Service (Amazon SNS) 摄取数据。将 EC2 实例放入 Application Load Balancer 后面的 Auto Scaling 组中。",
      "D": "将应用程序重新打包为容器。使用 Amazon Elastic Container Service (Amazon ECS) 使用 EC2 启动类型部署应用程序，并使用 Auto Scaling 组。"
    },
    "tags": [
      "Kinesis Data Streams",
      "Lambda",
      "API Gateway",
      "EC2",
      "ECS",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察如何构建可扩展的应用程序。当出现 503 错误时，表明应用程序无法处理请求。最佳方案是使用 Kinesis Data Streams 接收数据，并使用 Lambda 函数处理数据，实现高可扩展性。使用 API Gateway 可以限制流量，但不能解决根本问题。EC2 实例和 ECS 方案需要手动管理容量，扩展性不如 Kinesis 和 Lambda。",
      "why_correct": "Kinesis Data Streams 和 Lambda 函数的组合能够提供高可扩展性，并自动处理激增的数据。",
      "why_wrong": "选项 B 中 API Gateway 只能限制流量，不能解决容量不足的问题。选项 C 中 SNS 方案的扩展性不如 Kinesis。选项 D 中 ECS 方案手动管理容量。"
    },
    "related_terms": [
      "Amazon Kinesis Data Streams",
      "AWS Lambda",
      "Amazon API Gateway",
      "Amazon EC2",
      "Amazon Elastic Container Service",
      "Amazon ECS",
      "Auto Scaling"
    ]
  },
  {
    "id": 612,
    "topic": "1",
    "question_en": "A company has an application that runs on Amazon EC2 instances in a private subnet. The application needs to process sensitive information from an Amazon S3 bucket. The application must not use the internet to connect to the S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an internet gateway. Update the S3 bucket policy to allow access from the internet gateway. Update the application to use the new internet gateway.",
      "B": "Configure a VPN connection. Update the S3 bucket policy to allow access from the VPN connection. Update the application to use the new VPN connection.",
      "C": "Configure a NAT gateway. Update the S3 bucket policy to allow access from the NAT gateway. Update the application to use the new NAT gateway.",
      "D": "Configure a VPC endpoint. Update the S3 bucket policy to allow access from the VPC endpoint. Update the application to use the new VPC endpoint."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个应用程序，该程序运行在私有子网中的 Amazon EC2 实例上。该应用程序需要处理来自 Amazon S3 存储桶的敏感信息。该应用程序不得使用互联网连接到 S3 存储桶。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置一个互联网网关。更新 S3 存储桶策略以允许从互联网网关进行访问。更新应用程序以使用新的互联网网关。",
      "B": "配置一个 VPN 连接。更新 S3 存储桶策略以允许从 VPN 连接进行访问。更新应用程序以使用新的 VPN 连接。",
      "C": "配置一个 NAT 网关。更新 S3 存储桶策略以允许从 NAT 网关进行访问。更新应用程序以使用新的 NAT 网关。",
      "D": "配置一个 VPC endpoint。更新 S3 存储桶策略以允许从 VPC endpoint 进行访问。更新应用程序以使用新的 VPC endpoint。"
    },
    "tags": [
      "S3",
      "EC2",
      "VPC",
      "Internet Gateway",
      "VPN",
      "NAT Gateway",
      "VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查了在私有子网中的 EC2 实例访问 S3 存储桶时，如何避免使用互联网连接，保障安全性和合规性。",
      "why_correct": "VPC endpoint 允许私有子网中的 EC2 实例通过 Amazon 网络的私有连接访问 S3，而无需经过互联网。 这满足了题目中应用程序不得使用互联网的要求。同时，只需更新 S3 存储桶策略以允许从 VPC endpoint 进行访问，应用程序就可以使用 VPC endpoint 来访问 S3。",
      "why_wrong": "A 选项，互联网网关（Internet Gateway）提供了对互联网的访问，这与题目中“不得使用互联网”的要求相悖。B 选项，VPN 连接虽然提供了私有连接，但增加了额外的配置和管理复杂性，并且不如 VPC endpoint 直接。C 选项，NAT 网关允许私有子网的实例访问互联网，但同样违反了题目“不得使用互联网”的要求，并且也增加了额外的开销。"
    },
    "related_terms": [
      "Amazon EC2",
      "S3",
      "Internet Gateway",
      "VPN",
      "NAT Gateway",
      "VPC endpoint",
      "VPC",
      "S3 bucket policy",
      "private subnet"
    ]
  },
  {
    "id": 613,
    "topic": "1",
    "question_en": "A company uses Amazon Elastic Kubernetes Service (Amazon EKS) to run a container application. The EKS cluster stores sensitive information in the Kubernetes secrets object. The company wants to ensure that the information is encrypted. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use the container application to encrypt the information by using AWS Key Management Service (AWS KMS).",
      "B": "Enable secrets encryption in the EKS cluster by using AWS Key Management Service (AWS KMS).",
      "C": "Implement an AWS Lambda function to encrypt the information by using AWS Key Management Service (AWS KMS).",
      "D": "Use AWS Systems Manager Parameter Store to encrypt the information by using AWS Key Management Service (AWS KMS)."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon Elastic Kubernetes Service (Amazon EKS) 运行容器应用程序。EKS 集群将敏感信息存储在 Kubernetes 秘密对象中。该公司希望确保信息被加密。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用容器应用程序通过使用 AWS Key Management Service (AWS KMS) 加密信息。",
      "B": "通过使用 AWS Key Management Service (AWS KMS) 在 EKS 集群中打开秘密加密。",
      "C": "实施一个 AWS Lambda 函数，通过使用 AWS Key Management Service (AWS KMS) 加密信息。",
      "D": "使用 AWS Systems Manager Parameter Store 通过使用 AWS Key Management Service (AWS KMS) 加密信息。"
    },
    "tags": [
      "EKS",
      "KMS",
      "Secrets",
      "Lambda",
      "Parameter Store"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 Kubernetes 秘密对象的加密。在 EKS 中，可以使用 KMS 加密 Kubernetes 秘密。这可以通过在创建集群时启用 Secrets encryption 来实现，而无需编写额外的代码。Lambda 函数和 Systems Manager Parameter Store 都需要额外的配置和管理。",
      "why_correct": "使用 KMS 集成在 EKS 中加密秘密，无需额外的代码，简化了运维。",
      "why_wrong": "选项 A 中使用 KMS 加密信息需要在应用程序中实现。选项 C 和 D 中需要额外的 Lambda 函数或 Systems Manager Parameter Store 来加密。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service",
      "EKS",
      "AWS Key Management Service",
      "KMS",
      "AWS Lambda",
      "AWS Systems Manager Parameter Store",
      "Kubernetes Secrets"
    ]
  },
  {
    "id": 614,
    "topic": "1",
    "question_en": "A company is designing a new multi-tier web application that consists of the following components: • Web and application servers that run on Amazon EC2 instances as part of Auto Scaling groups • An Amazon RDS DB instance for data storage A solutions architect needs to limit access to the application servers so that only the web servers can access them. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy AWS PrivateLink in front of the application servers. Configure the network ACL to allow only the web servers to access the application servers.",
      "B": "Deploy a VPC endpoint in front of the application servers. Configure the security group to allow only the web servers to access the application servers.",
      "C": "Deploy a Network Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the network ACL to allow only the web servers to access the application servers.",
      "D": "Deploy an Application Load Balancer with a target group that contains the application servers' Auto Scaling group. Configure the security group to allow only the web servers to access the application servers."
    },
    "correct_answer": "D",
    "vote_percentage": "83%",
    "question_cn": "一家公司正在设计一个新的多层 Web 应用程序，该应用程序由以下组件组成：• 在作为 Auto Scaling 组一部分的 Amazon EC2 实例上运行的 Web 和应用程序服务器 • 用于数据存储的 Amazon RDS DB 实例 解决方案架构师需要限制对应用程序服务器的访问，以便只有 Web 服务器才能访问它们。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在应用程序服务器前面部署 AWS PrivateLink。配置网络 ACL 以允许只有 Web 服务器才能访问应用程序服务器。",
      "B": "在应用程序服务器前面部署 VPC 端点。配置安全组以允许只有 Web 服务器才能访问应用程序服务器。",
      "C": "部署具有包含应用程序服务器的 Auto Scaling 组的目标组的网络负载均衡器。配置网络 ACL 以允许只有 Web 服务器才能访问应用程序服务器。",
      "D": "部署具有包含应用程序服务器的 Auto Scaling 组的目标组的 Application Load Balancer。配置安全组以允许只有 Web 服务器才能访问应用程序服务器。"
    },
    "tags": [
      "ALB",
      "Security Group",
      "PrivateLink",
      "VPC endpoint",
      "Network ACL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 83%），解析仅供参考。】\n\n考察如何使用安全组和负载均衡器限制对 Amazon EC2 实例的访问，以满足应用程序服务器只被 Web 服务器访问的需求。",
      "why_correct": "选项 D 正确。使用 Application Load Balancer (ALB) 可以提供基于 HTTP/HTTPS 协议的流量管理和高级路由功能。通过配置 ALB 的 Target Group 包含应用程序服务器，并配置安全组只允许 Web 服务器访问 ALB，从而实现了对应用程序服务器的访问控制。",
      "why_wrong": "选项 A 错误，AWS PrivateLink 主要用于在 VPC 之间或与 AWS 服务之间私密地访问服务，在此场景下无法直接拦截来自 Web 服务器的流量。选项 B 错误，VPC 端点主要用于访问 AWS 服务，而非应用程序服务器。选项 C 错误，虽然 Network Load Balancer (NLB) 可以作为负载均衡器，但其主要功能是基于 TCP/UDP 的流量转发，不像 ALB 那样支持基于 HTTP/HTTPS 的路由，难以实现基于 HTTP 流量的精确控制和访问限制，而且 NLB 也不具有配置安全组的能力，所以此选项不满足题干要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "Web server",
      "Application Load Balancer (ALB)",
      "Target Group",
      "Security Group",
      "AWS PrivateLink",
      "VPC Endpoint",
      "Network Load Balancer (NLB)",
      "Network ACL"
    ]
  },
  {
    "id": 615,
    "topic": "1",
    "question_en": "A company runs a critical, customer-facing application on Amazon Elastic Kubernetes Service (Amazon EKS). The application has a microservices architecture. The company needs to implement a solution that collects, aggregates, and summarizes metrics and logs from the application in a centralized location. Which solution meets these requirements?",
    "options_en": {
      "A": "Run the Amazon CloudWatch agent in the existing EKS cluster. View the metrics and logs in the CloudWatch console.",
      "B": "Run AWS App Mesh in the existing EKS cluster. View the metrics and logs in the App Mesh console.",
      "C": "Configure AWS CloudTrail to capture data events. Query CloudTrail by using Amazon OpenSearch Service.",
      "D": "Configure Amazon CloudWatch Container Insights in the existing EKS cluster. View the metrics and logs in the CloudWatch console."
    },
    "correct_answer": "D",
    "vote_percentage": "90%",
    "question_cn": "一家公司在 Amazon Elastic Kubernetes Service (Amazon EKS) 上运行关键的面向客户的应用程序。该应用程序具有微服务架构。该公司需要实施一个解决方案，该解决方案从应用程序中收集、聚合和总结指标和日志，并将其集中存储。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "在现有的 EKS 集群中运行 Amazon CloudWatch 代理。在 CloudWatch 控制台中查看指标和日志。",
      "B": "在现有的 EKS 集群中运行 AWS App Mesh。在 App Mesh 控制台中查看指标和日志。",
      "C": "配置 AWS CloudTrail 以捕获数据事件。使用 Amazon OpenSearch Service 查询 CloudTrail。",
      "D": "在现有的 EKS 集群中配置 Amazon CloudWatch Container Insights。在 CloudWatch 控制台中查看指标和日志。"
    },
    "tags": [
      "EKS",
      "CloudWatch",
      "Container Insights",
      "CloudTrail",
      "OpenSearch Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 90%），解析仅供参考。】\n\n考查在 EKS 集群中收集、聚合和存储应用程序指标和日志的解决方案。",
      "why_correct": "Amazon CloudWatch Container Insights 专为 Kubernetes 环境设计，可以自动收集、聚合和总结 EKS 集群中的指标和日志。Container Insights 监控应用程序、容器、节点和集群层面的性能数据，并将数据集中存储在 CloudWatch 中，方便查看和分析，完全符合题目要求。",
      "why_wrong": "选项 A 仅使用 CloudWatch 代理，虽然可以收集指标和日志，但缺乏针对 EKS 的容器化环境的优化，无法提供 Container Insights 提供的深入分析能力。选项 B 使用 AWS App Mesh 更多关注于服务间的流量管理，而非指标和日志的收集与存储。选项 C 使用 CloudTrail 捕获数据事件，主要用于审计和合规，而非应用程序指标和日志的聚合与总结，并且 OpenSearch Service 的定位也并非指标的聚合与总结。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon CloudWatch",
      "Amazon CloudWatch Container Insights",
      "AWS App Mesh",
      "AWS CloudTrail",
      "Amazon OpenSearch Service",
      "Kubernetes",
      "EKS",
      "CloudWatch"
    ]
  },
  {
    "id": 616,
    "topic": "1",
    "question_en": "A company has deployed its newest product on AWS. The product runs in an Auto Scaling group behind a Network Load Balancer. The company stores the product’s objects in an Amazon S3 bucket. The company recently experienced malicious attacks against its systems. The company needs a solution that continuously monitors for malicious activity in the AWS account, workloads, and access patterns to the S3 bucket. The solution must also report suspicious activity and display the information on a dashboard. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure Amazon Macie to monitor and report findings to AWS Config.",
      "B": "Configure Amazon Inspector to monitor and report findings to AWS CloudTrail.",
      "C": "Configure Amazon GuardDuty to monitor and report findings to AWS Security Hub.",
      "D": "Configure AWS Config to monitor and report findings to Amazon EventBridge."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司已在 AWS 上部署了其最新产品。该产品在一个 Network Load Balancer 后的 Auto Scaling 组中运行。该公司将其产品的对象存储在 Amazon S3 存储桶中。该公司最近遭受了针对其系统的恶意攻击。该公司需要一个解决方案，该解决方案持续监控 AWS 账户、工作负载和对 S3 存储桶的访问模式中的恶意活动。该解决方案还必须报告可疑活动并在仪表板上显示信息。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Macie 以监控并将调查结果报告给 AWS Config。",
      "B": "配置 Amazon Inspector 以监控并将调查结果报告给 AWS CloudTrail。",
      "C": "配置 Amazon GuardDuty 以监控并将调查结果报告给 AWS Security Hub。",
      "D": "配置 AWS Config 以监控并将调查结果报告给 Amazon EventBridge。"
    },
    "tags": [
      "S3",
      "GuardDuty",
      "Security Hub",
      "Macie",
      "Inspector",
      "CloudTrail",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查利用 AWS 安全服务监控恶意活动，并报告和展示安全事件的能力。",
      "why_correct": "Amazon GuardDuty 能够持续监控 AWS 账户和工作负载，检测威胁。它能分析 CloudTrail 日志、VPC Flow Logs 和 DNS 查询等数据源。GuardDuty 检测到的威胁会被报告给 AWS Security Hub，在 Security Hub 仪表板上集中展示安全信息，满足题目对报告和展示的要求。",
      "why_wrong": "选项 A，Amazon Macie 主要用于发现和保护 S3 中的敏感数据，虽然也能报告安全问题，但其侧重点并非持续监控账户和工作负载中的恶意活动，且与 AWS Config 的结合不符合题意。选项 B，Amazon Inspector 专注于评估 EC2 实例的安全状况，并不直接监控账户活动和 S3 访问模式，报告给 CloudTrail 也无法满足对仪表盘展示的要求。选项 D，AWS Config 主要用于资源配置合规性监控，并不具备直接的威胁检测能力，与 Amazon EventBridge 结合也无法满足题目的核心需求，无法提供威胁检测和安全事件展示功能。"
    },
    "related_terms": [
      "Amazon S3",
      "Network Load Balancer",
      "Auto Scaling",
      "Amazon Macie",
      "AWS Config",
      "Amazon Inspector",
      "AWS CloudTrail",
      "Amazon GuardDuty",
      "AWS Security Hub",
      "Amazon EventBridge",
      "VPC Flow Logs",
      "DNS"
    ]
  },
  {
    "id": 617,
    "topic": "1",
    "question_en": "A company wants to migrate an on-premises data center to AWS. The data center hosts a storage server that stores data in an NFS-based file system. The storage server holds 200 GB of data. The company needs to migrate the data without interruption to existing services. Multiple resources in AWS must be able to access the data by using the NFS protocol. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon FSx for Lustre file system.",
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system.",
      "C": "Create an Amazon S3 bucket to receive the data.",
      "D": "Manually use an operating system copy command to push the data into the AWS destination",
      "E": "Install an AWS DataSync agent in the on-premises data center. Use a DataSync task between the on-premises location and AWS."
    },
    "correct_answer": "BE",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其本地数据中心迁移到 AWS。数据中心托管一个存储服务器，该服务器将数据存储在基于 NFS 的文件系统中。存储服务器包含 200 GB 的数据。公司需要在不中断现有服务的情况下迁移数据。AWS 中的多个资源必须能够通过使用 NFS 协议来访问数据。哪种步骤组合将以最具成本效益的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建 Amazon FSx for Lustre 文件系统。",
      "B": "创建 Amazon Elastic File System (Amazon EFS) 文件系统。",
      "C": "创建 Amazon S3 存储桶以接收数据。",
      "D": "手动使用操作系统复制命令将数据推送到 AWS 目标。",
      "E": "在本地数据中心安装 AWS DataSync 代理。使用 DataSync 任务在本地位置和 AWS 之间进行数据迁移。"
    },
    "tags": [
      "DataSync",
      "FSx for Lustre",
      "EFS",
      "S3",
      "NFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 100%），解析仅供参考。】\n\n考查了使用 NFS 协议迁移本地数据到 AWS，并允许多个 AWS 资源访问数据的成本效益方案。",
      "why_correct": "Amazon EFS 支持 NFS 协议，是为 Linux 工作负载设计的可扩展文件系统。创建 EFS 可以满足通过 NFS 访问数据的需求，并且易于扩展。AWS DataSync 能够安全、快速地将数据从本地文件系统迁移到 EFS。通过 DataSync，可以持续地同步数据，减少停机时间。",
      "why_wrong": "Amazon FSx for Lustre 虽然支持高性能计算场景，但相对 EFS 来说成本更高，对于 200GB 的数据，性价比不高。Amazon S3 是对象存储，不支持 NFS 协议，无法满足使用 NFS 协议访问数据的要求。手动使用操作系统复制命令迁移数据虽然可行，但效率低下，并且容易因为网络中断而失败，无法满足不中断现有服务的要求。"
    },
    "related_terms": [
      "Amazon FSx for Lustre",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon S3",
      "AWS DataSync",
      "NFS"
    ]
  },
  {
    "id": 618,
    "topic": "1",
    "question_en": "A company wants to use Amazon FSx for Windows File Server for its Amazon EC2 instances that have an SMB file share mounted as a volume in the us-east-1 Region. The company has a recovery point objective (RPO) of 5 minutes for planned system maintenance or unplanned service disruptions. The company needs to replicate the file system to the us-west-2 Region. The replicated data must not be deleted by any user for 5 years. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.",
      "B": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.",
      "C": "Create an FSx for Windows File Server file system in us-east-1 that has a Multi-AZ deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in compliance mode for a target vault in us-west-2. Configure a minimum duration of 5 years.",
      "D": "Create an FSx for Windows File Server file system in us-east-1 that has a Single-AZ 2 deployment type. Use AWS Backup to create a daily backup plan that includes a backup rule that copies the backup to us-west-2. Configure AWS Backup Vault Lock in governance mode for a target vault in us-west-2. Configure a minimum duration of 5 years."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望为其在 us-east-1 区域中作为卷挂载 SMB 文件共享的 Amazon EC2 实例使用 Amazon FSx for Windows File Server。该公司对计划内的系统维护或计划外的服务中断的恢复点目标 (RPO) 为 5 分钟。该公司需要将文件系统复制到 us-west-2 区域。复制的数据不得被任何用户删除，为期 5 年。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 us-east-1 中创建一个 FSx for Windows File Server 文件系统，该系统具有 Single-AZ 2 部署类型。使用 AWS Backup 创建一个每日备份计划，其中包括一个将备份复制到 us-west-2 的备份规则。为 us-west-2 中的目标 Vault 配置 AWS Backup Vault Lock 的合规模式。配置最短持续时间为 5 年。",
      "B": "在 us-east-1 中创建一个 FSx for Windows File Server 文件系统，该系统具有 Multi-AZ 部署类型。使用 AWS Backup 创建一个每日备份计划，其中包括一个将备份复制到 us-west-2 的备份规则。为 us-west-2 中的目标 Vault 配置 AWS Backup Vault Lock 的管理模式。配置最短持续时间为 5 年。",
      "C": "在 us-east-1 中创建一个 FSx for Windows File Server 文件系统，该系统具有 Multi-AZ 部署类型。使用 AWS Backup 创建一个每日备份计划，其中包括一个将备份复制到 us-west-2 的备份规则。为 us-west-2 中的目标 Vault 配置 AWS Backup Vault Lock 的合规模式。配置最短持续时间为 5 年。",
      "D": "在 us-east-1 中创建一个 FSx for Windows File Server 文件系统，该系统具有 Single-AZ 2 部署类型。使用 AWS Backup 创建一个每日备份计划，其中包括一个将备份复制到 us-west-2 的备份规则。为 us-west-2 中的目标 Vault 配置 AWS Backup Vault Lock 的管理模式。配置最短持续时间为 5 年。"
    },
    "tags": [
      "FSx for Windows File Server",
      "Backup",
      "RPO",
      "Backup Vault Lock",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察 FSx for Windows File Server 的灾难恢复和数据保护。要求 RPO 为 5 分钟，且数据复制到另一个区域，并防止删除 5 年。Multi-AZ 架构提供高可用性，AWS Backup 用于备份和跨区域复制，Backup Vault Lock 用于数据保护，合规模式满足不可删除要求。",
      "why_correct": "C 选项，Multi-AZ 提供高可用性，AWS Backup 用于备份和跨区域复制，Backup Vault Lock 的合规模式满足数据保护要求。",
      "why_wrong": "A 选项，Single-AZ 架构不具备高可用性。B 选项，管理模式允许管理员删除备份，不满足数据保护要求。D 选项，Single-AZ 架构不具备高可用性。管理模式允许管理员删除备份，不满足数据保护要求。"
    },
    "related_terms": [
      "FSx for Windows File Server",
      "AWS Backup",
      "Multi-AZ",
      "Backup Vault Lock",
      "Single-AZ"
    ]
  },
  {
    "id": 619,
    "topic": "1",
    "question_en": "A solutions architect is designing a security solution for a company that wants to provide developers with individual AWS accounts through AWS Organizations, while also maintaining standard security controls. Because the individual developers will have AWS account root user- level access to their own accounts, the solutions architect wants to ensure that the mandatory AWS CloudTrail configuration that is applied to new developer accounts is not modified. Which action meets these requirements?",
    "options_en": {
      "A": "Create an IAM policy that prohibits changes to CloudTrail. and attach it to the root user.",
      "B": "Create a new trail in CloudTrail from within the developer accounts with the organization trails option enabled.",
      "C": "Create a service control policy (SCP) that prohibits changes to CloudTrail, and attach it the developer accounts.",
      "D": "Create a service-linked role for CloudTrail with a policy condition that allows changes only from an Amazon Resource Name (ARN) in the management account."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为一家公司设计安全解决方案，该公司希望通过 AWS Organizations 为开发人员提供单独的 AWS 账户，同时维护标准安全控制。由于各个开发人员将拥有对其自己账户的 AWS 账户根用户级别的访问权限，因此解决方案架构师希望确保应用于新开发人员账户的强制性 AWS CloudTrail 配置不会被修改。哪个操作满足这些要求？",
    "options_cn": {
      "A": "创建一个 IAM 策略，禁止更改 CloudTrail，并将其附加到根用户。",
      "B": "在开发人员账户内从 CloudTrail 创建一个新的跟踪，并启用组织跟踪选项。",
      "C": "创建一个服务控制策略 (SCP)，禁止更改 CloudTrail，并将其附加到开发人员账户。",
      "D": "为 CloudTrail 创建一个服务关联角色，其策略条件仅允许从管理账户中的 Amazon 资源名称 (ARN) 进行更改。"
    },
    "tags": [
      "CloudTrail",
      "Organizations",
      "SCP",
      "IAM",
      "Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察 AWS Organizations 中 CloudTrail 的保护。通过服务控制策略 (SCP) 可以禁止在子账户中修改 CloudTrail 配置，从而保证了审计日志的完整性。",
      "why_correct": "SCP 可以防止开发人员账户修改 CloudTrail 配置，满足强制性配置的需求。",
      "why_wrong": "A 选项，IAM 策略仅限于单账户，无法在组织级别生效。B 选项，在开发人员账户中创建组织跟踪将导致多个 CloudTrail 跟踪，不符合要求。D 选项，服务关联角色通常用于服务代表您执行操作，无法满足禁止修改 CloudTrail 的要求。"
    },
    "related_terms": [
      "AWS CloudTrail",
      "IAM",
      "SCP",
      "ARN",
      "Organizations"
    ]
  },
  {
    "id": 620,
    "topic": "1",
    "question_en": "A company is planning to deploy a business-critical application in the AWS Cloud. The application requires durable storage with consistent, low-latency performance. Which type of storage should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Instance store volume",
      "B": "Amazon ElastiCache for Memcached cluster",
      "C": "Provisioned IOPS SSD Amazon Elastic Block Store (Amazon EBS) volume",
      "D": "Throughput Optimized HDD Amazon Elastic Block Store (Amazon EBS) volume"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划在 AWS 云中部署一项关键业务应用程序。该应用程序需要具有一致的低延迟性能的持久性存储。 解决方案架构师应该推荐哪种类型的存储来满足这些需求？",
    "options_cn": {
      "A": "实例存储卷",
      "B": "用于 Memcached 集群的 Amazon ElastiCache",
      "C": "预置 IOPS SSD Amazon Elastic Block Store (Amazon EBS) 卷",
      "D": "吞吐量优化 HDD Amazon Elastic Block Store (Amazon EBS) 卷"
    },
    "tags": [
      "EBS",
      "Instance Store",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n该题考察了持久性存储的特点。预置 IOPS SSD EBS 卷提供一致的低延迟性能，满足关键业务应用程序的需求。",
      "why_correct": "预置 IOPS SSD EBS 卷针对高 I/O 密集型工作负载提供了性能保证，满足一致的低延迟性能需求。",
      "why_wrong": "A 选项，实例存储卷是临时的，数据会丢失。B 选项，ElastiCache 适用于缓存，不适合持久性存储。D 选项，吞吐量优化 HDD EBS 卷侧重于吞吐量，而不是低延迟。"
    },
    "related_terms": [
      "Amazon ElastiCache",
      "Amazon EBS",
      "Instance Store"
    ]
  },
  {
    "id": 621,
    "topic": "1",
    "question_en": "An online photo-sharing company stores its photos in an Amazon S3 bucket that exists in the us-west-1 Region. The company needs to store a copy of all new photos in the us-east-1 Region. Which solution will meet this requirement with the LEAST operational effort?",
    "options_en": {
      "A": "Create a second S3 bucket in us-east-1. Use S3 Cross-Region Replication to copy photos from the existing S3 bucket to the second S3 bucket.",
      "B": "Create a cross-origin resource sharing (CORS) configuration of the existing S3 bucket. Specify us-east-1 in the CORS rule's AllowedOrigin element.",
      "C": "Create a second S3 bucket in us-east-1 across multiple Availability Zones. Create an S3 Lifecycle rule to save photos into the second S3 bucket.",
      "D": "Create a second S3 bucket in us-east-1. Configure S3 event notifications on object creation and update events to invoke an AWS Lambda function to copy photos from the existing S3 bucket to the second S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家在线照片分享公司将其照片存储在位于 us-west-1 区域的 Amazon S3 存储桶中。该公司需要在 us-east-1 区域存储所有新照片的副本。哪种解决方案将以最少的运营工作量满足此要求？",
    "options_cn": {
      "A": "在 us-east-1 中创建第二个 S3 存储桶。使用 S3 跨区域复制将照片从现有 S3 存储桶复制到第二个 S3 存储桶。",
      "B": "创建现有 S3 存储桶的跨源资源共享 (CORS) 配置。在 CORS 规则的 AllowedOrigin 元素中指定 us-east-1。",
      "C": "在 us-east-1 中创建跨多个可用区的第二个 S3 存储桶。创建 S3 生命周期规则以将照片保存到第二个 S3 存储桶中。",
      "D": "在 us-east-1 中创建第二个 S3 存储桶。配置 S3 事件通知，针对对象创建和更新事件，以调用 AWS Lambda 函数将照片从现有 S3 存储桶复制到第二个 S3 存储桶。"
    },
    "tags": [
      "S3",
      "Cross-Region Replication",
      "CORS",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n该题考查了 S3 存储桶的数据复制。S3 跨区域复制是最简单、运营成本最低的方案，可以自动复制新照片到另一个区域。",
      "why_correct": "S3 跨区域复制提供了一种自动将对象复制到另一个区域的解决方案，并且运营开销最小。",
      "why_wrong": "B 选项，CORS 主要用于跨源访问，而不是数据复制。C 选项，S3 生命周期规则用于管理存储桶中的对象生命周期，而不是复制数据。D 选项，使用 Lambda 复制文件，增加了额外的运营开销，并且并非最佳选择。"
    },
    "related_terms": [
      "S3",
      "CORS",
      "Lambda",
      "Cross-Region Replication"
    ]
  },
  {
    "id": 622,
    "topic": "1",
    "question_en": "A company is creating a new web application for its subscribers. The application will consist of a static single page and a persistent database layer. The application will have millions of users for 4 hours in the morning, but the application will have only a few thousand users during the rest of the day. The company's data architects have requested the ability to rapidly evolve their schema. Which solutions will meet these requirements and provide the MOST scalability? (Choose two.)",
    "options_en": {
      "A": "Deploy Amazon DynamoDB as the database solution. Provision on-demand capacity.",
      "B": "Deploy Amazon Aurora as the database solution. Choose the serverless DB engine mode.",
      "C": "Deploy Amazon DynamoDB as the database solution. Ensure that DynamoDB auto scaling is enabled.",
      "D": "Deploy the static content into an Amazon S3 bucket. Provision an Amazon CloudFront distribution with the S3 bucket as the origin",
      "E": "Deploy the web servers for static content across a fieet of Amazon EC2 instances in Auto Scaling groups. Configure the instances to periodically refresh the content from an Amazon Elastic File System (Amazon EFS) volume."
    },
    "correct_answer": "AD",
    "vote_percentage": "60%",
    "question_cn": "一家公司正在为其订阅者创建一个新的 Web 应用程序。该应用程序将包含一个静态单页和一个持久数据库层。该应用程序将在早晨的 4 个小时内拥有数百万用户，但在一天中的其余时间只有几千个用户。该公司的数据架构师要求能够快速演进其模式。哪些解决方案将满足这些要求并提供最大的可扩展性？（选择两个。）",
    "options_cn": {
      "A": "部署 Amazon DynamoDB 作为数据库解决方案。按需预置容量。",
      "B": "部署 Amazon Aurora 作为数据库解决方案。选择无服务器数据库引擎模式。",
      "C": "部署 Amazon DynamoDB 作为数据库解决方案。确保启用 DynamoDB 自动伸缩。",
      "D": "将静态内容部署到 Amazon S3 存储桶中。预置一个以 S3 存储桶为源的 Amazon CloudFront 分发。",
      "E": "将用于静态内容的 Web 服务器部署到 Auto Scaling 组中的一组 Amazon EC2 实例中。将这些实例配置为定期从 Amazon Elastic File System (Amazon EFS) 卷刷新内容。"
    },
    "tags": [
      "DynamoDB",
      "Aurora",
      "S3",
      "CloudFront",
      "EC2",
      "EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 60%），解析仅供参考。】\n\n考查了 Web 应用程序的可扩展性设计，包括数据库选择和静态内容存储与分发。重点关注 DynamoDB 的按需容量和 S3/CloudFront 的静态内容托管能力。",
      "why_correct": "Amazon DynamoDB 是一种完全托管的 NoSQL 数据库，非常适合具有可变负载的应用程序，其按需容量模式可以根据流量自动扩展，满足早晨高峰需求。Amazon S3 提供了高度可扩展的存储，而 Amazon CloudFront 是一种内容分发网络（CDN），可以将静态内容缓存到全球边缘站点，从而提高性能并处理大量并发用户。",
      "why_wrong": "Amazon Aurora 是一种关系型数据库服务，虽然性能良好，但其服务器无服务器模式也可能无法完全满足高峰期间的极高负载需求，不如 DynamoDB 的按需容量灵活。 DynamoDB 自动伸缩是另一种容量管理方式，虽然可以应对流量变化，但按需容量更适合本题中描述的流量模式。使用 EC2 实例存储静态内容并从 EFS 卷刷新内容不如 S3 和 CloudFront 组合的性能和扩展性好，也增加了运维复杂性，EC2 不擅长应付静态内容的并发访问。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon Aurora",
      "Amazon S3",
      "Amazon CloudFront",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon Elastic File System (Amazon EFS)",
      "CDN",
      "NoSQL"
    ]
  },
  {
    "id": 623,
    "topic": "1",
    "question_en": "A company uses Amazon API Gateway to manage its REST APIs that third-party service providers access. The company must protect the REST APIs from SQL injection and cross-site scripting attacks. What is the MOST operationally eficient solution that meets these requirements?",
    "options_en": {
      "A": "Configure AWS Shield.",
      "B": "Configure AWS WAF.",
      "C": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS Shield in CloudFront.",
      "D": "Set up API Gateway with an Amazon CloudFront distribution. Configure AWS WAF in CloudFront."
    },
    "correct_answer": "B",
    "vote_percentage": "89%",
    "question_cn": "一家公司使用 Amazon API Gateway 管理第三方服务提供商访问的 REST API。公司必须保护 REST API 免受 SQL 注入和跨站点脚本攻击。哪种解决方案在运营上效率最高，并且满足这些要求？",
    "options_cn": {
      "A": "配置 AWS Shield。",
      "B": "配置 AWS WAF。",
      "C": "使用 Amazon CloudFront 分发配置 API Gateway。在 CloudFront 中配置 AWS Shield。",
      "D": "使用 Amazon CloudFront 分发配置 API Gateway。在 CloudFront 中配置 AWS WAF。"
    },
    "tags": [
      "API Gateway",
      "WAF",
      "Shield",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 89%），解析仅供参考。】\n\n本题考察了使用 AWS WAF (Web Application Firewall) 防护 API Gateway 免受 Web 攻击，以及部署 API Gateway 的最佳实践。",
      "why_correct": "AWS WAF 是一种 Web 应用程序防火墙，可以保护 Web 应用程序免受常见的 Web 攻击，例如 SQL 注入和跨站点脚本攻击。配置 AWS WAF 后，可以针对 API Gateway 定义规则，从而实现安全防护。这种方案是专门为 Web 应用安全设计的，因此在安全性和运维效率上都表现出色。",
      "why_wrong": "选项 A 仅配置了 AWS Shield，AWS Shield 主要用于抵御 DDoS 攻击，虽然也能提供一定的保护，但对于 SQL 注入和跨站点脚本攻击的防护能力不足。选项 C 和 D 虽然都使用了 CloudFront，但 CloudFront 主要是一个内容分发网络（CDN），用于加速内容传输，并非必须。虽然可以在 CloudFront 中配置 Shield 或 WAF，但这种方式增加了复杂性，且如果只在 CloudFront 配置 WAF，而 API Gateway 本身未配置 WAF，防护的范围和效果会打折扣。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "REST API",
      "SQL injection",
      "cross-site scripting",
      "AWS WAF",
      "AWS Shield",
      "Amazon CloudFront"
    ]
  },
  {
    "id": 624,
    "topic": "1",
    "question_en": "A company wants to provide users with access to AWS resources. The company has 1,500 users and manages their access to on-premises resources through Active Directory user groups on the corporate network. However, the company does not want users to have to maintain another identity to access the resources. A solutions architect must manage user access to the AWS resources while preserving access to the on-premises resources. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an IAM user for each user in the company. Attach the appropriate policies to each user.",
      "B": "Use Amazon Cognito with an Active Directory user pool. Create roles with the appropriate policies attached.",
      "C": "Define cross-account roles with the appropriate policies attached. Map the roles to the Active Directory groups.",
      "D": "Configure Security Assertion Markup Language (SAML) 2 0-based federation. Create roles with the appropriate policies attached Map the roles to the Active Directory groups."
    },
    "correct_answer": "D",
    "vote_percentage": "92%",
    "question_cn": "一家公司希望为用户提供对 AWS 资源的访问权限。该公司有 1,500 名用户，并通过公司网络上的 Active Directory 用户组管理他们对本地资源的访问。但是，该公司不希望用户为了访问资源而维护另一个身份。一位解决方案架构师必须管理用户对 AWS 资源的访问权限，同时保留对本地资源的访问权限。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "为公司中的每个用户创建一个 IAM 用户。将适当的策略附加到每个用户。",
      "B": "使用 Amazon Cognito 和 Active Directory 用户池。创建带有附加的适当策略的角色。",
      "C": "定义跨账户角色，并附加适当的策略。将角色映射到 Active Directory 组。",
      "D": "配置基于安全断言标记语言 (SAML) 2.0 的联合身份验证。创建带有附加的适当策略的角色。将角色映射到 Active Directory 组。"
    },
    "tags": [
      "IAM",
      "Cognito",
      "SAML",
      "Active Directory"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 92%），解析仅供参考。】\n\n该题考察用户访问权限管理。SAML 2.0 联合身份验证与 Active Directory 结合，用户可以使用现有凭证访问 AWS 资源，无需维护额外的身份。",
      "why_correct": "SAML 2.0 联合身份验证允许用户使用 Active Directory 凭证登录 AWS，并映射到 IAM 角色，满足访问需求。",
      "why_wrong": "A 选项，为每个用户创建 IAM 用户会增加管理开销，并且用户需要记住新凭据。B 选项，Cognito 用户池不是为 Active Directory 身份提供联合而设计的。C 选项，跨账户角色没有解决用户需要维护另一个身份的问题。"
    },
    "related_terms": [
      "IAM",
      "Amazon Cognito",
      "SAML",
      "Active Directory"
    ]
  },
  {
    "id": 625,
    "topic": "1",
    "question_en": "A company is hosting a website behind multiple Application Load Balancers. The company has different distribution rights for its content around the world. A solutions architect needs to ensure that users are served the correct content without violating distribution rights. Which configuration should the solutions architect choose to meet these requirements?",
    "options_en": {
      "A": "Configure Amazon CloudFront with AWS WAF.",
      "B": "Configure Application Load Balancers with AWS WAF",
      "C": "Configure Amazon Route 53 with a geolocation policy",
      "D": "Configure Amazon Route 53 with a geoproximity routing policy"
    },
    "correct_answer": "C",
    "vote_percentage": "77%",
    "question_cn": "一家公司在其多个 Application Load Balancer 后面托管一个网站。该公司在全球范围内对其内容拥有不同的分发权。 解决方案架构师需要确保为用户提供正确的内容，且不违反分发权。 解决方案架构师应选择哪种配置来满足这些要求？",
    "options_cn": {
      "A": "使用 AWS WAF 配置 Amazon CloudFront。",
      "B": "使用 AWS WAF 配置 Application Load Balancer。",
      "C": "使用地理位置策略配置 Amazon Route 53。",
      "D": "使用地理邻近路由策略配置 Amazon Route 53。"
    },
    "tags": [
      "CloudFront",
      "WAF",
      "Route 53",
      "Geo-Restriction",
      "Geo Proximity"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 77%），解析仅供参考。】\n\n考察使用 Route 53 的地理位置策略来实现基于用户地理位置的内容分发，以满足分发权需求。",
      "why_correct": "地理位置策略（Geo Location routing policy）允许 Route 53 基于 DNS 查询的来源地理位置，将流量路由到不同的资源。这非常适合根据用户的位置提供不同的内容，以符合分发权的要求。通过配置地理位置策略，架构师可以确保来自特定国家/地区的用户访问到正确的内容。",
      "why_wrong": "选项 A 使用 AWS WAF 和 CloudFront，虽然 CloudFront 能够分发内容，但 AWS WAF 主要用于 Web 应用程序的保护，而不是基于地理位置的内容分发。选项 B 使用 AWS WAF 配置 Application Load Balancer，WAF 同样不具备基于地理位置的内容分发能力。选项 D 的地理邻近路由策略（Geolocation routing policy）并不存在，实际是地理位置策略（Geo Location routing policy），因此选项 D 错误。"
    },
    "related_terms": [
      "Application Load Balancer",
      "AWS WAF",
      "Amazon CloudFront",
      "Amazon Route 53",
      "Geo Location routing policy",
      "Geolocation routing policy"
    ]
  },
  {
    "id": 626,
    "topic": "1",
    "question_en": "A company stores its data on premises. The amount of data is growing beyond the company's available capacity. The company wants to migrate its data from the on-premises location to an Amazon S3 bucket. The company needs a solution that will automatically validate the integrity of the data after the transfer. Which solution will meet these requirements?",
    "options_en": {
      "A": "Order an AWS Snowball Edge device. Configure the Snowball Edge device to perform the online data transfer to an S3 bucket",
      "B": "Deploy an AWS DataSync agent on premises. Configure the DataSync agent to perform the online data transfer to an S3 bucket.",
      "C": "Create an Amazon S3 File Gateway on premises Configure the S3 File Gateway to perform the online data transfer to an S3 bucket",
      "D": "Configure an accelerator in Amazon S3 Transfer Acceleration on premises. Configure the accelerator to perform the online data transfer to an S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司将其数据存储在本地。数据量正在增长，超出了公司的可用容量。该公司希望将其数据从本地位置迁移到 Amazon S3 存储桶。该公司需要一个解决方案，该解决方案将在传输后自动验证数据的完整性。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "订购 AWS Snowball Edge 存储优化设备。将 Snowball Edge 设备配置为执行在线数据传输到 S3 存储桶。",
      "B": "在本地部署 AWS DataSync 代理。将 DataSync 代理配置为执行在线数据传输到 S3 存储桶。",
      "C": "在本地创建 Amazon S3 文件网关。配置 S3 文件网关以执行在线数据传输到 S3 存储桶。",
      "D": "在本地配置 Amazon S3 传输加速器。配置加速器以执行在线数据传输到 S3 存储桶。"
    },
    "tags": [
      "DataSync",
      "S3",
      "Snowball",
      "File Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察数据迁移和数据完整性校验。DataSync 支持在线数据迁移，并在传输完成后自动验证数据的完整性。",
      "why_correct": "AWS DataSync 可以在数据传输到 S3 存储桶后自动验证数据的完整性，满足要求。",
      "why_wrong": "A 选项，Snowball Edge 设备需要先物理运输设备，然后才能传输数据，不满足在线传输。C 选项，S3 文件网关主要用于本地访问 S3 对象，不直接支持在线数据传输。D 选项，S3 传输加速器仅加速数据传输，不进行完整性验证。"
    },
    "related_terms": [
      "AWS DataSync",
      "S3",
      "AWS Snowball Edge",
      "Amazon S3 File Gateway"
    ]
  },
  {
    "id": 627,
    "topic": "1",
    "question_en": "A company wants to migrate two DNS servers to AWS. The servers host a total of approximately 200 zones and receive 1 million requests each day on average. The company wants to maximize availability while minimizing the operational overhead that is related to the management of the two servers. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Create 200 new hosted zones in the Amazon Route 53 console Import zone files.",
      "B": "Launch a single large Amazon EC2 instance Import zone tiles. Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.",
      "C": "Migrate the servers to AWS by using AWS Server Migration Service (AWS SMS). Configure Amazon CloudWatch alarms and notifications to alert the company about any downtime.",
      "D": "Launch an Amazon EC2 instance in an Auto Scaling group across two Availability Zones. Import zone files. Set the desired capacity to 1 and the maximum capacity to 3 for the Auto Scaling group. Configure scaling alarms to scale based on CPU utilization."
    },
    "correct_answer": "A",
    "vote_percentage": "94%",
    "question_cn": "一家公司希望将两台 DNS 服务器迁移到 AWS。服务器总共托管大约 200 个区域，平均每天收到 100 万个请求。公司希望最大限度地提高可用性，同时最大限度地减少与管理这两台服务器相关的运营开销。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "在 Amazon Route 53 控制台中创建 200 个新的托管区域，并导入区域文件。",
      "B": "启动一个大型 Amazon EC2 实例，导入区域文件。配置 Amazon CloudWatch 告警和通知，以提醒公司任何停机时间。",
      "C": "使用 AWS Server Migration Service (AWS SMS) 将服务器迁移到 AWS。配置 Amazon CloudWatch 告警和通知，以提醒公司任何停机时间。",
      "D": "在跨两个可用区的 Auto Scaling 组中启动一个 Amazon EC2 实例。导入区域文件。将 Auto Scaling 组的所需容量设置为 1，最大容量设置为 3。配置基于 CPU 利用率进行扩展的告警。"
    },
    "tags": [
      "Route 53",
      "EC2",
      "Server Migration Service",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 94%），解析仅供参考。】\n\n该题考察 DNS 服务的迁移和可用性。Route 53 托管区域提供高可用性，并可轻松管理 DNS 记录，是最佳选择。",
      "why_correct": "使用 Route 53 可以托管 DNS 区域，并提供高可用性和易于管理，满足题干的需求。",
      "why_wrong": "B 选项，EC2 实例的单点故障风险高，运营开销大。C 选项，SMS 迁移服务器，增加了运营开销。D 选项，Auto Scaling 组需要维护 EC2 实例，增加了运营开销。"
    },
    "related_terms": [
      "Amazon Route 53",
      "EC2",
      "AWS Server Migration Service",
      "Auto Scaling"
    ]
  },
  {
    "id": 628,
    "topic": "1",
    "question_en": "A global company runs its applications in multiple AWS accounts in AWS Organizations. The company's applications use multipart uploads to upload data to multiple Amazon S3 buckets across AWS Regions. The company wants to report on incomplete multipart uploads for cost compliance purposes. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure AWS Config with a rule to report the incomplete multipart upload object count.",
      "B": "Create a service control policy (SCP) to report the incomplete multipart upload object count.",
      "C": "Configure S3 Storage Lens to report the incomplete multipart upload object count.",
      "D": "Create an S3 Multi-Region Access Point to report the incomplete multipart upload object count."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家全球性公司在 AWS Organizations 的多个 AWS 账户中运行其应用程序。该公司的应用程序使用 multipart uploads 将数据上传到跨 AWS 区域的多个 Amazon S3 存储桶。该公司希望报告未完成的 multipart uploads 以进行成本合规性。哪种解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用规则配置 AWS Config 来报告未完成的 multipart upload 对象计数。",
      "B": "创建服务控制策略 (SCP) 来报告未完成的 multipart upload 对象计数。",
      "C": "配置 S3 Storage Lens 来报告未完成的 multipart upload 对象计数。",
      "D": "创建 S3 Multi-Region Access Point 来报告未完成的 multipart upload 对象计数。"
    },
    "tags": [
      "S3",
      "Storage Lens",
      "Config",
      "Multi-Region Access Point",
      "Multipart Uploads"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查未完成 Multipart Uploads 的报告。S3 Storage Lens 可以报告存储桶的指标，包括未完成的分段上传对象计数，从而满足成本合规性的需求。",
      "why_correct": "S3 Storage Lens 能够报告未完成的 multipart upload 对象计数，符合成本合规性的要求，且运营开销最小。",
      "why_wrong": "A 选项，AWS Config 无法直接报告未完成的 multipart uploads。B 选项，服务控制策略（SCP）无法报告未完成的 multipart uploads。D 选项，S3 Multi-Region Access Point 主要用于跨区域访问，而不是报告未完成的 uploads。"
    },
    "related_terms": [
      "S3",
      "AWS Config",
      "S3 Storage Lens",
      "S3 Multi-Region Access Point",
      "Multipart Uploads"
    ]
  },
  {
    "id": 629,
    "topic": "1",
    "question_en": "A company runs a production database on Amazon RDS for MySQL. The company wants to upgrade the database version for security compliance reasons. Because the database contains critical data, the company wants a quick solution to upgrade and test functionality without losing any data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an RDS manual snapshot. Upgrade to the new version of Amazon RDS for MySQL.",
      "B": "Use native backup and restore. Restore the data to the upgraded new version of Amazon RDS for MySQL.",
      "C": "Use AWS Database Migration Service (AWS DMS) to replicate the data to the upgraded new version of Amazon RDS for MySQL.",
      "D": "Use Amazon RDS Blue/Green Deployments to deploy and test production changes."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Amazon RDS for MySQL 上运行生产数据库。该公司希望出于安全合规性的原因升级数据库版本。由于数据库包含关键数据，该公司希望有一个快速的解决方案来升级和测试功能，而不会丢失任何数据。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建 RDS 手动快照。升级到 Amazon RDS for MySQL 的新版本。",
      "B": "使用本机备份和还原。将数据还原到已升级的 Amazon RDS for MySQL 的新版本。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 将数据复制到已升级的 Amazon RDS for MySQL 的新版本。",
      "D": "使用 Amazon RDS Blue/Green 部署来部署和测试生产更改。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "Blue/Green Deployment",
      "DMS",
      "Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n该题考察 RDS 数据库版本的升级。RDS Blue/Green 部署提供了快速升级和测试的功能，可以减少运营开销，并在不影响生产数据库的情况下完成。",
      "why_correct": "RDS Blue/Green 部署允许在后台部署和测试数据库升级，而不会影响生产数据库的可用性。",
      "why_wrong": "A 选项，手动快照需要停机才能完成数据库版本升级，不符合快速升级和测试的要求。B 选项，本机备份和还原可能需要更长的停机时间。C 选项，DMS 可能会有数据复制延迟，并且需要进行额外的设置。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "AWS Database Migration Service",
      "RDS Blue/Green deployments"
    ]
  },
  {
    "id": 630,
    "topic": "1",
    "question_en": "A solutions architect is creating a data processing job that runs once daily and can take up to 2 hours to complete. If the job is interrupted, it has to restart from the beginning. How should the solutions architect address this issue in the MOST cost-effective manner?",
    "options_en": {
      "A": "Create a script that runs locally on an Amazon EC2 Reserved Instance that is triggered by a cron job.",
      "B": "Create an AWS Lambda function triggered by an Amazon EventBridge scheduled event.",
      "C": "Use an Amazon Elastic Container Service (Amazon ECS) Fargate task triggered by an Amazon EventBridge scheduled event.",
      "D": "Use an Amazon Elastic Container Service (Amazon ECS) task running on Amazon EC2 triggered by an Amazon EventBridge scheduled event."
    },
    "correct_answer": "C",
    "vote_percentage": "87%",
    "question_cn": "一位解决方案架构师正在创建一个每天运行一次且最多需要 2 小时才能完成的数据处理作业。如果该作业中断，则必须从头开始重新启动。解决方案架构师应如何以最具成本效益的方式解决此问题？",
    "options_cn": {
      "A": "创建一个在 Amazon EC2 预留实例上本地运行的脚本，该脚本由 cron 作业触发。",
      "B": "创建一个由 Amazon EventBridge 定时事件触发的 AWS Lambda 函数。",
      "C": "使用由 Amazon EventBridge 定时事件触发的 Amazon Elastic Container Service (Amazon ECS) Fargate 任务。",
      "D": "使用由 Amazon EventBridge 定时事件触发的在 Amazon EC2 上运行的 Amazon Elastic Container Service (Amazon ECS) 任务。"
    },
    "tags": [
      "EC2",
      "EventBridge",
      "Lambda",
      "ECS",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 87%），解析仅供参考。】\n\n此题考查如何以最具成本效益的方式运行周期性任务。使用 Amazon EventBridge 定时事件触发 AWS Lambda 函数或 Amazon ECS Fargate 任务是常见的解决方案，避免了管理服务器的开销。Fargate 模式下，用户无需管理服务器，按任务实际使用量付费，成本较低。",
      "why_correct": "C 选项使用了 EventBridge 定时事件触发 Fargate 任务。Fargate 是无服务器计算引擎，无需管理服务器，适用于周期性任务，且成本效益高。",
      "why_wrong": "A 选项使用 EC2 预留实例，成本高，需要手动管理服务器。B 选项使用 Lambda，虽然也是无服务器，但对于需要长时间运行的任务，可能不如 Fargate 成本效益高。D 选项使用 EC2 上的 ECS，需要管理 EC2 实例，成本相对较高。"
    },
    "related_terms": [
      "EC2",
      "cron",
      "EventBridge",
      "Lambda",
      "ECS",
      "Fargate"
    ]
  },
  {
    "id": 631,
    "topic": "1",
    "question_en": "A social media company wants to store its database of user profiles, relationships, and interactions in the AWS Cloud. The company needs an application to monitor any changes in the database. The application needs to analyze the relationships between the data entities and to provide recommendations to users. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon Neptune to store the information. Use Amazon Kinesis Data Streams to process changes in the database.",
      "B": "Use Amazon Neptune to store the information. Use Neptune Streams to process changes in the database.",
      "C": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Amazon Kinesis Data Streams to process changes in the database.",
      "D": "Use Amazon Quantum Ledger Database (Amazon QLDB) to store the information. Use Neptune Streams to process changes in the database."
    },
    "correct_answer": "B",
    "vote_percentage": "88%",
    "question_cn": "一家社交媒体公司希望将其用户档案、关系和互动数据库存储在 AWS 云中。该公司需要一个应用程序来监控数据库中的任何更改。该应用程序需要分析数据实体之间的关系，并向用户提供建议。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Neptune 存储信息。使用 Amazon Kinesis Data Streams 处理数据库中的更改。",
      "B": "使用 Amazon Neptune 存储信息。使用 Neptune Streams 处理数据库中的更改。",
      "C": "使用 Amazon Quantum Ledger Database (Amazon QLDB) 存储信息。使用 Amazon Kinesis Data Streams 处理数据库中的更改。",
      "D": "使用 Amazon Quantum Ledger Database (Amazon QLDB) 存储信息。使用 Neptune Streams 处理数据库中的更改。"
    },
    "tags": [
      "Neptune",
      "Kinesis Data Streams",
      "Neptune Streams",
      "QLDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 88%），解析仅供参考。】\n\n本题考察数据库和数据流的结合使用。Neptune 是一种图数据库，适用于存储社交关系数据。 Neptune Streams 提供了数据变更捕获功能，可以高效地处理数据库中的更改。 相比之下，Kinesis Data Streams 需要额外的配置和管理。",
      "why_correct": "B 选项使用 Neptune 存储数据，并使用 Neptune Streams 捕获数据库更改，这是针对图数据库优化的解决方案，能以最小的运营开销满足需求。",
      "why_wrong": "A 选项使用 Kinesis Data Streams 处理数据库更改，增加了复杂度和运营开销。C 选项使用 QLDB 存储数据，QLDB 适用于账本数据库，不适合社交关系数据的存储。D 选项同样使用了不适合的数据存储方式。"
    },
    "related_terms": [
      "Kinesis Data Streams",
      "QLDB",
      "Neptune",
      "Neptune Streams"
    ]
  },
  {
    "id": 632,
    "topic": "1",
    "question_en": "A company is creating a new application that will store a large amount of data. The data will be analyzed hourly and will be modified by several Amazon EC2 Linux instances that are deployed across multiple Availability Zones. The needed amount of storage space will continue to grow for the next 6 months. Which storage solution should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Store the data in Amazon S3 Glacier. Update the S3 Glacier vault policy to allow access to the application instances.",
      "B": "Store the data in an Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the application instances.",
      "C": "Store the data in an Amazon Elastic File System (Amazon EFS) file system. Mount the file system on the application instances.",
      "D": "Store the data in an Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS volume shared between the application instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在创建一个新应用程序，该应用程序将存储大量数据。数据将每小时进行分析，并将由部署在多个可用区中的多个 Amazon EC2 Linux 实例修改。所需的存储空间将在未来 6 个月内持续增长。解决方案架构师应该推荐哪种存储解决方案来满足这些要求？",
    "options_cn": {
      "A": "将数据存储在 Amazon S3 Glacier 中。更新 S3 Glacier 库策略以允许应用程序实例访问。",
      "B": "将数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。将 EBS 卷挂载到应用程序实例上。",
      "C": "将数据存储在 Amazon Elastic File System (Amazon EFS) 文件系统中。将文件系统挂载到应用程序实例上。",
      "D": "将数据存储在应用程序实例之间共享的 Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS 卷中。"
    },
    "tags": [
      "S3",
      "EBS",
      "EFS",
      "Glacier"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察存储解决方案的选择。EFS 是一种可扩展的、完全托管的、弹性的文件系统。它允许用户在多个 EC2 实例之间共享数据，并且可以根据需要自动扩展。对于需要随着时间推移增长的存储空间来说，EFS 是一个不错的选择。",
      "why_correct": "C 选项使用 EFS 文件系统，提供可扩展的存储空间，方便多个 EC2 实例访问，满足题目需求。",
      "why_wrong": "A 选项将数据存储在 Glacier 中，不适合频繁访问的数据。B 选项使用 EBS，不适合在多个实例之间共享数据。D 选项将数据存储在 EBS Provisioned IOPS 卷中，无法在多个实例间共享，不满足需求。"
    },
    "related_terms": [
      "S3",
      "Glacier",
      "EBS",
      "EFS"
    ]
  },
  {
    "id": 633,
    "topic": "1",
    "question_en": "A company manages an application that stores data on an Amazon RDS for PostgreSQL Multi-AZ DB instance. Increases in trafic are causing performance problems. The company determines that database queries are the primary reason for the slow performance. What should a solutions architect do to improve the application's performance?",
    "options_en": {
      "A": "Serve read trafic from the Multi-AZ standby replica.",
      "B": "Configure the DB instance to use Transfer Acceleration.",
      "C": "Create a read replica from the source DB instance. Serve read trafic from the read replica.",
      "D": "Use Amazon Kinesis Data Firehose between the application and Amazon RDS to increase the concurrency of database requests."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司管理一个应用程序，该应用程序将数据存储在 Amazon RDS for PostgreSQL 多可用区数据库实例上。流量的增加导致了性能问题。该公司确定数据库查询是性能缓慢的主要原因。解决方案架构师应该怎么做来提高应用程序的性能？",
    "options_cn": {
      "A": "从多可用区备用副本提供读取流量。",
      "B": "将数据库实例配置为使用 Transfer Acceleration。",
      "C": "从源数据库实例创建读取副本。从读取副本提供读取流量。",
      "D": "在应用程序和 Amazon RDS 之间使用 Amazon Kinesis Data Firehose 来增加数据库请求的并发性。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "Read Replicas",
      "Transfer Acceleration",
      "Kinesis Data Firehose"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察 RDS 性能优化。创建读取副本可以分担主数据库的读取负载，提高应用程序的性能。多可用区数据库实例本身提供了高可用性，但并不能直接提升读取性能。",
      "why_correct": "C 选项从源数据库实例创建读取副本，并提供读取流量，可以分担主数据库的负载，从而提高应用程序的性能。",
      "why_wrong": "A 选项从多可用区备用副本提供读取流量，不能提高读取性能。B 选项使用 Transfer Acceleration 仅加速数据传输，对数据库性能优化没有帮助。D 选项使用 Kinesis Data Firehose 增加并发性，无法提升数据库查询性能。"
    },
    "related_terms": [
      "RDS",
      "PostgreSQL",
      "Kinesis Data Firehose",
      "Transfer Acceleration"
    ]
  },
  {
    "id": 634,
    "topic": "1",
    "question_en": "A company collects 10 GB of telemetry data daily from various machines. The company stores the data in an Amazon S3 bucket in a source data account. The company has hired several consulting agencies to use this data for analysis. Each agency needs read access to the data for its analysts. The company must share the data from the source data account by choosing a solution that maximizes security and operational eficiency. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure S3 global tables to replicate data for each agency.",
      "B": "Make the S3 bucket public for a limited time. Inform only the agencies.",
      "C": "Configure cross-account access for the S3 bucket to the accounts that the agencies own.",
      "D": "Set up an IAM user for each analyst in the source data account. Grant each user access to the S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一家公司每天从各种机器收集 10 GB 的遥测数据。该公司将数据存储在源数据账户的 Amazon S3 存储桶中。该公司聘请了几家咨询机构来使用这些数据进行分析。每个机构都需要访问数据的读取权限，供其分析师使用。该公司必须通过选择一个能够最大化安全性和运营效率的解决方案，来共享源数据账户中的数据。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 S3 全局表，为每个机构复制数据。",
      "B": "将 S3 存储桶公开一段有限的时间。仅通知这些机构。",
      "C": "为 S3 存储桶配置跨账户访问权限，使其指向这些机构拥有的账户。",
      "D": "在源数据账户中为每个分析师设置一个 IAM 用户。授予每个用户访问 S3 存储桶的权限。"
    },
    "tags": [
      "S3",
      "IAM",
      "跨账户访问"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n本题考察 S3 数据的安全共享。跨账户访问是最安全、最有效率的方案，可以限制对数据的访问权限。 避免使用全局表和公开存储桶，会带来安全风险。IAM 用户权限的管理也会带来额外的运维成本。",
      "why_correct": "C 选项配置跨账户访问权限，使机构可以通过其拥有的账户访问数据，这种方式安全且高效。",
      "why_wrong": "A 选项配置 S3 全局表，无法满足只读访问的需求。B 选项将 S3 存储桶公开，存在安全风险。D 选项为每个分析师设置 IAM 用户，虽然安全，但会增加管理复杂性。"
    },
    "related_terms": [
      "S3",
      "IAM",
      "跨账户访问"
    ]
  },
  {
    "id": 635,
    "topic": "1",
    "question_en": "A company uses Amazon FSx for NetApp ONTAP in its primary AWS Region for CIFS and NFS file shares. Applications that run on Amazon EC2 instances access the file shares. The company needs a storage disaster recovery (DR) solution in a secondary Region. The data that is replicated in the secondary Region needs to be accessed by using the same protocols as the primary Region. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Lambda function to copy the data to an Amazon S3 bucket. Replicate the S3 bucket to the secondary Region.",
      "B": "Create a backup of the FSx for ONTAP volumes by using AWS Backup. Copy the volumes to the secondary Region. Create a new FSx for ONTAP instance from the backup.",
      "C": "Create an FSx for ONTAP instance in the secondary Region. Use NetApp SnapMirror to replicate data from the primary Region to the secondary Region.",
      "D": "Create an Amazon Elastic File System (Amazon EFS) volume. Migrate the current data to the volume. Replicate the volume to the secondary Region."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其主 AWS 区域中使用 Amazon FSx for NetApp ONTAP 用于 CIFS 和 NFS 文件共享。在 Amazon EC2 实例上运行的应用程序访问文件共享。该公司需要在辅助区域中建立存储灾难恢复 (DR) 解决方案。在辅助区域中复制的数据需要使用与主区域相同的协议进行访问。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数将数据复制到 Amazon S3 存储桶。将 S3 存储桶复制到辅助区域。",
      "B": "使用 AWS Backup 创建 FSx for ONTAP 卷的备份。将卷复制到辅助区域。从备份创建新的 FSx for ONTAP 实例。",
      "C": "在辅助区域中创建 FSx for ONTAP 实例。使用 NetApp SnapMirror 将数据从主区域复制到辅助区域。",
      "D": "创建一个 Amazon Elastic File System (Amazon EFS) 卷。将当前数据迁移到该卷。将卷复制到辅助区域。"
    },
    "tags": [
      "FSx",
      "ONTAP",
      "Lambda",
      "S3",
      "EFS",
      "SnapMirror"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查 FSx for ONTAP 的灾难恢复解决方案。NetApp SnapMirror 是 FSx for ONTAP 提供的原生数据复制技术，能够以最小的开销实现跨区域的数据复制。 其他选项增加了额外的复杂性和开销。",
      "why_correct": "C 选项使用 SnapMirror 将数据复制到辅助区域，这是 FSx for ONTAP 提供的原生灾备方案，具有最少的运营开销。",
      "why_wrong": "A 选项使用 Lambda 将数据复制到 S3，增加了复杂性和延迟。B 选项使用 AWS Backup 进行备份和恢复，增加了运营开销。D 选项使用 EFS，与题干中 FSx for ONTAP 的要求不符。"
    },
    "related_terms": [
      "FSx",
      "Lambda",
      "S3",
      "EFS",
      "ONTAP",
      "SnapMirror"
    ]
  },
  {
    "id": 636,
    "topic": "1",
    "question_en": "A development team is creating an event-based application that uses AWS Lambda functions. Events will be generated when files are added to an Amazon S3 bucket. The development team currently has Amazon Simple Notification Service (Amazon SNS) configured as the event target from Amazon S3. What should a solutions architect do to process the events from Amazon S3 in a scalable way?",
    "options_en": {
      "A": "Create an SNS subscription that processes the event in Amazon Elastic Container Service (Amazon ECS) before the event runs in Lambda.",
      "B": "Create an SNS subscription that processes the event in Amazon Elastic Kubernetes Service (Amazon EKS) before the event runs in Lambda",
      "C": "Create an SNS subscription that sends the event to Amazon Simple Queue Service (Amazon SQS). Configure the SOS queue to trigger a Lambda function.",
      "D": "Create an SNS subscription that sends the event to AWS Server Migration Service (AWS SMS). Configure the Lambda function to poll from the SMS event."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一个开发团队正在创建一个基于事件的应用程序，该应用程序使用 AWS Lambda 函数。当文件添加到 Amazon S3 存储桶时，将生成事件。该开发团队目前已将 Amazon Simple Notification Service (Amazon SNS) 配置为来自 Amazon S3 的事件目标。解决方案架构师应该怎么做才能以可扩展的方式处理来自 Amazon S3 的事件？",
    "options_cn": {
      "A": "创建一个 SNS 订阅，在 Lambda 中运行事件之前，在 Amazon Elastic Container Service (Amazon ECS) 中处理该事件。",
      "B": "创建一个 SNS 订阅，在 Lambda 中运行事件之前，在 Amazon Elastic Kubernetes Service (Amazon EKS) 中处理该事件",
      "C": "创建一个 SNS 订阅，将事件发送到 Amazon Simple Queue Service (Amazon SQS)。配置 SQS 队列以触发 Lambda 函数。",
      "D": "创建一个 SNS 订阅，将事件发送到 AWS Server Migration Service (AWS SMS)。配置 Lambda 函数以从 SMS 事件轮询。"
    },
    "tags": [
      "S3",
      "SNS",
      "SQS",
      "Lambda",
      "ECS",
      "EKS",
      "SMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查基于事件的应用程序设计。使用 SNS 和 SQS 结合，可以实现 S3 事件的异步处理，提高可扩展性。 SNS 负责将事件发布到 SQS 队列，Lambda 函数从队列中获取消息并进行处理。",
      "why_correct": "C 选项使用 SNS 将事件发送到 SQS，然后配置 SQS 队列触发 Lambda 函数，这是可扩展的异步处理方案。",
      "why_wrong": "A 选项和 B 选项在 Lambda 前使用 ECS 或 EKS 处理事件，增加了复杂性，不简洁。D 选项使用 SMS，不适用于 S3 事件。"
    },
    "related_terms": [
      "S3",
      "SNS",
      "SQS",
      "Lambda",
      "ECS",
      "EKS",
      "SMS"
    ]
  },
  {
    "id": 637,
    "topic": "1",
    "question_en": "A solutions architect is designing a new service behind Amazon API Gateway. The request patterns for the service will be unpredictable and can change suddenly from 0 requests to over 500 per second. The total size of the data that needs to be persisted in a backend database is currently less than 1 GB with unpredictable future growth. Data can be queried using simple key-value requests. Which combination ofAWS services would meet these requirements? (Choose two.)",
    "options_en": {
      "A": "AWS Fargate",
      "B": "AWS Lambda",
      "C": "Amazon DynamoDB",
      "D": "Amazon EC2 Auto Scaling",
      "E": "MySQL-compatible Amazon Aurora"
    },
    "correct_answer": "BC",
    "vote_percentage": "100%",
    "question_cn": "一个解决方案架构师正在设计一个位于 Amazon API Gateway 之后的新服务。该服务的请求模式将是不可预测的，并且可以从 0 个请求突然更改为每秒超过 500 个请求。目前需要在后端数据库中持久保存的数据总大小小于 1 GB，并且未来的增长不可预测。可以使用简单的键值请求来查询数据。哪种 AWS 服务组合可以满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "AWS Fargate",
      "B": "AWS Lambda",
      "C": "Amazon DynamoDB",
      "D": "Amazon EC2 Auto Scaling",
      "E": "MySQL-compatible Amazon Aurora"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "EC2 Auto Scaling",
      "Fargate",
      "Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 100%），解析仅供参考。】\n\n本题考查无服务器架构设计。Lambda 函数可以根据 API Gateway 的请求自动扩展，并且 DynamoDB 适用于存储小数据量的键值数据。 这两个服务组合能够满足弹性、可扩展性和成本效益的要求。",
      "why_correct": "B 选项和 C 选项， Lambda 和 DynamoDB 组合，满足了弹性、可扩展性和成本效益的要求，是最合适的无服务器方案。",
      "why_wrong": "A 选项使用 Fargate，增加了复杂性，不一定是最佳的。D 选项使用 EC2 Auto Scaling，需要管理 EC2 实例，增加了复杂性。E 选项使用 MySQL-compatible Amazon Aurora，数据库复杂性过高，不满足题意。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "DynamoDB",
      "EC2 Auto Scaling",
      "Fargate",
      "Aurora"
    ]
  },
  {
    "id": 638,
    "topic": "1",
    "question_en": "A company collects and shares research data with the company's employees all over the world. The company wants to collect and store the data in an Amazon S3 bucket and process the data in the AWS Cloud. The company will share the data with the company's employees. The company needs a secure solution in the AWS Cloud that minimizes operational overhead. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an AWS Lambda function to create an S3 presigned URL. Instruct employees to use the URL.",
      "B": "Create an IAM user for each employee. Create an IAM policy for each employee to allow S3 access. Instruct employees to use the AWS Management Console.",
      "C": "Create an S3 File Gateway. Create a share for uploading and a share for downloading. Allow employees to mount shares on their local computers to use S3 File Gateway.",
      "D": "Configure AWS Transfer Family SFTP endpoints. Select the custom identity provider options. Use AWS Secrets Manager to manage the user credentials Instruct employees to use Transfer Family."
    },
    "correct_answer": "D",
    "vote_percentage": "39%",
    "question_cn": "一家公司在全球范围内收集并与公司员工共享研究数据。该公司希望将数据收集并存储在 Amazon S3 存储桶中，并在 AWS 云中处理数据。该公司将与公司员工共享数据。该公司需要在 AWS 云中提供一个安全的解决方案，以最大限度地减少运营开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Lambda 函数创建 S3 预签名 URL。指示员工使用该 URL。",
      "B": "为每个员工创建 IAM 用户。为每个员工创建 IAM 策略以允许 S3 访问。指示员工使用 AWS 管理控制台。",
      "C": "创建 S3 文件网关。创建用于上传的共享和用于下载的共享。允许员工在其本地计算机上挂载共享以使用 S3 文件网关。",
      "D": "配置 AWS Transfer Family SFTP 端点。选择自定义身份提供程序选项。使用 AWS Secrets Manager 管理用户凭证。指示员工使用 Transfer Family。"
    },
    "tags": [
      "S3",
      "Lambda",
      "Transfer Family",
      "SFTP",
      "Secrets Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 39%），解析仅供参考。】\n\n本题考查安全的数据共享方案。AWS Transfer Family SFTP 端点结合自定义身份验证和 AWS Secrets Manager，可以在 AWS 云中提供安全的解决方案。Lambda 函数可以生成预签名的 URL，但需要额外的代码和运维。其他选项的安全性和可扩展性较差。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：D 选项配置 SFTP 端点，使用自定义身份提供程序和 Secrets Manager 管理凭证，这是一种安全且易于管理的方式，解决了安全性和运营开销问题。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项使用 Lambda 生成 S3 预签名 URL，无法满足安全性和运维开销要求。B 选项为每个员工创建 IAM 用户和策略，会增加管理复杂性。C 选项创建 S3 文件网关，不适用于大量员工。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "Transfer Family",
      "SFTP",
      "Secrets Manager"
    ]
  },
  {
    "id": 639,
    "topic": "1",
    "question_en": "A company is building a new furniture inventory application. The company has deployed the application on a fieet ofAmazon EC2 instances across multiple Availability Zones. The EC2 instances run behind an Application Load Balancer (ALB) in their VPC. A solutions architect has observed that incoming trafic seems to favor one EC2 instance, resulting in latency for some requests. What should the solutions architect do to resolve this issue?",
    "options_en": {
      "A": "Disable session afinity (sticky sessions) on the ALB",
      "B": "Replace the ALB with a Network Load Balancer",
      "C": "Increase the number of EC2 instances in each Availability Zone",
      "D": "Adjust the frequency of the health checks on the ALB's target group"
    },
    "correct_answer": "A",
    "vote_percentage": "89%",
    "question_cn": "一家公司正在构建一个新的家具库存应用程序。该公司已将该应用程序部署在一组 Amazon EC2 实例上，这些实例分布在多个可用区。 EC2 实例在其 VPC 中的 Application Load Balancer (ALB) 后面运行。 解决方案架构师观察到，传入的流量似乎偏向于一个 EC2 实例，导致某些请求的延迟。 解决方案架构师应该怎么做来解决这个问题？",
    "options_cn": {
      "A": "在 ALB 上禁用会话关联（粘性会话）",
      "B": "用 Network Load Balancer 替换 ALB",
      "C": "增加每个可用区中 EC2 实例的数量",
      "D": "调整 ALB 目标组上的健康检查频率"
    },
    "tags": [
      "ALB",
      "会话关联",
      "NLB",
      "EC2",
      "Auto Scaling",
      "健康检查"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 89%），解析仅供参考。】\n\n本题考查 ALB 的负载均衡问题。当出现流量偏向某个 EC2 实例时，通常是由于 ALB 的会话关联（粘性会话）导致。 禁用会话关联可以解决此问题。",
      "why_correct": "A 选项在 ALB 上禁用会话关联，可以解决流量偏向问题。",
      "why_wrong": "B 选项用 NLB 替换 ALB，NLB 仅提供网络层负载均衡，无法解决这个问题。C 选项增加 EC2 实例数量，无法解决负载不均的问题。D 选项调整健康检查频率，无法解决负载不均问题。"
    },
    "related_terms": [
      "ALB",
      "NLB",
      "EC2",
      "Auto Scaling",
      "会话关联",
      "健康检查"
    ]
  },
  {
    "id": 640,
    "topic": "1",
    "question_en": "A company has an application workfiow that uses an AWS Lambda function to download and decrypt files from Amazon S3. These files are encrypted using AWS Key Management Service (AWS KMS) keys. A solutions architect needs to design a solution that will ensure the required permissions are set correctly. Which combination of actions accomplish this? (Choose two.)",
    "options_en": {
      "A": "Attach the kms:decrypt permission to the Lambda function’s resource policy",
      "B": "Grant the decrypt permission for the Lambda IAM role in the KMS key's policy",
      "C": "Grant the decrypt permission for the Lambda resource policy in the KMS key's policy.",
      "D": "Create a new IAM policy with the kms:decrypt permission and attach the policy to the Lambda function",
      "E": "Create a new IAM role with the kms:decrypt permission and attach the execution role to the Lambda function."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家公司有一个应用程序工作流程，使用 AWS Lambda 函数从 Amazon S3 下载和解密文件。 这些文件使用 AWS Key Management Service (AWS KMS) 密钥进行加密。 解决方案架构师需要设计一个解决方案，以确保正确设置所需的权限。 哪些操作组合可以完成此操作？（选择两个。）",
    "options_cn": {
      "A": "将 kms:decrypt 权限附加到 Lambda 函数的资源策略。",
      "B": "在 KMS 密钥的策略中，授予 Lambda IAM 角色解密权限。",
      "C": "在 KMS 密钥的策略中，授予 Lambda 资源策略解密权限。",
      "D": "创建一个新的 IAM 策略，其中包含 kms:decrypt 权限，并将该策略附加到 Lambda 函数。",
      "E": "创建一个新的 IAM 角色，其中包含 kms:decrypt 权限，并将执行角色附加到 Lambda 函数。"
    },
    "tags": [
      "Lambda",
      "KMS",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n本题考查 Lambda 函数的权限设置。要让 Lambda 函数能够使用 KMS 解密 S3 中的加密文件，需要在 KMS 密钥的策略中授予 Lambda 函数的 IAM 角色解密权限，同时也要确保 Lambda 函数的执行角色有解密权限。  将 kms:decrypt 权限附加到 Lambda 函数的资源策略是不正确的做法。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：B 选项在 KMS 密钥的策略中，授予 Lambda IAM 角色解密权限。满足解密文件的要求，是正确做法。 D 选项，创建一个新的 IAM 策略，包含 kms:decrypt 权限，并将该策略附加到 Lambda 函数是不正确的做法。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项将 kms:decrypt 权限附加到 Lambda 函数的资源策略是不正确的做法。C 选项在 KMS 密钥的策略中，授予 Lambda 资源策略解密权限，错误。 D 选项创建了一个新的 IAM 策略，其中包含 kms:decrypt 权限，并将该策略附加到 Lambda 函数是不正确的做法。E 选项创建了一个新的 IAM 角色，其中包含 kms:decrypt 权限，并将执行角色附加到 Lambda 函数，也是错的。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Lambda",
      "KMS",
      "IAM"
    ]
  },
  {
    "id": 641,
    "topic": "1",
    "question_en": "A company wants to monitor its AWS costs for financial review. The cloud operations team is designing an architecture in the AWS Organizations management account to query AWS Cost and Usage Reports for all member accounts. The team must run this query once a month and provide a detailed analysis of the bill. Which solution is the MOST scalable and cost-effective way to meet these requirements?",
    "options_en": {
      "A": "Enable Cost and Usage Reports in the management account. Deliver reports to Amazon Kinesis. Use Amazon EMR for analysis.",
      "B": "Enable Cost and Usage Reports in the management account. Deliver the reports to Amazon S3 Use Amazon Athena for analysis.",
      "C": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon S3 Use Amazon Redshift for analysis.",
      "D": "Enable Cost and Usage Reports for member accounts. Deliver the reports to Amazon Kinesis. Use Amazon QuickSight tor analysis."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望监控其 AWS 成本以进行财务审查。云运营团队正在 AWS Organizations 管理账户中设计一个架构，以查询所有成员账户的 AWS 成本和使用情况报告。该团队必须每月运行一次此查询并提供账单的详细分析。哪种解决方案是满足这些要求的最具可扩展性和成本效益的方式？",
    "options_cn": {
      "A": "在管理账户中启用成本和使用情况报告。将报告发送到 Amazon Kinesis。使用 Amazon EMR 进行分析。",
      "B": "在管理账户中启用成本和使用情况报告。将报告发送到 Amazon S3。使用 Amazon Athena 进行分析。",
      "C": "为成员账户启用成本和使用情况报告。将报告发送到 Amazon S3。使用 Amazon Redshift 进行分析。",
      "D": "为成员账户启用成本和使用情况报告。将报告发送到 Amazon Kinesis。使用 Amazon QuickSight 进行分析。"
    },
    "tags": [
      "成本和使用情况报告",
      "Athena",
      "S3",
      "EMR",
      "Redshift",
      "QuickSight"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查成本监控和分析。将成本和使用情况报告存储在 S3 中，然后使用 Athena 进行查询，是进行成本分析最具可扩展性和成本效益的方式。 Athena 是一种无服务器查询服务，按查询付费。EMR 和 Redshift 需要维护集群，QuickSight 用于可视化，不涉及数据的存储与查询。",
      "why_correct": "B 选项在管理账户中启用成本和使用情况报告，将报告发送到 S3，并使用 Athena 进行分析，是最具成本效益和可扩展性的方案。",
      "why_wrong": "A 选项使用 Kinesis 和 EMR，增加了复杂性和成本。C 选项为成员账户启用报告，并使用 Redshift，增加了复杂性和成本。D 选项使用 Kinesis 和 QuickSight，不能直接进行数据分析。"
    },
    "related_terms": [
      "Athena",
      "S3",
      "EMR",
      "Redshift",
      "QuickSight",
      "成本和使用情况报告"
    ]
  },
  {
    "id": 642,
    "topic": "1",
    "question_en": "A company wants to run a gaming application on Amazon EC2 instances that are part of an Auto Scaling group in the AWS Cloud. The application will transmit data by using UDP packets. The company wants to ensure that the application can scale out and in as trafic increases and decreases. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Attach a Network Load Balancer to the Auto Scaling group.",
      "B": "Attach an Application Load Balancer to the Auto Scaling group.",
      "C": "Deploy an Amazon Route 53 record set with a weighted policy to route trafic appropriately.",
      "D": "Deploy a NAT instance that is configured with port forwarding to the EC2 instances in the Auto Scaling group."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在 AWS 云中运行作为 Auto Scaling 组一部分的 Amazon EC2 实例上的游戏应用程序。该应用程序将使用 UDP 数据包传输数据。该公司希望确保应用程序可以随着流量的增加和减少而横向扩展和缩减。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将 Network Load Balancer 附加到 Auto Scaling 组。",
      "B": "将 Application Load Balancer 附加到 Auto Scaling 组。",
      "C": "部署一个具有加权策略的 Amazon Route 53 记录集，以适当地路由流量。",
      "D": "部署一个配置了端口转发的 NAT 实例，该实例连接到 Auto Scaling 组中的 EC2 实例。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "NLB",
      "ALB",
      "UDP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查负载均衡和 UDP 流量。对于 UDP 流量，Network Load Balancer (NLB) 是最佳选择。Application Load Balancer (ALB) 主要针对 HTTP/HTTPS 流量。Route 53 和 NAT 实例不能提供负载均衡。",
      "why_correct": "A 选项将 Network Load Balancer 附加到 Auto Scaling 组，能够处理 UDP 流量，并实现负载均衡和自动扩展。",
      "why_wrong": "B 选项将 Application Load Balancer 附加到 Auto Scaling 组，ALB 主要针对 HTTP/HTTPS 流量。C 选项部署一个具有加权策略的 Amazon Route 53 记录集，无法实现负载均衡。D 选项部署一个配置了端口转发的 NAT 实例，无法实现负载均衡。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "NLB",
      "ALB",
      "UDP"
    ]
  },
  {
    "id": 643,
    "topic": "1",
    "question_en": "A company runs several websites on AWS for its different brands. Each website generates tens of gigabytes of web trafic logs each day. A solutions architect needs to design a scalable solution to give the company's developers the ability to analyze trafic patterns across all the company's websites. This analysis by the developers will occur on demand once a week over the course of several months. The solution must support queries with standard SQL. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store the logs in Amazon S3. Use Amazon Athena tor analysis.",
      "B": "Store the logs in Amazon RDS. Use a database client for analysis.",
      "C": "Store the logs in Amazon OpenSearch Service. Use OpenSearch Service for analysis.",
      "D": "Store the logs in an Amazon EMR cluster Use a supported open-source framework for SQL-based analysis."
    },
    "correct_answer": "A",
    "vote_percentage": "93%",
    "question_cn": "一家公司在其 AWS 上为其不同的品牌运营多个网站。每个网站每天生成数十 GB 的网络流量日志。一位解决方案架构师需要设计一个可扩展的解决方案，使公司的开发人员能够分析所有公司网站的流量模式。开发人员每周将按需进行此分析，持续数月。该解决方案必须支持使用标准 SQL 的查询。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "将日志存储在 Amazon S3 中。使用 Amazon Athena 进行分析。",
      "B": "将日志存储在 Amazon RDS 中。使用数据库客户端进行分析。",
      "C": "将日志存储在 Amazon OpenSearch Service 中。使用 OpenSearch Service 进行分析。",
      "D": "将日志存储在 Amazon EMR 集群中。使用支持的开源框架进行基于 SQL 的分析。"
    },
    "tags": [
      "S3",
      "Athena",
      "RDS",
      "OpenSearch Service",
      "EMR"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 93%），解析仅供参考。】\n\n本题考查日志分析。将日志存储在 S3 中，然后使用 Athena 进行 SQL 查询，是最经济有效的方案。Athena 是一种无服务器查询服务，可以针对 S3 上的数据进行查询。RDS、OpenSearch Service 和 EMR 的成本都相对较高，特别是对于周期性的按需分析。",
      "why_correct": "A 选项将日志存储在 S3 中，使用 Athena 进行分析，是成本效益最高的解决方案，因为 Athena 是一种按查询付费的服务。",
      "why_wrong": "B 选项将日志存储在 RDS 中，成本较高且不适合大规模数据分析。C 选项将日志存储在 OpenSearch Service 中，OpenSearch Service 主要用于日志索引和搜索。 D 选项将日志存储在 EMR 集群中，增加了运营成本。"
    },
    "related_terms": [
      "S3",
      "Athena",
      "RDS",
      "OpenSearch Service",
      "EMR"
    ]
  },
  {
    "id": 644,
    "topic": "1",
    "question_en": "An international company has a subdomain for each country that the company operates in. The subdomains are formatted as example.com, country1.example.com, and country2.example.com. The company's workloads are behind an Application Load Balancer. The company wants to encrypt the website data that is in transit. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use the AWS Certificate Manager (ACM) console to request a public certificate for the apex top domain example com and a wildcard certificate for *.example.com.",
      "B": "Use the AWS Certificate Manager (ACM) console to request a private certificate for the apex top domain example.com and a wildcard certificate for *.example.com.",
      "C": "Use the AWS Certificate Manager (ACM) console to request a public and private certificate for the apex top domain example.com.",
      "D": "Validate domain ownership by email address. Switch to DNS validation by adding the required DNS records to the DNS provider. E. Validate domain ownership for the domain by adding the required DNS records to the DNS provider."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家跨国公司为该公司运营的每个国家/地区都有一个子域。子域的格式为 example.com、country1.example.com 和 country2.example.com。该公司的负载位于 Application Load Balancer 之后。该公司希望对正在传输的网站数据进行加密。哪两种步骤的组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Certificate Manager (ACM) 控制台为顶级域 example.com 请求一个公共证书，并为 *.example.com 请求一个通配符证书。",
      "B": "使用 AWS Certificate Manager (ACM) 控制台为顶级域 example.com 请求一个私有证书，并为 *.example.com 请求一个通配符证书。",
      "C": "使用 AWS Certificate Manager (ACM) 控制台为顶级域 example.com 请求一个公共和私有证书。",
      "D": "通过电子邮件地址验证域所有权。通过将所需的 DNS 记录添加到 DNS 提供商来切换到 DNS 验证。 E. 通过将所需的 DNS 记录添加到 DNS 提供商来验证该域的域所有权。"
    },
    "tags": [
      "ACM",
      "SSL",
      "ALB",
      "通配符证书"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n本题考查 HTTPS 配置。使用 ACM 获取证书，并通过验证域所有权来配置 HTTPS。为了支持多个子域，可以使用通配符证书 (*.example.com)。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：A 选项使用 ACM 请求顶级域 example.com 的公共证书，并为 *.example.com 请求通配符证书，满足了安全性和多子域的需求，同时简化了证书管理。 E 选项，通过将所需的 DNS 记录添加到 DNS 提供商来验证该域的域所有权。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项使用私有证书，不适用。C 选项只请求了顶级域的证书，不能覆盖所有子域。D 选项和 E 选项是验证域所有权的步骤，不是部署HTTPS的完整方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ACM",
      "SSL",
      "ALB",
      "通配符证书"
    ]
  },
  {
    "id": 645,
    "topic": "1",
    "question_en": "A company is required to use cryptographic keys in its on-premises key manager. The key manager is outside of the AWS Cloud because of regulatory and compliance requirements. The company wants to manage encryption and decryption by using cryptographic keys that are retained outside of the AWS Cloud and that support a variety of external key managers from different vendors. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS CloudHSM key store backed by a CloudHSM cluster.",
      "B": "Use an AWS Key Management Service (AWS KMS) external key store backed by an external key manager.",
      "C": "Use the default AWS Key Management Service (AWS KMS) managed key store.",
      "D": "Use a custom key store backed by an AWS CloudHSM cluster."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司需要在其本地密钥管理器中使用加密密钥。由于法规和合规性要求，该密钥管理器位于 AWS Cloud 之外。该公司希望通过使用保留在 AWS Cloud 之外的加密密钥来管理加密和解密，并且支持来自不同供应商的各种外部密钥管理器。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用由 CloudHSM 集群支持的 AWS CloudHSM 密钥存储。",
      "B": "使用由外部密钥管理器支持的 AWS Key Management Service (AWS KMS) 外部密钥存储。",
      "C": "使用默认的 AWS Key Management Service (AWS KMS) 托管密钥存储。",
      "D": "使用由 AWS CloudHSM 集群支持的自定义密钥存储。"
    },
    "tags": [
      "AWS KMS",
      "external key store"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n此题考察使用外部密钥管理器（EKM）满足合规性要求。AWS KMS 外部密钥存储允许使用在 AWS 外部管理的密钥进行加密操作。这种方案可以满足法规和合规性要求，同时减少运营开销，因为密钥管理在 AWS 之外进行。",
      "why_correct": "选项 B 提供了最合适的解决方案，通过 AWS KMS 外部密钥存储来使用 AWS 外部的密钥管理器，满足了题目的要求。",
      "why_wrong": "选项 A 使用了 AWS CloudHSM，但 CloudHSM 仍然位于 AWS 内部。选项 C 使用 AWS KMS 托管密钥，不满足密钥在 AWS 之外的要求。选项 D 使用自定义密钥存储，也位于 AWS 内部。"
    },
    "related_terms": [
      "AWS KMS",
      "external key store",
      "CloudHSM",
      "AWS CloudHSM",
      "AWS CloudHSM 集群",
      "自定义密钥存储"
    ]
  },
  {
    "id": 646,
    "topic": "1",
    "question_en": "A solutions architect needs to host a high performance computing (HPC) workload in the AWS Cloud. The workload will run on hundreds of Amazon EC2 instances and will require parallel access to a shared file system to enable distributed processing of large datasets. Datasets will be accessed across multiple instances simultaneously. The workload requires access latency within 1 ms. After processing has completed, engineers will need access to the dataset for manual postprocessing. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Elastic File System (Amazon EFS) as a shared file system. Access the dataset from Amazon EFS.",
      "B": "Mount an Amazon S3 bucket to serve as the shared file system. Perform postprocessing directly from the S3 bucket.",
      "C": "Use Amazon FSx for Lustre as a shared file system. Link the file system to an Amazon S3 bucket for postprocessing.",
      "D": "Configure AWS Resource Access Manager to share an Amazon S3 bucket so that it can be mounted to all instances for processing and postprocessing."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要在 AWS 云中托管高性能计算 (HPC) 工作负载。该工作负载将在数百个 Amazon EC2 实例上运行，并将需要并行访问共享文件系统，以实现大型数据集的分布式处理。将同时跨多个实例访问数据集。该工作负载需要 1 毫秒内的访问延迟。处理完成后，工程师将需要访问数据集以进行手动后期处理。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic File System (Amazon EFS) 作为共享文件系统。从 Amazon EFS 访问数据集。",
      "B": "挂载一个 Amazon S3 存储桶作为共享文件系统。直接从 S3 存储桶执行后期处理。",
      "C": "使用 Amazon FSx for Lustre 作为共享文件系统。将文件系统链接到 Amazon S3 存储桶以进行后期处理。",
      "D": "配置 AWS Resource Access Manager 以共享一个 Amazon S3 存储桶，以便可以将其挂载到所有实例上进行处理和后期处理。"
    },
    "tags": [
      "Amazon FSx for Lustre",
      "Amazon S3",
      "HPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察 HPC 场景下的共享文件系统选择。Amazon FSx for Lustre 提供了高性能、低延迟的共享文件系统，非常适合 HPC 工作负载。FSx for Lustre 可以链接到 S3，实现数据的后期处理和存档。",
      "why_correct": "选项 C 使用 Amazon FSx for Lustre 作为共享文件系统，并将其链接到 S3 进行后期处理，满足了高性能计算、低延迟和数据后期处理的要求。",
      "why_wrong": "选项 A 使用 Amazon EFS，虽然易于使用，但延迟较高，不适合高性能计算。选项 B 直接使用 S3 作为共享文件系统，延迟无法满足要求。选项 D 无法提供高性能，也不适合 HPC 工作负载。"
    },
    "related_terms": [
      "Amazon FSx for Lustre",
      "Amazon S3",
      "Amazon EFS",
      "HPC",
      "EC2",
      "文件系统"
    ]
  },
  {
    "id": 647,
    "topic": "1",
    "question_en": "A gaming company is building an application with Voice over IP capabilities. The application will serve trafic to users across the world. The application needs to be highly available with an automated failover across AWS Regions. The company wants to minimize the latency of users without relying on IP address caching on user devices. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use AWS Global Accelerator with health checks.",
      "B": "Use Amazon Route 53 with a geolocation routing policy.",
      "C": "Create an Amazon CloudFront distribution that includes multiple origins.",
      "D": "Create an Application Load Balancer that uses path-based routing."
    },
    "correct_answer": "A",
    "vote_percentage": "96%",
    "question_cn": "一家游戏公司正在构建一个具有语音 over IP 功能的应用程序。 该应用程序将为世界各地的用户提供服务。 该应用程序需要具有高可用性，并在 AWS 区域之间进行自动故障转移。 公司希望最大限度地减少用户的延迟，而无需依赖用户设备上的 IP 地址缓存。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Global Accelerator 以及运行状况检查。",
      "B": "使用 Amazon Route 53 和地理位置路由策略。",
      "C": "创建包含多个 源的 Amazon CloudFront 分发。",
      "D": "创建使用基于路径的路由的 Application Load Balancer。"
    },
    "tags": [
      "AWS Global Accelerator",
      "Amazon Route 53",
      "Amazon CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 96%），解析仅供参考。】\n\n题目考查高可用性、低延迟的全球化应用程序的架构设计。 AWS Global Accelerator 通过在全球 AWS 边缘站点缓存流量来提高性能。 它使用 AWS 拥有的全球网络来优化到应用程序的路径，并提供故障转移功能。 此外，还减少了用户的延迟。 这满足了游戏公司对低延迟和高可用性的要求。",
      "why_correct": "选项 A 使用 AWS Global Accelerator，它提供了高可用性、低延迟和自动故障转移，满足了题目的要求。",
      "why_wrong": "选项 B 使用 Amazon Route 53 和地理位置路由策略，延迟无法保证，并且没有自动故障转移功能。选项 C 使用 Amazon CloudFront，主要用于内容分发，不具备自动故障转移功能。选项 D 使用 Application Load Balancer，它没有优化全球延迟，并且没有自动故障转移功能。"
    },
    "related_terms": [
      "AWS Global Accelerator",
      "Amazon Route 53",
      "Amazon CloudFront",
      "Application Load Balancer"
    ]
  },
  {
    "id": 648,
    "topic": "1",
    "question_en": "A weather forecasting company needs to process hundreds of gigabytes of data with sub-millisecond latency. The company has a high performance computing (HPC) environment in its data center and wants to expand its forecasting capabilities. A solutions architect must identify a highly available cloud storage solution that can handle large amounts of sustained throughput. Files that are stored in the solution should be accessible to thousands of compute instances that will simultaneously access and process the entire dataset. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Use Amazon FSx for Lustre scratch file systems.",
      "B": "Use Amazon FSx for Lustre persistent file systems.",
      "C": "Use Amazon Elastic File System (Amazon EFS) with Bursting Throughput mode.",
      "D": "Use Amazon Elastic File System (Amazon EFS) with Provisioned Throughput mode."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家天气预报公司需要以亚毫秒级的延迟处理数百 GB 的数据。该公司在其数据中心拥有一个高性能计算 (HPC) 环境，并希望扩展其预测能力。一位解决方案架构师必须确定一个高可用性的云存储解决方案，该解决方案能够处理大量持续的吞吐量。存储在该解决方案中的文件应可供数千个计算实例访问，这些实例将同时访问和处理整个数据集。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon FSx for Lustre 临时文件系统。",
      "B": "使用 Amazon FSx for Lustre 持久文件系统。",
      "C": "使用 Amazon Elastic File System (Amazon EFS)，采用突增吞吐量模式。",
      "D": "使用 Amazon Elastic File System (Amazon EFS)，采用预置吞吐量模式。"
    },
    "tags": [
      "Amazon FSx for Lustre",
      "Amazon EFS",
      "HPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察大规模数据存储的解决方案。Amazon FSx for Lustre 提供了高性能和高吞吐量，非常适合处理大数据集和 HPC 工作负载。这种解决方案能够满足所需的亚毫秒级延迟和高吞吐量。",
      "why_correct": "选项 B 使用 Amazon FSx for Lustre 持久文件系统，满足了高性能、高可用性、低延迟的要求，并且能够处理大量持续的吞吐量。",
      "why_wrong": "选项 A 使用 Amazon FSx for Lustre 临时文件系统，不适合存储需要长期保存的数据。选项 C 和 D 使用 Amazon EFS， EFS 的性能不如 FSx for Lustre，不能满足亚毫秒级延迟的需求。"
    },
    "related_terms": [
      "Amazon FSx for Lustre",
      "Amazon EFS",
      "HPC",
      "吞吐量"
    ]
  },
  {
    "id": 649,
    "topic": "1",
    "question_en": "An ecommerce company runs a PostgreSQL database on premises. The database stores data by using high IOPS Amazon Elastic Block Store (Amazon EBS) block storage. The daily peak I/O transactions per second do not exceed 15,000 IOPS. The company wants to migrate the database to Amazon RDS for PostgreSQL and provision disk IOPS performance independent of disk storage capacity. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure the General Purpose SSD (gp2) EBS volume storage type and provision 15,000 IOPS.",
      "B": "Configure the Provisioned IOPS SSD (io1) EBS volume storage type and provision 15,000 IOPS.",
      "C": "Configure the General Purpose SSD (gp3) EBS volume storage type and provision 15,000 IOPS.",
      "D": "Configure the EBS magnetic volume type to achieve maximum IOPS."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司在本地运行 PostgreSQL 数据库。该数据库使用高 IOPS Amazon Elastic Block Store (Amazon EBS) 块存储来存储数据。每天的峰值每秒 I/O 事务数不超过 15,000 IOPS。该公司希望将数据库迁移到 Amazon RDS for PostgreSQL 并提供独立于磁盘存储容量的磁盘 IOPS 性能。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "配置通用 SSD (gp2) EBS 卷存储类型并配置 15,000 IOPS。",
      "B": "配置预置 IOPS SSD (io1) EBS 卷存储类型并配置 15,000 IOPS。",
      "C": "配置通用 SSD (gp3) EBS 卷存储类型并配置 15,000 IOPS。",
      "D": "配置 EBS 磁性卷类型以实现最大 IOPS。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "Amazon EBS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察 RDS 存储类型的选择。Amazon RDS for PostgreSQL 提供了不同的 EBS 卷存储类型。在满足磁盘 IOPS 性能需求的同时，选择具有成本效益的存储类型是关键。gp3 存储类型相比 gp2 存储类型，性价比更高，并且可以独立配置 IOPS 和吞吐量。",
      "why_correct": "选项 C 使用 gp3 存储类型，既能满足 IOPS 性能要求，又具有成本效益。",
      "why_wrong": "选项 A 使用 gp2 存储类型，虽然能满足 IOPS 要求，但成本较高。选项 B 使用 io1 存储类型，虽然能满足 IOPS 要求，但成本更高。选项 D 不存在 EBS 磁性卷类型，并且 IOPS 性能无法满足要求。"
    },
    "related_terms": [
      "Amazon RDS",
      "PostgreSQL",
      "Amazon EBS",
      "IOPS",
      "gp2",
      "gp3",
      "io1"
    ]
  },
  {
    "id": 650,
    "topic": "1",
    "question_en": "A company wants to migrate its on-premises Microsoft SQL Server Enterprise edition database to AWS. The company's online application uses the database to process transactions. The data analysis team uses the same production database to run reports for analytical processing. The company wants to reduce operational overhead by moving to managed services wherever possible. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Migrate to Amazon RDS for Microsoft SOL Server. Use read replicas for reporting purposes",
      "B": "Migrate to Microsoft SQL Server on Amazon EC2. Use Always On read replicas for reporting purposes",
      "C": "Migrate to Amazon DynamoDB. Use DynamoDB on-demand replicas for reporting purposes",
      "D": "Migrate to Amazon Aurora MySQL. Use Aurora read replicas for reporting purposes"
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其本地 Microsoft SQL Server Enterprise 版本数据库迁移到 AWS。该公司的在线应用程序使用数据库来处理事务。数据分析团队使用相同的生产数据库运行报告以进行分析处理。该公司希望通过尽可能转移到托管服务来减少运营开销。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "迁移到 Amazon RDS for Microsoft SQL Server。 使用只读副本进行报告。",
      "B": "迁移到 Amazon EC2 上的 Microsoft SQL Server。 使用 Always On 只读副本进行报告。",
      "C": "迁移到 Amazon DynamoDB。 使用 DynamoDB 按需副本进行报告。",
      "D": "迁移到 Amazon Aurora MySQL。 使用 Aurora 只读副本进行报告。"
    },
    "tags": [
      "Amazon RDS",
      "Microsoft SQL Server",
      "Amazon EC2",
      "Amazon Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察数据库迁移到 AWS 的方案选择。Amazon RDS for Microsoft SQL Server 提供了托管服务，降低了运营开销。使用只读副本进行报告可以满足数据分析需求。",
      "why_correct": "选项 A 使用 Amazon RDS for Microsoft SQL Server，使用只读副本进行报告，能够以最少的运营开销满足需求。",
      "why_wrong": "选项 B 采用 Amazon EC2 上的 Microsoft SQL Server，需要自行管理数据库，增加了运营开销。选项 C 采用 Amazon DynamoDB，不适合迁移现有的 Microsoft SQL Server 数据库。选项 D 采用 Amazon Aurora MySQL，不适用 Microsoft SQL Server 数据库。"
    },
    "related_terms": [
      "Amazon RDS",
      "Microsoft SQL Server",
      "Amazon EC2",
      "Amazon Aurora",
      "只读副本"
    ]
  },
  {
    "id": 651,
    "topic": "1",
    "question_en": "A company stores a large volume of image files in an Amazon S3 bucket. The images need to be readily available for the first 180 days. The images are infrequently accessed for the next 180 days. After 360 days, the images need to be archived but must be available instantly upon request. After 5 years, only auditors can access the images. The auditors must be able to retrieve the images within 12 hours. The images cannot be lost during this process. A developer will use S3 Standard storage for the first 180 days. The developer needs to configure an S3 Lifecycle rule. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.",
      "B": "Transition the objects to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 180 days. S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.",
      "C": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Instant Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years.",
      "D": "Transition the objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days, S3 Glacier Flexible Retrieval after 360 days, and S3 Glacier Deep Archive after 5 years."
    },
    "correct_answer": "C",
    "vote_percentage": "82%",
    "question_cn": "一家公司将大量图像文件存储在 Amazon S3 存储桶中。这些图像需要在最初的 180 天内随时可用。在接下来的 180 天内，这些图像的访问频率较低。360 天后，这些图像需要被归档，但必须在请求时立即可用。5 年后，只有审计人员才能访问这些图像。审计人员必须能够在 12 小时内检索这些图像。在此过程中，图像不能丢失。开发人员将使用 S3 标准存储进行最初的 180 天的存储。开发人员需要配置一个 S3 生命周期规则。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "在 180 天后，将对象转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)。360 天后使用 S3 Glacier Instant Retrieval，5 年后使用 S3 Glacier Deep Archive。",
      "B": "在 180 天后，将对象转换为 S3 One Zone-Infrequent Access (S3 One Zone-IA)。360 天后使用 S3 Glacier Flexible Retrieval，5 年后使用 S3 Glacier Deep Archive。",
      "C": "在 180 天后，将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA)。360 天后使用 S3 Glacier Instant Retrieval，5 年后使用 S3 Glacier Deep Archive。",
      "D": "在 180 天后，将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA)。360 天后使用 S3 Glacier Flexible Retrieval，5 年后使用 S3 Glacier Deep Archive。"
    },
    "tags": [
      "Amazon S3",
      "S3 lifecycle"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 82%），解析仅供参考。】\n\n本题考察 Amazon S3 生命周期配置。S3 生命周期策略允许根据对象的使用频率和访问模式自动转换存储类型，以优化成本。根据题目的要求，需要在不同的时间段使用不同的存储类型。S3 Standard-IA、S3 Glacier Instant Retrieval、S3 Glacier Deep Archive 是合适的选择。",
      "why_correct": "选项 C 在 180 天后转换为 S3 Standard-IA，在 360 天后转换为 S3 Glacier Instant Retrieval，5 年后转换为 S3 Glacier Deep Archive，满足了成本效益和访问需求。",
      "why_wrong": "选项 A 和 B 在 360 天后使用了 S3 Glacier Flexible Retrieval，检索时间太长。选项 D 在 180 天后转换为 S3 Standard-IA，360 天后使用 S3 Glacier Flexible Retrieval，不满足审计人员需要在 12 小时内检索图像的需求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard-IA",
      "S3 Glacier Deep Archive",
      "S3 Glacier Flexible Retrieval",
      "S3 lifecycle",
      "S3 Glacier Instant Retrieval"
    ]
  },
  {
    "id": 652,
    "topic": "1",
    "question_en": "A company has a large data workload that runs for 6 hours each day. The company cannot lose any data while the process is running. A solutions architect is designing an Amazon EMR cluster configuration to support this critical data workload. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure a long-running cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.",
      "B": "Configure a transient cluster that runs the primary node and core nodes on On-Demand Instances and the task nodes on Spot Instances.",
      "C": "Configure a transient cluster that runs the primary node on an On-Demand Instance and the core nodes and task nodes on Spot Instances.",
      "D": "Configure a long-running cluster that runs the primary node on an On-Demand Instance, the core nodes on Spot Instances, and the task nodes on Spot Instances."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个大型数据工作负载，每天运行 6 个小时。公司在流程运行时不能丢失任何数据。一位解决方案架构师正在设计 Amazon EMR 集群配置以支持这项关键数据工作负载。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置一个长期运行的集群，该集群在按需实例上运行主节点和核心节点，并在竞价实例上运行任务节点。",
      "B": "配置一个瞬态集群，该集群在按需实例上运行主节点和核心节点，并在竞价实例上运行任务节点。",
      "C": "配置一个瞬态集群，该集群在按需实例上运行主节点，并在竞价实例上运行核心节点和任务节点。",
      "D": "配置一个长期运行的集群，该集群在按需实例上运行主节点，在竞价实例上运行核心节点，并在竞价实例上运行任务节点。"
    },
    "tags": [
      "Amazon EMR",
      "Spot Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 Amazon EMR 集群配置的成本优化方案。使用 Spot 实例（竞价实例）可以显著降低成本，但需要考虑数据丢失的风险。对于关键数据工作负载，必须避免数据丢失。",
      "why_correct": "选项 B 配置一个瞬态集群，主节点和核心节点使用按需实例，任务节点使用 Spot 实例，满足了避免数据丢失和成本优化的需求。",
      "why_wrong": "选项 A 配置了一个长期运行的集群，即使任务节点使用竞价实例，但整体成本仍然较高。选项 C 在按需实例上运行主节点，并在竞价实例上运行核心节点和任务节点，这会影响性能。选项 D 配置了一个长期运行的集群，即使使用了竞价实例，整体成本仍然较高。"
    },
    "related_terms": [
      "Amazon EMR",
      "竞价实例",
      "按需实例",
      "Spot Instances",
      "On-Demand Instances",
      "长期运行",
      "瞬态集群"
    ]
  },
  {
    "id": 653,
    "topic": "1",
    "question_en": "A company maintains an Amazon RDS database that maps users to cost centers. The company has accounts in an organization in AWS Organizations. The company needs a solution that will tag all resources that are created in a specific AWS account in the organization. The solution must tag each resource with the cost center ID of the user who created the resource. Which solution will meet these requirements?",
    "options_en": {
      "A": "Move the specific AWS account to a new organizational unit (OU) in Organizations from the management account. Create a service control policy (SCP) that requires all existing resources to have the correct cost center tag before the resources are created. Apply the SCP to the new OU.",
      "B": "Create an AWS Lambda function to tag the resources after the Lambda function looks up the appropriate cost center from the RDS database. Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function.",
      "C": "Create an AWS CloudFormation stack to deploy an AWS Lambda function. Configure the Lambda function to look up the appropriate cost center from the RDS database and to tag resources. Create an Amazon EventBridge scheduled rule to invoke the CloudFormation stack.",
      "D": "Create an AWS Lambda function to tag the resources with a default value. Configure an Amazon EventBridge rule that reacts to AWS CloudTrail events to invoke the Lambda function when a resource is missing the cost center tag."
    },
    "correct_answer": "A",
    "vote_percentage": "54%",
    "question_cn": "一家公司维护一个将用户映射到成本中心的 Amazon RDS 数据库。该公司在 AWS Organizations 中拥有组织内的账户。该公司需要一个解决方案来标记在该组织中特定 AWS 账户中创建的所有资源。该解决方案必须使用创建资源的用户的成本中心 ID 标记每个资源。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将特定 AWS 账户从管理账户移动到 Organizations 中的新组织单元 (OU)。创建一个服务控制策略 (SCP)，要求所有现有资源在创建资源之前具有正确的成本中心标签。将 SCP 应用于新的 OU。",
      "B": "创建一个 AWS Lambda 函数，以便在 Lambda 函数从 RDS 数据库中查找适当的成本中心后标记资源。配置一个 Amazon EventBridge 规则，该规则对 AWS CloudTrail 事件做出反应以调用 Lambda 函数。",
      "C": "创建一个 AWS CloudFormation 堆栈来部署一个 AWS Lambda 函数。将 Lambda 函数配置为从 RDS 数据库中查找适当的成本中心并标记资源。创建一个 Amazon EventBridge 定时规则来调用 CloudFormation 堆栈。",
      "D": "创建一个 AWS Lambda 函数，使用默认值标记资源。配置一个 Amazon EventBridge 规则，该规则对 AWS CloudTrail 事件做出反应，当资源缺少成本中心标签时调用 Lambda 函数。"
    },
    "tags": [
      "AWS Organizations",
      "AWS Lambda",
      "Amazon EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 54%），解析仅供参考。】\n\n考查通过 Service Control Policies (SCP) 和组织单元 (OU) 对 AWS Organizations 中的账户资源进行标记。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 采用 Service Control Policies (SCP) 来强制在组织单元 (OU) 内创建的资源必须具有正确的成本中心标签。通过将账户移动到新的 OU 并应用 SCP，可以确保新创建的资源都带有正确的标记。这种方法可以有效地在组织范围内强制执行标签策略。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 使用 Lambda 函数和 EventBridge 响应 CloudTrail 事件来标记资源，这增加了复杂性，并且可能导致标记出现延迟，无法保证所有新创建的资源都立即被正确标记。选项 C 使用 CloudFormation 和定时规则，效率较低，并且不能保证新资源创建时即被标记。选项 D 依赖于在创建后标记资源，无法保证所有资源一开始就符合规范，而且处理缺少标签的资源也比一开始就进行规范更复杂。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon RDS",
      "AWS Organizations",
      "OU",
      "SCP",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS CloudTrail",
      "AWS CloudFormation",
      "CloudFormation stack"
    ]
  },
  {
    "id": 654,
    "topic": "1",
    "question_en": "A company recently migrated its web application to the AWS Cloud. The company uses an Amazon EC2 instance to run multiple processes to host the application. The processes include an Apache web server that serves static content. The Apache web server makes requests to a PHP application that uses a local Redis server for user sessions. The company wants to redesign the architecture to be highly available and to use AWS managed solutions. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Elastic Beanstalk to host the static content and the PHP application. Configure Elastic Beanstalk to deploy its EC2 instance into a public subnet. Assign a public IP address.",
      "B": "Use AWS Lambda to host the static content and the PHP application. Use an Amazon API Gateway REST API to proxy requests to the Lambda function. Set the API Gateway CORS configuration to respond to the domain name. Configure Amazon ElastiCache for Redis to handle session information.",
      "C": "Keep the backend code on the EC2 instance. Create an Amazon ElastiCache for Redis cluster that has Multi-AZ enabled. Configure the ElastiCache for Redis cluster in cluster mode. Copy the frontend resources to Amazon S3. Configure the backend code to reference the EC2 instance.",
      "D": "Configure an Amazon CloudFront distribution with an Amazon S3 endpoint to an S3 bucket that is configured to host the static content. Configure an Application Load Balancer that targets an Amazon Elastic Container Service (Amazon ECS) service that runs AWS Fargate tasks for the PHP application. Configure the PHP application to use an Amazon ElastiCache for Redis cluster that runs in multiple Availability Zones."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司最近将其 Web 应用程序迁移到了 AWS 云。该公司使用 Amazon EC2 实例运行多个进程来托管该应用程序。这些进程包括一个 Apache Web 服务器，用于提供静态内容。Apache Web 服务器向一个 PHP 应用程序发出请求，该应用程序使用本地 Redis 服务器进行用户会话。该公司希望重新设计架构，使其具有高可用性并使用 AWS 托管解决方案。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Elastic Beanstalk 托管静态内容和 PHP 应用程序。将 Elastic Beanstalk 配置为其 EC2 实例部署到公有子网。分配一个公有 IP 地址。",
      "B": "使用 AWS Lambda 托管静态内容和 PHP 应用程序。使用 Amazon API Gateway REST API 代理对 Lambda 函数的请求。将 API Gateway CORS 配置设置为响应域名。配置 Amazon ElastiCache for Redis 以处理会话信息。",
      "C": "将后端代码保留在 EC2 实例上。创建一个启用了 Multi-AZ 的 Amazon ElastiCache for Redis 集群。在集群模式下配置 ElastiCache for Redis 集群。将前端资源复制到 Amazon S3。配置后端代码以引用 EC2 实例。",
      "D": "使用指向配置为托管静态内容的 S3 存储桶的 Amazon S3 端点的 Amazon CloudFront 分发。配置一个 Application Load Balancer，该负载均衡器将目标指向一个 Amazon Elastic Container Service (Amazon ECS) 服务，该服务为 PHP 应用程序运行 AWS Fargate 任务。配置 PHP 应用程序以使用在多个可用区中运行的 Amazon ElastiCache for Redis 集群。"
    },
    "tags": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "ElastiCache",
      "Amazon ECS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察 Web 应用程序的高可用性和托管方案。使用 Amazon CloudFront 分发静态内容，使用 Application Load Balancer 和 Amazon ECS 运行 PHP 应用程序，使用 Amazon ElastiCache for Redis 存储会话信息，这些服务组合能提供高可用性和可扩展性。",
      "why_correct": "选项 D 使用 Amazon CloudFront，Application Load Balancer，Amazon ECS，Amazon ElastiCache for Redis，满足了高可用性、可扩展性，并使用 AWS 托管解决方案的要求。",
      "why_wrong": "选项 A 使用 Elastic Beanstalk 部署应用程序，无法实现细粒度的架构控制。选项 B 使用 AWS Lambda 托管静态内容和 PHP 应用程序，无法满足应用程序的运行需求。选项 C 将后端代码保留在 EC2 实例上，不符合托管解决方案的要求。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Application Load Balancer",
      "ElastiCache",
      "Amazon ECS",
      "Elastic Beanstalk",
      "AWS Lambda"
    ]
  },
  {
    "id": 655,
    "topic": "1",
    "question_en": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group that has a target group. The company designed the application to work with session afinity (sticky sessions) for a better user experience. The application must be available publicly over the internet as an endpoint. A WAF must be applied to the endpoint for additional security. Session afinity (sticky sessions) must be configured on the endpoint. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create a public Network Load Balancer. Specify the application target group.",
      "B": "Create a Gateway Load Balancer. Specify the application target group.",
      "C": "Create a public Application Load Balancer. Specify the application target group.",
      "D": "Create a second target group. Add Elastic IP addresses to the EC2 instances",
      "E": "Create a web ACL in AWS WAF. Associate the web ACL with the endpoint"
    },
    "correct_answer": "CE",
    "vote_percentage": "",
    "question_cn": "一家公司在具有目标组的 Auto Scaling 组中运行 Amazon EC2 实例上的 Web 应用程序。该公司设计了该应用程序，使其可以使用会话关联（粘性会话）以获得更好的用户体验。该应用程序必须通过互联网公开可用作端点。必须将 WAF 应用于端点以获得额外的安全性。必须在端点上配置会话关联（粘性会话）。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建公共 Network Load Balancer。指定应用程序目标组。",
      "B": "创建 Gateway Load Balancer。指定应用程序目标组。",
      "C": "创建公共 Application Load Balancer。指定应用程序目标组。",
      "D": "创建第二个目标组。将弹性 IP 地址添加到 EC2 实例。",
      "E": "在 AWS WAF 中创建 Web ACL。将 Web ACL 与端点关联"
    },
    "tags": [
      "Application Load Balancer",
      "AWS WAF",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 —），解析仅供参考。】\n\n本题考察 Application Load Balancer (ALB) 与 AWS WAF 的集成以及 Auto Scaling 组的配置。使用 ALB 可以配置粘性会话，并且可以与 WAF 集成以提高安全性。 在 Auto Scaling 组中使用 ALB，可以实现弹性和高可用性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 CE。理由简述：选项 C 配置 Application Load Balancer，用于负载均衡和粘性会话；选项 E 在 AWS WAF 中配置 Web ACL，并与 ALB 关联，满足了题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 创建公共 Network Load Balancer，无法配置粘性会话。选项 B 创建 Gateway Load Balancer，不适用。选项 D 创建第二个目标组，无法解决负载均衡、粘性会话和安全性的问题。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Application Load Balancer",
      "AWS WAF",
      "Auto Scaling",
      "Web ACL",
      "弹性 IP",
      "目标组"
    ]
  },
  {
    "id": 656,
    "topic": "1",
    "question_en": "A company runs a website that stores images of historical events. Website users need the ability to search and view images based on the year that the event in the image occurred. On average, users request each image only once or twice a year. The company wants a highly available solution to store and deliver the images to users. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Store images in Amazon Elastic Block Store (Amazon EBS). Use a web server that runs on Amazon EC2.",
      "B": "Store images in Amazon Elastic File System (Amazon EFS). Use a web server that runs on Amazon EC2.",
      "C": "Store images in Amazon S3 Standard. Use S3 Standard to directly deliver images by using a static website.",
      "D": "Store images in Amazon S3 Standard-Infrequent Access (S3 Standard-IA). Use S3 Standard-IA to directly deliver images by using a static website."
    },
    "correct_answer": "D",
    "vote_percentage": "90%",
    "question_cn": "一家公司运营一个网站，存储历史事件的图像。网站用户需要能够根据图像中事件发生的年份来搜索和查看图像。平均而言，用户每年仅请求每张图像一两次。该公司希望使用一个高可用性的解决方案来存储和向用户提供图像。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将图像存储在 Amazon Elastic Block Store (Amazon EBS) 中。使用在 Amazon EC2 上运行的 Web 服务器。",
      "B": "将图像存储在 Amazon Elastic File System (Amazon EFS) 中。使用在 Amazon EC2 上运行的 Web 服务器。",
      "C": "将图像存储在 Amazon S3 Standard 中。 使用 S3 Standard 通过静态网站直接提供图像。",
      "D": "将图像存储在 Amazon S3 Standard-Infrequent Access (S3 Standard-IA) 中。使用 S3 Standard-IA 通过静态网站直接提供图像。"
    },
    "tags": [
      "Amazon S3",
      "S3 Standard",
      "S3 Standard-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 90%），解析仅供参考。】\n\n考察使用 S3 存储图像，并结合存储类别和访问频率优化成本。重点关注 S3 Standard-IA 的适用场景和静态网站托管。",
      "why_correct": "Amazon S3 Standard-IA 针对不常访问的数据进行了优化，非常适合每年仅访问一两次的图像数据。结合 S3 静态网站托管功能，可以以低成本提供高可用性和直接访问。此方案满足了高可用性和成本效益的要求，并能直接通过 Web 提供图像。",
      "why_wrong": "选项 A 和 B 使用 Amazon EBS 和 EFS，这两种存储服务主要用于 EC2 实例的块存储和共享文件系统，不适合存储静态图像并直接提供服务，成本也高于 S3。选项 C 使用 S3 Standard，虽然可以提供静态网站托管，但对于访问频率低的图像，其存储成本高于 S3 Standard-IA，因此不具备最佳的成本效益。"
    },
    "related_terms": [
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon EC2",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon S3 Standard",
      "Amazon S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3",
      "static website"
    ]
  },
  {
    "id": 657,
    "topic": "1",
    "question_en": "A company has multiple AWS accounts in an organization in AWS Organizations that different business units use. The company has multiple ofices around the world. The company needs to update security group rules to allow new ofice CIDR ranges or to remove old CIDR ranges across the organization. The company wants to centralize the management of security group rules to minimize the administrative overhead that updating CIDR ranges requires. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create VPC security groups in the organization's management account. Update the security groups when a CIDR range update is necessary.",
      "B": "Create a VPC customer managed prefix list that contains the list of CIDRs. Use AWS Resource Access Manager (AWS RAM) to share the prefix list across the organization. Use the prefix list in the security groups across the organization.",
      "C": "Create an AWS managed prefix list. Use an AWS Security Hub policy to enforce the security group update across the organization. Use an AWS Lambda function to update the prefix list automatically when the CIDR ranges change.",
      "D": "Create security groups in a central administrative AWS account. Create an AWS Firewall Manager common security group policy for the whole organization. Select the previously created security groups as primary groups in the policy."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS Organizations 中拥有多个 AWS 账户，不同的业务部门使用这些账户。该公司在全球范围内设有多个办事处。该公司需要更新安全组规则以允许新的办事处 CIDR 范围或删除整个组织中的旧 CIDR 范围。该公司希望集中管理安全组规则，以最大限度地减少更新 CIDR 范围所需的管理开销。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在组织的管理账户中创建 VPC 安全组。在需要更新 CIDR 范围时更新安全组。",
      "B": "创建一个包含 CIDR 列表的 VPC 客户托管前缀列表。使用 AWS Resource Access Manager (AWS RAM) 在整个组织中共享前缀列表。在整个组织的安全组中使用前缀列表。",
      "C": "创建一个 AWS 托管前缀列表。使用 AWS Security Hub 策略来强制整个组织的安全组更新。使用 AWS Lambda 函数在 CIDR 范围更改时自动更新前缀列表。",
      "D": "在中央管理 AWS 账户中创建安全组。为整个组织创建一个 AWS Firewall Manager 公共安全组策略。选择先前创建的安全组作为策略中的主要组。"
    },
    "tags": [
      "AWS Organizations",
      "VPC",
      "Security Group",
      "前缀列表",
      "AWS Firewall Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察如何在 AWS Organizations 中集中管理安全组规则。使用前缀列表可以在多个账户和 VPC 中共享 IP 范围，简化安全组规则的更新和管理。AWS RAM 可以用于共享前缀列表。",
      "why_correct": "选项 B 创建一个包含 CIDR 列表的 VPC 客户托管前缀列表，使用 AWS Resource Access Manager (AWS RAM) 在整个组织中共享前缀列表，并在整个组织的安全组中使用前缀列表，可以集中管理安全组规则。",
      "why_wrong": "选项 A 在组织的管理账户中创建 VPC 安全组，不能方便地在整个组织中共享。选项 C 使用 AWS 托管前缀列表和 AWS Security Hub 策略，无法实现集中管理。选项 D 使用 AWS Firewall Manager 公共安全组策略，无法实现集中管理和自动更新。"
    },
    "related_terms": [
      "AWS Organizations",
      "VPC",
      "AWS Firewall Manager",
      "CIDR",
      "AWS RAM",
      "Security Group",
      "前缀列表"
    ]
  },
  {
    "id": 658,
    "topic": "1",
    "question_en": "A company uses an on-premises network-attached storage (NAS) system to provide file shares to its high performance computing (HPC) workloads. The company wants to migrate its latency-sensitive HPC workloads and its storage to the AWS Cloud. The company must be able to provide NFS and SMB multi-protocol access from the file system. Which solution will meet these requirements with the LEAST latency? (Choose two.)",
    "options_en": {
      "A": "Deploy compute optimized EC2 instances into a cluster placement group.",
      "B": "Deploy compute optimized EC2 instances into a partition placement group.",
      "C": "Attach the EC2 instances to an Amazon FSx for Lustre file system.",
      "D": "Attach the EC2 instances to an Amazon FSx for OpenZFS file system",
      "E": "Attach the EC2 instances to an Amazon FSx for NetApp ONTAP file system."
    },
    "correct_answer": "AE",
    "vote_percentage": "",
    "question_cn": "一家公司使用本地网络附加存储 (NAS) 系统为其高性能计算 (HPC) 工作负载提供文件共享。该公司希望将其对延迟敏感的 HPC 工作负载及其存储迁移到 AWS 云。该公司必须能够从文件系统提供 NFS 和 SMB 多协议访问。哪种解决方案将以最低的延迟满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "将计算优化 EC2 实例部署到集群放置组中。",
      "B": "将计算优化 EC2 实例部署到分区放置组中。",
      "C": "将 EC2 实例连接到 Amazon FSx for Lustre 文件系统。",
      "D": "将 EC2 实例连接到 Amazon FSx for OpenZFS 文件系统。",
      "E": "将 EC2 实例连接到 Amazon FSx for NetApp ONTAP 文件系统。"
    },
    "tags": [
      "Amazon FSx for Lustre",
      "Amazon FSx for OpenZFS",
      "Amazon FSx for NetApp ONTAP",
      "NFS",
      "SMB",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 —），解析仅供参考。】\n\n本题考察高性能计算（HPC）存储解决方案。Amazon FSx for Lustre 和 Amazon FSx for NetApp ONTAP 文件系统都支持 NFS 和 SMB 多协议访问。为了满足延迟敏感的需求，需要选择与 EC2 实例部署在同一可用区内的方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：选项 A 将计算优化 EC2 实例部署到集群放置组中，保证了最低的延迟。选项 E 将 EC2 实例连接到 Amazon FSx for NetApp ONTAP 文件系统，满足了多协议访问的需求。这两个选项结合，能满足延迟和多协议的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 将 EC2 实例部署到分区放置组中，无法提供最低延迟。选项 C 将 EC2 实例连接到 Amazon FSx for Lustre 文件系统，虽然提供了高性能，但没有多协议访问的功能。选项 D 将 EC2 实例连接到 Amazon FSx for OpenZFS 文件系统，虽然支持多协议，但性能不如 FSx for Lustre。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon FSx for Lustre",
      "NFS",
      "SMB",
      "EC2",
      "集群放置组",
      "Amazon FSx for OpenZFS",
      "Amazon FSx for NetApp ONTAP",
      "分区放置组"
    ]
  },
  {
    "id": 659,
    "topic": "1",
    "question_en": "A company is relocating its data center and wants to securely transfer 50 TB of data to AWS within 2 weeks. The existing data center has a Site-to-Site VPN connection to AWS that is 90% utilized. Which AWS service should a solutions architect use to meet these requirements?",
    "options_en": {
      "A": "AWS DataSync with a VPC endpoint",
      "B": "AWS Direct Connect",
      "C": "AWS Snowball Edge Storage Optimized",
      "D": "AWS Storage Gateway"
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一家公司正在搬迁其数据中心，并希望在两周内安全地将 50 TB 的数据传输到 AWS。现有数据中心与 AWS 之间有一个站点到站点 VPN 连接，该连接已使用 90%。解决方案架构师应使用哪种 AWS 服务来满足这些要求？",
    "options_cn": {
      "A": "带有 VPC endpoint 的 AWS DataSync",
      "B": "AWS Direct Connect",
      "C": "AWS Snowball Edge 存储优化设备",
      "D": "AWS Storage Gateway"
    },
    "tags": [
      "AWS DataSync",
      "AWS Direct Connect",
      "AWS Snowball Edge",
      "AWS Storage Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n此题考察大容量数据传输的解决方案。AWS Snowball Edge 设备最适合大容量数据传输，尤其是当现有网络连接带宽受限时。 在两周内传输 50 TB 的数据，使用 Snowball Edge 存储优化设备是最佳选择。",
      "why_correct": "选项 C 使用 AWS Snowball Edge 存储优化设备，可以安全地传输 50 TB 的数据。",
      "why_wrong": "选项 A 使用 AWS DataSync，无法满足在两周内传输 50TB 数据的需求。选项 B 使用 AWS Direct Connect，不适用于初始数据传输。选项 D 使用 AWS Storage Gateway，不适用于大容量数据传输。"
    },
    "related_terms": [
      "AWS DataSync",
      "AWS Direct Connect",
      "AWS Snowball Edge",
      "AWS Storage Gateway",
      "数据传输",
      "VPC endpoint"
    ]
  },
  {
    "id": 660,
    "topic": "1",
    "question_en": "A company hosts an application on Amazon EC2 On-Demand Instances in an Auto Scaling group. Application peak hours occur at the same time each day. Application users report slow application performance at the start of peak hours. The application performs normally 2-3 hours after peak hours begin. The company wants to ensure that the application works properly at the start of peak hours. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an Application Load Balancer to distribute trafic properly to the instances.",
      "B": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on memory utilization.",
      "C": "Configure a dynamic scaling policy for the Auto Scaling group to launch new instances based on CPU utilization.",
      "D": "Configure a scheduled scaling policy for the Auto Scaling group to launch new instances before peak hours."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Auto Scaling 组中的 Amazon EC2 按需实例上托管一个应用程序。应用程序高峰时段每天在同一时间发生。应用程序用户报告说在高峰时段开始时应用程序性能缓慢。在高峰时段开始后的 2-3 小时内，应用程序运行正常。该公司希望确保应用程序在高峰时段开始时正常工作。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Application Load Balancer 以将流量正确分配给实例。",
      "B": "为 Auto Scaling 组配置动态伸缩策略，以根据内存利用率启动新实例。",
      "C": "为 Auto Scaling 组配置动态伸缩策略，以根据 CPU 利用率启动新实例。",
      "D": "为 Auto Scaling 组配置一个定时伸缩策略，在高峰时段之前启动新实例。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Auto Scaling Group",
      "Application Load Balancer",
      "CPU Utilization",
      "Memory Utilization",
      "Scheduled Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查了 Auto Scaling 组的伸缩策略选择，特别是针对周期性负载高峰的场景。需要理解不同伸缩策略的适用场景，并结合应用程序的性能表现进行分析，与定时伸缩策略、动态伸缩策略相关。",
      "why_correct": "选项 D 提供了最佳解决方案。由于应用程序高峰时段每天在同一时间发生，使用定时伸缩策略在高峰时段开始之前启动新实例可以确保在高峰时段开始时应用程序有足够的资源，从而避免性能下降。定时伸缩策略可以根据预定义的计划（如特定时间）自动调整 Auto Scaling 组的容量，满足周期性、可预测的负载变化需求。",
      "why_wrong": "选项 A 错误，Application Load Balancer (ALB) 主要负责流量分发，虽然可以提高应用程序的可用性和弹性，但无法解决高峰时段资源不足的问题。即使负载均衡器将流量均匀分配，如果后端 EC2 实例资源不足，应用程序性能仍然会下降。选项 B 和 C 错误，动态伸缩策略（基于内存利用率或 CPU 利用率）虽然可以根据实例的负载情况自动调整 Auto Scaling 组的容量，但它们响应负载变化需要时间。在高峰时段开始时，系统可能尚未响应过来，实例数量不足，无法立即满足需求。并且，高峰时段开始后 2-3 小时性能恢复正常，说明当前实例配置已足够应对峰值，因此动态伸缩策略并不十分必要，而定时策略则能提前准备资源，避免性能问题。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Auto Scaling Group",
      "CPU Utilization",
      "Memory Utilization"
    ]
  },
  {
    "id": 661,
    "topic": "1",
    "question_en": "A company runs applications on AWS that connect to the company's Amazon RDS database. The applications scale on weekends and at peak times of the year. The company wants to scale the database more effectively for its applications that connect to the database. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon DynamoDB with connection pooling with a target group configuration for the database. Change the applications to use the DynamoDB endpoint.",
      "B": "Use Amazon RDS Proxy with a target group for the database. Change the applications to use the RDS Proxy endpoint.",
      "C": "Use a custom proxy that runs on Amazon EC2 as an intermediary to the database. Change the applications to use the custom proxy endpoint.",
      "D": "Use an AWS Lambda function to provide connection pooling with a target group configuration for the database. Change the applications to use the Lambda function."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行连接到公司 Amazon RDS 数据库的应用程序。这些应用程序在周末和一年中的高峰时段进行扩展。该公司希望更有效地扩展连接到数据库的应用程序的数据库。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon DynamoDB 配合连接池以及数据库的目标组配置。更改应用程序以使用 DynamoDB 端点。",
      "B": "使用 Amazon RDS Proxy 配合数据库的目标组。更改应用程序以使用 RDS Proxy 端点。",
      "C": "使用在 Amazon EC2 上运行的自定义代理作为数据库的中介。更改应用程序以使用自定义代理端点。",
      "D": "使用 AWS Lambda 函数来提供连接池以及数据库的目标组配置。更改应用程序以使用 Lambda 函数。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "Database Connection Pooling",
      "EC2",
      "Lambda",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察数据库连接扩展与优化，重点在于理解不同方案的运营开销和适用性。涉及 Amazon RDS Proxy、自定义代理、以及 AWS Lambda 在数据库连接管理方面的应用，并需要结合数据库连接池和目标组配置来综合考量。",
      "why_correct": "Amazon RDS Proxy 提供了针对数据库连接的代理服务，它能够有效地管理数据库连接池，提高数据库的可伸缩性和可用性。RDS Proxy 在客户端和数据库之间建立连接，从而减少了应用程序直接连接数据库的开销，并能够处理突发流量，优化数据库性能。使用 RDS Proxy，无需修改数据库代码，只需更改应用程序连接端点即可。RDS Proxy 易于部署和管理，运营开销较低，非常适合应用程序需要弹性伸缩的场景。",
      "why_wrong": "选项 A 错误，因为 DynamoDB 是一个 NoSQL 数据库，而题目中指出是 RDS 数据库。虽然 DynamoDB 擅长处理高并发读写请求，但它并非关系型数据库的替代方案，无法与现有的 RDS 数据库兼容。 选项 C 错误，因为在 EC2 上部署自定义代理会增加运营开销，需要手动管理 EC2 实例的启动、配置和维护。此外，自定义代理的构建和维护成本也较高。 选项 D 错误，因为使用 Lambda 函数进行数据库连接池管理并不可行，Lambda 函数的执行时间和内存限制不适合处理持续的数据库连接。此外，Lambda 函数的冷启动时间也会影响应用程序的性能。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS Proxy",
      "DynamoDB",
      "EC2",
      "Lambda",
      "Database Connection Pooling"
    ]
  },
  {
    "id": 662,
    "topic": "1",
    "question_en": "A company uses AWS Cost Explorer to monitor its AWS costs. The company notices that Amazon Elastic Block Store (Amazon EBS) storage and snapshot costs increase every month. However, the company does not purchase additional EBS storage every month. The company wants to optimize monthly costs for its current storage usage. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use logs in Amazon CloudWatch Logs to monitor the storage utilization of Amazon EBS. Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes.",
      "B": "Use a custom script to monitor space usage. Use Amazon EBS Elastic Volumes to reduce the size of the EBS volumes.",
      "C": "Delete all expired and unused snapshots to reduce snapshot costs.",
      "D": "Delete all nonessential snapshots. Use Amazon Data Lifecycle Manager to create and manage the snapshots according to the company's snapshot policy requirements."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS Cost Explorer 来监控其 AWS 成本。该公司注意到 Amazon Elastic Block Store (Amazon EBS) 存储和快照成本每个月都在增加。然而，该公司每个月并未购买额外的 EBS 存储。该公司希望优化其当前存储使用的每月成本。哪个解决方案将以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch Logs 中的日志来监控 Amazon EBS 的存储利用率。使用 Amazon EBS 弹性卷来减小 EBS 卷的大小。",
      "B": "使用自定义脚本来监控空间使用情况。使用 Amazon EBS 弹性卷来减小 EBS 卷的大小。",
      "C": "删除所有已过期和未使用的快照以降低快照成本。",
      "D": "删除所有非必要的快照。使用 Amazon Data Lifecycle Manager 根据公司的快照策略要求创建和管理快照。"
    },
    "tags": [
      "Amazon EBS",
      "EBS",
      "Amazon Data Lifecycle Manager",
      "Cost Optimization",
      "Snapshots"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察 Amazon EBS 成本优化，特别是针对 EBS 卷及快照成本的优化。这与成本管理、存储管理策略、以及 AWS Data Lifecycle Manager 的使用密切相关。",
      "why_correct": "删除非必要的快照是降低快照成本的直接方法。Amazon Data Lifecycle Manager（DLM）可以自动化快照的创建、保留和删除流程，从而确保快照符合公司的策略需求，并减少手动管理带来的运营开销。DLM 还可以根据标签等标准实现更精细的快照管理，避免了手动维护的复杂性和潜在错误。",
      "why_wrong": "选项 A 和 B 都依赖手动监控存储利用率和调整 EBS 卷大小，这增加了运营开销，并且并非解决快照成本增加问题的直接方法。选项 A 使用 Amazon CloudWatch Logs 来监控 EBS 存储利用率，需要额外设置和维护，增加了复杂性。选项 B 依赖自定义脚本，则增加了维护和潜在故障的风险。选项 C 仅删除已过期和未使用的快照，虽然有助于降低快照成本，但缺乏自动化管理，无法满足“以最小的运营开销”的要求，且可能未能完全满足公司的快照策略需求。"
    },
    "related_terms": [
      "Amazon EBS",
      "EBS",
      "Amazon CloudWatch Logs",
      "Amazon Data Lifecycle Manager",
      "Snapshots",
      "EBS Volume",
      "Cost Explorer"
    ]
  },
  {
    "id": 663,
    "topic": "1",
    "question_en": "A company is developing a new application on AWS. The application consists of an Amazon Elastic Container Service (Amazon ECS) cluster, an Amazon S3 bucket that contains assets for the application, and an Amazon RDS for MySQL database that contains the dataset for the application. The dataset contains sensitive information. The company wants to ensure that only the ECS cluster can access the data in the RDS for MySQL database and the data in the S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a new AWS Key Management Service (AWS KMS) customer managed key to encrypt both the S3 bucket and the RDS for MySQL database. Ensure that the KMS key policy includes encrypt and decrypt permissions for the ECS task execution role.",
      "B": "Create an AWS Key Management Service (AWS KMS) AWS managed key to encrypt both the S3 bucket and the RDS for MySQL database. Ensure that the S3 bucket policy specifies the ECS task execution role as a user.",
      "C": "Create an S3 bucket policy that restricts bucket access to the ECS task execution role. Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in.",
      "D": "Create a VPC endpoint for Amazon RDS for MySQL. Update the RDS for MySQL security group to allow access from only the subnets that the ECS cluster will generate tasks in. Create a VPC endpoint for Amazon S3. Update the S3 bucket policy to allow access from only the S3 VPC endpoint."
    },
    "correct_answer": "A",
    "vote_percentage": "61%",
    "question_cn": "一家公司正在 AWS 上开发一个新应用程序。该应用程序包含一个 Amazon Elastic Container Service (Amazon ECS) 集群、一个包含应用程序资产的 Amazon S3 存储桶以及一个包含应用程序数据集的 Amazon RDS for MySQL 数据库。数据集包含敏感信息。该公司希望确保只有 ECS 集群可以访问 RDS for MySQL 数据库中的数据和 S3 存储桶中的数据。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 AWS Key Management Service (AWS KMS) 客户托管密钥，以加密 S3 存储桶和 RDS for MySQL 数据库。确保 KMS 密钥策略包含 ECS 任务执行角色的加密和解密权限。",
      "B": "创建一个 AWS Key Management Service (AWS KMS) AWS 托管密钥，以加密 S3 存储桶和 RDS for MySQL 数据库。确保 S3 存储桶策略将 ECS 任务执行角色指定为用户。",
      "C": "创建 S3 存储桶策略，将存储桶访问限制为 ECS 任务执行角色。为 Amazon RDS for MySQL 创建一个 VPC endpoint。更新 RDS for MySQL 安全组，仅允许来自 ECS 集群将在其中生成任务的子网的访问。",
      "D": "为 Amazon RDS for MySQL 创建一个 VPC endpoint。更新 RDS for MySQL 安全组，仅允许来自 ECS 集群将在其中生成任务的子网的访问。为 Amazon S3 创建一个 VPC endpoint。更新 S3 存储桶策略，仅允许来自 S3 VPC endpoint 的访问。"
    },
    "tags": [
      "Amazon S3",
      "Amazon RDS for MySQL",
      "Amazon ECS",
      "AWS KMS",
      "S3 Bucket Policy",
      "Security",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 61%），解析仅供参考。】\n\n考察如何通过 AWS KMS 和 IAM 角色实现对 S3 存储桶和 RDS for MySQL 数据库的访问控制，确保只有 ECS 集群能够访问敏感数据。这涉及到 KMS 密钥的创建和权限配置，以及与 S3 存储桶策略、数据库安全组等相关知识。",
      "why_correct": "选项 A 采用了客户托管密钥（Customer Managed Key, CMK）进行 S3 存储桶和 RDS for MySQL 数据库的加密。通过创建新的 KMS 密钥，能够完全控制加密密钥，并确保只有 ECS 任务执行角色具有加密和解密权限。这种方法能够实现细粒度的访问控制，符合题目中“只有 ECS 集群可以访问”的要求，并保障了数据的安全性和访问的隔离性。",
      "why_wrong": "选项 B 错误，因为它使用了 AWS 托管密钥（AWS Managed Key）。虽然 AWS 托管密钥使用起来更加便捷，但它无法提供细粒度的访问控制，并且加密操作的策略和权限不由用户控制。使用 AWS 托管密钥无法满足题目中对特定角色访问权限的精确要求。选项 C 错误，虽然配置了 S3 存储桶策略和 RDS 安全组，但并没有对数据进行加密，这意味着数据在存储和传输过程中可能面临安全风险。选项 D 错误，它仅配置了 VPC endpoint 和安全组策略，同样没有解决数据加密问题。VPC endpoint 主要用于安全地访问 AWS 服务，而无法单独提供数据的安全保护，因此不能满足安全需求。"
    },
    "related_terms": [
      "Amazon ECS",
      "Amazon S3",
      "AWS KMS",
      "S3",
      "ECS",
      "KMS",
      "IAM",
      "MySQL",
      "Amazon RDS for MySQL",
      "RDS for MySQL",
      "VPC endpoint",
      "S3 Bucket Policy",
      "Security Group",
      "Customer Managed Key",
      "AWS Managed Key",
      "Encryption",
      "Decryption"
    ]
  },
  {
    "id": 664,
    "topic": "1",
    "question_en": "A company has a web application that runs on premises. The application experiences latency issues during peak hours. The latency issues occur twice each month. At the start of a latency issue, the application's CPU utilization immediately increases to 10 times its normal amount. The company wants to migrate the application to AWS to improve latency. The company also wants to scale the application automatically when application demand increases. The company will use AWS Elastic Beanstalk for application deployment. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale based on requests.",
      "B": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale based on requests.",
      "C": "Configure an Elastic Beanstalk environment to use compute optimized instances. Configure the environment to scale on a schedule.",
      "D": "Configure an Elastic Beanstalk environment to use burstable performance instances in unlimited mode. Configure the environment to scale on predictive metrics."
    },
    "correct_answer": "A",
    "vote_percentage": "56%",
    "question_cn": "一家公司有一个在本地运行的 Web 应用程序。该应用程序在高峰时段遇到延迟问题。 延迟问题每月发生两次。 在延迟问题开始时，应用程序的 CPU 利用率立即增加到其正常数量的 10 倍。 公司希望将应用程序迁移到 AWS 以改善延迟。 公司还希望在应用程序需求增加时自动扩展应用程序。 公司将使用 AWS Elastic Beanstalk 进行应用程序部署。 哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Elastic Beanstalk 环境以使用无限模式的可突发性能实例。配置环境以根据请求进行扩展。",
      "B": "配置 Elastic Beanstalk 环境以使用计算优化实例。配置环境以根据请求进行扩展。",
      "C": "配置 Elastic Beanstalk 环境以使用计算优化实例。配置环境以按计划进行扩展。",
      "D": "配置 Elastic Beanstalk 环境以使用无限模式的可突发性能实例。配置环境以根据预测指标进行扩展。"
    },
    "tags": [
      "AWS Elastic Beanstalk",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 56%），解析仅供参考。】\n\n考查在 Elastic Beanstalk 上部署应用程序，并根据需求自动扩展以及选择合适的实例类型以解决延迟问题。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 选择了可突发性能实例的无限模式，这适合应对应用程序 CPU 利用率突然大幅增加的情况。同时，根据请求进行扩展可以响应应用程序需求的动态变化，从而改善延迟问题。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，计算优化实例通常用于处理计算密集型任务，而本题的延迟问题是由于 CPU 负载激增，可突发性能实例的弹性更合适。选项 C 错误，计划扩展无法及时响应每月发生两次的突发负载，不能满足按需扩展的需求。选项 D 错误，预测指标不适用于 CPU 利用率立即增加 10 倍的情况，难以准确预测，而根据请求扩展更能及时响应变化。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Elastic Beanstalk",
      "CPU",
      "burstable performance instances",
      "compute optimized instances",
      "AWS"
    ]
  },
  {
    "id": 665,
    "topic": "1",
    "question_en": "A company has customers located across the world. The company wants to use automation to secure its systems and network infrastructure. The company's security team must be able to track and audit all incremental changes to the infrastructure. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Organizations to set up the infrastructure. Use AWS Config to track changes.",
      "B": "Use AWS CloudFormation to set up the infrastructure. Use AWS Config to track changes.",
      "C": "Use AWS Organizations to set up the infrastructure. Use AWS Service Catalog to track changes.",
      "D": "Use AWS CloudFormation to set up the infrastructure. Use AWS Service Catalog to track changes."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在全球各地都有客户。该公司希望使用自动化来保护其系统和网络基础设施。该公司的安全团队必须能够跟踪和审计对基础设施的所有增量更改。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Organizations 设置基础设施。使用 AWS Config 跟踪更改。",
      "B": "使用 AWS CloudFormation 设置基础设施。使用 AWS Config 跟踪更改。",
      "C": "使用 AWS Organizations 设置基础设施。使用 AWS Service Catalog 跟踪更改。",
      "D": "使用 AWS CloudFormation 设置基础设施。使用 AWS Service Catalog 跟踪更改。"
    },
    "tags": [
      "AWS CloudFormation",
      "AWS Config",
      "Infrastructure as Code (IaC)",
      "Auditing"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查基础设施即代码（IaC）和配置变更追踪；与 CloudFormation 和 Config 的应用场景、功能特性相关。",
      "why_correct": "CloudFormation 允许您将基础设施定义为代码，实现自动化部署和管理。结合 AWS Config，可以持续监控和跟踪 CloudFormation 堆栈的配置更改，满足对基础设施变更进行审计的需求。CloudFormation 提供了基础设施的模板化，可以方便地进行版本控制和重复部署，Config 则记录了所有配置更改的历史，实现变更的审计和回溯。",
      "why_wrong": "选项 A 和 C 错误，虽然 AWS Organizations 提供了集中管理多个 AWS 账户的能力，但它本身并不直接用于基础设施的部署或变更追踪。AWS Service Catalog 用于创建和管理 AWS 服务的模板，主要用于服务目录的管理，而非跟踪底层基础设施的变更。选项 D 错误，虽然 CloudFormation 用于基础设施部署，但 Service Catalog 不具备跟踪基础设施变更的能力，因此无法满足题目的审计要求。"
    },
    "related_terms": [
      "AWS CloudFormation",
      "AWS Config",
      "AWS Organizations",
      "AWS Service Catalog"
    ]
  },
  {
    "id": 666,
    "topic": "1",
    "question_en": "A startup company is hosting a website for its customers on an Amazon EC2 instance. The website consists of a stateless Python application and a MySQL database. The website serves only a small amount of trafic. The company is concerned about the reliability of the instance and needs to migrate to a highly available architecture. The company cannot modify the application code. Which combination of actions should a solutions architect take to achieve high availability for the website? (Choose two.)",
    "options_en": {
      "A": "Provision an internet gateway in each Availability Zone in use.",
      "B": "Migrate the database to an Amazon RDS for MySQL Multi-AZ DB instance.",
      "C": "Migrate the database to Amazon DynamoDB, and enable DynamoDB auto scaling.",
      "D": "Use AWS DataSync to synchronize the database data across multiple EC2 instances",
      "E": "Create an Application Load Balancer to distribute trafic to an Auto Scaling group of EC2 instances that are distributed across two Availability Zones."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家初创公司正在 Amazon EC2 实例上为其客户托管一个网站。该网站由一个无状态的 Python 应用程序和一个 MySQL 数据库组成。该网站仅提供少量流量。该公司担心实例的可靠性，需要迁移到一个高可用性架构。该公司无法修改应用程序代码。解决方案架构师应采取哪些组合操作来实现网站的高可用性？（选择两个。）",
    "options_cn": {
      "A": "在使用的每个可用区中配置一个互联网网关。",
      "B": "将数据库迁移到 Amazon RDS for MySQL 多可用区数据库实例。",
      "C": "将数据库迁移到 Amazon DynamoDB，并启用 DynamoDB 自动伸缩。",
      "D": "使用 AWS DataSync 将数据库数据同步到多个 EC2 实例。",
      "E": "创建一个 Application Load Balancer，将流量分配给分布在两个可用区中的 EC2 实例的 Auto Scaling 组。"
    },
    "tags": [
      "Amazon EC2",
      "Availability",
      "Amazon RDS",
      "MySQL",
      "Application Load Balancer",
      "Auto Scaling",
      "Amazon DynamoDB",
      "AWS DataSync",
      "Internet Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n考察如何实现 EC2 实例上网站的高可用性，涉及数据库高可用性方案选择、负载均衡、以及对现有应用程序代码的约束。与数据库服务选型、负载均衡器类型、可用区配置相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：将数据库迁移到 Amazon RDS for MySQL 多可用区数据库实例，可以提供数据库层面的高可用性。RDS 多可用区部署通过在不同可用区中创建数据库实例的备用副本，实现故障转移，当主数据库实例出现问题时，RDS 会自动切换到备用实例，从而确保应用程序的持续运行。该方案不需要修改应用程序代码，符合题目要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A：在每个可用区配置一个 Internet Gateway，这与实现高可用性无关，Internet Gateway 用于允许 EC2 实例访问 Internet，无法解决数据库或应用程序实例的可用性问题。\n\n选项 C：将数据库迁移到 Amazon DynamoDB 并启用 DynamoDB 自动伸缩，虽然 DynamoDB 具有高可用性，但题目中使用的是 MySQL 数据库，并且用户无法修改代码。迁移到 DynamoDB 需要更改应用程序代码以使用 NoSQL 数据库，不符合题目要求。\n\n选项 D：使用 AWS DataSync 将数据库数据同步到多个 EC2 实例，这是一种数据同步解决方案，无法提供数据库本身的高可用性。DataSync 更多用于数据迁移和同步，而不是数据库的故障转移和冗余。同时，多个 EC2 实例同步数据库数据，需要手动管理数据库的复制和同步，实现复杂且容易出错。\n\n选项 E：创建一个 Application Load Balancer，将流量分配给分布在两个可用区中的 EC2 实例的 Auto Scaling 组。虽然使用 Application Load Balancer 和 Auto Scaling 可以提高应用程序的可用性，但此方案只解决了应用程序层的可用性问题，没有解决数据库层面的问题，因此不能单独构成高可用性架构。需要结合数据库高可用性方案，例如 RDS 多可用区部署。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon DynamoDB",
      "Auto Scaling",
      "Application Load Balancer",
      "Internet Gateway",
      "AWS DataSync",
      "Availability Zone"
    ]
  },
  {
    "id": 667,
    "topic": "1",
    "question_en": "A company is moving its data and applications to AWS during a multiyear migration project. The company wants to securely access data on Amazon S3 from the company's AWS Region and from the company's on-premises location. The data must not traverse the internet. The company has established an AWS Direct Connect connection between its Region and its on-premises location. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create gateway endpoints for Amazon S3. Use the gateway endpoints to securely access the data from the Region and the on-premises location.",
      "B": "Create a gateway in AWS Transit Gateway to access Amazon S3 securely from the Region and the on-premises location.",
      "C": "Create interface endpoints for Amazon S3. Use the interface endpoints to securely access the data from the Region and the on- premises location.",
      "D": "Use an AWS Key Management Service (AWS KMS) key to access the data securely from the Region and the on-premises location."
    },
    "correct_answer": "C",
    "vote_percentage": "82%",
    "question_cn": "一家公司正在进行一项多年迁移项目，将其数据和应用程序迁移到 AWS。该公司希望安全地从公司的 AWS 区域和公司的本地位置访问 Amazon S3 上的数据。数据不得遍历互联网。该公司已在其区域和本地位置之间建立了 AWS Direct Connect 连接。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Amazon S3 创建网关端点。使用网关端点从该区域和本地位置安全地访问数据。",
      "B": "在 AWS Transit Gateway 中创建一个网关，以便从该区域和本地位置安全地访问 Amazon S3。",
      "C": "为 Amazon S3 创建接口端点。使用接口端点从该区域和本地位置安全地访问数据。",
      "D": "使用 AWS Key Management Service (AWS KMS) 密钥从该区域和本地位置安全地访问数据。"
    },
    "tags": [
      "Amazon S3",
      "VPC Endpoint",
      "AWS Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 82%），解析仅供参考。】\n\n考察使用 AWS Direct Connect 和 Amazon S3 的安全访问方案。需要选择一个能够通过 Direct Connect 访问 S3 数据的解决方案，且数据不经过 Internet。",
      "why_correct": "选项 C 为 Amazon S3 创建接口端点能够满足需求。接口端点通过 AWS PrivateLink 提供对 S3 的私有连接，允许通过 VPC 访问 S3，并支持通过 Direct Connect 连接进行访问。这样数据流不会经过 Internet，满足了安全访问的需求。",
      "why_wrong": "选项 A 错误，网关端点仅支持从 VPC 访问 S3，不支持从本地位置通过 Direct Connect 直接访问。选项 B 错误，虽然 Transit Gateway 可以管理网络流量，但它本身不能直接提供访问 S3 的私有连接，且不能直接解决从本地位置访问 S3 的需求。选项 D 错误，AWS KMS 用于加密密钥管理，无法提供访问 S3 的网络连接，与题目需求不符。"
    },
    "related_terms": [
      "AWS Direct Connect",
      "Amazon S3",
      "AWS KMS",
      "VPC",
      "Transit Gateway",
      "Gateway Endpoint",
      "Interface Endpoint",
      "AWS PrivateLink"
    ]
  },
  {
    "id": 668,
    "topic": "1",
    "question_en": "A company created a new organization in AWS Organizations. The organization has multiple accounts for the company's development teams. The development team members use AWS IAM Identity Center (AWS Single Sign-On) to access the accounts. For each of the company's applications, the development teams must use a predefined application name to tag resources that are created. A solutions architect needs to design a solution that gives the development team the ability to create resources only if the application name tag has an approved value. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an IAM group that has a conditional Allow policy that requires the application name tag to be specified for resources to be created.",
      "B": "Create a cross-account role that has a Deny policy for any resource that has the application name tag.",
      "C": "Create a resource group in AWS Resource Groups to validate that the tags are applied to all resources in all accounts.",
      "D": "Create a tag policy in Organizations that has a list of allowed application names."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS Organizations 中创建了一个新组织。该组织拥有用于该公司开发团队的多个账户。开发团队成员使用 AWS IAM Identity Center (AWS Single Sign-On) 访问账户。对于该公司的每个应用程序，开发团队必须使用预定义的应用程序名称来标记创建的资源。解决方案架构师需要设计一个解决方案，该解决方案使开发团队能够仅在应用程序名称标签具有批准值时才能创建资源。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 IAM 组，该组具有条件允许策略，该策略要求必须为要创建的资源指定应用程序名称标签。",
      "B": "创建一个跨账户角色，该角色具有拒绝应用程序名称标签的任何资源的策略。",
      "C": "在 AWS 资源组中创建资源组，以验证标签是否应用于所有账户中的所有资源。",
      "D": "在 Organizations 中创建标签策略，其中包含允许的应用程序名称的列表。"
    },
    "tags": [
      "AWS Organizations",
      "Tag Policies",
      "IAM",
      "IAM Identity Center",
      "AWS Single Sign-On"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查通过 AWS Organizations 中的 Tag Policies 来强制实施资源标签的合规性。这与 IAM 的权限管理、AWS Single Sign-On 访问控制，以及资源标签的组织管理密切相关。",
      "why_correct": "Tag Policies 允许在整个组织或组织单元（OU）中集中管理资源标签。通过创建包含允许的应用程序名称的标签策略，可以强制开发团队在创建资源时必须使用这些批准的标签值。这满足了题目中对预定义标签名称的要求，并确保只有拥有正确标签的资源才能被创建。此方案实现了集中化的标签管理，避免了分散的策略配置，更易于维护和审计。",
      "why_wrong": "A 选项创建 IAM 组并使用条件允许策略，这只能限制账户内的资源创建，无法跨组织强制执行标签策略。它主要用于基于标签的访问控制，而非强制标签本身。B 选项创建跨账户角色并使用拒绝策略，这是一种不推荐的做法，因为它会阻止任何拥有该标签的资源创建，而不是限制允许的值。这不符合题意，且会带来维护和配置的复杂性。C 选项创建 AWS 资源组，资源组仅用于资源编排和管理，不能用于强制实施标签策略或验证标签值。它无法实现对资源创建时标签的验证，并且无法达到题目的要求。"
    },
    "related_terms": [
      "AWS Organizations",
      "IAM",
      "IAM Identity Center",
      "Tag Policies",
      "AWS Single Sign-On",
      "IAM Group",
      "AWS Resource Groups"
    ]
  },
  {
    "id": 669,
    "topic": "1",
    "question_en": "A company runs its databases on Amazon RDS for PostgreSQL. The company wants a secure solution to manage the master user password by rotating the password every 30 days. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon EventBridge to schedule a custom AWS Lambda function to rotate the password every 30 days.",
      "B": "Use the modify-db-instance command in the AWS CLI to change the password.",
      "C": "Integrate AWS Secrets Manager with Amazon RDS for PostgreSQL to automate password rotation.",
      "D": "Integrate AWS Systems Manager Parameter Store with Amazon RDS for PostgreSQL to automate password rotation."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Amazon RDS for PostgreSQL 上运行数据库。该公司希望通过每 30 天轮换一次密码来管理主用户密码的安全解决方案。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EventBridge 安排自定义 AWS Lambda 函数，每 30 天轮换一次密码。",
      "B": "使用 AWS CLI 中的 modify-db-instance 命令更改密码。",
      "C": "将 AWS Secrets Manager 与 Amazon RDS for PostgreSQL 集成，以自动轮换密码。",
      "D": "将 AWS Systems Manager Parameter Store 与 Amazon RDS for PostgreSQL 集成，以自动轮换密码。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "AWS Secrets Manager",
      "Password Rotation",
      "Lambda",
      "AWS Systems Manager Parameter Store",
      "EventBridge",
      "AWS CLI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查了在 Amazon RDS for PostgreSQL 上使用密码轮换的安全实践，以及不同 AWS 服务（Secrets Manager、Lambda、Parameter Store）在密码管理方面的应用与对比。",
      "why_correct": "AWS Secrets Manager 提供了对密钥、密码和其他机密信息的存储、检索和管理功能，并支持自动轮换这些机密信息。 通过将 Secrets Manager 与 Amazon RDS for PostgreSQL 集成，可以配置 Secrets Manager 定期（例如，每 30 天）自动轮换数据库主用户密码。 这是满足题目要求的、运营开销最低的方案，因为它无需手动干预，且轮换过程由 AWS 托管。",
      "why_wrong": "A. 使用 Amazon EventBridge 和 Lambda 函数需要手动编写和维护自定义代码来轮换密码，增加了运营开销和复杂性，且安全性不如 Secrets Manager。B. 使用 AWS CLI 命令手动修改密码不是自动化解决方案，不符合题意，并且手动操作容易出错。D. AWS Systems Manager Parameter Store 主要用于存储配置数据和参数，虽然可以存储密码，但它本身不提供自动轮换功能，因此需要额外的配置和实现，不能直接满足题目要求，并且不如 Secrets Manager 集成方便。"
    },
    "related_terms": [
      "Amazon RDS",
      "PostgreSQL",
      "AWS Secrets Manager",
      "Lambda",
      "AWS CLI",
      "EventBridge",
      "AWS Systems Manager Parameter Store"
    ]
  },
  {
    "id": 670,
    "topic": "1",
    "question_en": "A company performs tests on an application that uses an Amazon DynamoDB table. The tests run for 4 hours once a week. The company knows how many read and write operations the application performs to the table each second during the tests. The company does not currently use DynamoDB for any other use case. A solutions architect needs to optimize the costs for the table. Which solution will meet these requirements?",
    "options_en": {
      "A": "Choose on-demand mode. Update the read and write capacity units appropriately.",
      "B": "Choose provisioned mode. Update the read and write capacity units appropriately.",
      "C": "Purchase DynamoDB reserved capacity for a 1-year term.",
      "D": "Purchase DynamoDB reserved capacity for a 3-year term."
    },
    "correct_answer": "B",
    "vote_percentage": "75%",
    "question_cn": "一家公司对其使用 Amazon DynamoDB 表的应用程序进行测试。测试每周运行一次，持续 4 小时。该公司知道应用程序在测试期间每秒对表执行的读写操作数量。该公司目前没有将 DynamoDB 用于任何其他用例。解决方案架构师需要优化表的成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "选择按需模式。适当地更新读取和写入容量单位。",
      "B": "选择预置模式。适当地更新读取和写入容量单位。",
      "C": "购买 DynamoDB 预留容量，为期 1 年。",
      "D": "购买 DynamoDB 预留容量，为期 3 年。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB Capacity Mode",
      "DynamoDB Reserved Capacity"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 75%），解析仅供参考。】\n\n考查如何优化 Amazon DynamoDB 表的成本，重点在于选择合适的容量模式。需要根据应用程序的负载特性（周期性、已知读写量）来选择。",
      "why_correct": "预置模式允许您根据应用程序的已知负载量（每周运行一次测试，持续 4 小时，且已知每秒读写操作数量）预先配置容量，从而优化成本。通过精确配置读写容量单元，可以避免因过度配置而导致的浪费，并保证应用程序的性能需求。因此，根据已知负载量，适当地更新预置模式的读取和写入容量单元是最佳的解决方案。",
      "why_wrong": "按需模式适用于负载不可预测或流量不稳定的情况，而本题测试负载已知，因此使用按需模式可能会导致成本过高。DynamoDB 预留容量适用于长期、稳定的负载，本题的测试只持续 4 小时，即使购买 1 年或 3 年的预留容量也无法满足需求，且成本效益不高。预留容量通常不适用于短期、周期性负载。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB",
      "read capacity units",
      "write capacity units",
      "On-demand mode",
      "Provisioned mode",
      "DynamoDB reserved capacity"
    ]
  },
  {
    "id": 671,
    "topic": "1",
    "question_en": "A company runs its applications on Amazon EC2 instances. The company performs periodic financial assessments of its AWS costs. The company recently identified unusual spending. The company needs a solution to prevent unusual spending. The solution must monitor costs and notify responsible stakeholders in the event of unusual spending. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an AWS Budgets template to create a zero spend budget.",
      "B": "Create an AWS Cost Anomaly Detection monitor in the AWS Billing and Cost Management console.",
      "C": "Create AWS Pricing Calculator estimates for the current running workload pricing details.",
      "D": "Use Amazon CloudWatch to monitor costs and to identify unusual spending."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Amazon EC2 实例上运行其应用程序。该公司定期进行其 AWS 成本的财务评估。该公司最近发现了异常支出。该公司需要一个解决方案来防止异常支出。该解决方案必须监控成本，并在发生异常支出时通知相关利益相关者。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Budgets 模板创建一个零支出预算。",
      "B": "在 AWS Billing and Cost Management 控制台中创建 AWS Cost Anomaly Detection 监控。",
      "C": "为当前运行的工作负载定价详细信息创建 AWS 定价计算器估算。",
      "D": "使用 Amazon CloudWatch 监控成本并识别异常支出。"
    },
    "tags": [
      "AWS Cost Management",
      "AWS Budgets",
      "AWS Cost Anomaly Detection",
      "Amazon CloudWatch",
      "AWS Pricing Calculator",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了 AWS 成本监控和异常支出检测的解决方案，以及如何通知利益相关者。",
      "why_correct": "AWS Cost Anomaly Detection 提供了对成本的自动监控，使用机器学习技术检测异常支出。它可以配置通知，在检测到异常时发送警报给相关利益相关者。这直接满足了题目中监控成本并通知异常支出的需求。",
      "why_wrong": "选项 A，AWS Budgets 的零支出预算无法用于检测和通知异常支出，它主要用于限制支出，而非检测异常。选项 C，AWS Pricing Calculator 用于估算成本，并非用于实时监控和检测。选项 D，虽然 Amazon CloudWatch 可以用于监控指标，但其本身不提供专门的异常支出检测功能，需要手动配置，不如 Cost Anomaly Detection 方便。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS",
      "AWS Cost Anomaly Detection",
      "AWS Billing and Cost Management",
      "AWS Budgets",
      "AWS Pricing Calculator",
      "Amazon CloudWatch"
    ]
  },
  {
    "id": 672,
    "topic": "1",
    "question_en": "A marketing company receives a large amount of new clickstream data in Amazon S3 from a marketing campaign. The company needs to analyze the clickstream data in Amazon S3 quickly. Then the company needs to determine whether to process the data further in the data pipeline. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create external tables in a Spark catalog. Configure jobs in AWS Glue to query the data.",
      "B": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Athena to query the data.",
      "C": "Create external tables in a Hive metastore. Configure Spark jobs in Amazon EMR to query the data.",
      "D": "Configure an AWS Glue crawler to crawl the data. Configure Amazon Kinesis Data Analytics to use SQL to query the data."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家营销公司从营销活动中在 Amazon S3 中接收大量新的点击流数据。该公司需要快速分析 Amazon S3 中的点击流数据，然后确定是否需要在数据管道中进一步处理数据。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 Spark 目录中创建外部表。配置 AWS Glue 中的作业以查询数据。",
      "B": "配置 AWS Glue 爬虫程序以爬取数据。配置 Amazon Athena 以查询数据。",
      "C": "在 Hive 元存储中创建外部表。配置 Amazon EMR 中的 Spark 作业以查询数据。",
      "D": "配置 AWS Glue 爬虫程序以爬取数据。配置 Amazon Kinesis Data Analytics 使用 SQL 查询数据。"
    },
    "tags": [
      "Amazon S3",
      "AWS Glue",
      "Amazon Kinesis Data Analytics",
      "Clickstream data",
      "Data analysis"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n考查利用 AWS Glue 和 Amazon Athena 实现对 Amazon S3 数据的快速分析，以满足低运维成本和快速查询的需求。",
      "why_correct": "AWS Glue 爬虫程序可以自动发现并提取 S3 中的数据模式，创建数据目录。Amazon Athena 是一种无服务器查询服务，可以直接查询 S3 中的数据，无需预先创建复杂的 ETL 管道或管理基础设施。结合使用两者，能够以最小的运营开销快速分析 S3 中的点击流数据。",
      "why_wrong": "选项 A 涉及 Spark 和 AWS Glue，虽然可以查询，但需要维护 Spark 目录，增加了运营复杂性。选项 C 涉及 Hive 元存储和 Amazon EMR，EMR 需要管理集群，也增加了运营开销。选项 D 使用了 Amazon Kinesis Data Analytics，虽然可以对流数据进行分析，但并不适合直接分析存储在 S3 中的数据，且 Kinesis Data Analytics 侧重于实时数据流处理，与题目的需求不符。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Glue",
      "Amazon Athena",
      "Spark",
      "AWS Glue Crawler",
      "Hive",
      "Amazon EMR",
      "Amazon Kinesis Data Analytics",
      "SQL"
    ]
  },
  {
    "id": 673,
    "topic": "1",
    "question_en": "A company runs an SMB file server in its data center. The file server stores large files that the company frequently accesses for up to 7 days after the file creation date. After 7 days, the company needs to be able to access the files with a maximum retrieval time of 24 hours. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.",
      "B": "Create an Amazon S3 File Gateway to increase the company's storage space. Create an S3 Lifecycle policy to transition the data to S3 Glacier Deep Archive after 7 days.",
      "C": "Create an Amazon FSx File Gateway to increase the company's storage space. Create an Amazon S3 Lifecycle policy to transition the data after 7 days.",
      "D": "Configure access to Amazon S3 for each user. Create an S3 Lifecycle policy to transition the data to S3 Glacier Flexible Retrieval after 7 days."
    },
    "correct_answer": "B",
    "vote_percentage": "82%",
    "question_cn": "一家公司在其数据中心运行一个 SMB 文件服务器。该文件服务器存储大型文件，公司在文件创建日期后的 7 天内频繁访问这些文件。7 天后，公司需要能够访问这些文件，最大检索时间为 24 小时。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS DataSync 将超过 7 天的数据从 SMB 文件服务器复制到 AWS。",
      "B": "创建一个 Amazon S3 File Gateway 以增加公司的存储空间。创建一个 S3 生命周期策略，在 7 天后将数据转移到 S3 Glacier Deep Archive。",
      "C": "创建一个 Amazon FSx File Gateway 以增加公司的存储空间。创建一个 Amazon S3 生命周期策略，在 7 天后转移数据。",
      "D": "为每个用户配置对 Amazon S3 的访问权限。创建一个 S3 生命周期策略，在 7 天后将数据转移到 S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "Amazon S3",
      "S3",
      "S3 Lifecycle",
      "S3 Glacier Flexible Retrieval",
      "SMB File Server",
      "S3 File Gateway",
      "Amazon FSx",
      "AWS DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 82%），解析仅供参考。】\n\n考查 S3 生命周期策略（Lifecycle policy）与存储分层。考察如何将文件从本地文件服务器迁移到 AWS S3，并根据访问频率进行存储分层。",
      "why_correct": "Amazon S3 File Gateway 允许公司访问本地文件服务器中的数据，并将其存储在 S3 中。S3 生命周期策略可以自动将 7 天后访问频率较低的数据转移到 S3 Glacier Deep Archive，以满足低成本存储的需求。S3 Glacier Deep Archive 的最大检索时间为 12 小时，满足题目的 24 小时检索需求。",
      "why_wrong": "选项 A 使用 AWS DataSync 将数据迁移到 AWS，但未指定后续的存储分层，无法满足低成本存储的需求，也没有体现 24 小时检索时间限制。选项 C 使用了 Amazon FSx File Gateway，FSx 主要提供高性能文件系统，不适用于低成本存储的需求。选项 D 要求为每个用户配置对 Amazon S3 的访问权限，这增加了管理复杂性，且 S3 Glacier Flexible Retrieval 检索时间较长，不满足 24 小时检索需求。"
    },
    "related_terms": [
      "SMB",
      "AWS DataSync",
      "Amazon S3",
      "S3 File Gateway",
      "S3 Lifecycle policy",
      "S3 Glacier Deep Archive",
      "Amazon FSx File Gateway",
      "S3 Glacier Flexible Retrieval"
    ]
  },
  {
    "id": 674,
    "topic": "1",
    "question_en": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group. The application uses a database that runs on an Amazon RDS for PostgreSQL DB instance. The application performs slowly when trafic increases. The database experiences a heavy read load during periods of high trafic. Which actions should a solutions architect take to resolve these performance issues? (Choose two.)",
    "options_en": {
      "A": "Turn on auto scaling for the DB instance.",
      "B": "Create a read replica for the DB instance. Configure the application to send read trafic to the read replica.",
      "C": "Convert the DB instance to a Multi-AZ DB instance deployment. Configure the application to send read trafic to the standby DB instance.",
      "D": "Create an Amazon ElastiCache cluster. Configure the application to cache query results in the ElastiCache cluster",
      "E": "Configure the Auto Scaling group subnets to ensure that the EC2 instances are provisioned in the same Availability Zone as the DB instance."
    },
    "correct_answer": "BD",
    "vote_percentage": "86%",
    "question_cn": "一家公司在 Auto Scaling 组中的 Amazon EC2 实例上运行一个 Web 应用程序。该应用程序使用在 Amazon RDS for PostgreSQL 数据库实例上运行的数据库。当流量增加时，应用程序运行缓慢。在高峰时，数据库会经历繁重的读取负载。解决方案架构师应采取哪些措施来解决这些性能问题？（选择两个。）",
    "options_cn": {
      "A": "为数据库实例打开自动伸缩。",
      "B": "为数据库实例创建一个只读副本。配置应用程序将读取流量发送到只读副本。",
      "C": "将数据库实例转换为多可用区数据库实例部署。配置应用程序将读取流量发送到备用数据库实例。",
      "D": "创建一个 Amazon ElastiCache 集群。配置应用程序将查询结果缓存在 ElastiCache 集群中。",
      "E": "配置 Auto Scaling 组子网，以确保在与数据库实例相同的可用区中预置 EC2 实例。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "Auto Scaling",
      "Read Replicas",
      "ElastiCache",
      "Multi-AZ",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 86%），解析仅供参考。】\n\n考察如何优化 RDS for PostgreSQL 数据库的性能，特别是在高读取负载场景下，以及针对 Web 应用程序的优化措施。",
      "why_correct": "B. 创建只读副本（Read Replica）是解决数据库读取负载问题的常见方法。通过将读取流量分流到只读副本，可以减轻主数据库的压力，提高应用程序的响应速度。 配置应用程序将读取流量发送到只读副本，可以有效分担主数据库的压力。\nD. 使用 ElastiCache 集群可以缓存数据库查询结果。通过缓存常用查询结果，减少对数据库的读取操作，从而提高应用程序的性能。ElastiCache 能够有效降低数据库负载，提升响应速度。",
      "why_wrong": "A. RDS 数据库实例不支持自动伸缩。数据库实例的资源调整需要手动进行或通过配置升级操作实现，无法像 EC2 实例一样自动伸缩。 \nC. 将数据库转换为多可用区（Multi-AZ）部署，主要目的是提高数据库的可用性和容错能力，而不是为了提高读取性能。备用数据库实例通常用于故障转移，应用程序不能直接向其发送读取流量。\nE.  虽然在与数据库实例相同的可用区中预置 EC2 实例可以减少延迟，但并不能直接解决数据库的读取负载问题。该方法主要针对网络延迟，而不是数据库性能瓶颈。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS for PostgreSQL",
      "Web application",
      "Database instance",
      "Read Replica",
      "Amazon ElastiCache",
      "Multi-AZ",
      "Availability Zone"
    ]
  },
  {
    "id": 675,
    "topic": "1",
    "question_en": "A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to run an application. The company creates one snapshot of each EBS volume every day to meet compliance requirements. The company wants to implement an architecture that prevents the accidental deletion of EBS volume snapshots. The solution must not change the administrative rights of the storage administrator user. Which solution will meet these requirements with the LEAST administrative effort?",
    "options_en": {
      "A": "Create an IAM role that has permission to delete snapshots. Attach the role to a new EC2 instance. Use the AWS CLI from the new EC2 instance to delete snapshots.",
      "B": "Create an IAM policy that denies snapshot deletion. Attach the policy to the storage administrator user.",
      "C": "Add tags to the snapshots. Create retention rules in Recycle Bin for EBS snapshots that have the tags.",
      "D": "Lock the EBS snapshots to prevent deletion."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon EC2 实例和 Amazon Elastic Block Store (Amazon EBS) 卷来运行一个应用程序。该公司每天都会拍摄每个 EBS 卷的快照，以满足合规性要求。该公司希望实施一种架构，以防止意外删除 EBS 卷快照。该解决方案不得更改存储管理员用户的管理权限。哪种解决方案将以最小的管理工作量满足这些要求？",
    "options_cn": {
      "A": "创建一个具有删除快照权限的 IAM 角色。将该角色附加到新的 EC2 实例。从新的 EC2 实例中使用 AWS CLI 删除快照。",
      "B": "创建一个拒绝删除快照的 IAM 策略。将该策略附加到存储管理员用户。",
      "C": "为快照添加标签。在回收站中为具有这些标签的 EBS 快照创建保留规则。",
      "D": "锁定 EBS 快照以防止删除。"
    },
    "tags": [
      "Amazon EBS",
      "EBS Snapshots",
      "Amazon S3",
      "S3 Lifecycle Policies",
      "AWS Backup",
      "AWS IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考察了如何通过最小的管理工作量，保护 EBS 快照免受意外删除，同时不改变现有管理员用户的权限。",
      "why_correct": "选项 D，锁定 EBS 快照以防止删除，是满足要求的最佳选择。 EBS 快照锁定功能允许用户设置一个锁定期，在此期间内无法删除快照，从而有效地防止了意外删除，并且不需要修改用户权限或引入额外的基础设施。",
      "why_wrong": "选项 A 引入了新的 EC2 实例和 IAM 角色，增加了管理复杂性，并且仍然依赖于使用 CLI 删除快照，而不是直接防止删除。选项 B，创建拒绝删除快照的 IAM 策略，会影响存储管理员用户的权限，这与题目的要求相悖。选项 C，使用回收站和标签的方案，虽然可以恢复误删除的快照，但并未从根本上防止删除发生，也引入了额外的配置和管理工作量。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "EBS volume",
      "snapshot",
      "IAM",
      "IAM role",
      "IAM policy",
      "AWS CLI",
      "Recycle Bin",
      "EBS snapshot lock"
    ]
  },
  {
    "id": 676,
    "topic": "1",
    "question_en": "A company's application uses Network Load Balancers, Auto Scaling groups, Amazon EC2 instances, and databases that are deployed in an Amazon VPC. The company wants to capture information about trafic to and from the network interfaces in near real time in its Amazon VPC. The company wants to send the information to Amazon OpenSearch Service for analysis. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Streams to stream the logs from the log group to OpenSearch Service.",
      "B": "Create a log group in Amazon CloudWatch Logs. Configure VPC Flow Logs to send the log data to the log group. Use Amazon Kinesis Data Firehose to stream the logs from the log group to OpenSearch Service.",
      "C": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Streams to stream the logs from the trail to OpenSearch Service.",
      "D": "Create a trail in AWS CloudTrail. Configure VPC Flow Logs to send the log data to the trail. Use Amazon Kinesis Data Firehose to stream the logs from the trail to OpenSearch Service."
    },
    "correct_answer": "B",
    "vote_percentage": "92%",
    "question_cn": "一家公司的应用程序使用 Network Load Balancer、Auto Scaling 组、Amazon EC2 实例以及部署在 Amazon VPC 中的数据库。该公司希望在其 Amazon VPC 中近乎实时地捕获有关进出网络接口的流量信息。该公司希望将这些信息发送到 Amazon OpenSearch Service 进行分析。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 Amazon CloudWatch Logs 中创建一个日志组。配置 VPC Flow Logs 将日志数据发送到该日志组。使用 Amazon Kinesis Data Streams 将日志从该日志组流式传输到 OpenSearch Service。",
      "B": "在 Amazon CloudWatch Logs 中创建一个日志组。配置 VPC Flow Logs 将日志数据发送到该日志组。使用 Amazon Kinesis Data Firehose 将日志从该日志组流式传输到 OpenSearch Service。",
      "C": "在 AWS CloudTrail 中创建一个追踪。配置 VPC Flow Logs 将日志数据发送到该追踪。使用 Amazon Kinesis Data Streams 将日志从该追踪流式传输到 OpenSearch Service。",
      "D": "在 AWS CloudTrail 中创建一个追踪。配置 VPC Flow Logs 将日志数据发送到该追踪。使用 Amazon Kinesis Data Firehose 将日志从该追踪流式传输到 OpenSearch Service。"
    },
    "tags": [
      "VPC Flow Logs",
      "Amazon CloudWatch Logs",
      "Amazon Kinesis Data Firehose",
      "Amazon OpenSearch Service",
      "Network Load Balancer",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 92%），解析仅供参考。】\n\n本题考查 VPC Flow Logs 的配置与日志数据的传输，以及与 Amazon OpenSearch Service 的集成。与 VPC Flow Logs 的数据存储目标、Kinesis Data Streams 和 Kinesis Data Firehose 的选型相关。以及对 CloudWatch Logs 和 CloudTrail 的理解。",
      "why_correct": "选项 B 是最佳解决方案。VPC Flow Logs 产生的日志需要存储到 CloudWatch Logs 中。接着，Kinesis Data Firehose 提供了将数据流式传输到 Amazon OpenSearch Service 的便捷方式，它能够自动处理数据的批处理、转换和错误处理，简化了数据管道的构建和管理。Kinesis Data Firehose 支持直接将 CloudWatch Logs 中的数据作为源，而无需额外的中间组件。",
      "why_wrong": "选项 A 错误，因为从 CloudWatch Logs 中提取数据到 OpenSearch Service，推荐使用 Kinesis Data Firehose，而不是 Kinesis Data Streams。Kinesis Data Streams 需要更复杂的配置和管理，例如数据转换、错误处理等，不如 Kinesis Data Firehose 方便。选项 C 和 D 错误，因为 VPC Flow Logs 无法直接发送到 AWS CloudTrail。CloudTrail 主要用于捕获 AWS API 调用和用户活动，而不是网络流量信息。因此，将 VPC Flow Logs 写入 CloudTrail 是错误的选择，无法满足题目需求。"
    },
    "related_terms": [
      "Amazon CloudWatch Logs",
      "Amazon Kinesis Data Streams",
      "Amazon Kinesis Data Firehose",
      "Amazon OpenSearch Service",
      "Network Load Balancer",
      "Auto Scaling",
      "Amazon EC2",
      "AWS CloudTrail",
      "VPC Flow Logs",
      "Amazon VPC"
    ]
  },
  {
    "id": 677,
    "topic": "1",
    "question_en": "A company is developing an application that will run on a production Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has managed node groups that are provisioned with On-Demand Instances. The company needs a dedicated EKS cluster for development work. The company will use the development cluster infrequently to test the resiliency of the application. The EKS cluster must manage all the nodes. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a managed node group that contains only Spot Instances.",
      "B": "Create two managed node groups. Provision one node group with On-Demand Instances. Provision the second node group with Spot Instances.",
      "C": "Create an Auto Scaling group that has a launch configuration that uses Spot Instances. Configure the user data to add the nodes to the EKS cluster.",
      "D": "Create a managed node group that contains only On-Demand Instances."
    },
    "correct_answer": "A",
    "vote_percentage": "50%",
    "question_cn": "一家公司正在开发一个应用程序，该应用程序将在生产 Amazon Elastic Kubernetes Service (Amazon EKS) 集群上运行。EKS 集群具有使用按需实例配置的托管节点组。该公司需要一个用于开发工作的专用 EKS 集群。该公司将不频繁地使用开发集群来测试应用程序的弹性。EKS 集群必须管理所有节点。哪种解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个仅包含 Spot 实例的托管节点组。",
      "B": "创建两个托管节点组。使用按需实例配置一个节点组。使用 Spot 实例配置第二个节点组。",
      "C": "创建一个具有使用 Spot 实例的启动配置的自动伸缩组。配置用户数据以将节点添加到 EKS 集群。",
      "D": "创建一个仅包含按需实例的托管节点组。"
    },
    "tags": [
      "Amazon EKS",
      "EKS Managed Node Groups",
      "Amazon EC2 Spot Instances",
      "Amazon EC2 On-Demand Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 50%），解析仅供参考。】\n\n本题考查 EKS 集群的成本优化，侧重于 Spot 实例在开发环境中的应用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 提供了最具成本效益的解决方案，通过在托管节点组中使用 Spot 实例。Spot 实例提供了显著的折扣，非常适合不频繁使用的开发集群，且 EKS 能够管理所有节点，满足题目需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 虽然使用了 Spot 实例，但同时也创建了按需实例节点组，增加了不必要的成本，与题干中成本效益的要求相悖。选项 C 涉及使用自动伸缩组和启动配置，增加了管理复杂度，且并非最直接的 EKS 集群节点管理方式。选项 D 使用按需实例，成本最高，不符合成本效益的要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "EKS",
      "Spot Instances",
      "Managed node group",
      "On-Demand Instances",
      "Auto Scaling Group"
    ]
  },
  {
    "id": 678,
    "topic": "1",
    "question_en": "A company stores sensitive data in Amazon S3. A solutions architect needs to create an encryption solution. The company needs to fully control the ability of users to create, rotate, and disable encryption keys with minimal effort for any data that must be encrypted. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use default server-side encryption with Amazon S3 managed encryption keys (SSE-S3) to store the sensitive data.",
      "B": "Create a customer managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "C": "Create an AWS managed key by using AWS Key Management Service (AWS KMS). Use the new key to encrypt the S3 objects by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "D": "Download S3 objects to an Amazon EC2 instance. Encrypt the objects by using customer managed keys. Upload the encrypted objects back into Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司将敏感数据存储在 Amazon S3 中。一位解决方案架构师需要创建一个加密解决方案。该公司需要完全控制用户创建、轮换和禁用加密密钥的能力，并且对任何必须加密的数据的操作要将工作量降到最低。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用带有 Amazon S3 托管加密密钥 (SSE-S3) 的默认服务器端加密来存储敏感数据。",
      "B": "使用 AWS Key Management Service (AWS KMS) 创建客户托管密钥。使用新密钥通过使用带有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密来加密 S3 对象。",
      "C": "使用 AWS Key Management Service (AWS KMS) 创建 AWS 托管密钥。使用新密钥通过使用带有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密来加密 S3 对象。",
      "D": "将 S3 对象下载到 Amazon EC2 实例。使用客户托管密钥加密这些对象。将加密后的对象上传回 Amazon S3。"
    },
    "tags": [
      "Amazon S3",
      "SSE-S3",
      "SSE-KMS",
      "AWS KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查在 S3 上实现加密的方案，包括对密钥的管理、控制和操作的便捷性。",
      "why_correct": "选项 B 提供了对密钥的完全控制，允许用户创建、轮换和禁用密钥。 使用 SSE-KMS 加密 S3 对象，确保了数据在服务器端的加密，并且降低了操作的工作量，满足了题目对加密方案的要求。 使用 KMS 密钥还能追踪密钥的使用情况。",
      "why_wrong": "选项 A 使用 SSE-S3，由 AWS 管理密钥，无法满足公司需要完全控制密钥的要求。选项 C 使用 AWS 托管密钥，虽然使用 SSE-KMS，但密钥也由 AWS 管理，同样无法满足对密钥的完全控制。 选项 D 需要将数据下载到 EC2 实例进行加密，增加了操作复杂度和工作量，不符合题目中降低工作量的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "SSE-S3",
      "AWS Key Management Service (AWS KMS)",
      "SSE-KMS",
      "Amazon EC2",
      "customer managed key",
      "AWS managed key"
    ]
  },
  {
    "id": 679,
    "topic": "1",
    "question_en": "A company wants to back up its on-premises virtual machines (VMs) to AWS. The company's backup solution exports on-premises backups to an Amazon S3 bucket as objects. The S3 backups must be retained for 30 days and must be automatically deleted after 30 days. Which combination of steps will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Create an S3 bucket that has S3 Object Lock enabled.",
      "B": "Create an S3 bucket that has object versioning enabled.",
      "C": "Configure a default retention period of 30 days for the objects.",
      "D": "Configure an S3 Lifecycle policy to protect the objects for 30 days",
      "E": "Configure an S3 Lifecycle policy to expire the objects after 30 days",
      "F": "Configure the backup solution to tag the objects with a 30-day retention period"
    },
    "correct_answer": "ACE",
    "vote_percentage": "69%",
    "question_cn": "一家公司希望将其本地虚拟机 (VM) 备份到 AWS。该公司的备份解决方案将本地备份作为对象导出到 Amazon S3 存储桶。 S3 备份必须保留 30 天，并且必须在 30 天后自动删除。哪三种步骤的组合将满足这些要求？（选择三项。）",
    "options_cn": {
      "A": "创建一个已启用 S3 对象锁定的 S3 存储桶。",
      "B": "创建一个已启用对象版本控制的 S3 存储桶。",
      "C": "为这些对象配置 30 天的默认保留期。",
      "D": "配置一个 S3 生命周期策略以保护对象 30 天。",
      "E": "配置一个 S3 生命周期策略以在 30 天后使对象过期。",
      "F": "将备份解决方案配置为使用 30 天的保留期标记这些对象。"
    },
    "tags": [
      "Amazon S3",
      "S3 Object Lock",
      "S3 Versioning",
      "S3 Lifecycle"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACE（社区 69%），解析仅供参考。】\n\n考察使用 S3 实现备份存储，以及通过 S3 生命周期策略和对象锁定实现数据保留和自动删除的功能。",
      "why_correct": "A: 通过启用 S3 对象锁定功能，可以防止对象被意外删除或覆盖，确保数据的完整性。C: 配置 30 天的默认保留期，为 S3 对象设置了保留时间，满足了题目的要求。E: 通过 S3 生命周期策略，可以配置对象在 30 天后过期，实现自动删除，满足题目的要求。",
      "why_wrong": "B: 对象版本控制主要用于在 S3 桶中保留对象的多个版本，而不是用于实现数据保留和自动删除。D: S3 生命周期策略本身无法保护对象 30 天，而是通过配置规则来实现不同生命周期操作，如过期。F: 虽然可以使用标签进行管理，但 S3 不直接支持基于标签的自动删除或保留机制，需要通过其他方式结合使用，例如对象过期。这种方式不直接满足题目要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 object locking",
      "S3 bucket",
      "S3 lifecycle policy",
      "object versioning",
      "S3 objects"
    ]
  },
  {
    "id": 680,
    "topic": "1",
    "question_en": "A solutions architect needs to copy files from an Amazon S3 bucket to an Amazon Elastic File System (Amazon EFS) file system and another S3 bucket. The files must be copied continuously. New files are added to the original S3 bucket consistently. The copied files should be overwritten only if the source file changes. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system. Create a task for the destination S3 bucket and the EFS file system. Set the transfer mode to transfer only data that has changed.",
      "B": "Create an AWS Lambda function. Mount the file system to the function. Set up an S3 event notification to invoke the function when files are created and changed in Amazon S3. Configure the function to copy files to the file system and the destination S3 bucket.",
      "C": "Create an AWS DataSync location for both the destination S3 bucket and the EFS file system. Create a task for the destination S3 bucket and the EFS file system. Set the transfer mode to transfer all data.",
      "D": "Launch an Amazon EC2 instance in the same VPC as the file system. Mount the file system. Create a script to routinely synchronize all objects that changed in the origin S3 bucket to the destination S3 bucket and the mounted file system."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一名解决方案架构师需要将文件从一个 Amazon S3 存储桶复制到 Amazon EFS 文件系统和另一个 S3 存储桶。必须连续复制文件。新文件会持续添加到源 S3 存储桶。仅当 源 文件更改时才应覆盖已复制的文件。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "为目标 S3 存储桶和 EFS 文件系统创建 AWS DataSync 位置。为目标 S3 存储桶和 EFS 文件系统创建任务。将传输模式设置为仅传输已更改的数据。",
      "B": "创建一个 AWS Lambda 函数。将文件系统挂载到该函数。设置一个 S3 事件通知，以便在 Amazon S3 中创建和更改文件时调用该函数。配置该函数以将文件复制到文件系统和目标 S3 存储桶。",
      "C": "为目标 S3 存储桶和 EFS 文件系统创建 AWS DataSync 位置。为目标 S3 存储桶和 EFS 文件系统创建任务。将传输模式设置为传输所有数据。",
      "D": "在与文件系统相同的 VPC 中启动一个 Amazon EC2 实例。挂载文件系统。创建一个脚本，以定期同步源 S3 存储桶中更改的所有对象到目标 S3 存储桶和已挂载的文件系统。"
    },
    "tags": [
      "Amazon S3",
      "Amazon EFS",
      "Amazon EC2",
      "DataSync",
      "AWS Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查利用 AWS DataSync 实现文件同步，并结合传输模式、增量同步满足文件复制的持续性和覆盖需求。",
      "why_correct": "选项 A 使用 AWS DataSync 提供了最有效的解决方案。DataSync 支持增量复制，仅传输已更改的数据，满足了仅当源文件更改时才覆盖已复制文件的需求。同时，DataSync 支持将数据传输到 S3 存储桶和 EFS 文件系统，符合题目要求，并且运营开销最小。",
      "why_wrong": "选项 B 涉及 Lambda 函数，虽然可以实现文件复制，但需要自行编写代码处理 S3 事件、文件复制逻辑，运营开销较高，且无法保证数据一致性。选项 C 错误在于将传输模式设置为传输所有数据，这会导致每次都复制所有文件，不符合仅当源文件更改时才覆盖的要求，且效率低下。选项 D 使用 EC2 实例和自定义脚本，需要手动管理 EC2 实例和脚本，运营开销大，且脚本需要考虑同步逻辑和错误处理，复杂性高，不如 DataSync 自动化。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon EFS",
      "AWS DataSync",
      "AWS Lambda",
      "VPC",
      "Amazon EC2"
    ]
  },
  {
    "id": 681,
    "topic": "1",
    "question_en": "A company uses Amazon EC2 instances and stores data on Amazon Elastic Block Store (Amazon EBS) volumes. The company must ensure that all data is encrypted at rest by using AWS Key Management Service (AWS KMS). The company must be able to control rotation of the encryption keys. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a customer managed key. Use the key to encrypt the EBS volumes.",
      "B": "Use an AWS managed key to encrypt the EBS volumes. Use the key to configure automatic key rotation.",
      "C": "Create an external KMS key with imported key material. Use the key to encrypt the EBS volumes.",
      "D": "Use an AWS owned key to encrypt the EBS volumes."
    },
    "correct_answer": "A",
    "vote_percentage": "94%",
    "question_cn": "题目 1\n题干: 一家公司使用 Amazon EC2 实例并将数据存储在 Amazon Elastic Block Store (Amazon EBS) 卷上。该公司必须确保所有静态数据通过使用 AWS Key Management Service (AWS KMS) 进行加密。该公司必须能够控制加密密钥的轮换。哪种解决方案将以最低的运营开销满足这些要求？\n选项:\n   A. 创建一个客户托管密钥。使用该密钥加密 EBS 卷。\n   B. 使用 AWS 托管密钥加密 EBS 卷。使用该密钥配置自动密钥轮换。\n   C. 使用导入的密钥材料创建一个外部 KMS 密钥。使用该密钥加密 EBS 卷。\n   D. 使用 AWS 拥有的密钥加密 EBS 卷。",
    "options_cn": {
      "A": "创建一个客户托管密钥。使用该密钥加密 EBS 卷。",
      "B": "使用 AWS 托管密钥加密 EBS 卷。使用该密钥配置自动密钥轮换。",
      "C": "使用导入的密钥材料创建一个外部 KMS 密钥。使用该密钥加密 EBS 卷。",
      "D": "使用 AWS 拥有的密钥加密 EBS 卷。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS KMS",
      "KMS Key Rotation",
      "Customer managed keys",
      "AWS managed keys",
      "AWS owned keys"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 94%），解析仅供参考。】\n\n考察使用 AWS KMS 对 EBS 卷进行加密，并满足密钥轮换和运营开销最小化的要求。",
      "why_correct": "选项 A 提供了最低的运营开销，同时满足了加密和密钥轮换的要求。通过创建客户托管密钥 (Customer managed key)，公司可以完全控制密钥的生命周期，包括轮换。EBS 卷使用客户托管密钥进行加密，满足了静态数据加密的需求。",
      "why_wrong": "选项 B 错误，虽然 AWS 托管密钥 (AWS managed key) 提供了自动密钥轮换，但公司无法完全控制密钥的生命周期，且不满足题干中对密钥管理的控制要求。选项 C 错误，导入的密钥材料 (External KMS key) 需要额外的配置和管理开销，增加了运营复杂性。选项 D 错误，AWS 拥有的密钥 (AWS owned key) 公司无法控制其密钥轮换，不满足题干要求。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS Key Management Service",
      "AWS KMS",
      "EBS volume",
      "Customer managed key",
      "AWS managed key",
      "External KMS key",
      "AWS owned key"
    ]
  },
  {
    "id": 682,
    "topic": "1",
    "question_en": "A company needs a solution to enforce data encryption at rest on Amazon EC2 instances. The solution must automatically identify noncompliant resources and enforce compliance policies on findings. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Use an IAM policy that allows users to create only encrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Config and AWS Systems Manager to automate the detection and remediation of unencrypted EBS volumes.",
      "B": "Use AWS Key Management Service (AWS KMS) to manage access to encrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Lambda and Amazon EventBridge to automate the detection and remediation of unencrypted EBS volumes.",
      "C": "Use Amazon Macie to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes.",
      "D": "Use Amazon inspector to detect unencrypted Amazon Elastic Block Store (Amazon EBS) volumes. Use AWS Systems Manager Automation rules to automatically encrypt existing and new EBS volumes."
    },
    "correct_answer": "A",
    "vote_percentage": "94%",
    "question_cn": "一家公司需要一个解决方案，以强制对 Amazon EC2 实例上的静态数据进行加密。该解决方案必须自动识别不合规的资源，并对发现结果执行合规策略。哪个解决方案将以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "使用 IAM 策略，该策略允许用户仅创建已加密的 Amazon Elastic Block Store (Amazon EBS) 卷。使用 AWS Config 和 AWS Systems Manager 自动化检测和修复未加密的 EBS 卷。",
      "B": "使用 AWS Key Management Service (AWS KMS) 管理对已加密的 Amazon Elastic Block Store (Amazon EBS) 卷的访问。使用 AWS Lambda 和 Amazon EventBridge 自动化检测和修复未加密的 EBS 卷。",
      "C": "使用 Amazon Macie 检测未加密的 Amazon Elastic Block Store (Amazon EBS) 卷。使用 AWS Systems Manager Automation 规则自动加密现有和新的 EBS 卷。",
      "D": "使用 Amazon Inspector 检测未加密的 Amazon Elastic Block Store (Amazon EBS) 卷。使用 AWS Systems Manager Automation 规则自动加密现有和新的 EBS 卷。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon EBS",
      "AWS KMS",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Config",
      "AWS Systems Manager",
      "Amazon Macie",
      "Amazon Inspector",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 94%），解析仅供参考。】\n\n考察使用 AWS Config 和 AWS Systems Manager 自动化管理 EBS 卷加密的解决方案，以满足静态数据加密需求和合规性策略。",
      "why_correct": "IAM 策略可以限制用户仅创建加密的 EBS 卷，从源头上确保数据加密。AWS Config 可以持续监控 EBS 卷的加密状态，并识别不合规的资源。AWS Systems Manager 自动化提供了一个自动化修复框架，可以对未加密的 EBS 卷执行合规策略，例如加密它们，从而以最小的管理开销满足要求。",
      "why_wrong": "B 选项使用 AWS KMS 管理访问权限，这侧重于加密密钥的管理而非检测和修复未加密资源。使用 Lambda 和 EventBridge 检测 EBS 卷加密状态，增加了额外的复杂性和管理成本，不如 AWS Config 方便。C 选项使用 Amazon Macie 检测数据安全合规问题，而本题主要关注 EBS 卷的加密状态，Amazon Macie 并不是最佳选择。D 选项使用 Amazon Inspector 检测安全漏洞，但它不是设计用来检测 EBS 卷加密状态的。虽然 AWS Systems Manager Automation 可以用于加密卷，但是用 Amazon Inspector 来检测 EBS 卷加密状态，不如 AWS Config 契合需求。"
    },
    "related_terms": [
      "IAM",
      "Amazon EC2",
      "Amazon EBS",
      "AWS Config",
      "AWS Systems Manager",
      "AWS KMS",
      "AWS Lambda",
      "Amazon EventBridge",
      "Amazon Macie",
      "Amazon Inspector"
    ]
  },
  {
    "id": 683,
    "topic": "1",
    "question_en": "A company is migrating its multi-tier on-premises application to AWS. The application consists of a single-node MySQL database and a multi- node web tier. The company must minimize changes to the application during the migration. The company wants to improve application resiliency after the migration. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Migrate the web tier to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
      "B": "Migrate the database to Amazon EC2 instances in an Auto Scaling group behind a Network Load Balancer.",
      "C": "Migrate the database to an Amazon RDS Multi-AZ deployment.",
      "D": "Migrate the web tier to an AWS Lambda function",
      "E": "Migrate the database to an Amazon DynamoDB table."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "题目 2\n题干: 一家公司正在将其多层本地应用程序迁移到 AWS。该应用程序包含一个单节点 MySQL 数据库和一个多节点 Web 层。该公司必须最大限度地减少迁移期间对应用程序的更改。该公司希望在迁移后提高应用程序的弹性。哪些步骤的组合将满足这些要求？（选择两个。）\n选项:\n   A. 将 Web 层迁移到位于 Application Load Balancer 后面的 Auto Scaling 组中的 Amazon EC2 实例。\n   B. 将数据库迁移到位于 Network Load Balancer 后面的 Auto Scaling 组中的 Amazon EC2 实例。\n   C. 将数据库迁移到 Amazon RDS Multi-AZ 部署。\n   D. 将 Web 层迁移到 AWS Lambda 函数。\nE. 将数据库迁移到 Amazon DynamoDB 表。",
    "options_cn": {
      "A": "将 Web 层迁移到位于 Application Load Balancer 后面的 Auto Scaling 组中的 Amazon EC2 实例。",
      "B": "将数据库迁移到位于 Network Load Balancer 后面的 Auto Scaling 组中的 Amazon EC2 实例。",
      "C": "将数据库迁移到 Amazon RDS Multi-AZ 部署。",
      "D": "将 Web 层迁移到 AWS Lambda 函数。",
      "E": "将数据库迁移到 Amazon DynamoDB 表。"
    },
    "tags": [
      "Amazon RDS",
      "Multi-AZ",
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Network Load Balancer",
      "AWS Lambda",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考察将本地应用程序迁移到 AWS 的方案，包括如何减少对应用程序的更改以及提高应用程序的弹性。",
      "why_correct": "A 选项通过将 Web 层迁移到 Application Load Balancer 后面的 Auto Scaling 组中的 EC2 实例，可以实现负载均衡和自动伸缩，提高弹性。C 选项将数据库迁移到 RDS Multi-AZ 部署，提供了高可用性和故障转移能力，满足了弹性需求，并且最大程度减少了对数据库的更改。",
      "why_wrong": "B 选项将数据库迁移到 EC2 实例，虽然可以使用 Auto Scaling，但不如 RDS Multi-AZ 部署方便，并且需要管理数据库的维护和高可用性，与题目目标不符。D 选项将 Web 层迁移到 Lambda 函数，需要重写应用程序，不符合最小化更改的要求。E 选项将数据库迁移到 DynamoDB，需要对应用程序进行大量更改，不满足最小化更改的需求。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon RDS",
      "Multi-AZ",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Network Load Balancer"
    ]
  },
  {
    "id": 684,
    "topic": "1",
    "question_en": "A company wants to migrate its web applications from on premises to AWS. The company is located close to the eu-central-1 Region. Because of regulations, the company cannot launch some of its applications in eu-central-1. The company wants to achieve single-digit millisecond latency. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy the applications in eu-central-1. Extend the company’s VPC from eu-central-1 to an edge location in Amazon CloudFront.",
      "B": "Deploy the applications in AWS Local Zones by extending the company's VPC from eu-central-1 to the chosen Local Zone.",
      "C": "Deploy the applications in eu-central-1. Extend the company’s VPC from eu-central-1 to the regional edge caches in Amazon CloudFront.",
      "D": "Deploy the applications in AWS Wavelength Zones by extending the company’s VPC from eu-central-1 to the chosen Wavelength Zone."
    },
    "correct_answer": "B",
    "vote_percentage": "72%",
    "question_cn": "题目 3\n题干: 一家公司希望将其 Web 应用程序从本地迁移到 AWS。该公司位于 eu-central-1 区域附近。由于法规原因，该公司无法在 eu-central-1 中启动其部分应用程序。该公司希望实现个位数的毫秒级延迟。哪种解决方案将满足这些要求？\n选项:\n   A. 在 eu-central-1 中部署应用程序。将公司的 VPC 从 eu-central-1 扩展到 Amazon CloudFront 中的边缘站点。\n   B. 通过将公司的 VPC 从 eu-central-1 扩展到所选的 Local Zone，在 AWS Local Zones 中部署应用程序。\n   C. 在 eu-central-1 中部署应用程序。将公司的 VPC 从 eu-central-1 扩展到 Amazon CloudFront 中的区域边缘缓存。\n   D. 通过将公司的 VPC 从 eu-central-1 扩展到所选的 Wavelength Zone，在 AWS Wavelength Zones 中部署应用程序。",
    "options_cn": {
      "A": "在 eu-central-1 中部署应用程序。将公司的 VPC 从 eu-central-1 扩展到 Amazon CloudFront 中的边缘站点。",
      "B": "通过将公司的 VPC 从 eu-central-1 扩展到所选的 Local Zone，在 AWS Local Zones 中部署应用程序。",
      "C": "在 eu-central-1 中部署应用程序。将公司的 VPC 从 eu-central-1 扩展到 Amazon CloudFront 中的区域边缘缓存。",
      "D": "通过将公司的 VPC 从 eu-central-1 扩展到所选的 Wavelength Zone，在 AWS Wavelength Zones 中部署应用程序。"
    },
    "tags": [
      "AWS Local Zones",
      "Amazon CloudFront",
      "VPC",
      "AWS Wavelength Zones"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 72%），解析仅供参考。】\n\n考查在满足法规限制和低延迟需求下，选择合适的 AWS 基础设施服务。这涉及到对 Local Zones、CloudFront 和 Wavelength Zones 的理解，以及它们与 VPC 的关联和适用场景的对比。",
      "why_correct": "AWS Local Zones 允许将计算、存储、数据库和其他 AWS 服务放置在靠近人口中心或行业的地理位置，以满足个位数的毫秒级延迟需求。通过扩展 VPC 到 Local Zone，应用程序可以部署在更靠近最终用户的位置，从而减少延迟。由于题干说明了法规要求无法在 eu-central-1 启动部分应用程序，Local Zones 提供了替代方案，可以在其他区域中部署应用程序。",
      "why_wrong": {
        "A": "将应用程序部署在 eu-central-1，再扩展到 CloudFront 边缘站点，虽然 CloudFront 可以在边缘缓存内容，但无法直接满足应用程序部署的要求。CloudFront 主要用于内容分发，而非应用程序的完整部署。并且这无法解决题干中对于应用程序无法在 eu-central-1 中部署的需求。",
        "C": "将应用程序部署在 eu-central-1，再扩展到 CloudFront 的区域边缘缓存，与 A 类似，CloudFront 区域边缘缓存无法直接解决应用程序部署的要求，同样无法满足法规限制。",
        "D": "AWS Wavelength Zones 主要用于将应用程序部署在 5G 网络边缘，以满足超低延迟的需求。虽然 Wavelength Zones 也提供了低延迟，但题目中并未提及 5G 网络，并且也需要考虑是否可用。此外，考虑到法规限制，Local Zones 提供了更通用的解决方案，可以部署到多个区域而不仅仅是 5G 边缘。"
      }
    },
    "related_terms": [
      "Amazon CloudFront",
      "VPC",
      "EC2",
      "EBS",
      "eu-central-1",
      "AWS Local Zones",
      "AWS Wavelength Zones"
    ]
  },
  {
    "id": 685,
    "topic": "1",
    "question_en": "A company’s ecommerce website has unpredictable trafic and uses AWS Lambda functions to directly access a private Amazon RDS for PostgreSQL DB instance. The company wants to maintain predictable database performance and ensure that the Lambda invocations do not overload the database with too many connections. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions inside a VPC.",
      "B": "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions inside a VPC.",
      "C": "Point the client driver at an RDS custom endpoint. Deploy the Lambda functions outside a VPC.",
      "D": "Point the client driver at an RDS proxy endpoint. Deploy the Lambda functions outside a VPC."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "题目 4\n题干: 一家公司的电子商务网站具有不可预测的流量，并使用 AWS Lambda 函数直接访问私有 Amazon RDS for PostgreSQL 数据库实例。该公司希望保持可预测的数据库性能，并确保 Lambda 调用不会使用过多的连接使数据库过载。解决方案架构师应如何满足这些要求？\n选项:\n   A. 将客户端驱动程序指向 RDS 自定义终端节点。在 VPC 内部部署 Lambda 函数。\n   B. 将客户端驱动程序指向 RDS 代理终端节点。在 VPC 内部部署 Lambda 函数。\n   C. 将客户端驱动程序指向 RDS 自定义终端节点。在 VPC 外部部署 Lambda 函数。\n   D. 将客户端驱动程序指向 RDS 代理终端节点。在 VPC 外部部署 Lambda 函数。",
    "options_cn": {
      "A": "将客户端驱动程序指向 RDS 自定义终端节点。在 VPC 内部部署 Lambda 函数。",
      "B": "将客户端驱动程序指向 RDS 代理终端节点。在 VPC 内部部署 Lambda 函数。",
      "C": "将客户端驱动程序指向 RDS 自定义终端节点。在 VPC 外部部署 Lambda 函数。",
      "D": "将客户端驱动程序指向 RDS 代理终端节点。在 VPC 外部部署 Lambda 函数。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "AWS Lambda",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查 RDS Proxy 的应用场景与 Lambda 函数的 VPC 配置。与数据库连接管理、高可用性、性能优化相关。",
      "why_correct": "RDS Proxy 提供了针对数据库连接的连接池和多路复用功能，可以有效减少 Lambda 函数调用对数据库连接的压力。当 Lambda 函数在 VPC 内部署时，可以安全地访问 RDS for PostgreSQL 数据库实例。RDS Proxy 在 VPC 内部署，Lambda 函数也需要在 VPC 内部署才能正常访问。",
      "why_wrong": "A 选项，RDS 自定义终端节点不提供连接池功能，无法解决数据库连接过载的问题。C 选项，将 Lambda 函数部署在 VPC 外部，无法直接访问 VPC 内部的 RDS 数据库，因此连接将失败。D 选项，即使使用 RDS Proxy，但 Lambda 函数在 VPC 外部署时，也无法访问 VPC 内部的 RDS 数据库，会产生连接问题，与题干要求不符。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS Proxy",
      "Lambda",
      "VPC",
      "RDS for PostgreSQL"
    ]
  },
  {
    "id": 686,
    "topic": "1",
    "question_en": "A company is creating an application. The company stores data from tests of the application in multiple on-premises locations. The company needs to connect the on-premises locations to VPCs in an AWS Region in the AWS Cloud. The number of accounts and VPCs will increase during the next year. The network architecture must simplify the administration of new connections and must provide the ability to scale. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Create a peering connection between the VPCs. Create a VPN connection between the VPCs and the on-premises locations.",
      "B": "Launch an Amazon EC2 instance. On the instance, include VPN software that uses a VPN connection to connect all VPCs and on- premises locations.",
      "C": "Create a transit gateway. Create VPC attachments for the VPC connections. Create VPN attachments for the on-premises connections.",
      "D": "Create an AWS Direct Connect connection between the on-premises locations and a central VPC. Connect the central VPC to other VPCs by using peering connections."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "题目 5\n题干: 一家公司正在创建一个应用程序。该公司从应用程序测试中存储来自多个本地位置的数据。该公司需要将本地位置连接到 AWS 区域中的 VPC。帐户和 VPC 的数量将在明年增加。网络架构必须简化新连接的管理，并提供扩展能力。哪种解决方案将以最低的管理开销满足这些要求？\n选项:\n   A. 在 VPC 之间创建对等连接。在 VPC 和本地位置之间创建 VPN 连接。\n   B. 启动 Amazon EC2 实例。在该实例上，包括使用 VPN 连接连接所有 VPC 和本地位置的 VPN 软件。\n   C. 创建一个 transit gateway。为 VPC 连接创建 VPC 附件。为本地连接创建 VPN 附件。\n   D. 在本地位置和中央 VPC 之间创建 AWS Direct Connect 连接。使用对等连接将中央 VPC 连接到其他 VPC。",
    "options_cn": {
      "A": "在 VPC 之间创建对等连接。在 VPC 和本地位置之间创建 VPN 连接。",
      "B": "启动 Amazon EC2 实例。在该实例上，包括使用 VPN 连接连接所有 VPC 和本地位置的 VPN 软件。",
      "C": "创建一个 transit gateway。为 VPC 连接创建 VPC 附件。为本地连接创建 VPN 附件。",
      "D": "在本地位置和中央 VPC 之间创建 AWS Direct Connect 连接。使用对等连接将中央 VPC 连接到其他 VPC。"
    },
    "tags": [
      "VPC",
      "VPN",
      "Direct Connect",
      "Transit Gateway",
      "VPC Peering",
      "Network Connectivity"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查利用 AWS Transit Gateway 简化跨 VPC 和本地网络连接，并实现可扩展性的方案。",
      "why_correct": "Transit Gateway 是一种网络传输中心，可以简化和管理 VPC 之间的连接以及与本地网络的连接。通过 Transit Gateway，VPC 和本地位置只需要连接到 Transit Gateway，而不是两两互连。这种架构易于扩展，并且管理开销低。",
      "why_wrong": "选项 A 和 D 都涉及使用对等连接，随着 VPC 数量的增加，对等连接的管理复杂度会迅速增加，不满足题目对扩展性的需求。选项 B 涉及在 EC2 实例上运行 VPN 软件，这会引入管理和维护成本，并且容易成为单点故障，也增加了管理开销。选项 D 使用了 Direct Connect，虽然适合高性能连接，但增加了本地端配置复杂性，并且价格较高，也不符合题干中“最低的管理开销”要求。"
    },
    "related_terms": [
      "VPC",
      "VPN",
      "Amazon EC2",
      "Transit Gateway",
      "VPC attachments",
      "VPN attachments",
      "AWS Direct Connect"
    ]
  },
  {
    "id": 687,
    "topic": "1",
    "question_en": "A company that uses AWS needs a solution to predict the resources needed for manufacturing processes each month. The solution must use historical values that are currently stored in an Amazon S3 bucket. The company has no machine learning (ML) experience and wants to use a managed service for the training and predictions. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Deploy an Amazon SageMaker model. Create a SageMaker endpoint for inference.",
      "B": "Use Amazon SageMaker to train a model by using the historical data in the S3 bucket.",
      "C": "Configure an AWS Lambda function with a function URL that uses Amazon SageMaker endpoints to create predictions based on the inputs.",
      "D": "Configure an AWS Lambda function with a function URL that uses an Amazon Forecast predictor to create a prediction based on the inputs",
      "E": "Train an Amazon Forsecast predictor by using the historical data in the S3 bucket."
    },
    "correct_answer": "DE",
    "vote_percentage": "63%",
    "question_cn": "题目 6\n题干: 一家使用 AWS 的公司需要一个解决方案来预测每个月制造过程中所需的资源。该解决方案必须使用当前存储在 Amazon S3 存储桶中的历史值。该公司没有机器学习 (ML) 经验，并且希望使用托管服务进行训练和预测。哪些步骤的组合将满足这些要求？（选择两个。）\n选项:\n   A. 部署 Amazon SageMaker 模型。创建一个用于推断的 SageMaker 终端节点。\n   B. 使用 Amazon SageMaker 通过使用 S3 存储桶中的历史数据来训练模型。\n   C. 配置一个带有函数 URL 的 AWS Lambda 函数，该函数使用 Amazon SageMaker 终端节点根据输入创建预测。\n   D. 配置一个带有函数 URL 的 AWS Lambda 函数，该函数使用 Amazon Forecast 预测器根据输入创建预测。\nE. 使用 S3 存储桶中的历史数据来训练 Amazon Forsecast 预测器。",
    "options_cn": {
      "A": "部署 Amazon SageMaker 模型。创建一个用于推断的 SageMaker 终端节点。",
      "B": "使用 Amazon SageMaker 通过使用 S3 存储桶中的历史数据来训练模型。",
      "C": "配置一个带有函数 URL 的 AWS Lambda 函数，该函数使用 Amazon SageMaker 终端节点根据输入创建预测。",
      "D": "配置一个带有函数 URL 的 AWS Lambda 函数，该函数使用 Amazon Forecast 预测器根据输入创建预测。",
      "E": "使用 S3 存储桶中的历史数据来训练 Amazon Forsecast 预测器。"
    },
    "tags": [
      "Amazon S3",
      "Amazon SageMaker",
      "AWS Lambda",
      "Amazon Forecast"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 63%），解析仅供参考。】\n\n考察使用托管服务进行时间序列预测，并从 S3 存储桶中获取历史数据。同时，还考察了 Lambda 函数的配置和预测器的选择。",
      "why_correct": "选项 D 和 E 共同构成了解决问题的完整流程。选项 E 使用 S3 存储桶中的历史数据训练 Amazon Forecast 预测器。选项 D 配置了一个 Lambda 函数，该函数使用 Amazon Forecast 预测器根据输入创建预测，满足了托管服务、历史数据和预测的需求。",
      "why_wrong": "选项 A 错误，因为仅部署 SageMaker 模型和创建终端节点无法满足训练需求，并且不适合缺乏 ML 经验的公司。选项 B 错误，虽然 SageMaker 可以训练模型，但题目要求预测功能，B 缺少预测环节。选项 C 错误，虽然可以使用 Lambda 调用 SageMaker 终端节点，但题目要求使用托管服务进行预测，并且题干明确指出公司缺乏 ML 经验，使用 SageMaker 模型需要较多的人工配置。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon SageMaker",
      "AWS Lambda",
      "Amazon Forecast",
      "S3",
      "SageMaker",
      "Lambda",
      "Forecast",
      "SageMaker endpoint",
      "Forecast predictor",
      "function URL"
    ]
  },
  {
    "id": 688,
    "topic": "1",
    "question_en": "A company manages AWS accounts in AWS Organizations. AWS IAM Identity Center (AWS Single Sign-On) and AWS Control Tower are configured for the accounts. The company wants to manage multiple user permissions across all the accounts. The permissions will be used by multiple IAM users and must be split between the developer and administrator teams. Each team requires different permissions. The company wants a solution that includes new users that are hired on both teams. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create individual users in IAM Identity Center for each account. Create separate developer and administrator groups in IAM Identity Center. Assign the users to the appropriate groups. Create a custom IAM policy for each group to set fine-grained permissions.",
      "B": "Create individual users in IAM Identity Center for each account. Create separate developer and administrator groups in IAM Identity Center. Assign the users to the appropriate groups. Attach AWS managed IAM policies to each user as needed for fine-grained permissions.",
      "C": "Create individual users in IAM Identity Center. Create new developer and administrator groups in IAM Identity Center. Create new permission sets that include the appropriate IAM policies for each group. Assign the new groups to the appropriate accounts. Assign the new permission sets to the new groups. When new users are hired, add them to the appropriate group.",
      "D": "Create individual users in IAM Identity Center. Create new permission sets that include the appropriate IAM policies for each user. Assign the users to the appropriate accounts. Grant additional IAM permissions to the users from within specific accounts. When new users are hired, add them to IAM Identity Center and assign them to the accounts."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS Organizations 中管理 AWS 账户。这些账户已配置 AWS IAM Identity Center (AWS Single Sign-On) 和 AWS Control Tower。该公司希望跨所有账户管理多个用户权限。这些权限将由多个 IAM 用户使用，并且必须在开发人员和管理员团队之间划分。每个团队需要不同的权限。该公司希望一个解决方案，其中包括两支队伍中招聘的新用户。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在每个账户的 IAM Identity Center 中创建单个用户。在 IAM Identity Center 中创建单独的开发人员和管理员组。将用户分配到相应的组。为每个组创建一个自定义 IAM 策略以设置细粒度权限。",
      "B": "在每个账户的 IAM Identity Center 中创建单个用户。在 IAM Identity Center 中创建单独的开发人员和管理员组。将用户分配到相应的组。根据需要将 AWS 托管 IAM 策略附加到每个用户以实现细粒度权限。",
      "C": "在 IAM Identity Center 中创建单个用户。在 IAM Identity Center 中创建新的开发人员和管理员组。创建新的权限集，其中包括每个组的相应 IAM 策略。将新组分配给相应的账户。将新的权限集分配给新组。当招聘新用户时，将他们添加到相应的组。",
      "D": "在 IAM Identity Center 中创建单个用户。创建新的权限集，其中包括每个用户的相应 IAM 策略。将用户分配给相应的账户。从特定账户内向用户授予额外的 IAM 权限。当招聘新用户时，将他们添加到 IAM Identity Center 并将他们分配给账户。"
    },
    "tags": [
      "IAM Identity Center",
      "IAM",
      "AWS Organizations",
      "AWS Control Tower"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查如何在 AWS Organizations 和 AWS IAM Identity Center (AWS SSO) 中，使用最小的运营开销，跨多个账户管理用户权限。",
      "why_correct": "选项 C 提供了最高效的解决方案。它利用 IAM Identity Center 的组和权限集，集中管理用户权限，并简化了权限的分配过程。通过创建开发人员和管理员组，以及针对每个组定制的权限集，可以方便地为新用户分配正确的权限，从而满足了题目的要求，且具有最佳的可扩展性。",
      "why_wrong": "选项 A 的问题在于需要在每个账户中创建用户，这增加了管理开销。选项 B 虽然使用组，但依赖于将 AWS 托管策略附加到用户，这难以实现细粒度控制。选项 D 没有使用组，而是直接管理用户和权限集，这使得权限管理变得复杂，并且不利于批量用户管理。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS IAM Identity Center (AWS SSO)",
      "IAM",
      "IAM Policy",
      "IAM User",
      "AWS Control Tower",
      "Permissions set",
      "Account"
    ]
  },
  {
    "id": 689,
    "topic": "1",
    "question_en": "A company wants to standardize its Amazon Elastic Block Store (Amazon EBS) volume encryption strategy. The company also wants to minimize the cost and configuration effort required to operate the volume encryption check. Which solution will meet these requirements?",
    "options_en": {
      "A": "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Use Amazon EventBridge to schedule an AWS Lambda function to run the API calls.",
      "B": "Write API calls to describe the EBS volumes and to confirm the EBS volumes are encrypted. Run the API calls on an AWS Fargate task.",
      "C": "Create an AWS Identity and Access Management (IAM) policy that requires the use of tags on EBS volumes. Use AWS Cost Explorer to display resources that are not properly tagged. Encrypt the untagged resources manually.",
      "D": "Create an AWS Config rule for Amazon EBS to evaluate if a volume is encrypted and to fiag the volume if it is not encrypted."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "题目 7\n题干: 一家公司希望标准化其 Amazon Elastic Block Store (Amazon EBS) 卷加密策略。该公司还希望最大限度地减少操作卷加密检查所需的成本和配置工作量。哪种解决方案将满足这些要求？\n选项:\n   A. 编写 API 调用来描述 EBS 卷，并确认 EBS 卷已加密。使用 Amazon EventBridge 计划一个 AWS Lambda 函数来运行 API 调用。\n   B. 编写 API 调用来描述 EBS 卷，并确认 EBS 卷已加密。在 AWS Fargate 任务上运行 API 调用。\n   C. 创建一个 AWS Identity and Access Management (IAM) 策略，该策略要求在 EBS 卷上使用标签。使用 AWS Cost Explorer 显示未正确标记的资源。手动加密未标记的资源。\n   D. 为 Amazon EBS 创建一个 AWS Config 规则，以评估卷是否已加密，并在未加密时标记卷。",
    "options_cn": {
      "A": "编写 API 调用来描述 EBS 卷，并确认 EBS 卷已加密。使用 Amazon EventBridge 计划一个 AWS Lambda 函数来运行 API 调用。",
      "B": "编写 API 调用来描述 EBS 卷，并确认 EBS 卷已加密。在 AWS Fargate 任务上运行 API 调用。",
      "C": "创建一个 AWS Identity and Access Management (IAM) 策略，该策略要求在 EBS 卷上使用标签。使用 AWS Cost Explorer 显示未正确标记的资源。手动加密未标记的资源。",
      "D": "为 Amazon EBS 创建一个 AWS Config 规则，以评估卷是否已加密，并在未加密时标记卷。"
    },
    "tags": [
      "Amazon EBS",
      "IAM",
      "AWS Cost Explorer",
      "AWS Config",
      "Amazon EventBridge",
      "Lambda",
      "AWS Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察使用 AWS Config 规则来自动化 EBS 卷加密策略的合规性检查。",
      "why_correct": "AWS Config 允许创建规则来评估 EBS 卷的加密状态。当卷未加密时，Config 可以标记这些卷，从而实现自动化检测。这满足了题目中标准化、最小化成本和配置工作量的要求。",
      "why_wrong": "A 和 B 方案都需要编写自定义 API 调用和额外的计算资源（Lambda 和 Fargate）来检查加密状态，增加了复杂度和成本，不符合最小化成本的要求。C 方案侧重于标签和成本分析，并没有直接解决加密合规性的问题，并且需要手动加密，操作不够自动化。"
    },
    "related_terms": [
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS Lambda",
      "AWS Fargate",
      "AWS Identity and Access Management (IAM)",
      "AWS Cost Explorer",
      "AWS Config"
    ]
  },
  {
    "id": 690,
    "topic": "1",
    "question_en": "A company regularly uploads GB-sized files to Amazon S3. After the company uploads the files, the company uses a fieet of Amazon EC2 Spot Instances to transcode the file format. The company needs to scale throughput when the company uploads data from the on-premises data center to Amazon S3 and when the company downloads data from Amazon S3 to the EC2 instances. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use the S3 bucket access point instead of accessing the S3 bucket directly.",
      "B": "Upload the files into multiple S3 buckets.",
      "C": "Use S3 multipart uploads.",
      "D": "Fetch multiple byte-ranges of an object in parallel",
      "E": "Add a random prefix to each object when uploading the files."
    },
    "correct_answer": "CD",
    "vote_percentage": "100%",
    "question_cn": "题目 8\n题干: 一家公司定期将 GB 大小的文件上传到 Amazon S3。在公司上传文件后，该公司使用一组 Amazon EC2 Spot 实例来转码文件格式。当公司将数据从本地数据中心上传到 Amazon S3 以及当公司将数据从 Amazon S3 下载到 EC2 实例时，该公司需要扩展吞吐量。哪些解决方案将满足这些要求？（选择两个。）\n选项:\n   A. 使用 S3 存储桶访问点，而不是直接访问 S3 存储桶。\n   B. 将文件上传到多个 S3 存储桶。\n   C. 使用 S3 多部分上传。\n   D. 并行获取对象的多个字节范围。\nE. 在上传文件时，为每个对象添加一个随机前缀。",
    "options_cn": {
      "A": "使用 S3 存储桶访问点，而不是直接访问 S3 存储桶。",
      "B": "将文件上传到多个 S3 存储桶。",
      "C": "使用 S3 多部分上传。",
      "D": "并行获取对象的多个字节范围。",
      "E": "在上传文件时，为每个对象添加一个随机前缀。"
    },
    "tags": [
      "S3",
      "EC2",
      "Spot Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 100%），解析仅供参考。】\n\n考察如何通过优化 S3 上传和下载，提高吞吐量，满足大规模文件处理的需求。",
      "why_correct": "C 选项：S3 多部分上传 (Multipart Upload) 允许将大文件分成多个部分并行上传，从而提高上传速度和可靠性。D 选项：通过并行获取对象的多个字节范围，EC2 实例可以并行下载 S3 对象的不同部分，加快下载速度。",
      "why_wrong": "A 选项：S3 存储桶访问点 (Access Points) 主要用于管理对 S3 存储桶的访问权限，而非直接提高上传或下载吞吐量。B 选项：将文件上传到多个 S3 存储桶并不能直接提高单个文件上传或下载的速度。E 选项：在上传文件时添加随机前缀可以提高数据分布，但主要目的是为了避免密钥冲突，而不是提高吞吐量，同时该方法对提高下载速度没有帮助。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon EC2",
      "S3 Multipart Upload",
      "S3 Access Points"
    ]
  },
  {
    "id": 691,
    "topic": "1",
    "question_en": "A solutions architect is designing a shared storage solution for a web application that is deployed across multiple Availability Zones. The web application runs on Amazon EC2 instances that are in an Auto Scaling group. The company plans to make frequent changes to the content. The solution must have strong consistency in returning the new content as soon as the changes occur. Which solutions meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) block storage that is mounted to the individual EC2 instances.",
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system on the individual EC2 instances.",
      "C": "Create a shared Amazon Elastic Block Store (Amazon EBS) volume. Mount the EBS volume on the individual EC2 instances.",
      "D": "Use AWS DataSync to perform continuous synchronization of data between EC2 hosts in the Auto Scaling group",
      "E": "Create an Amazon S3 bucket to store the web content. Set the metadata for the Cache-Control header to no-cache. Use Amazon CloudFront to deliver the content."
    },
    "correct_answer": "BE",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为部署在多个可用区中的 Web 应用程序设计共享存储解决方案。Web 应用程序在 Auto Scaling 组中的 Amazon EC2 实例上运行。公司计划频繁更改内容。该解决方案必须在更改发生后立即以强一致性返回新内容。哪些解决方案符合这些要求？（选择两项。）",
    "options_cn": {
      "A": "使用 AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI) 块存储，该存储已挂载到各个 EC2 实例。",
      "B": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统。将 EFS 文件系统挂载到各个 EC2 实例上。",
      "C": "创建一个共享 Amazon Elastic Block Store (Amazon EBS) 卷。将 EBS 卷挂载到各个 EC2 实例上。",
      "D": "使用 AWS DataSync 在 Auto Scaling 组中的 EC2 主机之间执行数据的持续同步。",
      "E": "创建一个 Amazon S3 存储桶来存储 Web 内容。将 Cache-Control 标头的元数据设置为 no-cache。使用 Amazon CloudFront 交付内容。"
    },
    "tags": [
      "EFS",
      "EC2",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 100%），解析仅供参考。】\n\n考查在多可用区 Web 应用中，利用共享存储实现内容快速更新和强一致性的解决方案。",
      "why_correct": "选项 B：Amazon EFS 提供可在多个可用区中访问的共享文件系统，并支持强一致性。将 EFS 挂载到 EC2 实例上，应用程序可以直接读取和写入文件，满足快速内容更新需求。\n选项 E：Amazon S3 结合 CloudFront 提供内容分发，设置 Cache-Control 为 no-cache 可确保立即获取最新内容。 CloudFront 的全球分布可以提高内容访问速度，S3 的强一致性也保证了数据更新的可靠性。",
      "why_wrong": "选项 A：Storage Gateway Volume Gateway iSCSI 块存储虽然可以提供块存储服务，但不适合多实例共享，且一致性难以保证。\n选项 C：Amazon EBS 卷只能被单个 EC2 实例挂载，不满足多个可用区共享存储的需求。\n选项 D：AWS DataSync 适用于数据迁移或周期性同步，而不是满足快速更新和强一致性的需求，持续同步的开销也很大，无法满足快速更新的需求。"
    },
    "related_terms": [
      "AWS Storage Gateway Volume Gateway Internet Small Computer Systems Interface (iSCSI)",
      "Amazon EC2",
      "Auto Scaling",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS DataSync",
      "Amazon S3",
      "Cache-Control",
      "Amazon CloudFront"
    ]
  },
  {
    "id": 692,
    "topic": "1",
    "question_en": "A company is deploying an application in three AWS Regions using an Application Load Balancer. Amazon Route 53 will be used to distribute trafic between these Regions. Which Route 53 configuration should a solutions architect use to provide the MOST high-performing experience?",
    "options_en": {
      "A": "Create an A record with a latency policy.",
      "B": "Create an A record with a geolocation policy.",
      "C": "Create a CNAME record with a failover policy.",
      "D": "Create a CNAME record with a geoproximity policy."
    },
    "correct_answer": "A",
    "vote_percentage": "81%",
    "question_cn": "一家公司正在使用 Application Load Balancer 在三个 AWS 区域中部署应用程序。Amazon Route 53 将用于在这些区域之间分配流量。解决方案架构师应该使用哪种 Route 53 配置来提供最高的性能体验？",
    "options_cn": {
      "A": "创建一个带有延迟策略的 A 记录。",
      "B": "创建一个带有地理位置策略的 A 记录。",
      "C": "创建一个带有故障转移策略的 CNAME 记录。",
      "D": "创建一个带有地理邻近策略的 CNAME 记录。"
    },
    "tags": [
      "Route 53",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 81%），解析仅供参考。】\n\n本题考察 Route 53 的流量路由策略，侧重于性能优化。",
      "why_correct": "延迟策略（Latency Routing Policy）基于用户到 AWS 区域的延迟时间来路由流量。Route 53 会根据用户到不同区域的延迟时间自动将用户流量路由到延迟最低的区域，从而提供最佳的性能体验，适用于全球分布的应用程序。",
      "why_wrong": "地理位置策略（Geolocation Routing Policy）基于用户地理位置将流量路由到特定区域，不一定能保证最佳的性能，因为用户可能距离其所在地理位置最近的区域较远。故障转移策略（Failover Routing Policy）主要用于实现高可用性，而非性能优化。地理邻近策略（Geoproximity Routing Policy）基于用户位置和资源位置来路由流量，并允许您通过设置偏差（bias）值来控制流量偏向某个特定资源，但其主要目的是优化用户体验，而并非单纯的性能。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon Route 53",
      "Latency Routing Policy",
      "Geolocation Routing Policy",
      "Failover Routing Policy",
      "Geoproximity Routing Policy",
      "A record",
      "CNAME record",
      "AWS"
    ]
  },
  {
    "id": 693,
    "topic": "1",
    "question_en": "A company has a web application that includes an embedded NoSQL database. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an Amazon EC2 Auto Scaling group in a single Availability Zone. A recent increase in trafic requires the application to be highly available and for the database to be eventually consistent. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Replace the ALB with a Network Load Balancer. Maintain the embedded NoSQL database with its replication service on the EC2 instances.",
      "B": "Replace the ALB with a Network Load Balancer. Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS).",
      "C": "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Maintain the embedded NoSQL database with its replication service on the EC2 instances.",
      "D": "Modify the Auto Scaling group to use EC2 instances across three Availability Zones. Migrate the embedded NoSQL database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS)."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个Web应用程序，其中包含一个嵌入式NoSQL数据库。该应用程序在Application Load Balancer (ALB) 后的Amazon EC2实例上运行。这些实例在单个可用区中的Amazon EC2 Auto Scaling组中运行。最近流量的增加要求应用程序具有高可用性，并且数据库具有最终一致性。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将ALB替换为Network Load Balancer。在EC2实例上维护嵌入式NoSQL数据库及其复制服务。",
      "B": "将ALB替换为Network Load Balancer。使用AWS Database Migration Service (AWS DMS) 将嵌入式NoSQL数据库迁移到Amazon DynamoDB。",
      "C": "修改Auto Scaling组以使用跨三个可用区的EC2实例。在EC2实例上维护嵌入式NoSQL数据库及其复制服务。",
      "D": "修改Auto Scaling组以使用跨三个可用区的EC2实例。使用AWS Database Migration Service (AWS DMS) 将嵌入式NoSQL数据库迁移到Amazon DynamoDB。"
    },
    "tags": [
      "Application Load Balancer",
      "EC2",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查在确保高可用性和最终一致性的前提下，迁移嵌入式 NoSQL 数据库到 DynamoDB 的方案。同时考查 Auto Scaling 组跨可用区的配置。",
      "why_correct": "选项 D 提供了最佳解决方案。通过将 Auto Scaling 组配置为跨三个可用区，实现了EC2实例的高可用性。同时，使用 AWS Database Migration Service (AWS DMS) 将嵌入式NoSQL数据库迁移到 DynamoDB，满足了最终一致性需求，并减轻了运营负担。",
      "why_wrong": "选项 A 和 B 都错误地使用了 Network Load Balancer (NLB)，这与本题需求的高可用性关系不大。选项 A 保留了本地嵌入式数据库，无法实现高可用性和减轻运维负担。选项 B 虽然迁移到 DynamoDB，但没有考虑 EC2 实例的高可用性。选项 C 未迁移数据库，虽然扩展了可用区，但数据库的复制和维护增加了运维成本，也无法保证完全的高可用性，因为仍然依赖本地数据库的复制机制。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "NoSQL",
      "Amazon EC2 Auto Scaling",
      "Network Load Balancer (NLB)",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon DynamoDB",
      "EC2"
    ]
  },
  {
    "id": 694,
    "topic": "1",
    "question_en": "A company is building a shopping application on AWS. The application offers a catalog that changes once each month and needs to scale with trafic volume. The company wants the lowest possible latency from the application. Data from each user's shopping cart needs to be highly available. User session data must be available even if the user is disconnected and reconnects. What should a solutions architect do to ensure that the shopping cart data is preserved at all times?",
    "options_en": {
      "A": "Configure an Application Load Balancer to enable the sticky sessions feature (session afinity) for access to the catalog in Amazon Aurora.",
      "B": "Configure Amazon ElastiCache for Redis to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.",
      "C": "Configure Amazon OpenSearch Service to cache catalog data from Amazon DynamoDB and shopping cart data from the user's session.",
      "D": "Configure an Amazon EC2 instance with Amazon Elastic Block Store (Amazon EBS) storage for the catalog and shopping cart. Configure automated snapshots."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上构建一个购物应用程序。该应用程序提供每月更新一次的商品目录，并且需要随着流量的增加而扩展。公司希望应用程序的延迟尽可能低。每个用户的购物车数据需要具有高可用性。用户会话数据即使在用户断开连接并重新连接后也必须可用。解决方案架构师应该怎么做才能确保购物车数据始终被保留？",
    "options_cn": {
      "A": "配置 Application Load Balancer 以启用 Amazon Aurora 中对商品目录的访问的粘性会话功能（会话亲和性）。",
      "B": "配置 Amazon ElastiCache for Redis 以缓存来自 Amazon DynamoDB 的商品目录数据和来自用户会话的购物车数据。",
      "C": "配置 Amazon OpenSearch Service 以缓存来自 Amazon DynamoDB 的商品目录数据和来自用户会话的购物车数据。",
      "D": "配置一个带有 Amazon Elastic Block Store (Amazon EBS) 存储的 Amazon EC2 实例，用于商品目录和购物车。配置自动快照。"
    },
    "tags": [
      "ElastiCache",
      "DynamoDB",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n该问题考察了购物车数据的高可用性和用户会话数据的持久性。使用ElastiCache for Redis可以同时缓存商品目录和购物车数据，提供高可用性。",
      "why_correct": "配置ElastiCache for Redis，用于缓存DynamoDB的商品目录数据和用户会话的购物车数据，从而提供高可用性，用户会话数据在用户断开连接并重新连接后仍然可用。",
      "why_wrong": "选项A 错误，配置ALB启用粘性会话功能，只是将用户会话定向到特定的EC2实例，无法保证购物车数据的高可用性。选项C 错误，OpenSearch Service主要用于搜索和分析，不适合缓存用户会话数据。选项D 错误，使用EBS存储，没有高可用性，并且无法持久化用户会话数据。"
    },
    "related_terms": [
      "ElastiCache",
      "DynamoDB",
      "EC2",
      "Redis",
      "Aurora",
      "EBS",
      "OpenSearch Service"
    ]
  },
  {
    "id": 695,
    "topic": "1",
    "question_en": "A company is building a microservices-based application that will be deployed on Amazon Elastic Kubernetes Service (Amazon EKS). The microservices will interact with each other. The company wants to ensure that the application is observable to identify performance issues in the future. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the application to use Amazon ElastiCache to reduce the number of requests that are sent to the microservices.",
      "B": "Configure Amazon CloudWatch Container Insights to collect metrics from the EKS clusters. Configure AWS X-Ray to trace the requests between the microservices.",
      "C": "Configure AWS CloudTrail to review the API calls. Build an Amazon QuickSight dashboard to observe the microservice interactions.",
      "D": "Use AWS Trusted Advisor to understand the performance of the application."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在构建一个基于微服务的应用程序，该应用程序将部署在 Amazon Elastic Kubernetes Service (Amazon EKS) 上。微服务将相互交互。公司希望确保应用程序是可观察的，以便将来识别性能问题。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置应用程序以使用 Amazon ElastiCache 来减少发送到微服务的请求数量。",
      "B": "配置 Amazon CloudWatch Container Insights 以从 EKS 集群收集指标。配置 AWS X-Ray 以跟踪微服务之间的请求。",
      "C": "配置 AWS CloudTrail 以审查 API 调用。构建 Amazon QuickSight 仪表板以观察微服务交互。",
      "D": "使用 AWS Trusted Advisor 来了解应用程序的性能。"
    },
    "tags": [
      "EKS",
      "CloudWatch",
      "X-Ray"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察 Amazon EKS 应用程序的可观察性方案，包括指标收集和分布式跟踪。",
      "why_correct": "选项 B 提供了全面的可观察性解决方案。CloudWatch Container Insights 能够收集 EKS 集群的指标，包括 CPU、内存和网络利用率等。X-Ray 能够跟踪微服务之间的请求，帮助识别性能瓶颈和错误。",
      "why_wrong": "选项 A 错误，ElastiCache 用于缓存，无法提供可观察性功能。选项 C 错误，CloudTrail 主要用于审计 API 调用，QuickSight 用于数据可视化，但无法直接跟踪微服务请求。选项 D 错误，Trusted Advisor 主要用于优化 AWS 环境，不具备监控应用程序性能的能力。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon CloudWatch Container Insights",
      "AWS X-Ray",
      "Amazon ElastiCache",
      "AWS CloudTrail",
      "Amazon QuickSight",
      "AWS Trusted Advisor"
    ]
  },
  {
    "id": 696,
    "topic": "1",
    "question_en": "A company needs to provide customers with secure access to its data. The company processes customer data and stores the results in an Amazon S3 bucket. All the data is subject to strong regulations and security requirements. The data must be encrypted at rest. Each customer must be able to access only their data from their AWS account. Company employees must not be able to access the data. Which solution will meet these requirements?",
    "options_en": {
      "A": "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data client-side. In the private certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides.",
      "B": "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt the data server-side. In the S3 bucket policy, deny decryption of data for all principals except an IAM role that the customer provides.",
      "C": "Provision a separate AWS Key Management Service (AWS KMS) key for each customer. Encrypt the data server-side. In each KMS key policy, deny decryption of data for all principals except an IAM role that the customer provides.",
      "D": "Provision an AWS Certificate Manager (ACM) certificate for each customer. Encrypt the data client-side. In the public certificate policy, deny access to the certificate for all principals except an IAM role that the customer provides."
    },
    "correct_answer": "C",
    "vote_percentage": "73%",
    "question_cn": "一家公司需要为客户提供对其数据的安全访问。该公司处理客户数据并将结果存储在 Amazon S3 存储桶中。所有数据都受到严格的法规和安全要求。数据必须在静态时加密。每个客户必须只能从其 AWS 账户访问其数据。公司员工不得访问数据。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为每个客户预置一个 AWS Certificate Manager (ACM) 证书。在客户端加密数据。在私有证书策略中，拒绝除客户提供的 IAM 角色之外的所有主体的证书访问权限。",
      "B": "为每个客户预置一个单独的 AWS Key Management Service (AWS KMS) 密钥。在服务器端加密数据。在 S3 存储桶策略中，拒绝除客户提供的 IAM 角色之外的所有主体的数据解密权限。",
      "C": "为每个客户预置一个单独的 AWS Key Management Service (AWS KMS) 密钥。在服务器端加密数据。在每个 KMS 密钥策略中，拒绝除客户提供的 IAM 角色之外的所有主体的数据解密权限。",
      "D": "为每个客户预置一个 AWS Certificate Manager (ACM) 证书。在客户端加密数据。在公共证书策略中，拒绝除客户提供的 IAM 角色之外的所有主体的证书访问权限。"
    },
    "tags": [
      "S3",
      "KMS",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 73%），解析仅供参考。】\n\n考查在 S3 上实现静态数据加密，并根据客户隔离访问权限的解决方案。需要兼顾加密方式和访问控制。",
      "why_correct": "选项 C 提供了满足要求的解决方案。它使用 AWS KMS 密钥进行服务器端加密，确保数据在静态时加密。通过在 KMS 密钥策略中，仅允许客户的 IAM 角色解密数据，从而实现了客户访问的隔离，并防止了公司员工的访问。",
      "why_wrong": "选项 A 和 D 使用 ACM 证书进行客户端加密，这不适用于服务器端加密的需求，且 ACM 主要用于 SSL/TLS 证书管理，不直接用于数据加密。选项 A 错误在于客户端加密和使用 ACM 的方式不匹配，并且私有证书策略不是正确的访问控制方法。选项 D 错误在于使用公共证书策略来控制访问权限是不恰当的。选项 B 使用 KMS 密钥进行服务器端加密，但 S3 存储桶策略的适用范围不如 KMS 密钥策略精确，可能会出现安全隐患，并且无法完全满足员工不得访问数据的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS KMS",
      "IAM",
      "ACM",
      "S3 bucket policy",
      "KMS key policy",
      "server-side encryption",
      "client-side encryption"
    ]
  },
  {
    "id": 697,
    "topic": "1",
    "question_en": "A solutions architect creates a VPC that includes two public subnets and two private subnets. A corporate security mandate requires the solutions architect to launch all Amazon EC2 instances in a private subnet. However, when the solutions architect launches an EC2 instance that runs a web server on ports 80 and 443 in a private subnet, no external internet trafic can connect to the server. What should the solutions architect do to resolve this issue?",
    "options_en": {
      "A": "Attach the EC2 instance to an Auto Scaling group in a private subnet. Ensure that the DNS record for the website resolves to the Auto Scaling group identifier.",
      "B": "Provision an internet-facing Application Load Balancer (ALB) in a public subnet. Add the EC2 instance to the target group that is associated with the ALEnsure that the DNS record for the website resolves to the ALB.",
      "C": "Launch a NAT gateway in a private subnet. Update the route table for the private subnets to add a default route to the NAT gateway. Attach a public Elastic IP address to the NAT gateway.",
      "D": "Ensure that the security group that is attached to the EC2 instance allows HTTP trafic on port 80 and HTTPS trafic on port 443. Ensure that the DNS record for the website resolves to the public IP address of the EC2 instance."
    },
    "correct_answer": "B",
    "vote_percentage": "83%",
    "question_cn": "一位解决方案架构师创建了一个包含两个公共子网和两个私有子网的 VPC。 公司的安全规定要求解决方案架构师在私有子网中启动所有 Amazon EC2 实例。 然而，当解决方案架构师在私有子网中启动一个在端口 80 和 443 上运行 Web 服务器的 EC2 实例时，没有任何外部互联网流量可以连接到该服务器。 解决方案架构师应该怎么做才能解决这个问题？",
    "options_cn": {
      "A": "将 EC2 实例附加到私有子网中的 Auto Scaling 组。 确保网站的 DNS 记录解析为 Auto Scaling 组标识符。",
      "B": "在公共子网中配置一个面向互联网的 Application Load Balancer (ALB)。 将 EC2 实例添加到与 ALB 关联的目标组中。 确保网站的 DNS 记录解析到 ALB。",
      "C": "在私有子网中启动一个 NAT Gateway。 更新私有子网的路由表以添加指向 NAT Gateway 的默认路由。 将一个公有 Elastic IP 地址附加到 NAT Gateway。",
      "D": "确保附加到 EC2 实例的安全组允许端口 80 上的 HTTP 流量和端口 443 上的 HTTPS 流量。 确保网站的 DNS 记录解析为 EC2 实例的公有 IP 地址。"
    },
    "tags": [
      "VPC",
      "EC2",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 83%），解析仅供参考。】\n\n考察如何在私有子网中部署 Web 服务器，并使其能够通过互联网访问。",
      "why_correct": "选项 B 提供了最直接和安全的方式来解决问题。 使用 Application Load Balancer (ALB) 作为入口点，并将私有子网中的 EC2 实例注册为目标，ALB 负责将流量路由到后端 EC2 实例。DNS 记录指向 ALB 确保了流量可以到达 ALB，从而访问私有子网中的 Web 服务器。",
      "why_wrong": "选项 A 错误，将 EC2 实例加入 Auto Scaling 组无法解决外部访问的问题，并且 Auto Scaling 组本身不能直接暴露给互联网。选项 C 描述了 NAT Gateway 的配置，NAT Gateway 允许私有子网中的实例访问互联网，但并不能将外部流量路由到私有子网中的服务器。选项 D 错误，私有子网中的 EC2 实例没有公有 IP 地址，无法通过公有 IP 地址访问；安全组配置仅控制实例内部流量，并不能解决外部访问问题。"
    },
    "related_terms": [
      "VPC",
      "Amazon EC2",
      "Subnet",
      "Web server",
      "Application Load Balancer (ALB)",
      "DNS",
      "Auto Scaling",
      "NAT Gateway",
      "Elastic IP",
      "HTTP",
      "HTTPS"
    ]
  },
  {
    "id": 698,
    "topic": "1",
    "question_en": "A company is deploying a new application to Amazon Elastic Kubernetes Service (Amazon EKS) with an AWS Fargate cluster. The application needs a storage solution for data persistence. The solution must be highly available and fault tolerant. The solution also must be shared between multiple application containers. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create Amazon Elastic Block Store (Amazon EBS) volumes in the same Availability Zones where EKS worker nodes are placed. Register the volumes in a StorageClass object on an EKS cluster. Use EBS Multi-Attach to share the data between containers.",
      "B": "Create an Amazon Elastic File System (Amazon EFS) file system. Register the file system in a StorageClass object on an EKS cluster. Use the same file system for all containers.",
      "C": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Register the volume in a StorageClass object on an EKS cluster. Use the same volume for all containers.",
      "D": "Create Amazon Elastic File System (Amazon EFS) file systems in the same Availability Zones where EKS worker nodes are placed. Register the file systems in a StorageClass object on an EKS cluster. Create an AWS Lambda function to synchronize the data between file systems."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 AWS Fargate 集群将新应用程序部署到 Amazon Elastic Kubernetes Service (Amazon EKS)。该应用程序需要一个用于数据持久性的存储解决方案。该解决方案必须具备高可用性和容错能力。该解决方案还必须在多个应用程序容器之间共享。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 EKS 工作节点所在的可用区中创建 Amazon Elastic Block Store (Amazon EBS) 卷。在 EKS 集群的 StorageClass 对象中注册这些卷。使用 EBS Multi-Attach 在容器之间共享数据。",
      "B": "创建一个 Amazon Elastic File System (Amazon EFS) 文件系统。在 EKS 集群的 StorageClass 对象中注册该文件系统。所有容器使用相同的文件系统。",
      "C": "创建一个 Amazon Elastic Block Store (Amazon EBS) 卷。在 EKS 集群的 StorageClass 对象中注册该卷。所有容器使用相同的卷。",
      "D": "在 EKS 工作节点所在的可用区中创建 Amazon Elastic File System (Amazon EFS) 文件系统。在 EKS 集群的 StorageClass 对象中注册这些文件系统。创建一个 AWS Lambda 函数以在文件系统之间同步数据。"
    },
    "tags": [
      "EKS",
      "EBS",
      "EFS",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查在 EKS 中为 Fargate 应用程序选择合适的持久化存储方案，要求高可用性、容错性和多容器共享数据。",
      "why_correct": "Amazon EFS 提供了高可用性、容错能力，并且可以被多个容器共享。通过在 EKS 的 StorageClass 中注册 EFS 文件系统，可以直接在容器中使用。EFS 是为云原生应用程序设计的，具有较低的运营开销。",
      "why_wrong": "A 选项使用 EBS 卷，但 EBS 卷主要用于单个实例，无法在多个容器之间直接共享，即便使用 Multi-Attach，也会增加复杂性。C 选项与 A 选项类似，EBS 卷无法满足多容器共享的需求。D 选项在不同 EFS 之间使用 Lambda 同步数据，增加了不必要的复杂性和运营开销，并且并非最佳实践。"
    },
    "related_terms": [
      "AWS Fargate",
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon Elastic Block Store (Amazon EBS)",
      "StorageClass",
      "EBS Multi-Attach",
      "AWS Lambda"
    ]
  },
  {
    "id": 699,
    "topic": "1",
    "question_en": "A company has an application that uses Docker containers in its local data center. The application runs on a container host that stores persistent data in a volume on the host. The container instances use the stored persistent data. The company wants to move the application to a fully managed service because the company does not want to manage any servers or storage infrastructure. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with self-managed nodes. Create an Amazon Elastic Block Store (Amazon EBS) volume attached to an Amazon EC2 instance. Use the EBS volume as a persistent volume mounted in the containers.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers.",
      "C": "Use Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type. Create an Amazon S3 bucket. Map the S3 bucket as a persistent storage volume mounted in the containers.",
      "D": "Use Amazon Elastic Container Service (Amazon ECS) with an Amazon EC2 launch type. Create an Amazon Elastic File System (Amazon EFS) volume. Add the EFS volume as a persistent storage volume mounted in the containers."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家公司在其本地数据中心使用 Docker 容器构建应用程序。该应用程序运行在容器主机上，该主机将其持久数据存储在主机上的卷中。容器实例使用存储的持久数据。公司希望将应用程序迁移到完全托管的服务，因为它不想管理任何服务器或存储基础设施。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和自管理节点。创建一个附加到 Amazon EC2 实例的 Amazon Elastic Block Store (Amazon EBS) 卷。将 EBS 卷用作在容器中挂载的持久卷。",
      "B": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 启动类型。创建一个 Amazon Elastic File System (Amazon EFS) 卷。将 EFS 卷添加为在容器中挂载的持久存储卷。",
      "C": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 启动类型。创建一个 Amazon S3 存储桶。将 S3 存储桶映射为在容器中挂载的持久存储卷。",
      "D": "使用 Amazon Elastic Container Service (Amazon ECS) 和 Amazon EC2 启动类型。创建一个 Amazon Elastic File System (Amazon EFS) 卷。将 EFS 卷添加为在容器中挂载的持久存储卷。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n该问题考察了将容器应用程序迁移到完全托管的服务。使用ECS和Fargate启动类型，并将EFS卷挂载到容器中，满足了要求。",
      "why_correct": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 启动类型。创建一个 Amazon Elastic File System (Amazon EFS) 卷。将 EFS 卷添加为在容器中挂载的持久存储卷，满足了要求。",
      "why_wrong": "选项A 错误，EKS 使用自管理节点，不符合完全托管的要求。选项C 错误，ECS和Fargate启动类型不支持直接使用S3存储桶作为持久卷。选项D 错误，ECS和Amazon EC2启动类型不符合托管服务的要求。"
    },
    "related_terms": [
      "ECS",
      "Fargate",
      "EFS",
      "EKS",
      "EC2",
      "S3"
    ]
  },
  {
    "id": 700,
    "topic": "1",
    "question_en": "A gaming company wants to launch a new internet-facing application in multiple AWS Regions. The application will use the TCP and UDP protocols for communication. The company needs to provide high availability and minimum latency for global users. Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create internal Network Load Balancers in front of the application in each Region.",
      "B": "Create external Application Load Balancers in front of the application in each Region.",
      "C": "Create an AWS Global Accelerator accelerator to route trafic to the load balancers in each Region.",
      "D": "Configure Amazon Route 53 to use a geolocation routing policy to distribute the trafic",
      "E": "Configure Amazon CloudFront to handle the trafic and route requests to the application in each Region"
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "题目 9\n题干: 一家游戏公司希望在多个 AWS 区域中启动一个新的面向 Internet 的应用程序。该应用程序将使用 TCP 和 UDP 协议进行通信。该公司需要为全球用户提供高可用性和最低的延迟。解决方案架构师应采取哪些组合操作来满足这些要求？（选择两个。）\n选项:\n   A. 在每个区域中的应用程序前面创建内部 Network Load Balancer。\n   B. 在每个区域中的应用程序前面创建外部 Application Load Balancer。\n   C. 创建一个 AWS Global Accelerator 加速器，以将流量路由到每个区域中的负载均衡器。\n   D. 配置 Amazon Route 53 以使用地理位置路由策略来分配流量。\nE. 配置 Amazon CloudFront 以处理流量并将请求路由到每个区域中的应用程序",
    "options_cn": {
      "A": "在每个区域中的应用程序前面创建内部 Network Load Balancer。",
      "B": "在每个区域中的应用程序前面创建外部 Application Load Balancer。",
      "C": "创建一个 AWS Global Accelerator 加速器，以将流量路由到每个区域中的负载均衡器。",
      "D": "配置 Amazon Route 53 以使用地理位置路由策略来分配流量。",
      "E": "配置 Amazon CloudFront 以处理流量并将请求路由到每个区域中的应用程序"
    },
    "tags": [
      "Global Accelerator",
      "Application Load Balancer",
      "Route 53",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考察如何设计具有高可用性、低延迟，支持 TCP/UDP 协议的全球应用程序架构。重点关注负载均衡、全局加速、以及内容分发网络的选用。",
      "why_correct": "选项 A 和 C 提供了最佳的解决方案。选项 A 创建了内部 Network Load Balancer（NLB）来支持 TCP 和 UDP 协议，并实现区域内的负载均衡和高可用性。选项 C 使用 AWS Global Accelerator，通过其边缘站点将用户流量路由到各个区域的 NLB，从而实现全球加速和降低延迟。",
      "why_wrong": "选项 B 错误，Application Load Balancer (ALB) 主要用于 HTTP/HTTPS 协议，不支持 UDP。选项 D 使用地理位置路由策略分配流量，虽然 Route 53 可以实现流量分配，但无法保证最佳延迟，并且没有利用 Global Accelerator 的加速能力。选项 E，CloudFront 虽然可以提供内容分发和加速，但主要针对 HTTP/HTTPS 内容，不适用于直接的 TCP/UDP 应用程序，且没有利用 NLB 实现负载均衡。"
    },
    "related_terms": [
      "Network Load Balancer",
      "TCP",
      "UDP",
      "AWS Global Accelerator",
      "Application Load Balancer",
      "Amazon Route 53",
      "Amazon CloudFront",
      "HTTP",
      "HTTPS"
    ]
  },
  {
    "id": 701,
    "topic": "1",
    "question_en": "A city has deployed a web application running on Amazon EC2 instances behind an Application Load Balancer (ALB). The application's users have reported sporadic performance, which appears to be related to DDoS attacks originating from random IP addresses. The city needs a solution that requires minimal configuration changes and provides an audit trail for the DDoS sources. Which solution meets these requirements?",
    "options_en": {
      "A": "Enable an AWS WAF web ACL on the ALB, and configure rules to block trafic from unknown sources.",
      "B": "Subscribe to Amazon Inspector. Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.",
      "C": "Subscribe to AWS Shield Advanced. Engage the AWS DDoS Response Team (DRT) to integrate mitigating controls into the service.",
      "D": "Create an Amazon CloudFront distribution for the application, and set the ALB as the origin. Enable an AWS WAF web ACL on the distribution, and configure rules to block trafic from unknown sources"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一个城市部署了一个 Web 应用程序，该应用程序运行在 Application Load Balancer (ALB) 之后运行的 Amazon EC2 实例上。 应用程序的用户报告了零星的性能问题，这似乎与来自随机 IP 地址的 DDoS 攻击有关。 该城市需要一个解决方案，该方案需要最少的配置更改，并为 DDoS 源提供审计跟踪。 哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "在 ALB 上启用 AWS WAF Web ACL，并配置规则以阻止来自未知源的流量。",
      "B": "订阅 Amazon Inspector。 聘请 AWS DDoS 响应团队 (DRT) 将缓解控制措施集成到服务中。",
      "C": "订阅 AWS Shield Advanced。 聘请 AWS DDoS 响应团队 (DRT) 将缓解控制措施集成到服务中。",
      "D": "为应用程序创建 Amazon CloudFront 分发，并将 ALB 设置为源。 在分发上启用 AWS WAF Web ACL，并配置规则以阻止来自未知源的流量"
    },
    "tags": [
      "Application Load Balancer",
      "AWS WAF",
      "Shield Advanced"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用 AWS Shield Advanced 和 DDoS 响应团队 (DRT) 来缓解 DDoS 攻击，并满足最小配置变更和审计跟踪需求。",
      "why_correct": "AWS Shield Advanced 专门用于防御 DDoS 攻击，并提供了针对各种攻击类型的缓解措施。 聘请 AWS DDoS 响应团队 (DRT) 能够根据具体情况定制缓解措施。该方案满足了题目中所需的审计跟踪和最小配置变更的要求，因为 Shield Advanced 提供了详细的攻击信息和日志，并且 DRT 可以协助配置。",
      "why_wrong": "选项 A 仅使用 AWS WAF Web ACL，其防护能力不如 Shield Advanced 强大，且配置规则需要较多手动配置，可能不满足最小配置变更的需求。 选项 B 涉及 Amazon Inspector，主要用于安全评估，与 DDoS 攻击缓解的核心目标不符。 选项 D 使用 CloudFront 和 WAF，虽然能提供一定防护，但不如 Shield Advanced 的专业性和针对性强，且配置相对复杂，不符合题目中“最小配置更改”的要求。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Amazon EC2",
      "DDoS",
      "AWS WAF",
      "Web ACL",
      "Amazon Inspector",
      "AWS DDoS Response Team (DRT)",
      "AWS Shield Advanced",
      "Amazon CloudFront"
    ]
  },
  {
    "id": 702,
    "topic": "1",
    "question_en": "A company copies 200 TB of data from a recent ocean survey onto AWS Snowball Edge Storage Optimized devices. The company has a high performance computing (HPC) cluster that is hosted on AWS to look for oil and gas deposits. A solutions architect must provide the cluster with consistent sub-millisecond latency and high-throughput access to the data on the Snowball Edge Storage Optimized devices. The company is sending the devices back to AWS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an AWS Storage Gateway file gateway to use the S3 bucket. Access the file gateway from the HPC cluster instances.",
      "B": "Create an Amazon S3 bucket. Import the data into the S3 bucket. Configure an Amazon FSx for Lustre file system, and integrate it with the S3 bucket. Access the FSx for Lustre file system from the HPC cluster instances.",
      "C": "Create an Amazon S3 bucket and an Amazon Elastic File System (Amazon EFS) file system. Import the data into the S3 bucket. Copy the data from the S3 bucket to the EFS file system. Access the EFS file system from the HPC cluster instances.",
      "D": "Create an Amazon FSx for Lustre file system. Import the data directly into the FSx for Lustre file system. Access the FSx for Lustre file system from the HPC cluster instances."
    },
    "correct_answer": "D",
    "vote_percentage": "53%",
    "question_cn": "一家公司将 200 TB 的数据从最近的海洋调查复制到 AWS Snowball Edge 存储优化设备上。该公司拥有一个托管在 AWS 上的高性能计算 (HPC) 集群，用于寻找石油和天然气沉积。 解决方案架构师必须为集群提供一致的亚毫秒级延迟和对 Snowball Edge 存储优化设备上数据的高吞吐量访问。该公司将设备发送回 AWS。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon S3 存储桶。将数据导入 S3 存储桶。配置一个 AWS Storage Gateway 文件网关以使用 S3 存储桶。从 HPC 集群实例访问文件网关。",
      "B": "创建一个 Amazon S3 存储桶。将数据导入 S3 存储桶。配置 Amazon FSx for Lustre 文件系统，并将其与 S3 存储桶集成。从 HPC 集群实例访问 FSx for Lustre 文件系统。",
      "C": "创建一个 Amazon S3 存储桶和一个 Amazon Elastic File System (Amazon EFS) 文件系统。将数据导入 S3 存储桶。将数据从 S3 存储桶复制到 EFS 文件系统。从 HPC 集群实例访问 EFS 文件系统。",
      "D": "创建一个 Amazon FSx for Lustre 文件系统。将数据直接导入 FSx for Lustre 文件系统。从 HPC 集群实例访问 FSx for Lustre 文件系统。"
    },
    "tags": [
      "Snowball Edge",
      "FSx for Lustre",
      "S3",
      "EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 53%），解析仅供参考。】\n\n考查如何为 HPC 集群提供对 Snowball Edge 上数据的低延迟和高吞吐量访问。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 提供了最佳解决方案。FSx for Lustre 专为高性能计算工作负载设计，能够提供亚毫秒级延迟和高吞吐量。可以直接将数据从 Snowball Edge 导入到 FSx for Lustre，满足对数据快速访问的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 引入了 Storage Gateway，这并非最佳选择，因为它增加了额外的延迟。选项 B虽然使用 FSx for Lustre，但仍然需要先将数据导入 S3，再由 FSx for Lustre 访问，增加了额外步骤，且不如直接导入 FSx for Lustre 高效。选项 C 引入了 EFS，虽然 EFS 可以使用，但其性能不如 FSx for Lustre，不适用于高性能计算场景。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Snowball Edge",
      "Amazon S3",
      "HPC",
      "AWS Storage Gateway",
      "Amazon FSx for Lustre",
      "Amazon EFS"
    ]
  },
  {
    "id": 703,
    "topic": "1",
    "question_en": "A company has NFS servers in an on-premises data center that need to periodically back up small amounts of data to Amazon S3. Which solution meets these requirements and is MOST cost-effective?",
    "options_en": {
      "A": "Set up AWS Glue to copy the data from the on-premises servers to Amazon S3.",
      "B": "Set up an AWS DataSync agent on the on-premises servers, and sync the data to Amazon S3.",
      "C": "Set up an SFTP sync using AWS Transfer for SFTP to sync data from on premises to Amazon S3.",
      "D": "Set up an AWS Direct Connect connection between the on-premises data center and a VPC, and copy the data to Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在本地数据中心拥有 NFS 服务器，需要定期间隔地将少量数据备份到 Amazon S3。哪种解决方案满足这些要求且最具成本效益？",
    "options_cn": {
      "A": "设置 AWS Glue 将数据从本地服务器复制到 Amazon S3。",
      "B": "在本地服务器上设置 AWS DataSync 代理，并将数据同步到 Amazon S3。",
      "C": "使用 AWS Transfer for SFTP 设置 SFTP 同步，将数据从本地同步到 Amazon S3。",
      "D": "在本地数据中心和 VPC 之间设置 AWS Direct Connect 连接，并将数据复制到 Amazon S3。"
    },
    "tags": [
      "AWS DataSync",
      "S3",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查了将本地数据定期备份到 Amazon S3 的场景，并考察了不同解决方案的成本效益和适用性。",
      "why_correct": "AWS DataSync 代理专为本地到云的数据传输而设计，能够高效、安全地将数据同步到 Amazon S3。它支持定期调度，满足了定期备份的要求。DataSync 的计费方式也更具成本效益，适用于少量数据的传输。",
      "why_wrong": "选项 A，AWS Glue 主要用于 ETL 任务，而非直接的文件同步，且其成本结构可能不如 DataSync 经济。选项 C，AWS Transfer for SFTP 适用于通过 SFTP 协议传输文件，但在此场景下，可能不如 DataSync 直接和高效。选项 D，AWS Direct Connect 提供了本地数据中心与 AWS 之间的专用网络连接，但仅用于网络连接，无法直接完成数据同步，且成本较高，对于少量数据的备份而言，性价比不高。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS DataSync",
      "AWS Glue",
      "AWS Transfer for SFTP",
      "AWS Direct Connect",
      "VPC",
      "NFS"
    ]
  },
  {
    "id": 704,
    "topic": "1",
    "question_en": "An online video game company must maintain ultra-low latency for its game servers. The game servers run on Amazon EC2 instances. The company needs a solution that can handle millions of UDP internet trafic requests each second. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an Application Load Balancer with the required protocol and ports for the internet trafic. Specify the EC2 instances as the targets.",
      "B": "Configure a Gateway Load Balancer for the internet trafic. Specify the EC2 instances as the targets.",
      "C": "Configure a Network Load Balancer with the required protocol and ports for the internet trafic. Specify the EC2 instances as the targets.",
      "D": "Launch an identical set of game servers on EC2 instances in separate AWS Regions. Route internet trafic to both sets of EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家在线视频游戏公司必须为其游戏服务器保持超低延迟。游戏服务器在 Amazon EC2 实例上运行。该公司需要一个能够每秒处理数百万个 UDP 互联网流量请求的解决方案。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "配置一个 Application Load Balancer，并为互联网流量配置所需的协议和端口。将 EC2 实例指定为目标。",
      "B": "为互联网流量配置一个 Gateway Load Balancer。将 EC2 实例指定为目标。",
      "C": "配置一个 Network Load Balancer，并为互联网流量配置所需的协议和端口。将 EC2 实例指定为目标。",
      "D": "在不同的 AWS 区域的 EC2 实例上启动一组相同的游戏服务器。将互联网流量路由到两组 EC2 实例。"
    },
    "tags": [
      "Network Load Balancer",
      "Application Load Balancer",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查了如何选择最适合处理高吞吐量 UDP 流量的负载均衡器，以及 EC2 实例的配置。",
      "why_correct": "Network Load Balancer (NLB) 是设计用于处理需要极高性能和低延迟的流量的。NLB 能够处理每秒数百万个 UDP 请求，并且直接将流量路由到 EC2 实例，从而最大限度地降低延迟。这使得 NLB 成为满足游戏服务器对超低延迟要求的最佳选择。",
      "why_wrong": "Application Load Balancer (ALB) 针对 HTTP 和 HTTPS 流量进行了优化，不适合 UDP 流量和超低延迟需求。Gateway Load Balancer 主要用于第三方网络设备，不能直接满足题目的要求。在不同区域启动 EC2 实例并路由流量增加了复杂性和管理开销，并且无法直接解决低延迟的需求，也无法保证成本效益。"
    },
    "related_terms": [
      "Amazon EC2",
      "UDP",
      "Network Load Balancer",
      "Application Load Balancer",
      "Gateway Load Balancer",
      "EC2 instance"
    ]
  },
  {
    "id": 705,
    "topic": "1",
    "question_en": "A company runs a three-tier application in a VPC. The database tier uses an Amazon RDS for MySQL DB instance. The company plans to migrate the RDS for MySQL DB instance to an Amazon Aurora PostgreSQL DB cluster. The company needs a solution that replicates the data changes that happen during the migration to the new database. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Database Migration Service (AWS DMS) Schema Conversion to transform the database objects.",
      "B": "Use AWS Database Migration Service (AWS DMS) Schema Conversion to create an Aurora PostgreSQL read replica on the RDS for MySQL DB instance.",
      "C": "Configure an Aurora MySQL read replica for the RDS for MySQL DB instance.",
      "D": "Define an AWS Database Migration Service (AWS DMS) task with change data capture (CDC) to migrate the data",
      "E": "Promote the Aurora PostgreSQL read replica to a standalone Aurora PostgreSQL DB cluster when the replica lag is zero."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "题目 10\n题干: 一家公司在 VPC 中运行一个三层应用程序。数据库层使用 Amazon RDS for MySQL DB 实例。该公司计划将 RDS for MySQL 数据库实例迁移到 Amazon Aurora PostgreSQL 数据库集群。该公司需要一个解决方案，该解决方案可以复制在迁移期间发生到新数据库的数据更改。哪些步骤的组合将满足这些要求？（选择两个。）\n选项:\n   A. 使用 AWS Database Migration Service (AWS DMS) 模式转换来转换数据库对象。\n   B. 使用 AWS Database Migration Service (AWS DMS) 模式转换在 RDS for MySQL 数据库实例上创建 Aurora PostgreSQL 读副本。\n   C. 为 RDS for MySQL 数据库实例配置 Aurora MySQL 读副本。\n   D. 定义一个带有更改数据捕获 (CDC) 的 AWS Database Migration Service (AWS DMS) 任务来迁移数据。\nE. 当副本滞后为零时，将 Aurora PostgreSQL 读副本提升为独立的 Aurora PostgreSQL 数据库集群。",
    "options_cn": {
      "A": "使用 AWS Database Migration Service (AWS DMS) 模式转换来转换数据库对象。",
      "B": "使用 AWS Database Migration Service (AWS DMS) 模式转换在 RDS for MySQL 数据库实例上创建 Aurora PostgreSQL 读副本。",
      "C": "为 RDS for MySQL 数据库实例配置 Aurora MySQL 读副本。",
      "D": "定义一个带有更改数据捕获 (CDC) 的 AWS Database Migration Service (AWS DMS) 任务来迁移数据。",
      "E": "当副本滞后为零时，将 Aurora PostgreSQL 读副本提升为独立的 Aurora PostgreSQL 数据库集群。"
    },
    "tags": [
      "AWS DMS",
      "RDS",
      "Aurora PostgreSQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n考察使用 AWS Database Migration Service (DMS) 迁移数据库，并同步数据变更的方法。",
      "why_correct": "A. 迁移数据库时，需要转换数据库对象，以适应目标数据库 Aurora PostgreSQL 的要求，AWS DMS 模式转换可以实现此功能。D. AWS DMS 支持使用更改数据捕获 (CDC) 来持续复制源数据库中的数据更改到目标数据库，确保迁移期间的数据同步。",
      "why_wrong": "B. AWS DMS 模式转换不能直接用于创建读副本，读副本是由数据库引擎自身提供的功能。C. 题干要求迁移到 Aurora PostgreSQL，而此选项是配置 Aurora MySQL 读副本，数据库类型不匹配，无法满足迁移需求。E.  将读副本提升为独立集群是在数据迁移完成后才进行的操作，而不是在迁移期间同步数据更改的步骤。"
    },
    "related_terms": [
      "VPC",
      "Amazon RDS for MySQL",
      "DB instance",
      "Amazon Aurora PostgreSQL",
      "AWS Database Migration Service (AWS DMS)",
      "Database Migration Service (DMS)",
      "Aurora PostgreSQL",
      "Aurora MySQL",
      "Read Replica",
      "Change Data Capture (CDC)"
    ]
  },
  {
    "id": 706,
    "topic": "1",
    "question_en": "A company hosts a database that runs on an Amazon RDS instance that is deployed to multiple Availability Zones. The company periodically runs a script against the database to report new entries that are added to the database. The script that runs against the database negatively affects the performance of a critical application. The company needs to improve application performance with minimal costs. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Add functionality to the script to identify the instance that has the fewest active connections. Configure the script to read from that instance to report the total new entries.",
      "B": "Create a read replica of the database. Configure the script to query only the read replica to report the total new entries.",
      "C": "Instruct the development team to manually export the new entries for the day in the database at the end of each day.",
      "D": "Use Amazon ElastiCache to cache the common queries that the script runs against the database."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管一个数据库，该数据库运行在部署到多个可用区的 Amazon RDS 实例上。该公司定期间隔对数据库运行一个脚本，以报告添加到数据库的新条目。针对数据库运行的脚本会对关键应用程序的性能产生负面影响。该公司需要以最低的成本提高应用程序的性能。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "向脚本添加功能，以识别具有最少活动连接的实例。配置脚本以从该实例读取数据，以报告总的新条目。",
      "B": "创建数据库的只读副本。配置脚本仅查询只读副本以报告总的新条目。",
      "C": "指示开发团队在每天结束时手动导出数据库中每天的新条目。",
      "D": "使用 Amazon ElastiCache 缓存脚本针对数据库运行的常见查询。"
    },
    "tags": [
      "RDS",
      "Read Replica",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察如何以最低成本提高数据库性能。关键在于如何减少数据库的负载。只读副本可以分担读取流量，而 ElastiCache 可以缓存数据。选择只读副本可以减少对主数据库的读取操作的压力，从而提高应用程序性能。",
      "why_correct": "选项 B 正确，创建数据库的只读副本，并配置脚本查询只读副本，以减少对主数据库的压力，从而提高性能。",
      "why_wrong": "选项 A、C 和 D 错误。A 选项向脚本添加功能，让其识别活动连接最少的实例并读取数据，无法解决数据库性能问题。C 选项手动导出数据库中每天的新条目，不能提高性能。D 选项使用 ElastiCache 缓存脚本中针对数据库运行的常见查询，虽然可以提高性能，但没有只读副本成本低。"
    },
    "related_terms": [
      "RDS",
      "Read Replica",
      "ElastiCache"
    ]
  },
  {
    "id": 707,
    "topic": "1",
    "question_en": "A company is using an Application Load Balancer (ALB) to present its application to the internet. The company finds abnormal trafic access patterns across the application. A solutions architect needs to improve visibility into the infrastructure to help the company understand these abnormalities better. What is the MOST operationally eficient solution that meets these requirements?",
    "options_en": {
      "A": "Create a table in Amazon Athena for AWS CloudTrail logs. Create a query for the relevant information.",
      "B": "Enable ALB access logging to Amazon S3. Create a table in Amazon Athena, and query the logs.",
      "C": "Enable ALB access logging to Amazon S3. Open each file in a text editor, and search each line for the relevant information.",
      "D": "Use Amazon EMR on a dedicated Amazon EC2 instance to directly query the ALB to acquire trafic access log information."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "题目 11\n题干: 一家公司正在使用 Application Load Balancer (ALB) 向 Internet 呈现其应用程序。该公司发现应用程序的访问模式异常。解决方案架构师需要提高对基础设施的可见性，以帮助公司更好地了解这些异常情况。哪种解决方案最能满足这些要求？\n选项:\n   A. 在 Amazon Athena 中为 AWS CloudTrail 日志创建一个表。为相关信息创建一个查询。\n   B. 为 Amazon S3 打开 ALB 访问日志。在 Amazon Athena 中创建一个表，并查询日志。\n   C. 为 Amazon S3 打开 ALB 访问日志。在文本编辑器中打开每个文件，并搜索每一行以获取相关信息。\n   D. 在专用的 Amazon EC2 实例上使用 Amazon EMR 直接查询 ALB 以获取流量访问日志信息。",
    "options_cn": {
      "A": "在 Amazon Athena 中为 AWS CloudTrail 日志创建一个表。为相关信息创建一个查询。",
      "B": "为 Amazon S3 打开 ALB 访问日志。在 Amazon Athena 中创建一个表，并查询日志。",
      "C": "为 Amazon S3 打开 ALB 访问日志。在文本编辑器中打开每个文件，并搜索每一行以获取相关信息。",
      "D": "在专用的 Amazon EC2 实例上使用 Amazon EMR 直接查询 ALB 以获取流量访问日志信息。"
    },
    "tags": [
      "ALB",
      "Athena",
      "CloudTrail",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察使用 Application Load Balancer (ALB) 访问日志进行流量分析，以提高基础设施可见性。要求找到一个能够有效分析日志的方案。",
      "why_correct": "选项 B 是最佳解决方案。ALB 访问日志可以记录客户端的请求信息，将其保存在 S3 桶中。Athena 是一种无服务器查询服务，可以方便地查询 S3 中的日志文件，从而进行分析。",
      "why_wrong": "选项 A 错误，CloudTrail 主要记录 API 调用，无法直接提供应用程序流量访问的详细信息。选项 C 错误，在文本编辑器中逐个打开和搜索日志文件效率极低，不适用于大规模数据分析。选项 D 错误，虽然 EMR 可以处理大规模数据，但 ALB 无法直接查询。需要先将日志上传到 S3，再由 EMR 或 Athena 进行分析。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Internet",
      "Amazon Athena",
      "AWS CloudTrail",
      "Amazon S3",
      "Amazon EC2",
      "Amazon EMR"
    ]
  },
  {
    "id": 708,
    "topic": "1",
    "question_en": "A company wants to use NAT gateways in its AWS environment. The company's Amazon EC2 instances in private subnets must be able to connect to the public internet through the NAT gateways. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create public NAT gateways in the same private subnets as the EC2 instances.",
      "B": "Create private NAT gateways in the same private subnets as the EC2 instances.",
      "C": "Create public NAT gateways in public subnets in the same VPCs as the EC2 instances.",
      "D": "Create private NAT gateways in public subnets in the same VPCs as the EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在其 AWS 环境中使用 NAT 网关。该公司位于私有子网中的 Amazon EC2 实例必须能够通过 NAT 网关连接到公共互联网。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在与 EC2 实例相同的私有子网中创建公共 NAT 网关。",
      "B": "在与 EC2 实例相同的私有子网中创建私有 NAT 网关。",
      "C": "在与 EC2 实例相同的 VPC 的公共子网中创建公共 NAT 网关。",
      "D": "在与 EC2 实例相同的 VPC 的公共子网中创建私有 NAT 网关。"
    },
    "tags": [
      "NAT Gateway",
      "VPC",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察 NAT 网关在 VPC 中的部署位置以及其与私有子网 EC2 实例的连通性。需要确保私有子网中的实例能够通过 NAT 网关访问公网。",
      "why_correct": "选项 C 正确。NAT 网关必须部署在公共子网中，以便它可以拥有公网 IP 地址。私有子网中的 EC2 实例通过路由表配置将流量导向 NAT 网关，从而实现对公网的访问。这种配置允许私有子网中的实例发起与公网的连接，而不会暴露实例的公网 IP 地址。",
      "why_wrong": "选项 A 错误，因为即使 NAT 网关是公共的，但如果部署在与 EC2 实例相同的私有子网中，那么 NAT 网关也无法访问互联网。选项 B 错误，私有 NAT 网关的设计目的就是用于私有子网之间的通讯，无法提供对公网的访问。选项 D 错误，虽然 NAT 网关部署在公共子网，但如果 NAT 网关本身是私有的，则无法为私有子网中的 EC2 实例提供公网访问能力。"
    },
    "related_terms": [
      "NAT Gateway",
      "VPC",
      "EC2 instance",
      "Public subnet",
      "Private subnet",
      "Internet"
    ]
  },
  {
    "id": 709,
    "topic": "1",
    "question_en": "A company has an organization in AWS Organizations. The company runs Amazon EC2 instances across four AWS accounts in the root organizational unit (OU). There are three nonproduction accounts and one production account. The company wants to prohibit users from launching EC2 instances of a certain size in the nonproduction accounts. The company has created a service control policy (SCP) to deny access to launch instances that use the prohibited types. Which solutions to deploy the SCP will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Attach the SCP to the root OU for the organization.",
      "B": "Attach the SCP to the three nonproduction Organizations member accounts.",
      "C": "Attach the SCP to the Organizations management account.",
      "D": "Create an OU for the production account. Attach the SCP to the OU. Move the production member account into the new OU",
      "E": "Create an OU for the required accounts. Attach the SCP to the OU. Move the nonproduction member accounts into the new OU."
    },
    "correct_answer": "BE",
    "vote_percentage": "83%",
    "question_cn": "一家公司在 AWS Organizations 中有一个组织。该公司在根组织单元 (OU) 中的四个 AWS 账户上运行 Amazon EC2 实例。有三个非生产账户和一个生产账户。该公司希望禁止用户在非生产账户中启动特定大小的 EC2 实例。该公司创建了一个服务控制策略 (SCP) 以拒绝访问启动使用被禁止类型的实例。哪些部署 SCP 的解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将 SCP 附加到组织的根 OU。",
      "B": "将 SCP 附加到三个非生产 Organizations 成员账户。",
      "C": "将 SCP 附加到 Organizations 管理账户。",
      "D": "为生产账户创建一个 OU。将 SCP 附加到该 OU。将生产成员账户移动到新的 OU 中。",
      "E": "为所需账户创建一个 OU。将 SCP 附加到该 OU。将非生产成员账户移动到新的 OU 中。"
    },
    "tags": [
      "Organizations",
      "SCP",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 83%），解析仅供参考。】\n\n考查 AWS Organizations 中的服务控制策略 (SCP) 的应用，以限制 EC2 实例的启动类型。",
      "why_correct": "选项 B 正确，因为 SCP 必须直接附加到受影响的账户上，以阻止其启动被禁止的实例。将 SCP 附加到三个非生产 Organizations 成员账户，可以精确地针对目标账户实施限制。选项 E 正确，通过为非生产账户创建一个 OU，并将 SCP 附加到该 OU，可以有效地管理和应用策略到指定账户组。这种 OU 结构允许集中管理和更灵活的策略应用。",
      "why_wrong": "选项 A 错误，因为将 SCP 附加到根 OU 会影响组织内的所有账户，包括生产账户，这与仅限制非生产账户的要求相悖。选项 C 错误，因为将 SCP 附加到 Organizations 管理账户无法直接影响成员账户的资源操作。选项 D 错误，虽然为生产账户创建 OU 是一种组织方法，但这样做不能解决禁止在非生产账户中启动特定实例类型的需求，而且会影响生产账户的可用性。"
    },
    "related_terms": [
      "AWS Organizations",
      "OU",
      "SCP",
      "Amazon EC2",
      "EC2 instance"
    ]
  },
  {
    "id": 710,
    "topic": "1",
    "question_en": "A company’s website hosted on Amazon EC2 instances processes classified data stored in Amazon S3. Due to security concerns, the company requires a private and secure connection between its EC2 resources and Amazon S3. Which solution meets these requirements?",
    "options_en": {
      "A": "Set up S3 bucket policies to allow access from a VPC endpoint.",
      "B": "Set up an IAM policy to grant read-write access to the S3 bucket.",
      "C": "Set up a NAT gateway to access resources outside the private subnet.",
      "D": "Set up an access key ID and a secret access key to access the S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管在 Amazon EC2 实例上的网站处理存储在 Amazon S3 中的机密数据。由于安全问题，该公司要求其 EC2 资源和 Amazon S3 之间建立私密且安全的连接。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "设置 S3 存储桶策略以允许从 VPC endpoint 访问。",
      "B": "设置 IAM 策略以授予对 S3 存储桶的读写访问权限。",
      "C": "设置 NAT Gateway 以访问私有子网之外的资源。",
      "D": "设置访问密钥 ID 和秘密访问密钥以访问 S3 存储桶。"
    },
    "tags": [
      "S3",
      "VPC Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察如何通过私有连接保护 EC2 实例与 S3 存储桶之间的通信。S3 的 VPC 端点提供了一种私有方式访问 S3，避免了流量通过公共互联网。",
      "why_correct": "选项 A 正确，设置 S3 存储桶策略以允许从 VPC endpoint 访问，可以实现 EC2 资源和 Amazon S3 之间的私密且安全的连接。",
      "why_wrong": "选项 B、C 和 D 错误。B 选项设置 IAM 策略以授予对 S3 存储桶的读写访问权限，不能实现私密且安全的连接。C 选项设置 NAT Gateway 以访问私有子网之外的资源，不能实现私密且安全的连接。D 选项设置访问密钥 ID 和秘密访问密钥以访问 S3 存储桶，不能实现私密且安全的连接。"
    },
    "related_terms": [
      "S3",
      "VPC Endpoint"
    ]
  },
  {
    "id": 711,
    "topic": "1",
    "question_en": "An ecommerce company runs its application on AWS. The application uses an Amazon Aurora PostgreSQL cluster in Multi-AZ mode for the underlying database. During a recent promotional campaign, the application experienced heavy read load and write load. Users experienced timeout issues when they attempted to access the application. A solutions architect needs to make the application architecture more scalable and highly available. Which solution will meet these requirements with the LEAST downtime?",
    "options_en": {
      "A": "Create an Amazon EventBridge rule that has the Aurora cluster as a source. Create an AWS Lambda function to log the state change events of the Aurora cluster. Add the Lambda function as a target for the EventBridge rule. Add additional reader nodes to fail over to.",
      "B": "Modify the Aurora cluster and activate the zero-downtime restart (ZDR) feature. Use Database Activity Streams on the cluster to track the cluster status.",
      "C": "Add additional reader instances to the Aurora cluster. Create an Amazon RDS Proxy target group for the Aurora cluster.",
      "D": "Create an Amazon ElastiCache for Redis cache. Replicate data from the Aurora cluster to Redis by using AWS Database Migration Service (AWS DMS) with a write-around approach."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司在 AWS 上运行其应用程序。该应用程序使用 Multi-AZ 模式的 Amazon Aurora PostgreSQL 集群作为底层数据库。在最近的促销活动中，应用程序经历了繁重的读取负载和写入负载。用户在尝试访问该应用程序时遇到了超时问题。 解决方案架构师需要使应用程序架构更具可扩展性和高可用性。哪种解决方案将以最少的停机时间满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon EventBridge 规则，该规则将 Aurora 集群作为 源。创建一个 AWS Lambda 函数来记录 Aurora 集群的状态更改事件。将 Lambda 函数添加为 EventBridge 规则的目标。添加额外的读取节点以进行故障转移。",
      "B": "修改 Aurora 集群并激活零停机重启 (ZDR) 功能。 使用数据库活动流来跟踪集群状态。",
      "C": "为 Aurora 集群添加额外的读取实例。 为 Aurora 集群创建 Amazon RDS Proxy 目标组。",
      "D": "创建一个 Amazon ElastiCache for Redis 缓存。使用 AWS Database Migration Service (AWS DMS) 并采用旁路写入方式，将数据从 Aurora 集群复制到 Redis。"
    },
    "tags": [
      "Aurora PostgreSQL",
      "RDS Proxy",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察了 Amazon Aurora 集群的可扩展性和高可用性解决方案，以及如何应对高负载下的超时问题。",
      "why_correct": "选项 C 通过添加额外的读取实例来分担读取负载，提高应用程序的响应速度。Amazon RDS Proxy 能够有效管理数据库连接，减轻数据库压力，并实现更快的故障转移，从而提高应用程序的可用性。",
      "why_wrong": "选项 A 仅通过 EventBridge 和 Lambda 监控数据库状态，无法直接解决读取和写入负载的问题，也不能提供快速的故障转移。选项 B 的 ZDR 功能主要用于数据库重启，解决的是维护问题，而非负载问题。数据库活动流也并非解决高负载和可用性问题的直接手段。选项 D 引入了 ElastiCache，适用于缓存静态数据以减轻数据库压力，但需要通过 DMS 进行数据同步，会增加复杂性和延迟，并且旁路写入方式无法保证数据一致性，不适合作为首选方案来解决高负载和可用性问题。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL",
      "Multi-AZ",
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon RDS Proxy",
      "Amazon ElastiCache for Redis",
      "AWS Database Migration Service (AWS DMS)",
      "ZDR"
    ]
  },
  {
    "id": 712,
    "topic": "1",
    "question_en": "A company is designing a web application on AWS. The application will use a VPN connection between the company’s existing data centers and the company's VPCs. The company uses Amazon Route 53 as its DNS service. The application must use private DNS records to communicate with the on-premises services from a VPC. Which solution will meet these requirements in the MOST secure manner?",
    "options_en": {
      "A": "Create a Route 53 Resolver outbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC.",
      "B": "Create a Route 53 Resolver inbound endpoint. Create a resolver rule. Associate the resolver rule with the VPC.",
      "C": "Create a Route 53 private hosted zone. Associate the private hosted zone with the VPC.",
      "D": "Create a Route 53 public hosted zone. Create a record for each service to allow service communication"
    },
    "correct_answer": "A",
    "vote_percentage": "91%",
    "question_cn": "一家公司正在 AWS 上设计一个 Web 应用程序。该应用程序将使用公司现有数据中心与其 VPC 之间的 VPN 连接。该公司使用 Amazon Route 53 作为其 DNS 服务。该应用程序必须使用私有 DNS 记录才能从 VPC 与本地服务通信。哪种解决方案将以最安全的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个 Route 53 Resolver 出站端点。创建一个解析器规则。将该解析器规则与 VPC 关联。",
      "B": "创建一个 Route 53 Resolver 入站端点。创建一个解析器规则。将该解析器规则与 VPC 关联。",
      "C": "创建一个 Route 53 私有托管区域。将该私有托管区域与 VPC 关联。",
      "D": "创建一个 Route 53 公共托管区域。为每个服务创建记录以允许服务通信"
    },
    "tags": [
      "Route 53",
      "Resolver"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 91%），解析仅供参考。】\n\n考查使用 Route 53 Resolver 实现 VPC 与本地网络私有 DNS 记录解析，以及出站端点的使用。确保本地服务与 VPC 内部服务能够通过 DNS 互相解析。",
      "why_correct": "Route 53 Resolver 出站端点允许 VPC 中的资源解析本地网络中的 DNS 记录。创建一个解析器规则，将本地 DNS 查询转发到本地 DNS 服务器。将该解析器规则与 VPC 关联后，VPC 中的资源就可以通过私有 DNS 记录与本地服务通信，满足安全和私有 DNS 解析的要求。",
      "why_wrong": "选项 B，入站端点主要用于将本地 DNS 查询转发到 VPC 内的 DNS 解析器，不符合题意。选项 C，私有托管区域用于管理 VPC 内部的 DNS 记录，无法解决 VPC 访问本地 DNS 的问题。选项 D，公共托管区域无法解决私有 DNS 记录解析的需求，并且安全性较差，不符合题意。"
    },
    "related_terms": [
      "Amazon Route 53",
      "VPC",
      "DNS",
      "Route 53 Resolver",
      "Resolver outbound endpoint",
      "Resolver rule",
      "Private Hosted Zone",
      "Public Hosted Zone"
    ]
  },
  {
    "id": 713,
    "topic": "1",
    "question_en": "A company is running a photo hosting service in the us-east-1 Region. The service enables users across multiple countries to upload and view photos. Some photos are heavily viewed for months, and others are viewed for less than a week. The application allows uploads of up to 20 MB for each photo. The service uses the photo metadata to determine which photos to display to each user. Which solution provides the appropriate user access MOST cost-effectively?",
    "options_en": {
      "A": "Store the photos in Amazon DynamoDB. Turn on DynamoDB Accelerator (DAX) to cache frequently viewed items.",
      "B": "Store the photos in the Amazon S3 Intelligent-Tiering storage class. Store the photo metadata and its S3 location in DynamoDB.",
      "C": "Store the photos in the Amazon S3 Standard storage class. Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Use the object tags to keep track of metadata.",
      "D": "Store the photos in the Amazon S3 Glacier storage class. Set up an S3 Lifecycle policy to move photos older than 30 days to the S3 Glacier Deep Archive storage class. Store the photo metadata and its S3 location in Amazon OpenSearch Service."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 us-east-1 区域运行一个照片托管服务。该服务允许全球各地的用户上传和查看照片。有些照片会被频繁查看数月，而另一些照片的查看时间不到一周。该应用程序允许每个照片上传高达 20 MB 的内容。该服务使用照片元数据来确定要向每个用户显示哪些照片。哪种解决方案能以最具成本效益的方式提供适当的用户访问？",
    "options_cn": {
      "A": "将照片存储在 Amazon DynamoDB 中。打开 DynamoDB Accelerator (DAX) 以缓存经常查看的项目。",
      "B": "将照片存储在 Amazon S3 Intelligent-Tiering 存储类中。将照片元数据及其 S3 位置存储在 DynamoDB 中。",
      "C": "将照片存储在 Amazon S3 Standard 存储类中。设置 S3 生命周期策略，将超过 30 天的照片移动到 S3 Standard-Infrequent Access (S3 Standard-IA) 存储类。使用对象标签来跟踪元数据。",
      "D": "将照片存储在 Amazon S3 Glacier 存储类中。设置 S3 生命周期策略，将超过 30 天的照片移动到 S3 Glacier Deep Archive 存储类。将照片元数据及其 S3 位置存储在 Amazon OpenSearch Service 中。"
    },
    "tags": [
      "S3",
      "S3 Intelligent-Tiering",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察如何使用 Amazon S3 和其他 AWS 服务，以最具成本效益的方式存储和访问频繁和不频繁访问的对象。",
      "why_correct": "选项 B 提供了最佳的成本效益解决方案。S3 Intelligent-Tiering 存储类可以根据访问模式自动将对象移动到不同的访问层，无需手动配置。将照片元数据存储在 DynamoDB 中，可以快速检索元数据信息，提高应用程序的响应速度。",
      "why_wrong": "选项 A 错误，因为 DynamoDB 主要用于存储结构化数据，不适合存储大型照片文件。DAX 是一种缓存加速器，可以提高 DynamoDB 的读取性能，但无法解决存储成本问题。选项 C 错误，虽然 S3 Standard-IA 存储类成本较低，但需要手动配置生命周期策略，不如 Intelligent-Tiering 灵活。选项 D 错误，S3 Glacier 和 S3 Glacier Deep Archive 存储类主要用于数据存档，检索时间较长，不适合频繁访问的照片；将元数据存储在 OpenSearch Service 中，会增加额外的复杂性和成本，且与题目需求不符。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "S3 Standard",
      "S3 Standard-Infrequent Access (S3 Standard-IA)",
      "S3 lifecycle policy",
      "S3 Glacier",
      "S3 Glacier Deep Archive",
      "Amazon OpenSearch Service"
    ]
  },
  {
    "id": 714,
    "topic": "1",
    "question_en": "A company runs a highly available web application on Amazon EC2 instances behind an Application Load Balancer. The company uses Amazon CloudWatch metrics. As the trafic to the web application increases, some EC2 instances become overloaded with many outstanding requests. The CloudWatch metrics show that the number of requests processed and the time to receive the responses from some EC2 instances are both higher compared to other EC2 instances. The company does not want new requests to be forwarded to the EC2 instances that are already overloaded. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use the round robin routing algorithm based on the RequestCountPerTarget and ActiveConnectionCount CloudWatch metrics.",
      "B": "Use the least outstanding requests algorithm based on the RequestCountPerTarget and ActiveConnectionCount CloudWatch metrics.",
      "C": "Use the round robin routing algorithm based on the RequestCount and TargetResponseTime CloudWatch metrics.",
      "D": "Use the least outstanding requests algorithm based on the RequestCount and TargetResponseTime CloudWatch metrics."
    },
    "correct_answer": "B",
    "vote_percentage": "79%",
    "question_cn": "一家公司在 Application Load Balancer 后面的 Amazon EC2 实例上运行一个高可用性 Web 应用程序。该公司使用 Amazon CloudWatch 指标。随着 Web 应用程序的流量增加，一些 EC2 实例因大量未完成的请求而过载。CloudWatch 指标显示，与其它 EC2 实例相比，某些 EC2 实例处理的请求数量和接收响应的时间都更高。该公司不希望将新请求转发到已经过载的 EC2 实例。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "根据 RequestCountPerTarget 和 ActiveConnectionCount CloudWatch 指标使用轮询路由算法。",
      "B": "根据 RequestCountPerTarget 和 ActiveConnectionCount CloudWatch 指标使用最少未完成请求算法。",
      "C": "根据 RequestCount 和 TargetResponseTime CloudWatch 指标使用轮询路由算法。",
      "D": "根据 RequestCount 和 TargetResponseTime CloudWatch 指标使用最少未完成请求算法。"
    },
    "tags": [
      "ALB",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 79%），解析仅供参考。】\n\n考察 Application Load Balancer 的负载均衡算法配置，以及结合 CloudWatch 指标判断 EC2 实例的健康状态，避免将新请求路由到过载实例。",
      "why_correct": "选项 B 结合了最少未完成请求（Least Outstanding Requests）算法和 RequestCountPerTarget、ActiveConnectionCount 这两个 CloudWatch 指标。最少未完成请求算法优先将请求转发到未完成请求数量最少的实例，这能有效避免将请求发送到过载的 EC2 实例。RequestCountPerTarget 和 ActiveConnectionCount 可以反映 EC2 实例的负载情况，辅助负载均衡器做出决策。",
      "why_wrong": "选项 A 使用轮询算法，轮询算法不考虑实例的负载情况，无法避免将请求发送到过载实例。选项 C 使用轮询算法，并且使用的指标 RequestCount 和 TargetResponseTime 不足以全面反映 EC2 实例的过载状态。选项 D 使用最少未完成请求算法，但指标 RequestCount 和 TargetResponseTime 无法直接反映实例的未完成请求和连接状态，不能准确判断实例的负载情况，导致负载均衡效果不佳。"
    },
    "related_terms": [
      "Application Load Balancer",
      "Amazon EC2",
      "Amazon CloudWatch",
      "RequestCountPerTarget",
      "ActiveConnectionCount",
      "Least Outstanding Requests",
      "RequestCount",
      "TargetResponseTime"
    ]
  },
  {
    "id": 715,
    "topic": "1",
    "question_en": "A company uses Amazon EC2, AWS Fargate, and AWS Lambda to run multiple workloads in the company's AWS account. The company wants to fully make use of its Compute Savings Plans. The company wants to receive notification when coverage of the Compute Savings Plans drops. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create a daily budget for the Savings Plans by using AWS Budgets. Configure the budget with a coverage threshold to send notifications to the appropriate email message recipients.",
      "B": "Create a Lambda function that runs a coverage report against the Savings Plans. Use Amazon Simple Email Service (Amazon SES) to email the report to the appropriate email message recipients.",
      "C": "Create an AWS Budgets report for the Savings Plans budget. Set the frequency to daily.",
      "D": "Create a Savings Plans alert subscription. Enable all notification options. Enter an email address to receive notifications."
    },
    "correct_answer": "A",
    "vote_percentage": "76%",
    "question_cn": "一家公司使用 Amazon EC2、AWS Fargate 和 AWS Lambda 在其公司的 AWS 账户中运行多个工作负载。该公司希望充分利用其 Compute Savings Plans。该公司希望在 Compute Savings Plans 的覆盖率下降时收到通知。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Budgets 为 Savings Plans 创建每日预算。使用覆盖率阈值配置预算，以便向相应的电子邮件消息接收者发送通知。",
      "B": "创建一个 Lambda 函数，该函数针对 Savings Plans 运行覆盖率报告。使用 Amazon Simple Email Service (Amazon SES) 将报告通过电子邮件发送给相应的电子邮件消息接收者。",
      "C": "为 Savings Plans 预算创建 AWS Budgets 报告。将频率设置为每日。",
      "D": "创建 Savings Plans 警报订阅。启用所有通知选项。输入一个电子邮件地址以接收通知。"
    },
    "tags": [
      "Savings Plans",
      "AWS Budgets",
      "Lambda",
      "SES"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 76%），解析仅供参考。】\n\n考查如何使用 AWS Budgets 监控 Compute Savings Plans 的覆盖率，并在覆盖率低于预设阈值时收到通知。",
      "why_correct": "选项 A 使用 AWS Budgets 直接监控 Savings Plans 的覆盖率，并基于覆盖率阈值触发通知。这是一种原生的、运维效率最高的方案，无需额外编写代码或设置复杂的集成。通过配置预算和通知，可以自动化监控流程，并在覆盖率下降时及时收到告警。",
      "why_wrong": "选项 B 需要手动编写 Lambda 函数来生成 Savings Plans 的覆盖率报告，增加了运维复杂性，不如直接使用 AWS Budgets。选项 C 创建报告，但没有设置通知机制，无法满足题目的告警需求。选项 D 中“Savings Plans 警报订阅”并不存在，无法实现监控覆盖率并发出通知的功能。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Fargate",
      "AWS Lambda",
      "Compute Savings Plans",
      "AWS Budgets",
      "Amazon Simple Email Service (Amazon SES)",
      "Savings Plans"
    ]
  },
  {
    "id": 716,
    "topic": "1",
    "question_en": "A company runs a real-time data ingestion solution on AWS. The solution consists of the most recent version of Amazon Managed Streaming for Apache Kafka (Amazon MSK). The solution is deployed in a VPC in private subnets across three Availability Zones. A solutions architect needs to redesign the data ingestion solution to be publicly available over the internet. The data in transit must also be encrypted. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Configure public subnets in the existing VPC. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication.",
      "B": "Create a new VPC that has public subnets. Deploy an MSK cluster in the public subnets. Update the MSK cluster security settings to enable mutual TLS authentication.",
      "C": "Deploy an Application Load Balancer (ALB) that uses private subnets. Configure an ALB security group inbound rule to allow inbound trafic from the VPC CIDR block for HTTPS protocol.",
      "D": "Deploy a Network Load Balancer (NLB) that uses private subnets. Configure an NLB listener for HTTPS communication over the internet."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 上运行实时数据摄取解决方案。该解决方案包含最新版本的 Amazon Managed Streaming for Apache Kafka (Amazon MSK)。该解决方案部署在跨三个可用区的私有子网中的 VPC 中。解决方案架构师需要重新设计数据摄取解决方案，使其可以通过 Internet 公开访问。传输中的数据也必须加密。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "在现有的 VPC 中配置公共子网。在公共子网中部署 MSK 集群。更新 MSK 集群安全设置以启用相互 TLS 身份验证。",
      "B": "创建一个具有公共子网的新 VPC。在公共子网中部署 MSK 集群。更新 MSK 集群安全设置以启用相互 TLS 身份验证。",
      "C": "部署一个使用私有子网的 Application Load Balancer (ALB)。配置 ALB 安全组入站规则，以允许来自 VPC CIDR 块的 HTTPS 协议的入站流量。",
      "D": "部署一个使用私有子网的 Network Load Balancer (NLB)。为通过 Internet 的 HTTPS 通信配置 NLB 侦听器。"
    },
    "tags": [
      "MSK",
      "ALB",
      "NLB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查通过 Internet 访问 MSK 集群，以及数据传输加密的解决方案。需要考虑 VPC 配置、安全性和运营效率。",
      "why_correct": "选项 A 在现有 VPC 中创建公共子网，允许 MSK 集群通过 Internet 访问。通过相互 TLS (mTLS) 身份验证，确保传输中数据的加密和身份验证。这种方法避免了创建新的 VPC，提高了运营效率。",
      "why_wrong": "选项 B 涉及创建一个新的 VPC，这增加了配置的复杂性和管理开销，降低了运营效率。选项 C 使用 ALB，ALB 通常用于 HTTP/HTTPS 流量，而 MSK 使用 Kafka 协议，ALB 在此场景下不适用。选项 D 使用 NLB，NLB 能够支持 TCP 流量，但配置 HTTPS 侦听器并不能实现 MSK 所需的加密和身份验证。NLB 本身不提供 mTLS 功能。"
    },
    "related_terms": [
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK)",
      "VPC",
      "Public Subnet",
      "Private Subnet",
      "Internet",
      "mTLS",
      "Application Load Balancer (ALB)",
      "Network Load Balancer (NLB)",
      "HTTPS",
      "TLS",
      "CIDR"
    ]
  },
  {
    "id": 717,
    "topic": "1",
    "question_en": "A company wants to migrate an on-premises legacy application to AWS. The application ingests customer order files from an on-premises enterprise resource planning (ERP) system. The application then uploads the files to an SFTP server. The application uses a scheduled job that checks for order files every hour. The company already has an AWS account that has connectivity to the on-premises network. The new application on AWS must support integration with the existing ERP system. The new application must be secure and resilient and must use the SFTP protocol to process orders from the ERP system immediately. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an AWS Transfer Family SFTP internet-facing server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use S3 Event Notifications to send s3:ObjectCreated:* events to the Lambda function.",
      "B": "Create an AWS Transfer Family SFTP internet-facing server in one Availability Zone. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workfiow to invoke the Lambda function.",
      "C": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon Elastic File System (Amazon EFS) storage. Create an AWS Step Functions state machine to process order files. Use Amazon EventBridge Scheduler to invoke the state machine to periodically check Amazon EFS for order files.",
      "D": "Create an AWS Transfer Family SFTP internal server in two Availability Zones. Use Amazon S3 storage. Create an AWS Lambda function to process order files. Use a Transfer Family managed workfiow to invoke the Lambda function."
    },
    "correct_answer": "D",
    "vote_percentage": "76%",
    "question_cn": "一家公司希望将本地遗留应用程序迁移到 AWS。该应用程序从本地企业资源规划 (ERP) 系统摄取客户订单文件。然后，该应用程序将文件上传到 SFTP 服务器。该应用程序使用一个计划作业，每小时检查一次订单文件。该公司已经拥有一个 AWS 账户，该账户已连接到本地网络。AWS 上的新应用程序必须支持与现有 ERP 系统的集成。新应用程序必须安全且具有弹性，并且必须使用 SFTP 协议立即处理来自 ERP 系统的订单。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在两个可用区中创建一个面向互联网的 AWS Transfer Family SFTP 服务器。使用 Amazon S3 存储。创建 AWS Lambda 函数来处理订单文件。使用 S3 事件通知将 s3:ObjectCreated:* 事件发送到 Lambda 函数。",
      "B": "在一个可用区中创建一个面向互联网的 AWS Transfer Family SFTP 服务器。使用 Amazon Elastic File System (Amazon EFS) 存储。创建 AWS Lambda 函数来处理订单文件。使用 Transfer Family 托管工作流来调用 Lambda 函数。",
      "C": "在两个可用区中创建一个内部 AWS Transfer Family SFTP 服务器。使用 Amazon Elastic File System (Amazon EFS) 存储。创建 AWS Step Functions 状态机来处理订单文件。使用 Amazon EventBridge Scheduler 定期间隔调用状态机来检查 Amazon EFS 中的订单文件。",
      "D": "在两个可用区中创建一个内部 AWS Transfer Family SFTP 服务器。使用 Amazon S3 存储。创建 AWS Lambda 函数来处理订单文件。使用 Transfer Family 托管工作流来调用 Lambda 函数。"
    },
    "tags": [
      "Transfer Family",
      "S3",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 76%），解析仅供参考。】\n\n考查了使用 AWS Transfer Family 构建 SFTP 服务器，结合 Lambda 函数和 S3/EFS 存储，实现文件处理和集成的解决方案。",
      "why_correct": "选项 D 提供了最佳的解决方案。使用内部 AWS Transfer Family SFTP 服务器，确保了安全性，并且可以与本地网络连接。结合 S3 存储，提供了高可用性和弹性。Transfer Family 托管工作流允许立即处理来自 ERP 系统的订单，满足了题目的实时性要求，并调用 Lambda 函数进行处理。",
      "why_wrong": "选项 A 错误在于，虽然使用了 S3 和 Lambda，但面向互联网的 SFTP 服务器可能不符合题目的安全要求，并且需要外部 IP 地址。选项 B 错误，因为使用 Amazon EFS 虽然易于访问，但相对于 S3 来说，在可用性和扩展性上可能较弱，不适合作为高可用解决方案。选项 C 错误，它使用了 Amazon EFS 存储，并且使用 EventBridge Scheduler 定期轮询检查，这不如 Transfer Family 的托管工作流更高效，并且使用 Step Functions 增加了解决方案的复杂性，不如 Lambda 更适合文件处理。"
    },
    "related_terms": [
      "AWS Transfer Family",
      "SFTP",
      "Amazon S3",
      "AWS Lambda",
      "S3 Event Notifications",
      "Amazon Elastic File System (Amazon EFS)",
      "Transfer Family managed workflows",
      "AWS Step Functions",
      "Amazon EventBridge Scheduler",
      "ERP"
    ]
  },
  {
    "id": 718,
    "topic": "1",
    "question_en": "A company’s applications use Apache Hadoop and Apache Spark to process data on premises. The existing infrastructure is not scalable and is complex to manage. A solutions architect must design a scalable solution that reduces operational complexity. The solution must keep the data processing on premises. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Site-to-Site VPN to access the on-premises Hadoop Distributed File System (HDFS) data and application. Use an Amazon EMR cluster to process the data.",
      "B": "Use AWS DataSync to connect to the on-premises Hadoop Distributed File System (HDFS) cluster. Create an Amazon EMR cluster to process the data.",
      "C": "Migrate the Apache Hadoop application and the Apache Spark application to Amazon EMR clusters on AWS Outposts. Use the EMR clusters to process the data.",
      "D": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Create an Amazon EMR cluster to process the data."
    },
    "correct_answer": "C",
    "vote_percentage": "83%",
    "question_cn": "一家公司的应用程序使用 Apache Hadoop 和 Apache Spark 在本地处理数据。现有的基础设施不可扩展且难以管理。一位解决方案架构师必须设计一个可扩展的解决方案，以降低运营复杂性。该解决方案必须在本地保留数据处理。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Site-to-Site VPN 访问本地 Hadoop 分布式文件系统 (HDFS) 数据和应用程序。使用 Amazon EMR 集群处理数据。",
      "B": "使用 AWS DataSync 连接到本地 Hadoop 分布式文件系统 (HDFS) 集群。创建 Amazon EMR 集群来处理数据。",
      "C": "将 Apache Hadoop 应用程序和 Apache Spark 应用程序迁移到 AWS Outposts 上的 Amazon EMR 集群。使用 EMR 集群处理数据。",
      "D": "使用 AWS Snowball 设备将数据迁移到 Amazon S3 存储桶。创建 Amazon EMR 集群来处理数据。"
    },
    "tags": [
      "EMR",
      "Hadoop",
      "Spark",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 83%），解析仅供参考。】\n\n考查在本地保留数据处理的情况下，如何利用 AWS 服务实现可扩展的解决方案，并降低运营复杂性。",
      "why_correct": "选项 C 提供了在本地保留数据处理的解决方案，满足了题目需求。AWS Outposts 允许在客户的本地数据中心部署 AWS 基础设施，其中包括 Amazon EMR。通过在 Outposts 上运行 EMR 集群，可以利用 EMR 的可扩展性和易管理性，同时满足数据本地处理的需求。",
      "why_wrong": "选项 A 使用 Site-to-Site VPN 连接到本地 HDFS，虽然实现了数据访问，但并未提供本地计算资源，无法满足本地数据处理的需求。选项 B 使用 DataSync 传输数据，同样没有提供本地计算资源。选项 D 将数据迁移到 S3，违反了数据本地处理的要求，且 Snowball 更多用于大批量数据的迁移，并不适合持续的本地数据处理场景。"
    },
    "related_terms": [
      "Apache Hadoop",
      "Apache Spark",
      "AWS Site-to-Site VPN",
      "Amazon EMR",
      "HDFS",
      "AWS DataSync",
      "AWS Outposts",
      "Amazon S3",
      "AWS Snowball"
    ]
  },
  {
    "id": 719,
    "topic": "1",
    "question_en": "A company is migrating a large amount of data from on-premises storage to AWS. Windows, Mac, and Linux based Amazon EC2 instances in the same AWS Region will access the data by using SMB and NFS storage protocols. The company will access a portion of the data routinely. The company will access the remaining data infrequently. The company needs to design a solution to host the data. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon Elastic File System (Amazon EFS) volume that uses EFS Intelligent-Tiering. Use AWS DataSync to migrate the data to the EFS volume.",
      "B": "Create an Amazon FSx for ONTAP instance. Create an FSx for ONTAP file system with a root volume that uses the auto tiering policy. Migrate the data to the FSx for ONTAP volume.",
      "C": "Create an Amazon S3 bucket that uses S3 Intelligent-Tiering. Migrate the data to the S3 bucket by using an AWS Storage Gateway Amazon S3 File Gateway.",
      "D": "Create an Amazon FSx for OpenZFS file system. Migrate the data to the new volume."
    },
    "correct_answer": "B",
    "vote_percentage": "70%",
    "question_cn": "一家公司正在将大量数据从本地存储迁移到 AWS。 位于同一 AWS 区域的基于 Windows、Mac 和 Linux 的 Amazon EC2 实例将通过 SMB 和 NFS 存储协议访问数据。该公司将定期访问部分数据。该公司将不经常访问剩余的数据。该公司需要设计一个解决方案来托管数据。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个使用 EFS Intelligent-Tiering 的 Amazon Elastic File System (Amazon EFS) 卷。使用 AWS DataSync 将数据迁移到 EFS 卷。",
      "B": "创建 Amazon FSx for ONTAP 实例。创建一个 FSx for ONTAP 文件系统，其根卷使用自动分层策略。将数据迁移到 FSx for ONTAP 卷。",
      "C": "创建一个使用 S3 Intelligent-Tiering 的 Amazon S3 存储桶。使用 AWS Storage Gateway Amazon S3 File Gateway 将数据迁移到 S3 存储桶。",
      "D": "创建一个 Amazon FSx for OpenZFS 文件系统。将数据迁移到新卷。"
    },
    "tags": [
      "FSx for ONTAP",
      "S3",
      "EFS",
      "DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 70%），解析仅供参考。】\n\n考查了在 AWS 上托管具有不同访问模式的数据的解决方案，包括存储协议、成本优化和运营开销。",
      "why_correct": "Amazon FSx for ONTAP 提供了与 SMB 和 NFS 协议的兼容性，满足了题目的要求。FSx for ONTAP 的自动分层策略可以根据访问频率，在性能层和容量层之间自动移动数据，实现成本优化。这种解决方案能够以最少的运营开销，满足数据存储的需求。",
      "why_wrong": "选项 A 使用 EFS Intelligent-Tiering，虽然适用于存储成本优化，但 EFS 主要针对 Linux 工作负载，SMB 访问性能不如 FSx for ONTAP，且使用 DataSync 迁移数据增加了运营复杂性。选项 C 使用 S3 和 Storage Gateway，虽然价格更低，但其性能和访问延迟不如 FSx for ONTAP，并且文件网关的部署和维护增加了运营开销。选项 D 使用 FSx for OpenZFS，虽然 OpenZFS 支持 NFS，但其自动分层功能不如 FSx for ONTAP 的完善，可能无法实现最佳的成本优化，并且无法直接支持 SMB 协议。"
    },
    "related_terms": [
      "Amazon EC2",
      "SMB",
      "NFS",
      "Amazon EFS",
      "EFS Intelligent-Tiering",
      "AWS DataSync",
      "Amazon FSx for ONTAP",
      "FSx for ONTAP",
      "Amazon S3",
      "S3 Intelligent-Tiering",
      "AWS Storage Gateway",
      "Amazon S3 File Gateway",
      "Amazon FSx for OpenZFS",
      "OpenZFS"
    ]
  },
  {
    "id": 720,
    "topic": "1",
    "question_en": "A manufacturing company runs its report generation application on AWS. The application generates each report in about 20 minutes. The application is built as a monolith that runs on a single Amazon EC2 instance. The application requires frequent updates to its tightly coupled modules. The application becomes complex to maintain as the company adds new features. Each time the company patches a software module, the application experiences downtime. Report generation must restart from the beginning after any interruptions. The company wants to redesign the application so that the application can be fiexible, scalable, and gradually improved. The company wants to minimize application downtime. Which solution will meet these requirements?",
    "options_en": {
      "A": "Run the application on AWS Lambda as a single function with maximum provisioned concurrency.",
      "B": "Run the application on Amazon EC2 Spot Instances as microservices with a Spot Fleet default allocation strategy.",
      "C": "Run the application on Amazon Elastic Container Service (Amazon ECS) as microservices with service auto scaling.",
      "D": "Run the application on AWS Elastic Beanstalk as a single application environment with an all-at-once deployment strategy."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家制造公司在 AWS 上运行其报告生成应用程序。该应用程序大约需要 20 分钟生成每个报告。该应用程序被构建为一个整体，在单个 Amazon EC2 实例上运行。该应用程序需要频繁更新其紧密耦合的模块。当公司添加新功能时，该应用程序变得难以维护。每次公司修补一个软件模块时，该应用程序都会经历停机时间。报告生成在任何中断后都必须从头开始。公司希望重新设计应用程序，以便应用程序可以灵活、可扩展并逐步改进。公司希望最大限度地减少应用程序停机时间。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Lambda 上将应用程序作为具有最大预置并发的单个函数运行。",
      "B": "在 Amazon EC2 Spot Instances 上将应用程序作为微服务运行，并使用 Spot Fleet 默认分配策略。",
      "C": "在 Amazon Elastic Container Service (Amazon ECS) 上将应用程序作为微服务运行，并使用服务自动伸缩。",
      "D": "在 AWS Elastic Beanstalk 上将应用程序作为具有一次性部署策略的单个应用程序环境运行。"
    },
    "tags": [
      "EC2",
      "ECS",
      "Lambda",
      "Elastic Beanstalk",
      "Spot Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察如何将单体应用分解为微服务，并提高应用程序的弹性和可扩展性，同时减少停机时间。",
      "why_correct": "Amazon ECS 允许将应用程序分解为微服务，每个服务都可以独立部署和扩展。服务自动伸缩可以根据负载变化自动调整容器数量，确保应用程序的可用性和弹性。通过滚动更新等部署策略，可以实现无停机时间的更新。",
      "why_wrong": "A 选项，AWS Lambda 更适合短时间、事件驱动的函数执行，不适合长时间运行的报告生成任务，并且预置并发无法完全解决冷启动问题。B 选项，Amazon EC2 Spot Instances 虽然成本低廉，但容易被中断，不适合对报告生成时间有要求的应用。D 选项，AWS Elastic Beanstalk 适用于部署单体应用程序，不适合微服务架构，而且一次性部署策略会导致停机时间。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Lambda",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon EC2 Spot Instances",
      "Spot Fleet",
      "AWS Elastic Beanstalk",
      "Service Auto Scaling"
    ]
  },
  {
    "id": 721,
    "topic": "1",
    "question_en": "A company wants to rearchitect a large-scale web application to a serverless microservices architecture. The application uses Amazon EC2 instances and is written in Python. The company selected one component of the web application to test as a microservice. The component supports hundreds of requests each second. The company wants to create and test the microservice on an AWS solution that supports Python. The solution must also scale automatically and require minimal infrastructure and minimal operational support. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use a Spot Fleet with auto scaling of EC2 instances that run the most recent Amazon Linux operating system.",
      "B": "Use an AWS Elastic Beanstalk web server environment that has high availability configured.",
      "C": "Use Amazon Elastic Kubernetes Service (Amazon EKS). Launch Auto Scaling groups of self-managed EC2 instances.",
      "D": "Use an AWS Lambda function that runs custom developed code."
    },
    "correct_answer": "D",
    "vote_percentage": "80%",
    "question_cn": "一家公司希望将大规模 Web 应用程序重新架构为无服务器微服务架构。该应用程序使用 Amazon EC2 实例，并使用 Python 编写。该公司选择 Web 应用程序的一个组件作为微服务进行测试。该组件每秒支持数百个请求。该公司希望在支持 Python 的 AWS 解决方案上创建和测试该微服务。该解决方案还必须自动扩展，并需要最少的 infrastructure 和最少的运营支持。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Spot Fleet 和 EC2 实例的自动伸缩，运行最新的 Amazon Linux 操作系统。",
      "B": "使用配置了高可用性的 AWS Elastic Beanstalk Web 服务器环境。",
      "C": "使用 Amazon Elastic Kubernetes Service (Amazon EKS)。启动自管理的 EC2 实例的 Auto Scaling 组。",
      "D": "使用运行自定义开发代码的 AWS Lambda 函数。"
    },
    "tags": [
      "EC2",
      "EKS",
      "Elastic Beanstalk",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 80%），解析仅供参考。】\n\n考查无服务器微服务架构的构建，以及针对 Python 应用的 AWS 服务选择，包括自动扩展、最小化基础设施和运营支持等需求。",
      "why_correct": "AWS Lambda 是一种无服务器计算服务，非常适合运行微服务。Lambda 函数可以自动扩展，以满足每秒数百个请求的需求，并支持 Python 运行时环境。Lambda 提供了最少的基础设施管理，因为 AWS 负责服务器的provisioning、管理和扩展，从而最大程度地减少了运营支持。",
      "why_wrong": "选项 A 使用 EC2 实例，需要手动管理服务器和操作系统，不符合最小化基础设施和运营支持的要求。选项 B 使用 Elastic Beanstalk，虽然简化了部署，但仍然需要管理底层 EC2 实例，并且不如 Lambda 具有更精细的自动扩展能力。选项 C 使用 Amazon EKS，虽然提供了容器编排能力，但需要管理 Kubernetes 集群，增加了复杂性，不符合最小化运营支持的要求，并且扩展性不如 Lambda 方便。"
    },
    "related_terms": [
      "AWS Lambda",
      "EC2",
      "Spot Fleet",
      "Amazon Linux",
      "Elastic Beanstalk",
      "Amazon EKS",
      "Auto Scaling",
      "Python"
    ]
  },
  {
    "id": 722,
    "topic": "1",
    "question_en": "A company has an AWS Direct Connect connection from its on-premises location to an AWS account. The AWS account has 30 different VPCs in the same AWS Region. The VPCs use private virtual interfaces (VIFs). Each VPC has a CIDR block that does not overlap with other networks under the company's control. The company wants to centrally manage the networking architecture while still allowing each VPC to communicate with all other VPCs and on- premises networks. Which solution will meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Create a transit gateway, and associate the Direct Connect connection with a new transit VIF. Turn on the transit gateway's route propagation feature.",
      "B": "Create a Direct Connect gateway. Recreate the private VIFs to use the new gateway. Associate each VPC by creating new virtual private gateways.",
      "C": "Create a transit VPConnect the Direct Connect connection to the transit VPCreate a peering connection between all other VPCs in the Region. Update the route tables.",
      "D": "Create AWS Site-to-Site VPN connections from on premises to each VPC. Ensure that both VPN tunnels are UP for each connection. Turn on the route propagation feature."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司通过 AWS Direct Connect 连接将其本地位置连接到 AWS 账户。AWS 账户在同一 AWS 区域中拥有 30 个不同的 VPC。这些 VPC 使用私有虚拟接口 (VIF)。每个 VPC 都有一个 CIDR 块，该块与公司控制下的其他网络不重叠。该公司希望集中管理网络架构，同时仍然允许每个 VPC 与所有其他 VPC 和本地网络通信。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Transit Gateway，并将 Direct Connect 连接与新的 Transit VIF 关联。打开 Transit Gateway 的路由传播功能。",
      "B": "创建一个 Direct Connect 网关。重新创建私有 VIF 以使用新网关。通过创建新的虚拟私有网关来关联每个 VPC。",
      "C": "创建一个 Transit VPC，并将 Direct Connect 连接到 Transit VPC。在区域内的所有其他 VPC 之间创建对等连接。更新路由表。",
      "D": "从本地到每个 VPC 创建 AWS Site-to-Site VPN 连接。确保每个连接的两个 VPN 隧道都处于 UP 状态。打开路由传播功能。"
    },
    "tags": [
      "Direct Connect",
      "VPC",
      "Transit Gateway",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考察使用 Transit Gateway 集中管理 VPC 之间及 VPC 与本地网络通信的网络架构。要求以最少的运营开销实现连接。",
      "why_correct": "Transit Gateway 是一种网络传输中心，可以简化 VPC 之间的连接以及与本地网络的连接。将 Direct Connect 与 Transit VIF 关联，并启用路由传播，可以自动将路由信息传播到所有连接的 VPC，实现集中管理，并减少运营开销。满足题目要求，同时简化了网络管理。",
      "why_wrong": "B 选项使用 Direct Connect 网关和虚拟私有网关，增加了配置复杂性，需要为每个 VPC 创建和管理连接，运营开销较高，不符合题意。C 选项创建 Transit VPC 并使用 VPC 对等连接，需要手动配置和管理 VPC 对等连接，随着 VPC 数量的增加，管理复杂度会迅速上升。D 选项使用 Site-to-Site VPN，需要为每个 VPC 创建 VPN 连接，增加了配置和维护的复杂性，不符合最少运营开销的要求。"
    },
    "related_terms": [
      "AWS Direct Connect",
      "VPC",
      "Private VIF",
      "Transit Gateway",
      "Transit VIF",
      "Direct Connect Gateway",
      "Virtual Private Gateway",
      "VPC Peering",
      "Site-to-Site VPN",
      "CIDR"
    ]
  },
  {
    "id": 723,
    "topic": "1",
    "question_en": "A company has applications that run on Amazon EC2 instances. The EC2 instances connect to Amazon RDS databases by using an IAM role that has associated policies. The company wants to use AWS Systems Manager to patch the EC2 instances without disrupting the running applications. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a new IAM role. Attach the AmazonSSMManagedInstanceCore policy to the new IAM role. Attach the new IAM role to the EC2 instances and the existing IAM role.",
      "B": "Create an IAM user. Attach the AmazonSSMManagedInstanceCore policy to the IAM user. Configure Systems Manager to use the IAM user to manage the EC2 instances.",
      "C": "Enable Default Host Configuration Management in Systems Manager to manage the EC2 instances.",
      "D": "Remove the existing policies from the existing IAM role. Add the AmazonSSMManagedInstanceCore policy to the existing IAM role."
    },
    "correct_answer": "C",
    "vote_percentage": "60%",
    "question_cn": "一家公司拥有运行在 Amazon EC2 实例上的应用程序。EC2 实例通过使用具有关联策略的 IAM 角色连接到 Amazon RDS 数据库。该公司希望使用 AWS Systems Manager 补丁 EC2 实例，而不会中断正在运行的应用程序。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 IAM 角色。将 AmazonSSMManagedInstanceCore 策略附加到新的 IAM 角色。将新的 IAM 角色附加到 EC2 实例和现有的 IAM 角色。",
      "B": "创建一个 IAM 用户。将 AmazonSSMManagedInstanceCore 策略附加到 IAM 用户。配置 Systems Manager 以使用 IAM 用户来管理 EC2 实例。",
      "C": "在 Systems Manager 中启用默认主机配置管理来管理 EC2 实例。",
      "D": "从现有 IAM 角色中删除现有策略。将 AmazonSSMManagedInstanceCore 策略添加到现有的 IAM 角色。"
    },
    "tags": [
      "EC2",
      "RDS",
      "IAM",
      "Systems Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 60%），解析仅供参考。】\n\n题目考察了使用 AWS Systems Manager (SSM) 补丁 EC2 实例，同时保持应用程序的正常运行。SSM 需要 IAM 权限才能管理 EC2 实例。确保 IAM 权限正确配置，并结合 SSM 的功能，可以在不中断应用程序的情况下进行补丁操作。",
      "why_correct": "选项 C，在 Systems Manager 中启用默认主机配置管理来管理 EC2 实例。启用默认主机配置管理，可以让 SSM 自动管理 EC2 实例的补丁，而无需手动配置 IAM 角色，能够满足题目中的需求，并且运营开销最小。",
      "why_wrong": "A 选项，为 EC2 实例创建新的 IAM 角色，增加了复杂性，且可能产生权限问题；B 选项，IAM 用户不用于管理 EC2 实例；D 选项，直接修改现有的 IAM 角色可能导致权限配置错误。"
    },
    "related_terms": [
      "EC2",
      "RDS",
      "IAM",
      "Systems Manager",
      "IAM Role"
    ]
  },
  {
    "id": 724,
    "topic": "1",
    "question_en": "A company runs container applications by using Amazon Elastic Kubernetes Service (Amazon EKS) and the Kubernetes Horizontal Pod Autoscaler. The workload is not consistent throughout the day. A solutions architect notices that the number of nodes does not automatically scale out when the existing nodes have reached maximum capacity in the cluster, which causes performance issues. Which solution will resolve this issue with the LEAST administrative overhead?",
    "options_en": {
      "A": "Scale out the nodes by tracking the memory usage.",
      "B": "Use the Kubernetes Cluster Autoscaler to manage the number of nodes in the cluster.",
      "C": "Use an AWS Lambda function to resize the EKS cluster automatically.",
      "D": "Use an Amazon EC2 Auto Scaling group to distribute the workload."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 Kubernetes Horizontal Pod Autoscaler 运行容器应用程序。工作负载在一天中并不一致。一位解决方案架构师注意到，当现有节点已达到集群中的最大容量时，节点的数量不会自动横向扩展，这会导致性能问题。哪个解决方案将以最少的管理开销解决此问题？",
    "options_cn": {
      "A": "通过跟踪内存使用情况来横向扩展节点。",
      "B": "使用 Kubernetes 集群自动伸缩器来管理集群中的节点数量。",
      "C": "使用 AWS Lambda 函数自动调整 EKS 集群的大小。",
      "D": "使用 Amazon EC2 Auto Scaling 组来分发工作负载。"
    },
    "tags": [
      "EKS",
      "Kubernetes",
      "Auto Scaling",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n题目考察了如何在 EKS 集群中进行自动伸缩，以应对动态的工作负载。Kubernetes 集群自动伸缩器是管理节点数量的最佳方式。其他选项可能需要额外的管理和配置。",
      "why_correct": "选项 B，使用 Kubernetes 集群自动伸缩器来管理集群中的节点数量。Kubernetes 集群自动伸缩器是 Kubernetes 提供的原生功能，用于根据资源需求自动伸缩节点，是 EKS 集群的推荐方案，最符合题目需求，且管理开销最少。",
      "why_wrong": "A 选项，通过跟踪内存使用情况来横向扩展节点。虽然可以使用，但是不如 Kubernetes 集群自动伸缩器方便；C 选项，使用 AWS Lambda 函数自动调整 EKS 集群的大小。实现起来较为复杂，且需要额外的管理；D 选项，使用 Amazon EC2 Auto Scaling 组来分发工作负载。EC2 Auto Scaling 组在 EKS 中不是直接用来管理节点数量的。"
    },
    "related_terms": [
      "EKS",
      "Kubernetes",
      "Auto Scaling",
      "Lambda",
      "EC2 Auto Scaling Group"
    ]
  },
  {
    "id": 725,
    "topic": "1",
    "question_en": "A company maintains about 300 TB in Amazon S3 Standard storage month after month. The S3 objects are each typically around 50 GB in size and are frequently replaced with multipart uploads by their global application. The number and size of S3 objects remain constant, but the company's S3 storage costs are increasing each month. How should a solutions architect reduce costs in this situation?",
    "options_en": {
      "A": "Switch from multipart uploads to Amazon S3 Transfer Acceleration.",
      "B": "Enable an S3 Lifecycle policy that deletes incomplete multipart uploads.",
      "C": "Configure S3 inventory to prevent objects from being archived too quickly.",
      "D": "Configure Amazon CloudFront to reduce the number of objects stored in Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司每月在 Amazon S3 标准存储中维护约 300 TB 的数据。S3 对象的大小通常约为 50 GB，并且由其全球应用程序使用 multipart uploads 频繁替换。S3 对象的数量和大小保持不变，但公司的 S3 存储成本每月都在增加。解决方案架构师应如何在这种情况下降低成本？",
    "options_cn": {
      "A": "从 multipart uploads 切换到 Amazon S3 Transfer Acceleration。",
      "B": "打开一个 S3 生命周期策略，该策略将删除未完成的 multipart uploads。",
      "C": "配置 S3 inventory 以防止对象过快地被归档。",
      "D": "配置 Amazon CloudFront 以减少存储在 Amazon S3 中的对象数量。"
    },
    "tags": [
      "S3",
      "成本优化",
      "multipart uploads"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何通过 S3 生命周期策略管理未完成的 multipart uploads，以降低 S3 存储成本。",
      "why_correct": "选项 B 正确，因为频繁的 multipart uploads 可能导致未完成的上传对象占据存储空间，增加存储成本。S3 生命周期策略可以自动删除未完成的 multipart uploads，从而优化 S3 存储成本，符合题意中存储成本增加的情况。",
      "why_wrong": "选项 A 错误，Amazon S3 Transfer Acceleration 用于加速数据传输，对降低存储成本没有直接作用，与题目需求不符。选项 C 错误，S3 inventory 用于提供 S3 对象清单报告，防止对象被过快归档与降低存储成本无关。选项 D 错误，Amazon CloudFront 是内容分发网络（CDN），用于缓存内容，减少从 S3 读取的流量，与减少 S3 存储成本的解决方案不符，且题目中未提及访问流量的问题。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard",
      "multipart uploads",
      "S3 lifecycle policy",
      "Amazon S3 Transfer Acceleration",
      "S3 inventory",
      "Amazon CloudFront"
    ]
  },
  {
    "id": 726,
    "topic": "1",
    "question_en": "A company has deployed a multiplayer game for mobile devices. The game requires live location tracking of players based on latitude and longitude. The data store for the game must support rapid updates and retrieval of locations. The game uses an Amazon RDS for PostgreSQL DB instance with read replicas to store the location data. During peak usage periods, the database is unable to maintain the performance that is needed for reading and writing updates. The game's user base is increasing rapidly. What should a solutions architect do to improve the performance of the data tier?",
    "options_en": {
      "A": "Take a snapshot of the existing DB instance. Restore the snapshot with Multi-AZ enabled.",
      "B": "Migrate from Amazon RDS to Amazon OpenSearch Service with OpenSearch Dashboards.",
      "C": "Deploy Amazon DynamoDB Accelerator (DAX) in front of the existing DB instance. Modify the game to use DAX.",
      "D": "Deploy an Amazon ElastiCache for Redis cluster in front of the existing DB instance. Modify the game to use Redis."
    },
    "correct_answer": "D",
    "vote_percentage": "60%",
    "question_cn": "一家公司为移动设备部署了一款多人游戏。该游戏需要根据纬度和经度对玩家进行实时位置追踪。该游戏的数据存储必须支持对位置数据进行快速更新和检索。该游戏使用一个 Amazon RDS for PostgreSQL 数据库实例以及只读副本，来存储位置数据。在高峰使用时段，数据库无法保持读取和写入更新所需的性能。该游戏的玩家数量正在迅速增加。解决方案架构师应该怎么做来提高数据层的性能？",
    "options_cn": {
      "A": "拍摄现有数据库实例的快照。使用启用 Multi-AZ 的方式恢复该快照。",
      "B": "从 Amazon RDS 迁移到 Amazon OpenSearch Service，并使用 OpenSearch Dashboards。",
      "C": "在现有的数据库实例之前部署 Amazon DynamoDB Accelerator (DAX)。修改游戏以使用 DAX。",
      "D": "在现有的数据库实例之前部署 Amazon ElastiCache for Redis 集群。修改游戏以使用 Redis。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "DynamoDB",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 60%），解析仅供参考。】\n\n题目考察了如何提高数据库性能。对于需要快速更新和检索位置数据的多人游戏，数据库的性能至关重要。使用 ElastiCache 缓存可以有效减轻数据库的负载，从而提高性能。",
      "why_correct": "选项 D，在现有的数据库实例之前部署 Amazon ElastiCache for Redis 集群。修改游戏以使用 Redis。Redis 是一种内存中的数据存储，可以缓存经常访问的数据，提高数据库读取性能，尤其是在游戏场景中非常有效。",
      "why_wrong": "A 选项，拍摄现有数据库实例的快照。使用启用 Multi-AZ 的方式恢复该快照。Multi-AZ 主要是提供高可用性，对于性能提升有限；B 选项，从 Amazon RDS 迁移到 Amazon OpenSearch Service，并使用 OpenSearch Dashboards。OpenSearch 适合搜索和分析，不适合实时位置数据更新；C 选项，在现有的数据库实例之前部署 Amazon DynamoDB Accelerator (DAX)。修改游戏以使用 DAX。DAX 适用于 DynamoDB，不能用于 RDS for PostgreSQL。"
    },
    "related_terms": [
      "RDS",
      "PostgreSQL",
      "DynamoDB",
      "ElastiCache",
      "Redis",
      "DAX"
    ]
  },
  {
    "id": 727,
    "topic": "1",
    "question_en": "A company stores critical data in Amazon DynamoDB tables in the company's AWS account. An IT administrator accidentally deleted a DynamoDB table. The deletion caused a significant loss of data and disrupted the company's operations. The company wants to prevent this type of disruption in the future. Which solution will meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure a trail in AWS CloudTrail. Create an Amazon EventBridge rule for delete actions. Create an AWS Lambda function to automatically restore deleted DynamoDB tables.",
      "B": "Create a backup and restore plan for the DynamoDB tables. Recover the DynamoDB tables manually.",
      "C": "Configure deletion protection on the DynamoDB tables.",
      "D": "Enable point-in-time recovery on the DynamoDB tables."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 AWS 账户中将关键数据存储在 Amazon DynamoDB 表中。一位 IT 管理员意外删除了一个 DynamoDB 表。删除操作导致大量数据丢失，并中断了公司的运营。该公司希望在将来防止此类中断。以下哪种解决方案以最小的运营开销满足此要求？",
    "options_cn": {
      "A": "在 AWS CloudTrail 中配置一个追踪。为删除操作创建 Amazon EventBridge 规则。创建一个 AWS Lambda 函数以自动恢复已删除的 DynamoDB 表。",
      "B": "为 DynamoDB 表创建备份和恢复计划。手动恢复 DynamoDB 表。",
      "C": "在 DynamoDB 表上配置删除保护。",
      "D": "在 DynamoDB 表上打开时间点恢复。"
    },
    "tags": [
      "DynamoDB",
      "备份",
      "恢复",
      "CloudTrail"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查如何通过 DynamoDB 提供的功能来防止数据意外删除，以最小的运营开销满足业务需求。",
      "why_correct": "选项 C 正确，因为在 DynamoDB 表上配置删除保护能够阻止意外删除表的操作。这是 DynamoDB 内置的功能，无需额外的复杂配置，有效地降低了运营开销，并直接满足了题目的要求：防止数据丢失和运营中断。",
      "why_wrong": "选项 A 错误，因为它涉及 CloudTrail、EventBridge 和 Lambda 函数，需要更复杂的设置和维护，运营开销较高，不能满足“最小运营开销”的要求。选项 B 错误，因为它需要手动恢复，这违反了“最小运营开销”的要求，且恢复过程耗时较长，可能无法及时恢复数据。选项 D 错误，虽然时间点恢复可以用于数据恢复，但它并不能防止表的意外删除，无法满足题目中“防止此类中断”的要求。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "AWS CloudTrail",
      "Amazon EventBridge",
      "AWS Lambda",
      "Point-in-time recovery",
      "delete protection"
    ]
  },
  {
    "id": 728,
    "topic": "1",
    "question_en": "A company has an on-premises data center that is running out of storage capacity. The company wants to migrate its storage infrastructure to AWS while minimizing bandwidth costs. The solution must allow for immediate retrieval of data at no additional cost. How can these requirements be met?",
    "options_en": {
      "A": "Deploy Amazon S3 Glacier Vault and enable expedited retrieval. Enable provisioned retrieval capacity for the workload.",
      "B": "Deploy AWS Storage Gateway using cached volumes. Use Storage Gateway to store data in Amazon S3 while retaining copies of frequently accessed data subsets locally.",
      "C": "Deploy AWS Storage Gateway using stored volumes to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3.",
      "D": "Deploy AWS Direct Connect to connect with the on-premises data center. Configure AWS Storage Gateway to store data locally. Use Storage Gateway to asynchronously back up point-in-time snapshots of the data to Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "56%",
    "question_cn": "一家公司本地数据中心即将耗尽存储容量。该公司希望将其存储基础设施迁移到 AWS，同时最大限度地降低带宽成本。该解决方案必须允许立即检索数据，且不产生额外费用。如何满足这些要求？",
    "options_cn": {
      "A": "部署 Amazon S3 Glacier Vault，并启用加速检索。为工作负载启用预置检索容量。",
      "B": "使用缓存卷部署 AWS Storage Gateway。使用 Storage Gateway 将数据存储在 Amazon S3 中，同时在本地保留经常访问的数据子集的副本。",
      "C": "使用存储卷部署 AWS Storage Gateway 以在本地存储数据。使用 Storage Gateway 异步备份数据的即时点快照到 Amazon S3。",
      "D": "部署 AWS Direct Connect 以连接本地数据中心。配置 AWS Storage Gateway 以在本地存储数据。使用 Storage Gateway 异步备份数据的即时点快照到 Amazon S3。"
    },
    "tags": [
      "Storage Gateway",
      "S3",
      "数据迁移",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 56%），解析仅供参考。】\n\n题目考察了如何将本地存储迁移到 AWS，并最大限度地降低带宽成本。AWS Storage Gateway 提供了将本地数据与 Amazon S3 集成的解决方案，同时支持数据的缓存和即时访问。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B，使用缓存卷部署 AWS Storage Gateway。使用 Storage Gateway 将数据存储在 Amazon S3 中，同时在本地保留经常访问的数据子集的副本。此方案允许快速访问数据，且只存储常用数据，可以节省带宽成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，部署 Amazon S3 Glacier Vault，并启用加速检索。为工作负载启用预置检索容量。Glacier 主要用于归档数据，检索时间较长，不满足立即检索数据的需求；C 选项，使用存储卷部署 AWS Storage Gateway 以在本地存储数据。使用 Storage Gateway 异步备份数据的即时点快照到 Amazon S3。此方案数据存储在本地，不满足数据迁移到 AWS 的需求；D 选项，部署 AWS Direct Connect 以连接本地数据中心。配置 AWS Storage Gateway 以在本地存储数据。使用 Storage Gateway 异步备份数据的即时点快照到 Amazon S3。此方案需要 Direct Connect，增加了成本，且数据存储在本地，不满足数据迁移到 AWS 的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Storage Gateway",
      "S3",
      "Direct Connect"
    ]
  },
  {
    "id": 729,
    "topic": "1",
    "question_en": "A company runs a three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances run in an Auto Scaling group for the application tier. The company needs to make an automated scaling plan that will analyze each resource's daily and weekly historical workload trends. The configuration must scale resources appropriately according to both the forecast and live changes in utilization. Which scaling strategy should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Implement dynamic scaling with step scaling based on average CPU utilization from the EC2 instances.",
      "B": "Enable predictive scaling to forecast and scale. Configure dynamic scaling with target tracking",
      "C": "Create an automated scheduled scaling action based on the trafic patterns of the web application.",
      "D": "Set up a simple scaling policy. Increase the cooldown period based on the EC2 instance startup time."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其VPC中跨多个可用区运行一个三层Web应用程序。Amazon EC2实例在应用程序层的一个Auto Scaling组中运行。该公司需要制定一个自动化扩展计划，该计划将分析每个资源的每日和每周的历史工作负载趋势。该配置必须根据预测和利用率的实时变化来适当地扩展资源。解决方案架构师应该推荐哪种扩展策略来满足这些要求？",
    "options_cn": {
      "A": "实施基于EC2实例平均CPU利用率的阶梯式动态扩展。",
      "B": "启用预测式扩展以进行预测和扩展。使用目标跟踪配置动态扩展。",
      "C": "根据Web应用程序的流量模式创建自动计划的扩展操作。",
      "D": "设置一个简单的扩展策略。根据EC2实例启动时间增加冷却期。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "预测式扩展",
      "动态伸缩"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何通过 AWS Auto Scaling 实现基于历史工作负载趋势和实时变化的资源自动扩展，并结合预测功能。",
      "why_correct": "选项 B 结合了预测式扩展和目标跟踪的动态扩展策略。预测式扩展能够分析历史工作负载趋势，并根据预测提前调整 EC2 实例的数量。目标跟踪策略则能够根据实时的利用率变化动态调整实例数量，确保资源充分利用。",
      "why_wrong": "选项 A 仅基于 EC2 实例平均 CPU 利用率，没有考虑历史趋势和预测，无法满足题目中对每日和每周工作负载趋势分析的要求。选项 C 仅基于预定义的计划，无法响应实时的利用率变化。选项 D 是一个简单的扩展策略，冷却期设置无法有效地预测和适应负载变化。"
    },
    "related_terms": [
      "VPC",
      "Amazon EC2",
      "Auto Scaling",
      "EC2",
      "CPU",
      "AWS",
      "Auto Scaling Group",
      "Predictive Scaling",
      "Target Tracking"
    ]
  },
  {
    "id": 730,
    "topic": "1",
    "question_en": "A package delivery company has an application that uses Amazon EC2 instances and an Amazon Aurora MySQL DB cluster. As the application becomes more popular, EC2 instance usage increases only slightly. DB cluster usage increases at a much faster rate. The company adds a read replica, which reduces the DB cluster usage for a short period of time. However, the load continues to increase. The operations that cause the increase in DB cluster usage are all repeated read statements that are related to delivery details. The company needs to alleviate the effect of repeated reads on the DB cluster. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Implement an Amazon ElastiCache for Redis cluster between the application and the DB cluster.",
      "B": "Add an additional read replica to the DB cluster.",
      "C": "Configure Aurora Auto Scaling for the Aurora read replicas.",
      "D": "Modify the DB cluster to have multiple writer instances."
    },
    "correct_answer": "A",
    "vote_percentage": "86%",
    "question_cn": "一家包裹递送公司有一个使用 Amazon EC2 实例和 Amazon Aurora MySQL 数据库集群的应用程序。随着应用程序越来越受欢迎，EC2 实例的使用量略有增加。数据库集群的使用量以更快的速度增加。该公司添加了一个只读副本，这在短时间内减少了数据库集群的使用量。然而，负载持续增加。导致数据库集群使用量增加的操作都是与递送详细信息相关的重复读取语句。该公司需要减轻重复读取对数据库集群的影响。哪种解决方案将最经济高效地满足这些要求？",
    "options_cn": {
      "A": "在应用程序和数据库集群之间实施一个 Amazon ElastiCache for Redis 集群。",
      "B": "为数据库集群添加一个额外的只读副本。",
      "C": "为 Aurora 只读副本配置 Aurora Auto Scaling。",
      "D": "修改数据库集群以具有多个写入实例。"
    },
    "tags": [
      "ElastiCache",
      "Redis",
      "Aurora",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 86%），解析仅供参考。】\n\n题目考察了如何优化数据库性能，尤其是解决重复读取的问题。ElastiCache for Redis 是一种内存缓存，可以有效缓解数据库的负载，从而提高读取性能。添加只读副本和配置 Aurora Auto Scaling 只能缓解部分问题，并不能根本性地解决重复读取问题。",
      "why_correct": "选项 A，在应用程序和数据库集群之间实施一个 Amazon ElastiCache for Redis 集群。Redis 可以缓存常用的数据，可以减少数据库的压力，从而提高读取性能，并且最经济高效。",
      "why_wrong": "B 选项，为数据库集群添加一个额外的只读副本。只读副本可以提高读取性能，但是并不能根本性解决重复读取导致的问题；C 选项，为 Aurora 只读副本配置 Aurora Auto Scaling。Aurora Auto Scaling 主要用于根据负载自动调整只读副本的数量，无法解决重复读取导致的问题；D 选项，修改数据库集群以具有多个写入实例。这会增加维护成本，并且对于解决重复读取问题效果有限。"
    },
    "related_terms": [
      "ElastiCache",
      "Redis",
      "Aurora",
      "MySQL"
    ]
  },
  {
    "id": 731,
    "topic": "1",
    "question_en": "A company has an application that uses an Amazon DynamoDB table for storage. A solutions architect discovers that many requests to the table are not returning the latest data. The company's users have not reported any other issues with database performance. Latency is in an acceptable range. Which design change should the solutions architect recommend?",
    "options_en": {
      "A": "Add read replicas to the table.",
      "B": "Use a global secondary index (GSI).",
      "C": "Request strongly consistent reads for the table.",
      "D": "Request eventually consistent reads for the table."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个应用程序，使用 Amazon DynamoDB 表进行存储。一位解决方案架构师发现，许多对该表的请求没有返回最新的数据。该公司用户没有报告数据库性能方面的任何其他问题。延迟在可接受的范围内。解决方案架构师应该建议哪种设计更改？",
    "options_cn": {
      "A": "为该表添加读取副本。",
      "B": "使用全局二级索引 (GSI)。",
      "C": "为该表请求强一致性读取。",
      "D": "为该表请求最终一致性读取。"
    },
    "tags": [
      "DynamoDB",
      "一致性",
      "读取"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n题目考察了 DynamoDB 的一致性读取。当用户未获得最新数据时，通常需要调整读取一致性。强一致性读取可以确保读取到最新数据，但会增加延迟。",
      "why_correct": "选项 C，为该表请求强一致性读取。强一致性读取保证返回最新数据，满足了用户获取不到最新数据的需求。",
      "why_wrong": "A 选项，为该表添加读取副本。读取副本主要用于提高读取吞吐量，不能确保数据一致性；B 选项，使用全局二级索引 (GSI)。GSI 用于加快不同属性的查询，对于确保一致性没有直接帮助；D 选项，为该表请求最终一致性读取。最终一致性读取可能会读取到旧的数据，不符合题目需求。"
    },
    "related_terms": [
      "DynamoDB",
      "GSI",
      "一致性",
      "读取"
    ]
  },
  {
    "id": 732,
    "topic": "1",
    "question_en": "A company has deployed its application on Amazon EC2 instances with an Amazon RDS database. The company used the principle of least privilege to configure the database access credentials. The company's security team wants to protect the application and the database from SQL injection and other web-based attacks. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use security groups and network ACLs to secure the database and application servers.",
      "B": "Use AWS WAF to protect the application. Use RDS parameter groups to configure the security settings.",
      "C": "Use AWS Network Firewall to protect the application and the database.",
      "D": "Use different database accounts in the application code for different functions. Avoid granting excessive privileges to the database users."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司已将其应用程序部署在带有 Amazon RDS 数据库的 Amazon EC2 实例上。该公司使用最小权限原则来配置数据库访问凭据。该公司的安全团队希望保护应用程序和数据库免受 SQL 注入和其他基于 Web 的攻击。哪种解决方案将以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用安全组和网络 ACL 来保护数据库和应用程序服务器。",
      "B": "使用 AWS WAF 保护应用程序。使用 RDS 参数组来配置安全设置。",
      "C": "使用 AWS Network Firewall 来保护应用程序和数据库。",
      "D": "在应用程序代码中使用不同的数据库账户，用于不同的功能。避免向数据库用户授予过多的权限。"
    },
    "tags": [
      "EC2",
      "RDS",
      "WAF",
      "安全组",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察使用 AWS WAF 防御 Web 应用程序攻击，并结合 RDS 配置安全设置的方案。",
      "why_correct": "AWS WAF 是一种 Web 应用程序防火墙，可以帮助保护应用程序免受常见的 Web 攻击，例如 SQL 注入。将其与 RDS 参数组结合使用，可以配置数据库的安全设置，从而满足最小运营开销的要求。通过 AWS WAF 和 RDS 参数组的组合，可以有效保护应用程序和数据库安全。",
      "why_wrong": "选项 A 使用安全组和网络 ACL，只能提供基本的网络层保护，无法有效防御 SQL 注入等 Web 应用程序攻击。选项 C 使用 AWS Network Firewall 虽然也可以提供安全保护，但其配置和管理复杂性较高，运营开销较大，不符合题干中“最小的运营开销”的要求。选项 D 建议在应用程序代码中实现权限控制，虽然有助于安全性，但主要职责是开发人员，与保护应用程序和数据库免受 SQL 注入的初衷不符，且操作开销也较大。"
    },
    "related_terms": [
      "Amazon RDS",
      "Amazon EC2",
      "SQL injection",
      "AWS WAF",
      "RDS parameter group",
      "Security Group",
      "Network ACL",
      "AWS Network Firewall"
    ]
  },
  {
    "id": 733,
    "topic": "1",
    "question_en": "An ecommerce company runs applications in AWS accounts that are part of an organization in AWS Organizations. The applications run on Amazon Aurora PostgreSQL databases across all the accounts. The company needs to prevent malicious activity and must identify abnormal failed and incomplete login attempts to the databases. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Attach service control policies (SCPs) to the root of the organization to identity the failed login attempts.",
      "B": "Enable the Amazon RDS Protection feature in Amazon GuardDuty for the member accounts of the organization.",
      "C": "Publish the Aurora general logs to a log group in Amazon CloudWatch Logs. Export the log data to a central Amazon S3 bucket.",
      "D": "Publish all the Aurora PostgreSQL database events in AWS CloudTrail to a central Amazon S3 bucket."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家电子商务公司在其 AWS 账户中运行应用程序，这些账户是 AWS Organizations 组织的一部分。应用程序在所有账户的 Amazon Aurora PostgreSQL 数据库上运行。该公司需要防止恶意活动，并且必须识别数据库中异常的登录尝试失败和未完成的登录尝试。哪种解决方案将以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "将服务控制策略 (SCP) 附加到组织的根目录，以识别登录尝试失败的情况。",
      "B": "在组织的成员账户中启用 Amazon GuardDuty 的 Amazon RDS 保护功能。",
      "C": "将 Aurora 常规日志发布到 Amazon CloudWatch Logs 中的一个日志组。将日志数据导出到中央 Amazon S3 存储桶。",
      "D": "将所有 Aurora PostgreSQL 数据库事件发布到 AWS CloudTrail 中，并将其发布到中央 Amazon S3 存储桶。"
    },
    "tags": [
      "Aurora",
      "GuardDuty",
      "CloudTrail",
      "Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n题目考察了监控和保护 Amazon Aurora 数据库。AWS GuardDuty 可以检测恶意活动和异常行为。CloudTrail 用于审计和日志记录，但 GuardDuty 提供了更便捷的解决方案，能够更有效地识别 Aurora 数据库的异常登录尝试。",
      "why_correct": "选项 B，在组织的成员账户中启用 Amazon GuardDuty 的 Amazon RDS 保护功能。GuardDuty 可以检测异常登录和恶意活动，是 Aurora 数据库的有效保护手段，可以满足题目需求。",
      "why_wrong": "A 选项，将服务控制策略 (SCP) 附加到组织的根目录，以识别登录尝试失败的情况。SCP 主要用于控制组织的权限，无法识别 Aurora 数据库的异常登录尝试；C 选项，将 Aurora 常规日志发布到 Amazon CloudWatch Logs 中的一个日志组。将日志数据导出到中央 Amazon S3 存储桶。日志收集和分析需要额外配置，且较为复杂；D 选项，将所有 Aurora PostgreSQL 数据库事件发布到 AWS CloudTrail 中，并将其发布到中央 Amazon S3 存储桶。虽然 CloudTrail 可以记录数据库事件，但配置起来比较复杂，且需要额外的分析。"
    },
    "related_terms": [
      "Aurora",
      "GuardDuty",
      "CloudTrail",
      "SCP",
      "RDS",
      "Organizations"
    ]
  },
  {
    "id": 734,
    "topic": "1",
    "question_en": "A company has an AWS Direct Connect connection from its corporate data center to its VPC in the us-east-1 Region. The company recently acquired a corporation that has several VPCs and a Direct Connect connection between its on-premises data center and the eu-west-2 Region. The CIDR blocks for the VPCs of the company and the corporation do not overlap. The company requires connectivity between two Regions and the data centers. The company needs a solution that is scalable while reducing operational overhead. What should a solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Set up inter-Region VPC peering between the VPC in us-east-1 and the VPCs in eu-west-2.",
      "B": "Create private virtual interfaces from the Direct Connect connection in us-east-1 to the VPCs in eu-west-2.",
      "C": "Establish VPN appliances in a fully meshed VPN network hosted by Amazon EC2. Use AWS VPN CloudHub to send and receive data between the data centers and each VPC.",
      "D": "Connect the existing Direct Connect connection to a Direct Connect gateway. Route trafic from the virtual private gateways of the VPCs in each Region to the Direct Connect gateway."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司通过 AWS Direct Connect 连接，将其企业数据中心连接到 us-east-1 区域的 VPC。该公司最近收购了一家公司，该公司拥有多个 VPC，并拥有一个 Direct Connect 连接，将其本地数据中心连接到 eu-west-2 区域。该公司和被收购公司的 VPC 的 CIDR 块不重叠。该公司需要在两个区域和数据中心之间建立连接。该公司需要一个可扩展的解决方案，同时减少运营开销。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在 us-east-1 中的 VPC 和 eu-west-2 中的 VPC 之间设置区域间 VPC 对等连接。",
      "B": "从 us-east-1 中的 Direct Connect 连接创建到 eu-west-2 中的 VPC 的私有虚拟接口。",
      "C": "在由 Amazon EC2 托管的全网状 VPN 网络中建立 VPN 设备。使用 AWS VPN CloudHub 在数据中心和每个 VPC 之间发送和接收数据。",
      "D": "将现有的 Direct Connect 连接到 Direct Connect 网关。将来自每个区域中 VPC 的虚拟私有网关的流量路由到 Direct Connect 网关。"
    },
    "tags": [
      "Direct Connect",
      "VPC",
      "Direct Connect Gateway",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n题目考察了跨区域连接 VPC 和数据中心。Direct Connect Gateway 提供了跨区域和跨账户连接的能力。相比之下，其他方案需要更多配置和管理。",
      "why_correct": "选项 D，将现有的 Direct Connect 连接到 Direct Connect 网关。将来自每个区域中 VPC 的虚拟私有网关的流量路由到 Direct Connect 网关。Direct Connect Gateway 提供了连接不同区域 VPC 的最佳方案，并且具有良好的扩展性。",
      "why_wrong": "A 选项，在 us-east-1 中的 VPC 和 eu-west-2 中的 VPC 之间设置区域间 VPC 对等连接。VPC 对等连接仅限于同一区域，不能跨区域；B 选项，从 us-east-1 中的 Direct Connect 连接创建到 eu-west-2 中的 VPC 的私有虚拟接口。私有虚拟接口连接到特定区域的 VPC，不能满足跨区域的需求；C 选项，在由 Amazon EC2 托管的全网状 VPN 网络中建立 VPN 设备。使用 AWS VPN CloudHub 在数据中心和每个 VPC 之间发送和接收数据。VPN 方案扩展性和维护成本较高。"
    },
    "related_terms": [
      "Direct Connect",
      "VPC",
      "Direct Connect Gateway",
      "VPN"
    ]
  },
  {
    "id": 735,
    "topic": "1",
    "question_en": "A company is developing a mobile game that streams score updates to a backend processor and then posts results on a leaderboard. A solutions architect needs to design a solution that can handle large trafic spikes, process the mobile game updates in order of receipt, and store the processed updates in a highly available database. The company also wants to minimize the management overhead required to maintain the solution. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Push score updates to Amazon Kinesis Data Streams. Process the updates in Kinesis Data Streams with AWS Lambda. Store the processed updates in Amazon DynamoDB.",
      "B": "Push score updates to Amazon Kinesis Data Streams. Process the updates with a fieet of Amazon EC2 instances set up for Auto Scaling. Store the processed updates in Amazon Redshift.",
      "C": "Push score updates to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to process the updates. Store the processed updates in a SQL database running on Amazon EC2.",
      "D": "Push score updates to an Amazon Simple Queue Service (Amazon SQS) queue. Use a fieet of Amazon EC2 instances with Auto Scaling to process the updates in the SQS queue. Store the processed updates in an Amazon RDS Multi-AZ DB instance."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在开发一款手机游戏，该游戏将分数更新流式传输到后端处理器，然后在排行榜上发布结果。一位解决方案架构师需要设计一个解决方案，该解决方案能够处理大型流量峰值，按接收顺序处理手机游戏更新，并将已处理的更新存储在一个高可用性数据库中。该公司还希望最大限度地减少维护该解决方案所需的管理开销。解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "将分数更新推送到 Amazon Kinesis Data Streams。 使用 AWS Lambda 处理 Kinesis Data Streams 中的更新。 将已处理的更新存储在 Amazon DynamoDB 中。",
      "B": "将分数更新推送到 Amazon Kinesis Data Streams。使用一组设置为自动扩缩的 Amazon EC2 实例来处理更新。将已处理的更新存储在 Amazon Redshift 中。",
      "C": "将分数更新推送到 Amazon Simple Notification Service (Amazon SNS) 主题。 将 AWS Lambda 函数订阅到 SNS 主题以处理更新。将已处理的更新存储在 Amazon EC2 上运行的 SQL 数据库中。",
      "D": "将分数更新推送到 Amazon Simple Queue Service (Amazon SQS) 队列。 使用一组具有自动扩缩的 Amazon EC2 实例来处理 SQS 队列中的更新。 将已处理的更新存储在 Amazon RDS 多可用区数据库实例中。"
    },
    "tags": [
      "Kinesis Data Streams",
      "Lambda",
      "DynamoDB",
      "SNS",
      "SQS",
      "EC2",
      "Redshift",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考察使用 AWS 服务构建流处理架构，满足高吞吐量、顺序处理、持久化存储以及最小化运维成本的需求。",
      "why_correct": "Amazon Kinesis Data Streams 可以处理高吞吐量的数据流，并保证数据的顺序。AWS Lambda 易于扩展，可以按需处理流数据，且是无服务器服务，减少运维成本。Amazon DynamoDB 是一个高可用、可扩展的 NoSQL 数据库，适合存储游戏分数等数据。",
      "why_wrong": "选项 B 中，使用 Amazon EC2 实例需要管理服务器，增加了运维负担，且 Amazon Redshift 主要用于数据仓库，不适合这种需要低延迟和频繁更新的应用。选项 C 中，Amazon SNS 主要用于发布-订阅模式，不保证消息的顺序，且使用 EC2 上运行的 SQL 数据库，增加了管理开销。选项 D 中，使用 Amazon SQS 队列可能无法保证消息的严格顺序，且需要 EC2 实例来处理消息，增加了管理负担。Amazon RDS 多可用区数据库实例虽然高可用，但管理开销较大。"
    },
    "related_terms": [
      "Amazon Kinesis Data Streams",
      "AWS Lambda",
      "Amazon DynamoDB",
      "Amazon EC2",
      "Amazon Redshift",
      "Amazon Simple Notification Service (Amazon SNS)",
      "SQL",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon RDS",
      "Auto Scaling"
    ]
  },
  {
    "id": 736,
    "topic": "1",
    "question_en": "A company has multiple AWS accounts with applications deployed in the us-west-2 Region. Application logs are stored within Amazon S3 buckets in each account. The company wants to build a centralized log analysis solution that uses a single S3 bucket. Logs must not leave us- west-2, and the company wants to incur minimal operational overhead. Which solution meets these requirements and is MOST cost-effective?",
    "options_en": {
      "A": "Create an S3 Lifecycle policy that copies the objects from one of the application S3 buckets to the centralized S3 bucket.",
      "B": "Use S3 Same-Region Replication to replicate logs from the S3 buckets to another S3 bucket in us-west-2. Use this S3 bucket for log analysis.",
      "C": "Write a script that uses the PutObject API operation every day to copy the entire contents of the buckets to another S3 bucket in us- west-2. Use this S3 bucket for log analysis.",
      "D": "Write AWS Lambda functions in these accounts that are triggered every time logs are delivered to the S3 buckets (s3:ObjectCreated:* event). Copy the logs to another S3 bucket in us-west-2. Use this S3 bucket for log analysis."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有多个 AWS 账户，其应用程序部署在 us-west-2 区域。应用程序日志存储在每个账户的 Amazon S3 存储桶中。该公司希望构建一个使用单个 S3 存储桶的集中式日志分析解决方案。日志不得离开 us-west-2，并且该公司希望产生最少的运营开销。以下哪个解决方案满足这些要求并且最具成本效益？",
    "options_cn": {
      "A": "创建一个 S3 生命周期策略，将对象从其中一个应用程序 S3 存储桶复制到集中式 S3 存储桶。",
      "B": "使用 S3 同区域复制将日志从 S3 存储桶复制到 us-west-2 中的另一个 S3 存储桶。使用此 S3 存储桶进行日志分析。",
      "C": "编写一个脚本，每天使用 PutObject API 操作将存储桶的全部内容复制到 us-west-2 中的另一个 S3 存储桶。使用此 S3 存储桶进行日志分析。",
      "D": "在这些账户中编写 AWS Lambda 函数，这些函数在每次将日志传递到 S3 存储桶（s3:ObjectCreated:* 事件）时触发。将日志复制到 us-west-2 中的另一个 S3 存储桶。使用此 S3 存储桶进行日志分析。"
    },
    "tags": [
      "S3",
      "S3 同区域复制",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 S3 存储桶的集中日志分析方案。选项 A 使用 S3 生命周期策略复制对象，实现成本较低，但是复制效率不高。选项 B 使用 S3 同区域复制，复制过程快速，并符合日志不离开 us-west-2 区域的要求。选项 C 使用脚本复制，需要手动维护，不具备自动化。选项 D 使用 Lambda 触发复制，增加了维护成本，且不够高效。",
      "why_correct": "选项 B 使用 S3 同区域复制，可以满足日志在同一区域的要求，并且通过 S3 的内置复制功能，无需编写额外的代码，具有成本效益和运维优势。",
      "why_wrong": "选项 A 错误在于，S3 生命周期策略的复制过程相对较慢，而且可能产生额外的成本。选项 C 错误在于，编写脚本复制的方式需要手动管理和维护，不具备自动化的特性。选项 D 错误在于，使用 Lambda 触发复制会增加额外的成本和复杂度，且性能不如 S3 的内置复制功能。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Lambda",
      "S3 同区域复制",
      "S3 存储桶",
      "us-west-2"
    ]
  },
  {
    "id": 737,
    "topic": "1",
    "question_en": "A company has an application that delivers on-demand training videos to students around the world. The application also allows authorized content developers to upload videos. The data is stored in an Amazon S3 bucket in the us-east-2 Region. The company has created an S3 bucket in the eu-west-2 Region and an S3 bucket in the ap-southeast-1 Region. The company wants to replicate the data to the new S3 buckets. The company needs to minimize latency for developers who upload videos and students who stream videos near eu-west-2 and ap-southeast-1. Which combination of steps will meet these requirements with the FEWEST changes to the application? (Choose two.)",
    "options_en": {
      "A": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the us-east-2 S3 bucket to the ap-southeast-1 S3 bucket.",
      "B": "Configure one-way replication from the us-east-2 S3 bucket to the eu-west-2 S3 bucket. Configure one-way replication from the eu-west- 2 S3 bucket to the ap-southeast-1 S3 bucket.",
      "C": "Configure two-way (bidirectional) replication among the S3 buckets that are in all three Regions.",
      "D": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming. Do not modify the application for video uploads",
      "E": "Create an S3 Multi-Region Access Point. Modify the application to use the Amazon Resource Name (ARN) of the Multi-Region Access Point for video streaming and uploads."
    },
    "correct_answer": "CE",
    "vote_percentage": "85%",
    "question_cn": "一家公司有一个应用程序，为世界各地的学生提供点播培训视频。该应用程序还允许授权内容开发人员上传视频。数据存储在 us-east-2 区域的 Amazon S3 存储桶中。该公司已在 eu-west-2 区域和 ap-southeast-1 区域创建了 S3 存储桶。该公司希望将数据复制到新的 S3 存储桶。该公司需要最大限度地减少靠近 eu-west-2 和 ap-southeast-1 的开发人员上传视频和学生流式传输视频的延迟。以下哪种组合步骤将以对应用程序的更改最少的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "配置从 us-east-2 S3 存储桶到 eu-west-2 S3 存储桶的单向复制。配置从 us-east-2 S3 存储桶到 ap-southeast-1 S3 存储桶的单向复制。",
      "B": "配置从 us-east-2 S3 存储桶到 eu-west-2 S3 存储桶的单向复制。配置从 eu-west-2 S3 存储桶到 ap-southeast-1 S3 存储桶的单向复制。",
      "C": "在所有三个区域的 S3 存储桶之间配置双向复制。",
      "D": "创建一个 S3 多区域访问点。修改应用程序以使用多区域访问点的 Amazon Resource Name (ARN) 进行视频流。不要修改应用程序进行视频上传。",
      "E": "创建一个 S3 多区域访问点。修改应用程序以使用多区域访问点的 Amazon Resource Name (ARN) 进行视频流和上传。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "多区域访问点"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CE（社区 85%），解析仅供参考。】\n\n考察 S3 跨区域复制和 S3 多区域访问点（Multi-Region Access Point）的配置，以及它们对应用程序延迟的影响。",
      "why_correct": "选项 C 通过在所有三个区域的 S3 存储桶之间配置双向复制，保证了所有区域的数据同步，从而降低了开发人员上传和学生观看视频的延迟。选项 E 创建了 S3 多区域访问点，并修改应用程序以同时使用多区域访问点进行视频流和上传，能够根据客户端的地理位置自动选择最近的 S3 存储桶，从而降低延迟。",
      "why_wrong": "选项 A 和 B 仅配置了单向复制，无法满足开发人员在所有区域上传视频的需求，并不能保证数据的实时同步。选项 D 仅修改了视频流，没有修改上传，导致开发人员上传的视频可能无法及时同步到其他区域的存储桶，无法完全降低延迟。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 bucket",
      "us-east-2",
      "eu-west-2",
      "ap-southeast-1",
      "S3 Multi-Region Access Point",
      "Amazon Resource Name (ARN)"
    ]
  },
  {
    "id": 738,
    "topic": "1",
    "question_en": "A company has a new mobile app. Anywhere in the world, users can see local news on topics they choose. Users also can post photos and videos from inside the app. Users access content often in the first minutes after the content is posted. New content quickly replaces older content, and then the older content disappears. The local nature of the news means that users consume 90% of the content within the AWS Region where it is uploaded. Which solution will optimize the user experience by providing the LOWEST latency for content uploads?",
    "options_en": {
      "A": "Upload and store content in Amazon S3. Use Amazon CloudFront for the uploads.",
      "B": "Upload and store content in Amazon S3. Use S3 Transfer Acceleration for the uploads.",
      "C": "Upload content to Amazon EC2 instances in the Region that is closest to the user. Copy the data to Amazon S3.",
      "D": "Upload and store content in Amazon S3 in the Region that is closest to the user. Use multiple distributions of Amazon CloudFront."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "题目 12\n题干: 一家公司有一个新的移动应用程序。在世界任何地方，用户都可以看到他们选择的本地新闻。用户还可以从应用程序内部发布照片和视频。用户经常在发布内容后的几分钟内访问内容。新内容会很快替换旧内容，然后旧内容会消失。新闻的本地性质意味着用户在上传内容的 AWS 区域内消耗了 90% 的内容。哪个解决方案将通过为内容上传提供最低的延迟来优化用户体验？\n选项:\n   A. 上传并将内容存储在 Amazon S3 中。使用 Amazon CloudFront 进行上传。\n   B. 上传并将内容存储在 Amazon S3 中。使用 S3 Transfer Acceleration 进行上传。\n   C. 将内容上传到最靠近用户的区域中的 Amazon EC2 实例。将数据复制到 Amazon S3。\n   D. 在最靠近用户的区域中的 Amazon S3 中上传和存储内容。使用 Amazon CloudFront 的多个分发。",
    "options_cn": {
      "A": "上传并将内容存储在 Amazon S3 中。使用 Amazon CloudFront 进行上传。",
      "B": "上传并将内容存储在 Amazon S3 中。使用 S3 Transfer Acceleration 进行上传。",
      "C": "将内容上传到最靠近用户的区域中的 Amazon EC2 实例。将数据复制到 Amazon S3。",
      "D": "在最靠近用户的区域中的 Amazon S3 中上传和存储内容。使用 Amazon CloudFront 的多个分发。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "S3 Transfer Acceleration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查如何通过优化上传延迟来提升用户体验，主要关注内容上传和存储的方案选择。",
      "why_correct": "S3 Transfer Acceleration 通过利用 Amazon 的全球边缘站点来加速 Amazon S3 的上传。它使用优化的网络路径来缩短上传时间。由于题目要求尽可能降低上传延迟，S3 Transfer Acceleration 是最适合的选择。",
      "why_wrong": "选项 A 使用 CloudFront 进行上传，CloudFront 通常用于内容分发，而不是上传加速，它专注于下载加速。选项 C 将内容上传到 EC2 实例，然后再复制到 S3，增加了复杂性和延迟，且未直接解决上传加速问题。选项 D 在最靠近用户的区域上传，虽然可以减少部分上传延迟，但其并未提供上传加速机制，不如 S3 Transfer Acceleration 高效。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "S3 Transfer Acceleration",
      "Amazon EC2"
    ]
  },
  {
    "id": 739,
    "topic": "1",
    "question_en": "A company is building a new application that uses serverless architecture. The architecture will consist of an Amazon API Gateway REST API and AWS Lambda functions to manage incoming requests. The company wants to add a service that can send messages received from the API Gateway REST API to multiple target Lambda functions for processing. The service must offer message filtering that gives the target Lambda functions the ability to receive only the messages the functions need. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Send the requests from the API Gateway REST API to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe Amazon Simple Queue Service (Amazon SQS) queues to the SNS topic. Configure the target Lambda functions to poll the different SQS queues.",
      "B": "Send the requests from the API Gateway REST API to Amazon EventBridge. Configure EventBridge to invoke the target Lambda functions.",
      "C": "Send the requests from the API Gateway REST API to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Configure Amazon MSK to publish the messages to the target Lambda functions.",
      "D": "Send the requests from the API Gateway REST API to multiple Amazon Simple Queue Service (Amazon SQS) queues. Configure the target Lambda functions to poll the different SQS queues."
    },
    "correct_answer": "A",
    "vote_percentage": "68%",
    "question_cn": "题目 13\n题干: 一家公司正在构建一个使用无服务器架构的新应用程序。该架构将由 Amazon API Gateway REST API 和 AWS Lambda 函数组成，以管理传入的请求。该公司希望添加一项服务，该服务可以将从 API Gateway REST API 接收的消息发送到多个目标 Lambda 函数进行处理。该服务必须提供消息筛选，使目标 Lambda 函数能够仅接收函数需要的消息。哪个解决方案将以最低的运营开销满足这些要求？\n选项:\n   A. 将 API Gateway REST API 中的请求发送到 Amazon Simple Notification Service (Amazon SNS) 主题。将 Amazon Simple Queue Service (Amazon SQS) 队列订阅到 SNS 主题。配置目标 Lambda 函数以轮询不同的 SQS 队列。\n   B. 将 API Gateway REST API 中的请求发送到 Amazon EventBridge。配置 EventBridge 以调用目标 Lambda 函数。\n   C. 将 API Gateway REST API 中的请求发送到 Amazon Managed Streaming for Apache Kafka (Amazon MSK)。配置 Amazon MSK 以将消息发布到目标 Lambda 函数。\n   D. 将 API Gateway REST API 中的请求发送到多个 Amazon Simple Queue Service (Amazon SQS) 队列。配置目标 Lambda 函数以轮询不同的 SQS 队列。",
    "options_cn": {
      "A": "将 API Gateway REST API 中的请求发送到 Amazon Simple Notification Service (Amazon SNS) 主题。将 Amazon Simple Queue Service (Amazon SQS) 队列订阅到 SNS 主题。配置目标 Lambda 函数以轮询不同的 SQS 队列。",
      "B": "将 API Gateway REST API 中的请求发送到 Amazon EventBridge。配置 EventBridge 以调用目标 Lambda 函数。",
      "C": "将 API Gateway REST API 中的请求发送到 Amazon Managed Streaming for Apache Kafka (Amazon MSK)。配置 Amazon MSK 以将消息发布到目标 Lambda 函数。",
      "D": "将 API Gateway REST API 中的请求发送到多个 Amazon Simple Queue Service (Amazon SQS) 队列。配置目标 Lambda 函数以轮询不同的 SQS 队列。"
    },
    "tags": [
      "API Gateway",
      "SNS",
      "SQS",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 68%），解析仅供参考。】\n\n考察使用无服务器架构实现消息传递和筛选，以满足 API Gateway REST API 接收的消息发送到多个 Lambda 函数的需求，并降低运营开销。",
      "why_correct": "选项 A 提供了通过 SNS 主题和 SQS 队列进行消息传递的解决方案。API Gateway 将消息发送到 SNS 主题，然后不同的 Lambda 函数通过轮询订阅的 SQS 队列来接收消息，实现消息的扇出和筛选。这种方案具有高可扩展性和可靠性，并且 SQS 队列可以作为缓冲，降低 Lambda 函数的压力。",
      "why_wrong": "选项 B 使用 EventBridge，虽然也能实现消息路由，但 EventBridge 的主要应用场景是事件驱动架构，用于处理来自 AWS 服务或其他应用的事件，其开销相对较高。选项 C 使用 MSK，MSK 适用于大规模数据流，用于处理高吞吐量的数据，对于本题的需求，过于复杂，运营开销较高。选项 D 将消息直接发送到多个 SQS 队列，没有消息筛选机制，无法满足题目中“消息筛选”的要求；同时，直接发送到多个队列也增加了管理的复杂性，不利于维护。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "REST API",
      "AWS Lambda",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon EventBridge",
      "Amazon Managed Streaming for Apache Kafka (Amazon MSK)"
    ]
  },
  {
    "id": 740,
    "topic": "1",
    "question_en": "A company migrated millions of archival files to Amazon S3. A solutions architect needs to implement a solution that will encrypt all the archival data by using a customer-provided key. The solution must encrypt existing unencrypted objects and future objects. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a list of unencrypted objects by filtering an Amazon S3 Inventory report. Configure an S3 Batch Operations job to encrypt the objects from the list with a server-side encryption with a customer-provided key (SSE-C). Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C).",
      "B": "Use S3 Storage Lens metrics to identify unencrypted S3 buckets. Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).",
      "C": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure an AWS Batch job to encrypt the objects from the list with a server-side encryption with AWS KMS keys (SSE-KMS). Configure the S3 default encryption feature to use a server-side encryption with AWS KMS keys (SSE-KMS).",
      "D": "Create a list of unencrypted objects by filtering the AWS usage report for Amazon S3. Configure the S3 default encryption feature to use a server-side encryption with a customer-provided key (SSE-C)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司将数百万个存档文件迁移到 Amazon S3。 一位解决方案架构师需要实施一个解决方案，该解决方案将使用客户提供的密钥加密所有存档数据。 该解决方案必须加密现有的未加密对象和未来的对象。 哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过过滤 Amazon S3 清单报告创建未加密对象的列表。配置一个 S3 批量操作作业，使用客户提供的密钥（SSE-C）进行服务器端加密来加密列表中的对象。配置 S3 默认加密功能以使用客户提供的密钥（SSE-C）进行服务器端加密。",
      "B": "使用 S3 Storage Lens 指标来识别未加密的 S3 存储桶。 配置 S3 默认加密功能以使用 AWS KMS 密钥（SSE-KMS）进行服务器端加密。",
      "C": "通过过滤 Amazon S3 的 AWS 使用情况报告来创建未加密对象的列表。 配置一个 AWS Batch 作业，使用 AWS KMS 密钥（SSE-KMS）进行服务器端加密来加密列表中的对象。 配置 S3 默认加密功能以使用 AWS KMS 密钥（SSE-KMS）进行服务器端加密。",
      "D": "通过过滤 Amazon S3 的 AWS 使用情况报告来创建未加密对象的列表。配置 S3 默认加密功能以使用客户提供的密钥（SSE-C）进行服务器端加密。"
    },
    "tags": [
      "S3",
      "KMS",
      "SSE-C",
      "批量操作"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查使用客户提供的密钥加密 S3 对象，并同时对现有对象和未来对象进行加密的解决方案。",
      "why_correct": "选项 A 提供了完整的解决方案。 首先，通过 S3 清单报告识别未加密对象。然后，使用 S3 批量操作作业，利用客户提供的密钥（SSE-C）对未加密对象进行加密。最后，配置 S3 默认加密功能，确保所有未来上传的对象都使用客户提供的密钥（SSE-C）进行加密。",
      "why_wrong": "选项 B 错误之处在于使用了 S3 Storage Lens 来识别未加密存储桶，而题目要求的是针对单个对象进行加密，Storage Lens 无法提供具体对象层面的信息。 并且使用了 SSE-KMS 而不是 SSE-C，不满足题目要求。选项 C 也使用了 SSE-KMS 而不是 SSE-C，且使用 AWS Batch 加密对象，不如 S3 批量操作作业更高效。 选项 D 缺少对现有未加密对象的加密过程，只有 S3 默认加密的配置，无法满足题目要求，缺少了针对现有对象的加密过程。"
    },
    "related_terms": [
      "Amazon S3",
      "SSE-C",
      "S3 Inventory Report",
      "S3 Batch Operations",
      "SSE-KMS",
      "S3 Storage Lens",
      "AWS KMS",
      "AWS Batch",
      "S3 default encryption"
    ]
  },
  {
    "id": 741,
    "topic": "1",
    "question_en": "The DNS provider that hosts a company's domain name records is experiencing outages that cause service disruption for a website running on AWS. The company needs to migrate to a more resilient managed DNS service and wants the service to run on AWS. What should a solutions architect do to rapidly migrate the DNS hosting service?",
    "options_en": {
      "A": "Create an Amazon Route 53 public hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.",
      "B": "Create an Amazon Route 53 private hosted zone for the domain name. Import the zone file containing the domain records hosted by the previous provider.",
      "C": "Create a Simple AD directory in AWS. Enable zone transfer between the DNS provider and AWS Directory Service for Microsoft Active Directory for the domain records.",
      "D": "Create an Amazon Route 53 Resolver inbound endpoint in the VPC. Specify the IP addresses that the provider's DNS will forward DNS queries to. Configure the provider's DNS to forward DNS queries for the domain to the IP addresses that are specified in the inbound endpoint."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "托管公司域名记录的 DNS 提供商正经历中断，导致在 AWS 上运行的网站服务中断。该公司需要迁移到更具弹性的托管 DNS 服务，并且希望该服务在 AWS 上运行。解决方案架构师应该怎么做才能快速迁移 DNS 托管服务？",
    "options_cn": {
      "A": "为域名创建一个 Amazon Route 53 公共托管区域。导入包含先前提供商托管的域记录的区域文件。",
      "B": "为域名创建一个 Amazon Route 53 私有托管区域。导入包含先前提供商托管的域记录的区域文件。",
      "C": "在 AWS 中创建一个 Simple AD 目录。在 DNS 提供商和 AWS Directory Service for Microsoft Active Directory 之间为域记录启用区域传输。",
      "D": "在 VPC 中创建一个 Amazon Route 53 Resolver 入站端点。指定提供商的 DNS 将 DNS 查询转发到的 IP 地址。配置提供商的 DNS，以将域的 DNS 查询转发到入站端点中指定的 IP 地址。"
    },
    "tags": [
      "Route 53",
      "DNS",
      "Microsoft Active Directory"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在 AWS 上快速迁移 DNS 托管服务，主要关注 Amazon Route 53 的配置和区域文件导入。",
      "why_correct": "Amazon Route 53 公共托管区域是托管公共 DNS 记录的首选服务。创建公共托管区域后，可以从现有的 DNS 提供商导入区域文件，从而快速复制域名记录。这种方法简单高效，满足了快速迁移 DNS 托管服务的需求。",
      "why_wrong": "选项 B 创建了私有托管区域，这不适用于公开访问的网站。选项 C 涉及 Simple AD 和区域传输，过程较为复杂，且与直接迁移 DNS 记录的需求不符。选项 D 涉及 Route 53 Resolver 和转发 DNS 查询，主要用于解析 VPC 内部 DNS，而不是迁移 DNS 托管服务，无法满足题目的需求。"
    },
    "related_terms": [
      "Amazon Route 53",
      "DNS",
      "Simple AD",
      "AWS Directory Service for Microsoft Active Directory",
      "VPC",
      "Route 53 Resolver"
    ]
  },
  {
    "id": 742,
    "topic": "1",
    "question_en": "A company is building an application on AWS that connects to an Amazon RDS database. The company wants to manage the application configuration and to securely store and retrieve credentials for the database and other services. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Use AWS AppConfig to store and manage the application configuration. Use AWS Secrets Manager to store and retrieve the credentials.",
      "B": "Use AWS Lambda to store and manage the application configuration. Use AWS Systems Manager Parameter Store to store and retrieve the credentials.",
      "C": "Use an encrypted application configuration file. Store the file in Amazon S3 for the application configuration. Create another S3 file to store and retrieve the credentials.",
      "D": "Use AWS AppConfig to store and manage the application configuration. Use Amazon RDS to store and retrieve the credentials."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 上构建一个应用程序，该应用程序连接到 Amazon RDS 数据库。该公司希望管理应用程序配置，并安全地存储和检索数据库和其他服务的凭据。哪种解决方案以最小的管理开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS AppConfig 存储和管理应用程序配置。使用 AWS Secrets Manager 存储和检索凭据。",
      "B": "使用 AWS Lambda 存储和管理应用程序配置。使用 AWS Systems Manager Parameter Store 存储和检索凭据。",
      "C": "使用加密的应用程序配置文件。将文件存储在 Amazon S3 中用于应用程序配置。创建另一个 S3 文件来存储和检索凭据。",
      "D": "使用 AWS AppConfig 存储和管理应用程序配置。使用 Amazon RDS 存储和检索凭据。"
    },
    "tags": [
      "AppConfig",
      "Secrets Manager",
      "Systems Manager Parameter Store",
      "S3",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查了使用 AWS 服务安全管理应用程序配置和数据库凭据的能力，以及最小化管理开销的需求。",
      "why_correct": "AWS AppConfig 提供了集中式应用程序配置管理，而 AWS Secrets Manager 提供了安全地存储和检索凭据的功能。结合使用 AppConfig 和 Secrets Manager 能够满足题目的需求，并减少管理开销，AppConfig 易于部署和管理，Secrets Manager 集成了加密和访问控制。",
      "why_wrong": "选项 B 错误在于，虽然 AWS Systems Manager Parameter Store 可以存储凭据，但 Lambda 在配置管理方面不如 AppConfig 灵活，并且会增加代码复杂度和维护成本。选项 C 的做法不安全，使用 S3 存储配置文件和凭据不提供加密和访问控制的集中化管理，且增加了额外的维护工作。选项 D 错误在于，Amazon RDS 并非用于存储凭据的工具，其主要功能是数据库服务。"
    },
    "related_terms": [
      "AWS AppConfig",
      "AWS Secrets Manager",
      "Amazon RDS",
      "AWS Lambda",
      "AWS Systems Manager Parameter Store",
      "Amazon S3"
    ]
  },
  {
    "id": 743,
    "topic": "1",
    "question_en": "To meet security requirements, a company needs to encrypt all of its application data in transit while communicating with an Amazon RDS MySQL DB instance. A recent security audit revealed that encryption at rest is enabled using AWS Key Management Service (AWS KMS), but data in transit is not enabled. What should a solutions architect do to satisfy the security requirements?",
    "options_en": {
      "A": "Enable IAM database authentication on the database.",
      "B": "Provide self-signed certificates. Use the certificates in all connections to the RDS instance.",
      "C": "Take a snapshot of the RDS instance. Restore the snapshot to a new instance with encryption enabled.",
      "D": "Download AWS-provided root certificates. Provide the certificates in all connections to the RDS instance."
    },
    "correct_answer": "D",
    "vote_percentage": "81%",
    "question_cn": "为了满足安全要求，一家公司需要对所有应用程序在与 Amazon RDS MySQL 数据库实例通信时传输的数据进行加密。最近的安全审计显示，静态加密已使用 AWS Key Management Service (AWS KMS) 启用，但传输中的数据未启用。解决方案架构师应该怎么做才能满足安全要求？",
    "options_cn": {
      "A": "在数据库上启用 IAM 数据库身份验证。",
      "B": "提供自签名证书。在与 RDS 实例的所有连接中使用这些证书。",
      "C": "拍摄 RDS 实例的快照。将快照恢复到启用了加密的新实例。",
      "D": "下载 AWS 提供的根证书。在与 RDS 实例的所有连接中使用这些证书。"
    },
    "tags": [
      "RDS",
      "IAM",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 81%），解析仅供参考。】\n\n题目考察了 RDS MySQL 数据库实例数据传输加密的配置方法。",
      "why_correct": "选项 D 提供了正确配置数据传输加密的方法。通过下载 AWS 提供的根证书，客户端可以使用 TLS/SSL 加密与 RDS MySQL 实例的连接。这种方法满足了“传输中的数据”加密的安全需求，且不需要停机或创建新的数据库实例。",
      "why_wrong": "选项 A 错误，IAM 数据库身份验证用于控制对数据库的访问，而不是加密数据传输。选项 B 错误，使用自签名证书虽然可以加密连接，但管理和更新证书的复杂性以及不受信任机构颁发的风险，使其不适合作为首选方案。选项 C 错误，拍摄快照并恢复到新实例只适用于静态加密，与解决传输中数据加密的需求无关。"
    },
    "related_terms": [
      "Amazon RDS",
      "MySQL",
      "AWS KMS",
      "IAM",
      "TLS/SSL",
      "Database Authentication"
    ]
  },
  {
    "id": 744,
    "topic": "1",
    "question_en": "A company is designing a new web service that will run on Amazon EC2 instances behind an Elastic Load Balancing (ELB) load balancer. However, many of the web service clients can only reach IP addresses authorized on their firewalls. What should a solutions architect recommend to meet the clients’ needs?",
    "options_en": {
      "A": "A Network Load Balancer with an associated Elastic IP address.",
      "B": "An Application Load Balancer with an associated Elastic IP address.",
      "C": "An A record in an Amazon Route 53 hosted zone pointing to an Elastic IP address.",
      "D": "An EC2 instance with a public IP address running as a proxy in front of the load balancer."
    },
    "correct_answer": "A",
    "vote_percentage": "92%",
    "question_cn": "一家公司正在设计一个新的 Web 服务，该服务将在弹性负载均衡（ELB）负载均衡器后面的 Amazon EC2 实例上运行。但是，许多 Web 服务客户端只能访问其防火墙上授权的 IP 地址。 解决方案架构师应该推荐什么来满足客户的需求？",
    "options_cn": {
      "A": "带有相关弹性 IP 地址的网络负载均衡器。",
      "B": "带有相关弹性 IP 地址的 Application Load Balancer。",
      "C": "在 Amazon Route 53 托管区域中指向弹性 IP 地址的 A 记录。",
      "D": "一个 EC2 实例，带有公共 IP 地址，作为负载均衡器之前的代理运行。"
    },
    "tags": [
      "ELB",
      "EC2",
      "Route 53"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 92%），解析仅供参考。】\n\n考查如何配置负载均衡器，以满足客户端基于 IP 地址的访问控制需求。",
      "why_correct": "Network Load Balancer (NLB) 支持分配弹性 IP 地址，允许客户端通过预先定义的静态 IP 地址访问后端 EC2 实例。 这满足了题目中客户端需要基于授权 IP 访问服务的需求。NLB 的这种特性，确保了客户端能够基于已知的 IP 地址进行访问控制。",
      "why_wrong": "Application Load Balancer (ALB) 不支持直接配置弹性 IP 地址，因此无法满足基于 IP 地址的访问控制需求。Route 53 的 A 记录指向弹性 IP 地址，这种方式无法实现负载均衡，并且 A 记录本身无法解决客户端访问控制问题。使用 EC2 实例作为代理，会引入单点故障，并且管理和维护成本较高，不如直接使用负载均衡器。"
    },
    "related_terms": [
      "Elastic Load Balancing (ELB)",
      "Amazon EC2",
      "Network Load Balancer (NLB)",
      "Elastic IP Address",
      "Application Load Balancer",
      "Amazon Route 53",
      "A record"
    ]
  },
  {
    "id": 745,
    "topic": "1",
    "question_en": "A company has established a new AWS account. The account is newly provisioned and no changes have been made to the default settings. The company is concerned about the security of the AWS account root user. What should be done to secure the root user?",
    "options_en": {
      "A": "Create IAM users for daily administrative tasks. Disable the root user.",
      "B": "Create IAM users for daily administrative tasks. Enable multi-factor authentication on the root user.",
      "C": "Generate an access key for the root user. Use the access key for daily administration tasks instead of the AWS Management Console.",
      "D": "Provide the root user credentials to the most senior solutions architect. Have the solutions architect use the root user for daily administration tasks."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司建立了一个新的 AWS 账户。该账户是新配置的，并且没有对默认设置进行任何更改。该公司担心 AWS 账户根用户的安全性。应该怎么做来保护根用户？",
    "options_cn": {
      "A": "为日常管理任务创建 IAM 用户。禁用根用户。",
      "B": "为日常管理任务创建 IAM 用户。在根用户上启用多因素身份验证。",
      "C": "为根用户生成访问密钥。使用访问密钥进行日常管理任务，而不是 AWS 管理控制台。",
      "D": "将根用户凭据提供给最资深解决方案架构师。让解决方案架构师使用根用户进行日常管理任务。"
    },
    "tags": [
      "IAM",
      "根用户"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察 AWS 账户根用户的安全最佳实践，包括 IAM 用户创建、MFA 启用等。",
      "why_correct": "选项 B 提供了保护根用户的最佳实践。创建 IAM 用户用于日常管理任务，避免使用根用户，降低风险。同时，在根用户上启用多因素身份验证（MFA），增强账户的安全性，防止账户被盗用。",
      "why_wrong": "选项 A 错误，虽然创建 IAM 用户是正确的，但是完全禁用根用户会带来潜在的维护和恢复问题，不推荐。选项 C 错误，不应使用根用户的访问密钥进行日常管理任务，且访问密钥比密码更难管理。选项 D 错误，不应该将根用户凭据提供给任何其他人，这违反了安全最佳实践，会导致账户安全风险。"
    },
    "related_terms": [
      "AWS Account",
      "Root User",
      "IAM User",
      "MFA",
      "AWS Management Console",
      "Access Key"
    ]
  },
  {
    "id": 746,
    "topic": "1",
    "question_en": "A company is deploying an application that processes streaming data in near-real time. The company plans to use Amazon EC2 instances for the workload. The network architecture must be configurable to provide the lowest possible latency between nodes. Which combination of network solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Enable and configure enhanced networking on each EC2 instance.",
      "B": "Group the EC2 instances in separate accounts.",
      "C": "Run the EC2 instances in a cluster placement group.",
      "D": "Attach multiple elastic network interfaces to each EC2 instanc",
      "E": "E. Use Amazon Elastic Block Store (Amazon EBS) optimized instance types."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在部署一个应用程序，该应用程序近乎实时地处理流数据。该公司计划使用 Amazon EC2 实例来处理工作负载。网络架构必须可配置，以在节点之间提供尽可能低的延迟。哪种网络解决方案组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在每个 EC2 实例上启用并配置增强型网络。",
      "B": "将 EC2 实例分组到不同的账户中。",
      "C": "在集群放置组中运行 EC2 实例。",
      "D": "将多个弹性网络接口附加到每个 EC2 实例。",
      "E": "使用 Amazon Elastic Block Store (Amazon EBS) 优化实例类型。"
    },
    "tags": [
      "EC2",
      "增强型网络",
      "放置组",
      "弹性网络接口"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考查在 EC2 实例中实现低延迟网络配置的方案。要求选择两个满足低延迟需求的网络配置方法。",
      "why_correct": "选项 A，启用并配置 Enhanced Networking，通过使用单根 I/O 虚拟化 (SR-IOV) 显著提高 EC2 实例的网络性能，降低延迟。选项 C，在 Cluster Placement Group 中运行 EC2 实例，将实例放置在单个可用区内的物理硬件上，从而减少节点之间的网络延迟，并提供更一致的性能。",
      "why_wrong": "选项 B，将 EC2 实例分组到不同的账户中，这与网络延迟优化无关，反而可能引入额外的账户间通信延迟。选项 D，附加多个 Elastic Network Interfaces (ENIs)，主要用于满足实例的多 IP 地址、网络隔离或网络流量管理需求，而非直接优化延迟。选项 E，使用 Amazon Elastic Block Store (EBS) 优化实例类型，EBS 主要影响存储性能，与网络延迟优化无关。"
    },
    "related_terms": [
      "Amazon EC2",
      "Enhanced Networking",
      "Cluster Placement Group",
      "Elastic Network Interfaces (ENIs)",
      "Amazon Elastic Block Store (Amazon EBS)"
    ]
  },
  {
    "id": 747,
    "topic": "1",
    "question_en": "A financial services company wants to shut down two data centers and migrate more than 100 TB of data to AWS. The data has an intricate directory structure with millions of small files stored in deep hierarchies of subfolders. Most of the data is unstructured, and the company’s file storage consists of SMB-based storage types from multiple vendors. The company does not want to change its applications to access the data after migration. What should a solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Direct Connect to migrate the data to Amazon S3.",
      "B": "Use AWS DataSync to migrate the data to Amazon FSx for Lustre.",
      "C": "Use AWS DataSync to migrate the data to Amazon FSx for Windows File Server.",
      "D": "Use AWS Direct Connect to migrate the data on-premises file storage to an AWS Storage Gateway volume gateway."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家金融服务公司希望关闭两个数据中心，并将超过 100 TB 的数据迁移到 AWS。这些数据具有复杂的目录结构，其中数百万个小文件存储在深层次的子文件夹层次结构中。大多数数据是非结构化的，并且该公司的文件存储由来自多个供应商的基于 SMB 的存储类型组成。该公司不想在迁移后更改其应用程序以访问数据。解决方案架构师应该怎么做才能以最少的运营开销来满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Direct Connect 将数据迁移到 Amazon S3。",
      "B": "使用 AWS DataSync 将数据迁移到 Amazon FSx for Lustre。",
      "C": "使用 AWS DataSync 将数据迁移到 Amazon FSx for Windows File Server。",
      "D": "使用 AWS Direct Connect 将本地文件存储中的数据迁移到 AWS Storage Gateway 卷网关。"
    },
    "tags": [
      "DataSync",
      "FSx for Lustre",
      "FSx for Windows File Server",
      "Storage Gateway",
      "SMB",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查了大规模数据迁移场景下，对于复杂文件结构、SMB 协议支持以及最小运营开销的解决方案选择。",
      "why_correct": "AWS DataSync 适用于将数据迁移到 AWS。它支持 SMB 协议，能够处理复杂的文件目录结构，并可以自动管理数据传输、验证和加密。将数据迁移到 Amazon FSx for Windows File Server 符合题目中“不想在迁移后更改其应用程序以访问数据”的要求，因为 FSx for Windows File Server 提供了 Windows 文件服务器接口。",
      "why_wrong": "A 选项，AWS Direct Connect 提供了专用的网络连接，适合于网络传输加速，但它本身并不提供数据迁移的功能。B 选项，Amazon FSx for Lustre 虽然具有高性能，但不支持 SMB 协议，与题目中“来自多个供应商的基于 SMB 的存储类型”不符。D 选项，AWS Storage Gateway 卷网关主要用于提供本地应用程序对云存储的访问，而不是大规模的数据迁移。同时，它可能带来额外的运营开销，与题目要求不符。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon FSx for Windows File Server",
      "SMB",
      "AWS Direct Connect",
      "Amazon S3",
      "Amazon FSx for Lustre",
      "AWS Storage Gateway"
    ]
  },
  {
    "id": 748,
    "topic": "1",
    "question_en": "A company uses an organization in AWS Organizations to manage AWS accounts that contain applications. The company sets up a dedicated monitoring member account in the organization. The company wants to query and visualize observability data across the accounts by using Amazon CloudWatch. Which solution will meet these requirements?",
    "options_en": {
      "A": "Enable CloudWatch cross-account observability for the monitoring account. Deploy an AWS CloudFormation template provided by the monitoring account in each AWS account to share the data with the monitoring account.",
      "B": "Set up service control policies (SCPs) to provide access to CloudWatch in the monitoring account under the Organizations root organizational unit (OU).",
      "C": "Configure a new IAM user in the monitoring account. In each AWS account, configure an IAM policy to have access to query and visualize the CloudWatch data in the account. Attach the new IAM policy to the new IAM user.",
      "D": "Create a new IAM user in the monitoring account. Create cross-account IAM policies in each AWS account. Attach the IAM policies to the new IAM user."
    },
    "correct_answer": "A",
    "vote_percentage": "86%",
    "question_cn": "一家公司使用 AWS Organizations 中的一个组织来管理包含应用程序的 AWS 账户。该公司在组织中设置了一个专门的监控成员账户。该公司希望使用 Amazon CloudWatch 查询和可视化跨账户的可观察性数据。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为监控账户打开 CloudWatch 跨账户可观察性。在每个 AWS 账户中部署监控账户提供的 AWS CloudFormation 模板，以便与监控账户共享数据。",
      "B": "设置服务控制策略 (SCP)，以在 Organizations 根组织单元 (OU) 下为监控账户提供对 CloudWatch 的访问权限。",
      "C": "在监控账户中配置一个新的 IAM 用户。在每个 AWS 账户中，配置一个 IAM 策略，以便有权查询和可视化该账户中的 CloudWatch 数据。将新的 IAM 策略附加到新的 IAM 用户。",
      "D": "在监控账户中创建一个新的 IAM 用户。在每个 AWS 账户中创建跨账户 IAM 策略。将 IAM 策略附加到新的 IAM 用户。"
    },
    "tags": [
      "CloudWatch",
      "跨账户可观察性",
      "Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 86%），解析仅供参考。】\n\n考查 CloudWatch 跨账户可观察性的配置方法，包括数据共享和访问权限的设置。",
      "why_correct": "选项 A 使用 CloudWatch 跨账户可观察性功能，允许监控账户收集来自其他账户的 CloudWatch 指标和日志。通过部署 CloudFormation 模板，可以简化其他账户的数据共享配置，使得监控账户可以查询和可视化这些数据，满足题干要求。",
      "why_wrong": "选项 B 错误，SCP 主要用于限制账户的权限，而不是跨账户的数据共享。虽然可以赋予监控账户访问 CloudWatch 的权限，但无法实现数据跨账户的自动收集。选项 C 和 D 均使用 IAM 用户和策略实现跨账户访问，配置复杂且易出错。选项 C 描述的 IAM 策略需要每个账户单独配置，扩展性差；选项 D 描述的跨账户 IAM 策略虽然能实现跨账户访问，但需要手动配置，无法与 CloudWatch 跨账户可观察性功能高效配合。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS Account",
      "Amazon CloudWatch",
      "跨账户可观察性",
      "AWS CloudFormation",
      "服务控制策略 (SCP)",
      "IAM",
      "IAM user",
      "IAM 策略",
      "OU"
    ]
  },
  {
    "id": 749,
    "topic": "1",
    "question_en": "A company’s website is used to sell products to the public. The site runs on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). There is also an Amazon CloudFront distribution, and AWS WAF is being used to protect against SQL injection attacks. The ALB is the origin for the CloudFront distribution. A recent review of security logs revealed an external malicious IP that needs to be blocked from accessing the website. What should a solutions architect do to protect the application?",
    "options_en": {
      "A": "Modify the network ACL on the CloudFront distribution to add a deny rule for the malicious IP address.",
      "B": "Modify the configuration of AWS WAF to add an IP match condition to block the malicious IP address.",
      "C": "Modify the network ACL for the EC2 instances in the target groups behind the ALB to deny the malicious IP address.",
      "D": "Modify the security groups for the EC2 instances in the target groups behind the ALB to deny the malicious IP address."
    },
    "correct_answer": "B",
    "vote_percentage": "81%",
    "question_cn": "一家公司的网站用于向公众销售产品。该网站在 Application Load Balancer (ALB) 之后，在 Auto Scaling 组中的 Amazon EC2 实例上运行。 还有一个 Amazon CloudFront 分发，并且正在使用 AWS WAF 来防御 SQL 注入攻击。 ALB 是 CloudFront 分发的源。 最近对安全日志的审查显示了一个需要阻止访问该网站的外部恶意 IP。 解决方案架构师应该怎么做来保护应用程序？",
    "options_cn": {
      "A": "修改 CloudFront 分发上的网络 ACL，以添加拒绝恶意 IP 地址的规则。",
      "B": "修改 AWS WAF 的配置，以添加 IP 匹配条件来阻止恶意 IP 地址。",
      "C": "修改 ALB 之后的目标组中 EC2 实例的网络 ACL，以拒绝恶意 IP 地址。",
      "D": "修改 ALB 之后的目标组中 EC2 实例的安全组，以拒绝恶意 IP 地址。"
    },
    "tags": [
      "CloudFront",
      "WAF",
      "ALB",
      "安全组"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 81%），解析仅供参考。】\n\n考察使用 AWS WAF 阻止恶意 IP 地址访问 Web 应用程序的配置方法。",
      "why_correct": "AWS WAF 允许您根据各种标准（包括 IP 地址）创建规则来阻止 Web 流量。通过在 AWS WAF 中配置 IP 匹配条件，您可以轻松阻止来自特定 IP 地址的请求。因为题干中已使用 AWS WAF，所以这是最直接且有效的解决方案。",
      "why_wrong": "选项 A 错误，因为 CloudFront 的网络 ACL 主要用于控制边缘级别的流量，而非专门针对单个 IP 地址的快速封禁。选项 C 和 D 错误，因为 ALB 后面的 EC2 实例的网络 ACL 和安全组配置会在 CloudFront 和 ALB 之间，或者 ALB 本身接收流量后才生效，效率较低，且无法充分利用 AWS WAF 的优势进行快速响应。"
    },
    "related_terms": [
      "Application Load Balancer (ALB)",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon CloudFront",
      "AWS WAF",
      "IP address",
      "Network ACL",
      "Security Group",
      "Target Group"
    ]
  },
  {
    "id": 750,
    "topic": "1",
    "question_en": "A company sets up an organization in AWS Organizations that contains 10 AWS accounts. A solutions architect must design a solution to provide access to the accounts for several thousand employees. The company has an existing identity provider (IdP). The company wants to use the existing IdP for authentication to AWS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create IAM users for the employees in the required AWS accounts. Connect IAM users to the existing IdP. Configure federated authentication for the IAM users.",
      "B": "Set up AWS account root users with user email addresses and passwords that are synchronized from the existing IdP.",
      "C": "Configure AWS IAM Identity Center (AWS Single Sign-On). Connect IAM Identity Center to the existing IdP. Provision users and groups from the existing IdP.",
      "D": "Use AWS Resource Access Manager (AWS RAM) to share access to the AWS accounts with the users in the existing IdP."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS Organizations 中设置了一个组织，其中包含 10 个 AWS 账户。 一位解决方案架构师必须设计一个解决方案，为数千名员工提供对这些账户的访问权限。该公司有一个现有的身份提供商 (IdP)。该公司希望使用现有的 IdP 进行 AWS 身份验证。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在所需的 AWS 账户中为员工创建 IAM 用户。将 IAM 用户连接到现有的 IdP。为 IAM 用户配置联合身份验证。",
      "B": "使用从现有的 IdP 同步的用户电子邮件地址和密码设置 AWS 账户根用户。",
      "C": "配置 AWS IAM Identity Center (AWS Single Sign-On)。将 IAM Identity Center 连接到现有的 IdP。从现有的 IdP 预置用户和组。",
      "D": "使用 AWS Resource Access Manager (AWS RAM) 与现有的 IdP 中的用户共享对 AWS 账户的访问权限。"
    },
    "tags": [
      "IAM",
      "Organizations",
      "IdP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查通过身份提供商(IdP)实现跨多个AWS账户的用户访问权限，并整合IAM Identity Center (AWS SSO) 的方案。",
      "why_correct": "AWS IAM Identity Center (AWS Single Sign-On) 专为集中管理用户对多个 AWS 账户和应用程序的访问而设计。 将 IAM Identity Center 连接到现有的 IdP 后，可以从 IdP 预置用户和组，实现统一的身份验证和授权，满足题干对访问控制和现有 IdP 集成的需求。",
      "why_wrong": "选项 A 错误，因为手动为每个员工在每个账户中创建和管理 IAM 用户，不具备可扩展性，维护成本高，且无法充分利用 IdP。选项 B 错误，直接使用根用户账户不符合安全最佳实践，也不便于集中管理。选项 D 错误，AWS Resource Access Manager (RAM) 用于共享 AWS 资源，而非用户身份验证和访问控制。 RAM 无法直接与 IdP 结合进行身份验证。因此，RAM 不适合此场景。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS Account",
      "AWS Identity and Access Management (IAM)",
      "IAM User",
      "IdP",
      "AWS IAM Identity Center (AWS Single Sign-On)",
      "AWS Resource Access Manager (AWS RAM)"
    ]
  },
  {
    "id": 751,
    "topic": "1",
    "question_en": "A solutions architect is designing an AWS Identity and Access Management (IAM) authorization model for a company's AWS account. The company has designated five specific employees to have full access to AWS services and resources in the AWS account. The solutions architect has created an IAM user for each of the five designated employees and has created an IAM user group. Which solution will meet these requirements?",
    "options_en": {
      "A": "Attach the AdministratorAccess resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
      "B": "Attach the SystemAdministrator identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
      "C": "Attach the AdministratorAccess identity-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group.",
      "D": "Attach the SystemAdministrator resource-based policy to the IAM user group. Place each of the five designated employee IAM users in the IAM user group."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为公司 AWS 账户设计 AWS Identity and Access Management (IAM) 授权模型。该公司已指定五名特定员工完全访问 AWS 账户中的 AWS 服务和资源。解决方案架构师为这五名指定员工中的每位创建了一个 IAM 用户，并创建了一个 IAM 用户组。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 AdministratorAccess 基于资源的策略附加到 IAM 用户组。将五名指定员工的每个 IAM 用户放入 IAM 用户组。",
      "B": "将 SystemAdministrator 基于身份的策略附加到 IAM 用户组。将五名指定员工的每个 IAM 用户放入 IAM 用户组。",
      "C": "将 AdministratorAccess 基于身份的策略附加到 IAM 用户组。将五名指定员工的每个 IAM 用户放入 IAM 用户组。",
      "D": "将 SystemAdministrator 基于资源的策略附加到 IAM 用户组。将五名指定员工的每个 IAM 用户放入 IAM 用户组。"
    },
    "tags": [
      "IAM",
      "Policy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察 IAM 授权模型的设计。通过 IAM 用户组管理权限，并将 AdministratorAccess 策略附加到用户组可以快速授权权限。AdminstratorAccess 提供完全访问权限，而基于身份的策略直接附加到用户组更易于管理。为避免使用根账户进行操作，需要创建 IAM 用户。",
      "why_correct": "将 AdministratorAccess 基于身份的策略附加到 IAM 用户组，可以为组内的用户提供完全的 AWS 访问权限，是最直接的授权方式。",
      "why_wrong": "A 选项中，基于资源的策略用于控制对资源的访问，而非用户组。B 选项，SystemAdministrator 通常是预定义的 AWS 托管策略，但可能不完全符合需求。D 选项，基于资源的策略不适用于这种场景。"
    },
    "related_terms": [
      "IAM",
      "Policy",
      "AdministratorAccess",
      "SystemAdministrator"
    ]
  },
  {
    "id": 752,
    "topic": "1",
    "question_en": "A company has a multi-tier payment processing application that is based on virtual machines (VMs). The communication between the tiers occurs asynchronously through a third-party middleware solution that guarantees exactly-once delivery. The company needs a solution that requires the least amount of infrastructure management. The solution must guarantee exactly-once delivery for application messaging. Which combination of actions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Lambda for the compute layers in the architecture.",
      "B": "Use Amazon EC2 instances for the compute layers in the architecture.",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) as the messaging component between the compute layers.",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) FIFO queues as the messaging component between the compute layers",
      "E": "Use containers that are based on Amazon Elastic Kubernetes Service (Amazon EKS) for the compute layers in the architecture."
    },
    "correct_answer": "AD",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个基于虚拟机 (VM) 的多层支付处理应用程序。各层之间的通信通过第三方中间件解决方案异步进行，该解决方案保证了仅一次交付。该公司需要一个基础设施管理需求最少的解决方案。该解决方案必须保证应用程序消息传递的仅一次交付。哪两种操作的组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在架构中使用 AWS Lambda 作为计算层。",
      "B": "在架构中使用 Amazon EC2 实例作为计算层。",
      "C": "使用 Amazon Simple Notification Service (Amazon SNS) 作为计算层之间的消息传递组件。",
      "D": "使用 Amazon Simple Queue Service (Amazon SQS) FIFO 队列作为计算层之间的消息传递组件。",
      "E": "在架构中使用基于 Amazon Elastic Kubernetes Service (Amazon EKS) 的容器作为计算层。"
    },
    "tags": [
      "Lambda",
      "SQS",
      "SNS",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 100%），解析仅供参考。】\n\n此题考察如何设计一个基础设施管理需求最少的仅一次交付消息传递解决方案。AWS Lambda 提供了无服务器计算，按需执行，无需管理服务器。SQS FIFO 队列保证消息的顺序和仅一次处理。 Lambda 和 SQS 结合使用可以创建一个可靠的消息传递系统。 SNS 不支持 FIFO。",
      "why_correct": "Lambda 是无服务器计算，按需执行，减少了基础设施管理的需求。SQS FIFO 队列提供仅一次交付保证。",
      "why_wrong": "B 选项使用 EC2 实例需要管理服务器，增加了运营开销。C 选项，Amazon SNS 不保证消息的仅一次交付。D 选项，SQS 不是 FIFO，不保证消息传递顺序，无法保证仅一次交付。 E 选项，EKS 需要维护 Kubernetes 集群。"
    },
    "related_terms": [
      "Lambda",
      "SQS",
      "SNS",
      "EC2",
      "EKS",
      "FIFO"
    ]
  },
  {
    "id": 753,
    "topic": "1",
    "question_en": "A company has a nightly batch processing routine that analyzes report files that an on-premises file system receives daily through SFTP. The company wants to move the solution to the AWS Cloud. The solution must be highly available and resilient. The solution also must minimize operational effort. Which solution meets these requirements?",
    "options_en": {
      "A": "Deploy AWS Transfer for SFTP and an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Amazon EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation.",
      "B": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic Block Store (Amazon EBS) volume for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.",
      "C": "Deploy an Amazon EC2 instance that runs Linux and an SFTP service. Use an Amazon Elastic File System (Amazon EFS) file system for storage. Use an Auto Scaling group with the minimum number of instances and desired number of instances set to 1.",
      "D": "Deploy AWS Transfer for SFTP and an Amazon S3 bucket for storage. Modify the application to pull the batch files from Amazon S3 to an Amazon EC2 instance for processing. Use an EC2 instance in an Auto Scaling group with a scheduled scaling policy to run the batch operation."
    },
    "correct_answer": "A",
    "vote_percentage": "63%",
    "question_cn": "题目 14\n题干: 一家公司有一个夜间批处理例程，该例程分析本地文件系统每天通过 SFTP 接收的报告文件。该公司希望将解决方案转移到 AWS Cloud。该解决方案必须具有高可用性和弹性。该解决方案还必须最大限度地减少运营工作量。哪个解决方案满足这些要求？\n选项:\n   A. 部署 AWS Transfer for SFTP 和一个 Amazon Elastic File System (Amazon EFS) 文件系统进行存储。使用 Auto Scaling 组中的 Amazon EC2 实例（具有计划的扩展策略）来运行批处理操作。\n   B. 部署一个运行 Linux 和 SFTP 服务的 Amazon EC2 实例。使用 Amazon Elastic Block Store (Amazon EBS) 卷进行存储。使用 Auto Scaling 组，并将实例的最小数量和所需实例数量设置为 1。\n   C. 部署一个运行 Linux 和 SFTP 服务的 Amazon EC2 实例。使用 Amazon Elastic File System (Amazon EFS) 文件系统进行存储。使用 Auto Scaling 组，并将实例的最小数量和所需实例数量设置为 1。\n   D. 部署 AWS Transfer for SFTP 和一个 Amazon S3 存储桶进行存储。修改应用程序，从 Amazon S3 将批处理文件拉取到 Amazon EC2 实例进行处理。使用 Auto Scaling 组中的 EC2 实例（具有计划的扩展策略）来运行批处理操作。",
    "options_cn": {
      "A": "部署 AWS Transfer for SFTP 和一个 Amazon Elastic File System (Amazon EFS) 文件系统进行存储。使用 Auto Scaling 组中的 Amazon EC2 实例（具有计划的扩展策略）来运行批处理操作。",
      "B": "部署一个运行 Linux 和 SFTP 服务的 Amazon EC2 实例。使用 Amazon Elastic Block Store (Amazon EBS) 卷进行存储。使用 Auto Scaling 组，并将实例的最小数量和所需实例数量设置为 1。",
      "C": "部署一个运行 Linux 和 SFTP 服务的 Amazon EC2 实例。使用 Amazon Elastic File System (Amazon EFS) 文件系统进行存储。使用 Auto Scaling 组，并将实例的最小数量和所需实例数量设置为 1。",
      "D": "部署 AWS Transfer for SFTP 和一个 Amazon S3 存储桶进行存储。修改应用程序，从 Amazon S3 将批处理文件拉取到 Amazon EC2 实例进行处理。使用 Auto Scaling 组中的 EC2 实例（具有计划的扩展策略）来运行批处理操作。"
    },
    "tags": [
      "EC2",
      "SFTP",
      "EFS",
      "EBS",
      "Transfer for SFTP",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 63%），解析仅供参考。】\n\n考察了使用 AWS 服务构建高可用性、弹性，并最小化运营负担的 SFTP 文件处理解决方案。",
      "why_correct": "选项 A 结合了 AWS Transfer for SFTP，简化了 SFTP 文件接收。EFS 提供可扩展的存储，并与 EC2 实例配合，利用 Auto Scaling 实现弹性。使用计划的 Auto Scaling 策略可以针对夜间批处理进行优化，满足了高可用性、弹性，并减少了运营工作量。",
      "why_wrong": "选项 B 采用 EBS 卷，无法实现EFS的弹性，并且最小实例数和所需实例数设置为 1 无法实现高可用性和弹性。选项 C 也将实例的最小数量和所需实例数量设置为 1，无法实现高可用性和弹性。选项 D 需要修改应用程序以从 S3 提取文件，增加了额外的开发工作，并且引入了不必要的复杂性。"
    },
    "related_terms": [
      "AWS Transfer for SFTP",
      "Amazon Elastic File System (Amazon EFS)",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon S3",
      "SFTP"
    ]
  },
  {
    "id": 754,
    "topic": "1",
    "question_en": "A company has users all around the world accessing its HTTP-based application deployed on Amazon EC2 instances in multiple AWS Regions. The company wants to improve the availability and performance of the application. The company also wants to protect the application against common web exploits that may affect availability, compromise security, or consume excessive resources. Static IP addresses are required. What should a solutions architect recommend to accomplish this?",
    "options_en": {
      "A": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create an accelerator using AWS Global Accelerator and register the NLBs as endpoints.",
      "B": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Deploy AWS WAF on the ALBs. Create an accelerator using AWS Global Accelerator and register the ALBs as endpoints.",
      "C": "Put the EC2 instances behind Network Load Balancers (NLBs) in each Region. Deploy AWS WAF on the NLBs. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the NLBs.",
      "D": "Put the EC2 instances behind Application Load Balancers (ALBs) in each Region. Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the ALBs. Deploy AWS WAF on the CloudFront distribution."
    },
    "correct_answer": "B",
    "vote_percentage": "83%",
    "question_cn": "题目 15\n题干: 一家公司拥有遍布全球的用户，他们通过 HTTP 访问部署在多个 AWS 区域中的 Amazon EC2 实例上的基于 HTTP 的应用程序。该公司希望提高应用程序的可用性和性能。该公司还希望保护应用程序免受可能影响可用性、损害安全性或消耗过多资源的常见 Web 漏洞的侵害。需要静态 IP 地址。解决方案架构师应建议采取什么措施来实现此目的？\n选项:\n   A. 将 EC2 实例置于每个区域中的 Network Load Balancer (NLB) 后面。在 NLB 上部署 AWS WAF。使用 AWS Global Accelerator 创建一个加速器，并将 NLB 注册为终端节点。\n   B. 将 EC2 实例置于每个区域中的 Application Load Balancer (ALB) 后面。在 ALB 上部署 AWS WAF。使用 AWS Global Accelerator 创建一个加速器，并将 ALB 注册为终端节点。\n   C. 将 EC2 实例置于每个区域中的 Network Load Balancer (NLB) 后面。在 NLB 上部署 AWS WAF。创建一个 Amazon CloudFront 分配，其源使用 Amazon Route 53 基于延迟的路由将请求路由到 NLB。\n   D. 将 EC2 实例置于每个区域中的 Application Load Balancer (ALB) 后面。创建一个 Amazon CloudFront 分配，其源使用 Amazon Route 53 基于延迟的路由将请求路由到 ALB。在 CloudFront 分配上部署 AWS WAF。",
    "options_cn": {
      "A": "将 EC2 实例置于每个区域中的 Network Load Balancer (NLB) 后面。在 NLB 上部署 AWS WAF。使用 AWS Global Accelerator 创建一个加速器，并将 NLB 注册为终端节点。",
      "B": "将 EC2 实例置于每个区域中的 Application Load Balancer (ALB) 后面。在 ALB 上部署 AWS WAF。使用 AWS Global Accelerator 创建一个加速器，并将 ALB 注册为终端节点。",
      "C": "将 EC2 实例置于每个区域中的 Network Load Balancer (NLB) 后面。在 NLB 上部署 AWS WAF。创建一个 Amazon CloudFront 分配，其源使用 Amazon Route 53 基于延迟的路由将请求路由到 NLB。",
      "D": "将 EC2 实例置于每个区域中的 Application Load Balancer (ALB) 后面。创建一个 Amazon CloudFront 分配，其源使用 Amazon Route 53 基于延迟的路由将请求路由到 ALB。在 CloudFront 分配上部署 AWS WAF。"
    },
    "tags": [
      "CloudFront",
      "Global Accelerator",
      "ALB",
      "NLB",
      "WAF",
      "Route 53",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 83%），解析仅供参考。】\n\n考查通过使用 Global Accelerator 和 Web Application Firewall (WAF) 提升应用程序可用性、性能和安全性的架构设计。同时，题目要求提供静态 IP 地址。",
      "why_correct": "选项 B 提供了最佳的解决方案。使用 Application Load Balancer (ALB) 可以提供更高级的 HTTP 负载均衡功能，包括基于内容的路由。在 ALB 上部署 AWS WAF 可以保护应用程序免受常见的 Web 漏洞攻击。使用 AWS Global Accelerator 可以提供全局加速，并通过静态 IP 地址提高应用程序的性能和可用性，满足了题目的要求。",
      "why_wrong": "选项 A 错误，因为 Network Load Balancer (NLB) 主要用于 TCP、UDP 和 TLS 流量的负载均衡，不适用于 HTTP 应用程序的精细化路由需求。选项 C 错误，虽然 CloudFront 可以提供内容分发和加速，但题干需要静态 IP，CloudFront 无法直接满足此需求。选项 D 错误，虽然 CloudFront 和 ALB 可以结合使用，但在 CloudFront 上部署 WAF 无法像在 ALB 上部署 WAF 一样提供细粒度的保护，同时没有使用 Global Accelerator 无法提供静态 IP 地址和全局加速，不符合题干需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "HTTP",
      "Application Load Balancer (ALB)",
      "AWS WAF",
      "AWS Global Accelerator",
      "Network Load Balancer (NLB)",
      "Amazon CloudFront",
      "Amazon Route 53"
    ]
  },
  {
    "id": 755,
    "topic": "1",
    "question_en": "A company’s data platform uses an Amazon Aurora MySQL database. The database has multiple read replicas and multiple DB instances across different Availability Zones. Users have recently reported errors from the database that indicate that there are too many connections. The company wants to reduce the failover time by 20% when a read replica is promoted to primary writer. Which solution will meet this requirement?",
    "options_en": {
      "A": "Switch from Aurora to Amazon RDS with Multi-AZ cluster deployment.",
      "B": "Use Amazon RDS Proxy in front of the Aurora database.",
      "C": "Switch to Amazon DynamoDB with DynamoDB Accelerator (DAX) for read connections.",
      "D": "Switch to Amazon Redshift with relocation capability."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司的数据平台使用 Amazon Aurora MySQL 数据库。该数据库在不同的可用区中拥有多个只读副本和多个数据库实例。用户最近报告了来自数据库的错误，表明连接过多。该公司希望在将只读副本提升为主写库时，将故障转移时间缩短 20%。哪种解决方案将满足此要求？",
    "options_cn": {
      "A": "从 Aurora 切换到具有 Multi-AZ 集群部署的 Amazon RDS。",
      "B": "在 Aurora 数据库前面使用 Amazon RDS Proxy。",
      "C": "切换到 Amazon DynamoDB，并使用 DynamoDB Accelerator (DAX) 进行读取连接。",
      "D": "切换到具有重新定位功能的 Amazon Redshift。"
    },
    "tags": [
      "Aurora",
      "RDS",
      "DynamoDB",
      "Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了 Aurora 数据库在故障转移场景下的优化策略，重点关注缩短故障转移时间的方法。",
      "why_correct": "Amazon RDS Proxy 是一种数据库代理服务，可以帮助应用程序更有效地扩展数据库连接。它缓存数据库连接，并将连接请求路由到可用的数据库实例。当 Aurora 进行故障转移时，RDS Proxy 可以更快地将连接切换到新的主数据库，从而缩短故障转移时间。",
      "why_wrong": "选项 A 错误，将 Aurora 切换到 RDS 不利于故障转移时间的缩短，反而可能增加停机时间。选项 C 错误，DynamoDB 是一种 NoSQL 数据库，不适用于 MySQL 数据库的场景，且 DAX 主要用于缓存 DynamoDB 的读取操作，不能解决 MySQL 的连接问题。选项 D 错误，Redshift 是一种数据仓库服务，不适用于事务型数据库的场景，更不能缩短 Aurora MySQL 的故障转移时间。"
    },
    "related_terms": [
      "Amazon Aurora MySQL",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon RDS Proxy",
      "Amazon DynamoDB",
      "DynamoDB Accelerator (DAX)",
      "Amazon Redshift"
    ]
  },
  {
    "id": 756,
    "topic": "1",
    "question_en": "A company stores text files in Amazon S3. The text files include customer chat messages, date and time information, and customer personally identifiable information (PII). The company needs a solution to provide samples of the conversations to an external service provider for quality control. The external service provider needs to randomly pick sample conversations up to the most recent conversation. The company must not share the customer PII with the external service provider. The solution must scale when the number of customer conversations increases. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Object Lambda Access Point. Create an AWS Lambda function that redacts the PII when the function reads the file. Instruct the external service provider to access the Object Lambda Access Point.",
      "B": "Create a web application on an Amazon EC2 instance that presents a list of the files, redacts the PII from the files, and allows the external service provider to download new versions of the files that have the PII redacted.",
      "D": "Create an Amazon DynamoDB table. Create an AWS Lambda function that reads only the data in the files that does not contain PII. Configure the Lambda function to store the non-PII data in the DynamoDB table when a new file is written to Amazon S3. Grant the external service provider access to the DynamoDB table."
    },
    "correct_answer": "A",
    "vote_percentage": "91%",
    "question_cn": "一家公司将文本文件存储在 Amazon S3 中。文本文件包含客户聊天消息、日期和时间信息以及客户个人身份信息 (PII)。该公司需要一个解决方案，向外部服务提供商提供对话样本以进行质量控制。外部服务提供商需要随机选取最新的对话样本。该公司不得与外部服务提供商共享客户 PII。当客户对话数量增加时，该解决方案必须能够扩展。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Object Lambda Access Point。创建一个 AWS Lambda 函数，该函数在读取文件时会编辑掉 PII。指示外部服务提供商访问 Object Lambda Access Point。",
      "B": "在 Amazon EC2 实例上创建一个 Web 应用程序，该应用程序会显示文件列表，从文件中编辑掉 PII，并允许外部服务提供商下载已编辑掉 PII 的文件的新版本。",
      "D": "创建一个 Amazon DynamoDB 表。创建一个 AWS Lambda 函数，该函数仅读取文件中不包含 PII 的数据。将 Lambda 函数配置为在将新文件写入 Amazon S3 时将非 PII 数据存储在 DynamoDB 表中。授予外部服务提供商访问 DynamoDB 表的权限。"
    },
    "tags": [
      "S3",
      "Lambda",
      "DynamoDB",
      "Object Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 91%），解析仅供参考。】\n\n考查使用 Object Lambda 和 Lambda 函数处理 S3 对象，以及如何满足数据安全和可扩展性需求。",
      "why_correct": "选项 A 使用 Object Lambda Access Point 和 Lambda 函数，可以在 S3 层面对数据进行动态处理。Lambda 函数可以移除 PII 信息，确保外部服务提供商无法访问敏感数据。这种架构可以轻松扩展，以适应不断增长的客户对话数量。",
      "why_wrong": "选项 B 在 EC2 实例上运行 Web 应用程序，涉及实例管理和维护，运营开销较高，且可扩展性不如 S3 原生方案。选项 D 将数据存储在 DynamoDB 中，而不是直接提供原始 S3 文本数据，无法满足“对话样本”需求，且数据冗余，增加了复杂性，不适用于文件读取和编辑场景。"
    },
    "related_terms": [
      "Amazon S3",
      "Object Lambda Access Point",
      "AWS Lambda",
      "PII",
      "Amazon EC2",
      "Web application",
      "Amazon DynamoDB"
    ]
  },
  {
    "id": 757,
    "topic": "1",
    "question_en": "A company is running a legacy system on an Amazon EC2 instance. The application code cannot be modified, and the system cannot run on more than one instance. A solutions architect must design a resilient solution that can improve the recovery time for the system. What should the solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Enable termination protection for the EC2 instance.",
      "B": "Configure the EC2 instance for Multi-AZ deployment.",
      "C": "Create an Amazon CloudWatch alarm to recover the EC2 instance in case of failure.",
      "D": "Launch the EC2 instance with two Amazon Elastic Block Store (Amazon EBS) volumes that use RAID configurations for storage redundancy."
    },
    "correct_answer": "C",
    "vote_percentage": "68%",
    "question_cn": "一家公司正在 Amazon EC2 实例上运行一个旧系统。应用程序代码无法修改，并且系统无法在多个实例上运行。解决方案架构师必须设计一个弹性解决方案，以改善系统的恢复时间。解决方案架构师应推荐什么来满足这些要求？",
    "options_cn": {
      "A": "为 EC2 实例启用终止保护。",
      "B": "为多可用区 (Multi-AZ) 部署配置 EC2 实例。",
      "C": "创建一个 Amazon CloudWatch 警报，以在发生故障时恢复 EC2 实例。",
      "D": "启动具有两个 Amazon Elastic Block Store (Amazon EBS) 卷的 EC2 实例，这些卷使用 RAID 配置来实现存储冗余。"
    },
    "tags": [
      "EC2",
      "Recovery",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 68%），解析仅供参考。】\n\n考查在 EC2 实例上运行的旧系统，如何在无法修改代码且不支持多实例部署的情况下，通过增强恢复能力来提高弹性。需要关注故障恢复方案。",
      "why_correct": "选项 C 提供了针对 EC2 实例的故障恢复方案。通过创建 Amazon CloudWatch 警报，可以在 EC2 实例出现故障时自动触发恢复操作，如重启实例或启动新实例，从而缩短恢复时间。这满足了题目中应用程序代码不可修改且系统无法在多个实例上运行的需求，通过监控和自动恢复机制来增强弹性。",
      "why_wrong": "选项 A，启用终止保护，仅防止 EC2 实例被意外终止，无法解决实例故障。选项 B，多可用区部署需要应用程序支持多实例运行，而题目要求系统无法在多个实例上运行，因此不适用。选项 D，使用 RAID 配置的两个 EBS 卷，仅提供了存储冗余，无法解决实例故障导致的系统不可用问题。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 instance",
      "Amazon CloudWatch",
      "Amazon EBS",
      "Multi-AZ",
      "RAID"
    ]
  },
  {
    "id": 758,
    "topic": "1",
    "question_en": "A company wants to deploy its containerized application workloads to a VPC across three Availability Zones. The company needs a solution that is highly available across Availability Zones. The solution must require minimal changes to the application. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS Service Auto Scaling to use target tracking scaling. Set the minimum capacity to 3. Set the task placement strategy type to spread with an Availability Zone attribute.",
      "B": "Use Amazon Elastic Kubernetes Service (Amazon EKS) self-managed nodes. Configure Application Auto Scaling to use target tracking scaling. Set the minimum capacity to 3.",
      "C": "Use Amazon EC2 Reserved Instances. Launch three EC2 instances in a spread placement group. Configure an Auto Scaling group to use target tracking scaling. Set the minimum capacity to 3.",
      "D": "Use an AWS Lambda function. Configure the Lambda function to connect to a VPC. Configure Application Auto Scaling to use Lambda as a scalable target. Set the minimum capacity to 3."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其容器化应用程序工作负载部署到跨越三个可用区的 VPC 中。该公司需要一个在可用区之间具有高可用性的解决方案。该解决方案必须对应用程序进行最少的更改。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic Container Service (Amazon ECS)。配置 Amazon ECS 服务自动伸缩以使用目标跟踪伸缩。将最小容量设置为 3。将任务放置策略类型设置为按可用区属性进行分散。",
      "B": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 自管理节点。配置 Application Auto Scaling 以使用目标跟踪伸缩。将最小容量设置为 3。",
      "C": "使用 Amazon EC2 预留实例。在分散放置组中启动三个 EC2 实例。配置一个 Auto Scaling 组以使用目标跟踪伸缩。将最小容量设置为 3。",
      "D": "使用 AWS Lambda 函数。配置 Lambda 函数以连接到 VPC。配置 Application Auto Scaling 以使用 Lambda 作为可伸缩目标。将最小容量设置为 3。"
    },
    "tags": [
      "ECS",
      "EKS",
      "Auto Scaling",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在多可用区 VPC 中部署容器化应用程序，并实现高可用性和最小运维开销的解决方案。",
      "why_correct": "Amazon ECS 提供了托管的容器编排服务，可以轻松部署和管理容器化应用程序。通过配置 Amazon ECS 服务自动伸缩，并结合任务放置策略，可以在多个可用区中实现高可用性。将最小容量设置为 3 确保了在所有可用区中都有任务运行，以满足题目对高可用性的要求。",
      "why_wrong": "B 选项使用 Amazon EKS 自管理节点，增加了运维复杂性，与题目要求的“最少运营开销”相悖。C 选项使用 EC2 实例和 Auto Scaling 组，虽然可以实现高可用性，但相比 ECS，需要更多的配置和管理，增加了运维负担。D 选项使用 AWS Lambda 函数，Lambda 函数不直接支持容器化应用程序，因此无法满足题目的需求，且 Lambda 的部署方式与容器化应用程序的部署方式不符。"
    },
    "related_terms": [
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Application Auto Scaling",
      "Amazon EC2",
      "Auto Scaling",
      "AWS Lambda",
      "VPC"
    ]
  },
  {
    "id": 759,
    "topic": "1",
    "question_en": "A media company stores movies in Amazon S3. Each movie is stored in a single video file that ranges from 1 GB to 10 GB in size. The company must be able to provide the streaming content of a movie within 5 minutes of a user purchase. There is higher demand for movies that are less than 20 years old than for movies that are more than 20 years old. The company wants to minimize hosting service costs based on demand. Which solution will meet these requirements?",
    "options_en": {
      "A": "Store all media content in Amazon S3. Use S3 Lifecycle policies to move media data into the Infrequent Access tier when the demand for a movie decreases.",
      "B": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Standard-infrequent Access (S3 Standard-IA). When a user orders an older movie, retrieve the video file by using standard retrieval.",
      "C": "Store newer movie video files in S3 Intelligent-Tiering. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using expedited retrieval.",
      "D": "Store newer movie video files in S3 Standard. Store older movie video files in S3 Glacier Flexible Retrieval. When a user orders an older movie, retrieve the video file by using bulk retrieval."
    },
    "correct_answer": "C",
    "vote_percentage": "50%",
    "question_cn": "一家媒体公司将电影存储在 Amazon S3 中。每部电影存储在一个单独的视频文件中，大小从 1 GB 到 10 GB 不等。该公司必须能够在用户购买后 5 分钟内提供电影的流媒体内容。对 20 年以下的电影的需求高于对 20 年以上的电影的需求。该公司希望根据需求最大限度地降低托管服务成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将所有媒体内容存储在 Amazon S3 中。使用 S3 生命周期策略将媒体数据移动到不频繁访问层，当对电影的需求下降时。",
      "B": "将较新的电影视频文件存储在 S3 Standard 中。将较旧的电影视频文件存储在 S3 Standard-infrequent Access (S3 Standard-IA) 中。当用户订购一部较旧的电影时，使用标准检索方式检索视频文件。",
      "C": "将较新的电影视频文件存储在 S3 Intelligent-Tiering 中。将较旧的电影视频文件存储在 S3 Glacier Flexible Retrieval 中。当用户订购一部较旧的电影时，使用加速检索方式检索视频文件。",
      "D": "将较新的电影视频文件存储在 S3 Standard 中。将较旧的电影视频文件存储在 S3 Glacier Flexible Retrieval 中。当用户订购一部较旧的电影时，使用批量检索方式检索视频文件。"
    },
    "tags": [
      "S3",
      "Lifecycle",
      "Intelligent-Tiering",
      "Glacier"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 50%），解析仅供参考。】\n\n考查 S3 存储类的选择和检索方式，以及如何根据访问频率和成本优化存储。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：S3 Intelligent-Tiering 能够自动优化存储成本，适合对访问模式不确定的数据。S3 Glacier Flexible Retrieval 适合存档不常访问的数据。使用加速检索方式能够满足在 5 分钟内提供流媒体内容的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 仅使用 S3，未考虑成本优化。选项 B 虽然使用了 S3 Standard-IA，但无法满足 5 分钟内提供流媒体内容的要求，因为检索时间较长。选项 D 的 S3 Glacier Flexible Retrieval 搭配批量检索，同样不满足 5 分钟内提供流媒体内容的要求，检索时间过长。选项 B 和 D 的问题均在于无法快速检索归档数据。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard",
      "S3 Standard-infrequent Access (S3 Standard-IA)",
      "S3 Intelligent-Tiering",
      "S3 Glacier Flexible Retrieval",
      "Accelerated retrieval",
      "Bulk retrieval",
      "S3 lifecycle policy"
    ]
  },
  {
    "id": 760,
    "topic": "1",
    "question_en": "A solutions architect needs to design the architecture for an application that a vendor provides as a Docker container image. The container needs 50 GB of storage available for temporary files. The infrastructure must be serverless. Which solution meets these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Lambda function that uses the Docker container image with an Amazon S3 mounted volume that has more than 50 GB of space.",
      "B": "Create an AWS Lambda function that uses the Docker container image with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space.",
      "C": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the AWS Fargate launch type. Create a task definition for the container image with an Amazon Elastic File System (Amazon EFS) volume. Create a service with that task definition.",
      "D": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that uses the Amazon EC2 launch type with an Amazon Elastic Block Store (Amazon EBS) volume that has more than 50 GB of space. Create a task definition for the container image. Create a service with that task definition."
    },
    "correct_answer": "C",
    "vote_percentage": "91%",
    "question_cn": "解决方案架构师需要为供应商以 Docker 容器镜像形式提供的应用程序设计架构。容器需要 50 GB 的可用存储空间用于临时文件。基础设施必须是无服务器的。哪个解决方案能以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，该函数使用 Docker 容器镜像，并挂载一个具有超过 50 GB 空间的 Amazon S3 卷。",
      "B": "创建一个 AWS Lambda 函数，该函数使用 Docker 容器镜像，并挂载一个具有超过 50 GB 空间的 Amazon Elastic Block Store (Amazon EBS) 卷。",
      "C": "创建一个 Amazon Elastic Container Service (Amazon ECS) 集群，该集群使用 AWS Fargate 启动类型。为容器镜像创建一个任务定义，其中包含一个 Amazon Elastic File System (Amazon EFS) 卷。创建一个使用该任务定义的服务。",
      "D": "创建一个 Amazon Elastic Container Service (Amazon ECS) 集群，该集群使用 Amazon EC2 启动类型，其中包含一个 Amazon Elastic Block Store (Amazon EBS) 卷，该卷具有超过 50 GB 的空间。为容器镜像创建一个任务定义。创建一个使用该任务定义的服务。"
    },
    "tags": [
      "Lambda",
      "Docker",
      "ECS",
      "EFS",
      "EBS",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 91%），解析仅供参考。】\n\n考查在无服务器环境下运行 Docker 容器镜像的需求，以及选择合适的存储解决方案。",
      "why_correct": "选项 C 使用了 AWS Fargate 和 Amazon EFS。Fargate 是无服务器的容器运行引擎，满足了题目的无服务器要求。Amazon EFS 提供了可扩展的、持久的文件存储，适合 Docker 容器的临时文件需求，并可以灵活地扩展存储容量以满足 50 GB 的要求。",
      "why_wrong": "选项 A 错误，AWS Lambda 函数不支持直接挂载 S3 卷。选项 B 错误，Lambda 函数虽然支持使用容器镜像，但无法挂载 EBS 卷。选项 D 错误，虽然 ECS 支持 EBS 卷和 Docker 容器，但使用了 EC2 启动类型，不符合无服务器的要求，且需要额外的运维开销管理 EC2 实例。"
    },
    "related_terms": [
      "Docker",
      "Amazon S3",
      "AWS Lambda",
      "Amazon EBS",
      "Amazon Elastic Container Service (Amazon ECS)",
      "AWS Fargate",
      "Amazon Elastic File System (Amazon EFS)",
      "EC2"
    ]
  },
  {
    "id": 761,
    "topic": "1",
    "question_en": "A company needs to use its on-premises LDAP directory service to authenticate its users to the AWS Management Console. The directory service is not compatible with Security Assertion Markup Language (SAML). Which solution meets these requirements?",
    "options_en": {
      "A": "Enable AWS IAM Identity Center (AWS Single Sign-On) between AWS and the on-premises LDAP.",
      "B": "Create an IAM policy that uses AWS credentials, and integrate the policy into LDAP.",
      "C": "Set up a process that rotates the IAM credentials whenever LDAP credentials are updated.",
      "D": "Develop an on-premises custom identity broker application or process that uses AWS Security Token Service (AWS STS) to get short- lived credentials."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要使用其本地 LDAP 目录服务对用户进行 AWS Management Console 的身份验证。 该目录服务与 Security Assertion Markup Language (SAML) 不兼容。哪种解决方案符合这些要求？",
    "options_cn": {
      "A": "在 AWS 和本地 LDAP 之间启用 AWS IAM Identity Center (AWS Single Sign-On)。",
      "B": "创建一个使用 AWS 凭证的 IAM policy，并将该 policy 集成到 LDAP 中。",
      "C": "设置一个流程，以便在 LDAP 凭证更新时轮换 IAM 凭证。",
      "D": "开发一个本地自定义身份代理应用程序或流程，该应用程序或流程使用 AWS Security Token Service (AWS STS) 获取短期凭证。"
    },
    "tags": [
      "LDAP",
      "IAM",
      "STS",
      "Security Token Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查通过本地 LDAP 目录服务对 AWS Management Console 进行身份验证的解决方案，且该目录服务不兼容 SAML。",
      "why_correct": "选项 D 提供了正确的解决方案。通过开发一个自定义身份代理，该代理使用 AWS STS 获得短期凭证，从而允许用户使用其本地 LDAP 凭证进行身份验证。这种方法绕过了对 SAML 的需求，满足了题目对目录服务不兼容 SAML 的要求，并且使用了 AWS 提供的服务来保证安全性。",
      "why_wrong": "选项 A 错误，因为 AWS IAM Identity Center (AWS Single Sign-On) 通常用于 SAML 集成，而题目中 LDAP 目录服务不兼容 SAML。选项 B 错误，IAM policy 集成到 LDAP 中不可行，IAM policy 与用户凭证的结合方式不正确。选项 C 错误，轮换 IAM 凭证的方案无法解决本地 LDAP 身份验证的问题，并且不安全，通常不推荐直接更新 IAM 凭证。"
    },
    "related_terms": [
      "AWS Management Console",
      "LDAP",
      "SAML",
      "AWS IAM Identity Center (AWS Single Sign-On)",
      "IAM policy",
      "AWS Security Token Service (AWS STS)",
      "IAM"
    ]
  },
  {
    "id": 762,
    "topic": "1",
    "question_en": "A company stores multiple Amazon Machine Images (AMIs) in an AWS account to launch its Amazon EC2 instances. The AMIs contain critical data and configurations that are necessary for the company’s operations. The company wants to implement a solution that will recover accidentally deleted AMIs quickly and eficiently. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create Amazon Elastic Block Store (Amazon EBS) snapshots of the AMIs. Store the snapshots in a separate AWS account.",
      "B": "Copy all AMIs to another AWS account periodically.",
      "C": "Create a retention rule in Recycle Bin.",
      "D": "Upload the AMIs to an Amazon S3 bucket that has Cross-Region Replication."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 账户中存储多个 Amazon Machine Images (AMI) 以启动其 Amazon EC2 实例。 AMI 包含对公司运营至关重要的关键数据和配置。该公司希望实施一个解决方案，以便快速有效地恢复意外删除的 AMI。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "为 AMI 创建 Amazon Elastic Block Store (Amazon EBS) 快照。将快照存储在单独的 AWS 账户中。",
      "B": "定期将所有 AMI 复制到另一个 AWS 账户。",
      "C": "在回收站中创建保留规则。",
      "D": "将 AMI 上传到已启用 S3 跨区域复制的 Amazon S3 存储桶。"
    },
    "tags": [
      "AMI",
      "EBS",
      "S3",
      "Cross-Region Replication"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在 AWS 环境中，针对 AMI 意外删除场景，使用最少运营开销实现快速恢复的解决方案。",
      "why_correct": "选项 C 正确，因为在回收站中创建保留规则能够有效保护 AMI。通过启用回收站，被删除的 AMI 将被保留一段时间，如果需要恢复，则可以快速将其从回收站中恢复，满足了快速恢复的需求。此外，回收站的管理开销较低，符合题干中“最少的运营开销”的要求。",
      "why_wrong": "选项 A 错误，虽然 EBS 快照可以用于数据备份，但恢复 AMI 的过程比回收站更复杂，并且快照需要单独管理和维护，运营开销较高。选项 B 错误，复制 AMI 到另一个账户需要额外的存储成本和管理成本，且需要手动或自动化脚本进行复制，运维成本高。选项 D 错误，虽然将 AMI 上传到 S3 可以实现数据冗余，但从 S3 恢复 AMI 的过程较为复杂，且无法直接快速恢复 AMI，操作开销较高，不满足快速恢复的需求。"
    },
    "related_terms": [
      "Amazon Machine Images (AMI)",
      "Amazon EC2",
      "Amazon Elastic Block Store (Amazon EBS)",
      "AWS",
      "Amazon S3",
      "S3 cross-region replication",
      "Recycle Bin"
    ]
  },
  {
    "id": 763,
    "topic": "1",
    "question_en": "A company has 150 TB of archived image data stored on-premises that needs to be moved to the AWS Cloud within the next month. The company’s current network connection allows up to 100 Mbps uploads for this purpose during the night only. What is the MOST cost-effective mechanism to move this data and meet the migration deadline?",
    "options_en": {
      "A": "Use AWS Snowmobile to ship the data to AWS.",
      "B": "Order multiple AWS Snowball devices to ship the data to AWS.",
      "C": "Enable Amazon S3 Transfer Acceleration and securely upload the data.",
      "D": "Create an Amazon S3 VPC endpoint and establish a VPN to upload the data."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有 150 TB 的存档图像数据存储在本地，需要在下个月内将其移动到 AWS 云。该公司当前的网路连接仅允许夜间上传，速度高达 100 Mbps。哪种机制最经济高效，可以将这些数据移动并满足迁移截止日期？",
    "options_cn": {
      "A": "使用 AWS Snowmobile 将数据运送到 AWS。",
      "B": "订购多个 AWS Snowball 设备以将数据运送到 AWS。",
      "C": "打开 Amazon S3 Transfer Acceleration 并安全地上传数据。",
      "D": "创建 Amazon S3 VPC endpoint 并建立 VPN 以上传数据。"
    },
    "tags": [
      "Snowmobile",
      "Snowball",
      "S3",
      "Transfer Acceleration",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查了大规模数据迁移场景下的 AWS 服务选择，以及数据传输速度、成本效益和截止日期等关键因素的综合考量。",
      "why_correct": "选项 B 使用 AWS Snowball 设备是经济高效且适合的选择。Snowball 设备可以离线运送大量数据，绕过网络带宽限制，满足 150 TB 数据量在截止日期前迁移的需求。通过订购多个 Snowball 设备，可以并行上传数据，进一步缩短迁移时间。",
      "why_wrong": "选项 A，AWS Snowmobile 适用于百 PB 级别的数据迁移，对于 150 TB 的数据量来说，成本过高。选项 C，Amazon S3 Transfer Acceleration 虽然可以加速上传，但受限于 100 Mbps 的网络带宽，无法在截止日期前完成迁移。选项 D，通过 VPC endpoint 和 VPN 上传数据，同样受限于带宽，且配置较为复杂，不适合当前场景，也无法满足截止日期要求。"
    },
    "related_terms": [
      "AWS Snowmobile",
      "AWS Snowball",
      "Amazon S3 Transfer Acceleration",
      "Amazon S3 VPC endpoint",
      "VPN",
      "AWS"
    ]
  },
  {
    "id": 764,
    "topic": "1",
    "question_en": "A company wants to migrate its three-tier application from on premises to AWS. The web tier and the application tier are running on third- party virtual machines (VMs). The database tier is running on MySQL. The company needs to migrate the application by making the fewest possible changes to the architecture. The company also needs a database solution that can restore data to a specific point in time. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Migrate the web tier and the application tier to Amazon EC2 instances in private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.",
      "B": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to EC2 instances in private subnets. Migrate the database tier to Amazon Aurora MySQL in private subnets.",
      "C": "Migrate the web tier to Amazon EC2 instances in public subnets. Migrate the application tier to EC2 instances in private subnets. Migrate the database tier to Amazon RDS for MySQL in private subnets.",
      "D": "Migrate the web tier and the application tier to Amazon EC2 instances in public subnets. Migrate the database tier to Amazon Aurora MySQL in public subnets."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司希望将其三层应用程序从本地环境迁移到 AWS。Web 层和应用程序层运行在第三方虚拟机 (VM) 上。数据库层运行在 MySQL 上。该公司需要通过对架构进行最少的更改来迁移应用程序。该公司还需要一个数据库解决方案，该解决方案可以将数据恢复到特定的时间点。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将 Web 层和应用程序层迁移到私有子网中的 Amazon EC2 实例。将数据库层迁移到私有子网中的 Amazon RDS for MySQL。",
      "B": "将 Web 层迁移到公共子网中的 Amazon EC2 实例。将应用程序层迁移到私有子网中的 EC2 实例。将数据库层迁移到私有子网中的 Amazon Aurora MySQL。",
      "C": "将 Web 层迁移到公共子网中的 Amazon EC2 实例。将应用程序层迁移到私有子网中的 EC2 实例。将数据库层迁移到私有子网中的 Amazon RDS for MySQL。",
      "D": "将 Web 层和应用程序层迁移到公共子网中的 Amazon EC2 实例。将数据库层迁移到公共子网中的 Amazon Aurora MySQL。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Aurora",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考查了如何将三层应用程序迁移到 AWS，并满足最小架构更改和时间点恢复的要求。",
      "why_correct": "选项 B 提供了最佳解决方案。使用 Amazon Aurora MySQL 数据库，满足时间点恢复的需求，并且 Aurora 在性能和扩展性方面通常优于 RDS。将 Web 层放置在公共子网中，应用程序层放置在私有子网中，符合常见的安全最佳实践。",
      "why_wrong": "选项 A 错误，虽然 RDS for MySQL 具备时间点恢复能力，但相比 Aurora MySQL 性能和扩展性较差。选项 C 与选项 A 相似，且未充分利用 Aurora 的优势。选项 D 错误，将数据库层放置在公共子网中违反了安全最佳实践，且 Web 层和应用程序层都放在了公共子网中，降低了安全性。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS for MySQL",
      "MySQL",
      "Amazon Aurora MySQL",
      "Web layer",
      "Application layer",
      "Database layer",
      "Private subnet",
      "Public subnet",
      "VM"
    ]
  },
  {
    "id": 765,
    "topic": "1",
    "question_en": "A development team is collaborating with another company to create an integrated product. The other company needs to access an Amazon Simple Queue Service (Amazon SQS) queue that is contained in the development team's account. The other company wants to poll the queue without giving up its own account permissions to do so. How should a solutions architect provide access to the SQS queue?",
    "options_en": {
      "A": "Create an instance profile that provides the other company access to the SQS queue.",
      "B": "Create an IAM policy that provides the other company access to the SQS queue.",
      "C": "Create an SQS access policy that provides the other company access to the SQS queue.",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) access policy that provides the other company access to the SQS queue."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一个开发团队正与另一家公司合作创建一个集成产品。另一家公司需要访问开发团队账户中包含的 Amazon Simple Queue Service (Amazon SQS) 队列。另一家公司希望轮询该队列，而不放弃其自身的账户权限。解决方案架构师应该如何提供对 SQS 队列的访问权限？",
    "options_cn": {
      "A": "创建一个实例配置文件，为另一家公司提供对 SQS 队列的访问权限。",
      "B": "创建一个 IAM 策略，为另一家公司提供对 SQS 队列的访问权限。",
      "C": "创建一个 SQS 访问策略，为另一家公司提供对 SQS 队列的访问权限。",
      "D": "创建一个 Amazon Simple Notification Service (Amazon SNS) 访问策略，为另一家公司提供对 SQS 队列的访问权限。"
    },
    "tags": [
      "Amazon SQS",
      "IAM",
      "IAM Role",
      "Instance Profile"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查如何为其他 AWS 账户安全地授予对 SQS 队列的访问权限。",
      "why_correct": "SQS 访问策略允许您控制谁可以访问 SQS 队列以及如何访问。通过创建针对另一家公司的账户的 SQS 访问策略，可以安全地授予该账户轮询队列的权限，而无需共享凭证或放弃账户权限。这种方法符合题目的需求，即安全访问和避免权限泄露。",
      "why_wrong": "选项 A 涉及实例配置文件，这主要用于授予 EC2 实例访问权限，与此场景不符，且无法直接授权给其他账户。选项 B 涉及 IAM 策略，IAM 策略用于控制账户内的访问权限，不能直接跨账户使用。选项 D 涉及 SNS，SNS 用于发布和订阅消息，与 SQS 的队列轮询功能不相关，并且不能用于授予 SQS 队列的访问权限。"
    },
    "related_terms": [
      "Amazon Simple Queue Service (Amazon SQS)",
      "SQS",
      "IAM",
      "Amazon Simple Notification Service (Amazon SNS)",
      "EC2"
    ]
  },
  {
    "id": 766,
    "topic": "1",
    "question_en": "A company’s developers want a secure way to gain SSH access on the company's Amazon EC2 instances that run the latest version of Amazon Linux. The developers work remotely and in the corporate ofice. The company wants to use AWS services as a part of the solution. The EC2 instances are hosted in a VPC private subnet and access the internet through a NAT gateway that is deployed in a public subnet. What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a bastion host in the same subnet as the EC2 instances. Grant the ec2:CreateVpnConnection IAM permission to the developers. Install EC2 Instance Connect so that the developers can connect to the EC2 instances.",
      "B": "Create an AWS Site-to-Site VPN connection between the corporate network and the VPC. Instruct the developers to use the Site-to-Site VPN connection to access the EC2 instances when the developers are on the corporate network. Instruct the developers to set up another VPN connection for access when they work remotely.",
      "C": "Create a bastion host in the public subnet of the VPConfigure the security groups and SSH keys of the bastion host to only allow connections and SSH authentication from the developers’ corporate and remote networks. Instruct the developers to connect through the bastion host by using SSH to reach the EC2 instances.",
      "D": "Attach the AmazonSSMManagedInstanceCore IAM policy to an IAM role that is associated with the EC2 instances. Instruct the developers to use AWS Systems Manager Session Manager to access the EC2 instances."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司的开发人员希望通过安全的方式访问在其运行最新版 Amazon Linux 的公司 Amazon EC2 实例上的 SSH。开发人员可以在远程和公司办公室工作。该公司希望将 AWS 服务用作解决方案的一部分。EC2 实例托管在 VPC 私有子网中，并通过部署在公共子网中的 NAT 网关访问互联网。解决方案架构师应如何以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "在与 EC2 实例相同的子网中创建一个堡垒主机。授予开发人员 ec2:CreateVpnConnection IAM 权限。安装 EC2 Instance Connect，以便开发人员可以连接到 EC2 实例。",
      "B": "在公司网络和 VPC 之间创建 AWS Site-to-Site VPN 连接。指示开发人员使用 Site-to-Site VPN 连接来访问 EC2 实例（当开发人员在公司网络上时）。指示开发人员设置另一个 VPN 连接以供他们在远程工作时访问。",
      "C": "在 VPC 的公共子网中创建一个堡垒主机。配置堡垒主机的安全组和 SSH 密钥，仅允许来自开发人员的公司和远程网络的连接和 SSH 身份验证。指示开发人员通过堡垒主机使用 SSH 连接到 EC2 实例。",
      "D": "将 AmazonSSMManagedInstanceCore IAM 策略附加到与 EC2 实例关联的 IAM 角色。指示开发人员使用 AWS Systems Manager Session Manager 访问 EC2 实例。"
    },
    "tags": [
      "Amazon EC2",
      "VPC",
      "Site-to-Site VPN",
      "SSH",
      "NAT Gateway",
      "AWS Systems Manager Session Manager",
      "Bastion Host",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察使用 AWS Systems Manager Session Manager 安全、经济高效地访问 EC2 实例。",
      "why_correct": "选项 D 提供了最经济高效的解决方案。使用 AWS Systems Manager Session Manager 可以通过 AWS 提供的管理通道访问 EC2 实例，无需使用公网 IP、堡垒机或 VPN。 只需要将 IAM 策略附加到 EC2 实例的角色上，开发人员即可通过 AWS Management Console 或 AWS CLI 安全地访问实例。",
      "why_wrong": "选项 A 在相同的私有子网中创建堡垒主机是不必要的，且需要额外的运维成本。赋予开发人员 `ec2:CreateVpnConnection` 权限是不必要的，并且与题干的安全要求不符。选项 B 使用 Site-to-Site VPN 需要配置和维护 VPN 连接，增加了复杂性和成本。选项 C 虽然使用了堡垒机，但需要手动管理安全组和 SSH 密钥，相比 Session Manager 增加了管理负担和潜在的安全风险。"
    },
    "related_terms": [
      "Amazon EC2",
      "SSH",
      "VPC",
      "NAT Gateway",
      "Amazon Linux",
      "IAM",
      "EC2 Instance Connect",
      "AWS Site-to-Site VPN",
      "Security Group",
      "AWS Systems Manager Session Manager",
      "AmazonSSMManagedInstanceCore"
    ]
  },
  {
    "id": 767,
    "topic": "1",
    "question_en": "A pharmaceutical company is developing a new drug. The volume of data that the company generates has grown exponentially over the past few months. The company's researchers regularly require a subset of the entire dataset to be immediately available with minimal lag. However, the entire dataset does not need to be accessed on a daily basis. All the data currently resides in on-premises storage arrays, and the company wants to reduce ongoing capital expenses. Which storage solution should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Run AWS DataSync as a scheduled cron job to migrate the data to an Amazon S3 bucket on an ongoing basis.",
      "B": "Deploy an AWS Storage Gateway file gateway with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.",
      "C": "Deploy an AWS Storage Gateway volume gateway with cached volumes with an Amazon S3 bucket as the target storage. Migrate the data to the Storage Gateway appliance.",
      "D": "Configure an AWS Site-to-Site VPN connection from the on-premises environment to AWS. Migrate data to an Amazon Elastic File System (Amazon EFS) file system."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家制药公司正在开发一种新药。在过去几个月中，该公司生成的数据量呈指数级增长。该公司的研究人员需要定期获得整个数据集的子集，并立即可用，且延迟最小。但是，无需每天访问整个数据集。所有数据目前都驻留在本地存储阵列中，并且该公司希望减少持续的资本支出。解决方案架构师应该推荐哪种存储解决方案来满足这些要求？",
    "options_cn": {
      "A": "将 AWS DataSync 作为计划好的 cron 作业运行，以持续将数据迁移到 Amazon S3 存储桶。",
      "B": "部署一个 AWS Storage Gateway 文件网关，并将 Amazon S3 存储桶作为目标存储。将数据迁移到 Storage Gateway 设备。",
      "C": "部署一个带有缓存卷的 AWS Storage Gateway 卷网关，并将 Amazon S3 存储桶作为目标存储。将数据迁移到 Storage Gateway 设备。",
      "D": "配置从本地环境到 AWS 的 AWS Site-to-Site VPN 连接。将数据迁移到 Amazon Elastic File System (Amazon EFS) 文件系统。"
    },
    "tags": [
      "AWS Storage Gateway",
      "Amazon S3",
      "File Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查了如何选择合适的存储解决方案，以满足数据量指数级增长、需要快速访问子集数据、减少资本支出等需求。",
      "why_correct": "选项 C 正确，因为它利用了 Storage Gateway 的缓存卷功能。卷网关可以缓存常用的数据到本地，以便快速访问。将数据存储在 Amazon S3 中，满足了减少资本支出的需求。研究人员可以快速获取数据集的子集，因为数据已缓存在本地。",
      "why_wrong": "选项 A 错误，因为虽然 DataSync 可以将数据迁移到 S3，但它无法提供快速访问已迁移数据的子集的能力。选项 B 错误，文件网关的设计目标是提供对 S3 数据的基于文件的访问，但它并不侧重于快速访问子集数据，并且性能不如缓存卷。选项 D 错误，虽然 EFS 提供文件系统访问，但它没有本地缓存功能，无法满足快速访问子集数据的需求，并且成本较高。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon S3",
      "AWS Storage Gateway",
      "AWS Storage Gateway File Gateway",
      "AWS Storage Gateway Volume Gateway",
      "Amazon Elastic File System (Amazon EFS)",
      "AWS Site-to-Site VPN"
    ]
  },
  {
    "id": 768,
    "topic": "1",
    "question_en": "A company has a business-critical application that runs on Amazon EC2 instances. The application stores data in an Amazon DynamoDB table. The company must be able to revert the table to any point within the last 24 hours. Which solution meets these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure point-in-time recovery for the table.",
      "B": "Use AWS Backup for the table.",
      "C": "Use an AWS Lambda function to make an on-demand backup of the table every hour.",
      "D": "Turn on streams on the table to capture a log of all changes to the table in the last 24 hours. Store a copy of the stream in an Amazon S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个在 Amazon EC2 实例上运行的关键业务应用程序。该应用程序将数据存储在 Amazon DynamoDB 表中。该公司必须能够将表恢复到过去 24 小时内的任何时间点。哪个解决方案能以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "为表配置时间点恢复。",
      "B": "将 AWS Backup 用于该表。",
      "C": "使用 AWS Lambda 函数每小时对该表进行按需备份。",
      "D": "打开表的流，以捕获过去 24 小时内对表的所有更改的日志。在 Amazon S3 存储桶中存储流的副本。"
    },
    "tags": [
      "Amazon DynamoDB",
      "DynamoDB Point-in-time Recovery",
      "AWS Lambda",
      "Amazon S3",
      "DynamoDB Streams",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 Amazon DynamoDB 的时间点恢复 (Point-in-time recovery, PITR) 功能。该功能允许将 DynamoDB 表恢复到过去 35 天内的任何时间点。",
      "why_correct": "为 DynamoDB 表配置时间点恢复 (PITR) 是满足需求的最佳解决方案。PITR 允许在过去 35 天内的任何时间点恢复 DynamoDB 表，满足了将表恢复到过去 24 小时内任何时间点的要求。配置简单，运营开销低，是针对此场景最直接、最有效的解决方案。",
      "why_wrong": "选项 B 使用 AWS Backup 用于 DynamoDB 表，虽然可以实现备份和恢复，但增加了额外的管理和配置复杂性，运营开销高于直接使用 PITR。选项 C 使用 AWS Lambda 函数进行按需备份，需要自定义实现备份逻辑，增加了维护工作量和潜在的错误风险，运营开销较高。选项 D 打开表的流并将日志存储在 Amazon S3 中，这是一种数据变更捕获的方法，但不是直接的备份和恢复解决方案，需要自定义开发恢复逻辑，实现复杂，且恢复效率较低。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon DynamoDB",
      "AWS Backup",
      "AWS Lambda",
      "Amazon S3",
      "Point-in-time recovery (PITR)"
    ]
  },
  {
    "id": 769,
    "topic": "1",
    "question_en": "A company hosts an application used to upload files to an Amazon S3 bucket. Once uploaded, the files are processed to extract metadata, which takes less than 5 seconds. The volume and frequency of the uploads varies from a few files each hour to hundreds of concurrent uploads. The company has asked a solutions architect to design a cost-effective architecture that will meet these requirements. What should the solutions architect recommend?",
    "options_en": {
      "A": "Configure AWS CloudTrail trails to log S3 API calls. Use AWS AppSync to process the files.",
      "B": "Configure an object-created event notification within the S3 bucket to invoke an AWS Lambda function to process the files.",
      "C": "Configure Amazon Kinesis Data Streams to process and send data to Amazon S3. Invoke an AWS Lambda function to process the files.",
      "D": "Configure an Amazon Simple Notification Service (Amazon SNS) topic to process the files uploaded to Amazon S3. Invoke an AWS Lambda function to process the files."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司托管一个用于将文件上传到 Amazon S3 存储桶的应用程序。上传后，将处理文件以提取元数据，这需要不到 5 秒的时间。上传量和频率从每小时几个文件到数百个并发上传不等。该公司已要求解决方案架构师设计一个具有成本效益的架构，以满足这些要求。解决方案架构师应推荐什么？",
    "options_cn": {
      "A": "配置 AWS CloudTrail 追踪以记录 S3 API 调用。使用 AWS AppSync 处理文件。",
      "B": "在 S3 存储桶中配置对象创建事件通知，以调用 AWS Lambda 函数来处理文件。",
      "C": "配置 Amazon Kinesis Data Streams 来处理数据并将其发送到 Amazon S3。调用 AWS Lambda 函数来处理文件。",
      "D": "配置一个 Amazon Simple Notification Service (Amazon SNS) 主题来处理上传到 Amazon S3 的文件。调用 AWS Lambda 函数来处理文件。"
    },
    "tags": [
      "Amazon S3",
      "Kinesis Data Streams",
      "Lambda",
      "Event-driven architecture"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何设计一个具有成本效益的架构，用于处理上传到 Amazon S3 的文件，并提取元数据。",
      "why_correct": "选项 B 提供了最经济高效的解决方案。 S3 对象创建事件通知会触发 Lambda 函数，实现异步处理，符合并发上传的需求。这种架构具有良好的可扩展性，并且只在文件上传时才消耗资源，避免了持续的成本。",
      "why_wrong": "选项 A 错误，因为 CloudTrail 用于审计 S3 API 调用，而不是处理上传的文件。AppSync 用于构建 API，与文件处理的需求不符。选项 C 使用 Kinesis Data Streams 处理数据，这通常用于流式数据，而本题场景是文件上传。 Kinesis 引入了额外的复杂性和成本，不符合题目的成本效益要求。选项 D 使用 SNS，但 SNS 本身不直接处理 S3 存储桶中的对象。虽然 SNS 可以触发 Lambda，但其本身并不是触发文件处理的最直接和最有效的方案，而且配置相对复杂。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Lambda",
      "AWS CloudTrail",
      "AWS AppSync",
      "Amazon Kinesis Data Streams",
      "Amazon SNS"
    ]
  },
  {
    "id": 770,
    "topic": "1",
    "question_en": "A company’s application is deployed on Amazon EC2 instances and uses AWS Lambda functions for an event-driven architecture. The company uses nonproduction development environments in a different AWS account to test new features before the company deploys the features to production. The production instances show constant usage because of customers in different time zones. The company uses nonproduction instances only during business hours on weekdays. The company does not use the nonproduction instances on the weekends. The company wants to optimize the costs to run its application on AWS. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use On-Demand Instances for the production instances. Use Dedicated Hosts for the nonproduction instances on weekends only.",
      "B": "Use Reserved Instances for the production instances and the nonproduction instances. Shut down the nonproduction instances when not in use.",
      "C": "Use Compute Savings Plans for the production instances. Use On-Demand Instances for the nonproduction instances. Shut down the nonproduction instances when not in use.",
      "D": "Use Dedicated Hosts for the production instances. Use EC2 Instance Savings Plans for the nonproduction instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的应用程序部署在 Amazon EC2 实例上，并使用 AWS Lambda 函数来实现事件驱动架构。该公司使用位于不同 AWS 账户中的非生产开发环境来测试新功能，然后再将这些功能部署到生产环境。由于来自不同时区的客户，生产实例显示持续使用。该公司仅在工作日的工作时间使用非生产实例。该公司在周末不使用非生产实例。该公司希望优化在 AWS 上运行其应用程序的成本。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "为生产实例使用按需实例。仅在周末为非生产实例使用专用主机。",
      "B": "为生产实例和非生产实例使用预留实例。在不使用时关闭非生产实例。",
      "C": "为生产实例使用计算节省计划。为非生产实例使用按需实例。在不使用时关闭非生产实例。",
      "D": "为生产实例使用专用主机。为非生产实例使用 EC2 实例节省计划。"
    },
    "tags": [
      "EC2",
      "EC2 Dedicated Hosts",
      "EC2 Instance Savings Plans",
      "AWS Lambda",
      "EC2 On-Demand Instances",
      "EC2 Reserved Instances",
      "Compute Savings Plans",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在不同环境下优化 EC2 实例成本的策略，包括选择实例类型、购买选项以及根据使用模式进行优化。",
      "why_correct": "计算节省计划提供灵活的计算资源定价模型，适用于持续使用的生产实例。对于仅在工作日使用的非生产实例，按需实例更具成本效益，因为可以根据需要启动和关闭实例。关闭非生产实例可以进一步降低成本。",
      "why_wrong": "A 选项中，专用主机通常用于满足合规性需求，而非优化成本；在周末使用专用主机对于仅在工作日使用的非生产环境来说成本过高。B 选项中，预留实例虽然可以降低成本，但需要预先承诺计算资源，如果资源利用率不高，则无法充分发挥其优势。D 选项中，专用主机成本较高，不适合非生产环境；EC2 实例节省计划无法动态关闭实例以节省成本。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Lambda",
      "EC2 instances",
      "AWS Account",
      "On-Demand Instances",
      "Dedicated Hosts",
      "Compute Savings Plans",
      "Reserved Instances",
      "EC2 Instance Savings Plans"
    ]
  },
  {
    "id": 771,
    "topic": "1",
    "question_en": "A company stores data in an on-premises Oracle relational database. The company needs to make the data available in Amazon Aurora PostgreSQL for analysis. The company uses an AWS Site-to-Site VPN connection to connect its on-premises network to AWS. The company must capture the changes that occur to the source database during the migration to Aurora PostgreSQL. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use the AWS Database Migration Service (AWS DMS) full-load migration task to migrate the data.",
      "B": "Use AWS DataSync to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL by using the Aurora PostgreSQL aws_s3 extension.",
      "C": "Use the AWS Schema Conversion Tool (AWS SCT) to convert the Oracle schema to Aurora PostgreSQL schema. Use AWS Database Migration Service (AWS DMS) to migrate the existing data and replicate the ongoing changes.",
      "D": "Use an AWS Snowball device to migrate the data to an Amazon S3 bucket. Import the S3 data to Aurora PostgreSQL by using the Aurora PostgreSQL aws_s3 extension."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司将其数据存储在本地 Oracle 关系数据库中。该公司需要将数据提供给 Amazon Aurora PostgreSQL 以进行分析。该公司使用 AWS Site-to-Site VPN 连接将其本地网络连接到 AWS。该公司必须捕获在迁移到 Aurora PostgreSQL 期间对 源 数据库发生的更改。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Schema Conversion Tool (AWS SCT) 将 Oracle 模式转换为 Aurora PostgreSQL 模式。使用 AWS Database Migration Service (AWS DMS) 的完全加载迁移任务来迁移数据。",
      "B": "使用 AWS DataSync 将数据迁移到 Amazon S3 存储桶。使用 Aurora PostgreSQL aws_s3 扩展将 S3 数据导入到 Aurora PostgreSQL。",
      "C": "使用 AWS Schema Conversion Tool (AWS SCT) 将 Oracle 模式转换为 Aurora PostgreSQL 模式。使用 AWS Database Migration Service (AWS DMS) 迁移现有数据并复制正在进行的更改。",
      "D": "使用 AWS Snowball 设备将数据迁移到 Amazon S3 存储桶。使用 Aurora PostgreSQL aws_s3 扩展将 S3 数据导入到 Aurora PostgreSQL。"
    },
    "tags": [
      "AWS DMS",
      "AWS SCT",
      "Amazon Aurora PostgreSQL",
      "Oracle",
      "AWS DataSync",
      "Amazon S3",
      "AWS Snowball",
      "VPN",
      "aws_s3 extension"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查了使用 AWS Database Migration Service (AWS DMS) 进行数据库迁移，并捕获源数据库更改的需求。",
      "why_correct": "选项 C 提供了最完整的解决方案。AWS Schema Conversion Tool (AWS SCT) 用于将 Oracle 模式转换为 Aurora PostgreSQL 模式。AWS Database Migration Service (AWS DMS) 可以迁移现有数据，并在数据迁移过程中复制源数据库发生的持续更改，满足了题目的所有要求。",
      "why_wrong": "选项 A 只提供了完全加载的迁移任务，无法满足捕获源数据库更改的要求。选项 B 使用 AWS DataSync，但并未提供持续更改捕获的机制，且不直接处理数据库迁移。选项 D 使用了 Snowball，这更适合大数据量的情况，对于需要持续捕获更改的需求，没有提供相应的解决方案，并且与 Site-to-Site VPN 的场景关联性弱。"
    },
    "related_terms": [
      "AWS Schema Conversion Tool (AWS SCT)",
      "Amazon Aurora PostgreSQL",
      "AWS Database Migration Service (AWS DMS)",
      "Oracle",
      "AWS Site-to-Site VPN",
      "AWS DataSync",
      "Amazon S3",
      "aws_s3",
      "AWS Snowball"
    ]
  },
  {
    "id": 772,
    "topic": "1",
    "question_en": "A company built an application with Docker containers and needs to run the application in the AWS Cloud. The company wants to use a managed service to host the application. The solution must scale in and out appropriately according to demand on the individual container services. The solution also must not result in additional operational overhead or infrastructure to manage. Which solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate.",
      "B": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate.",
      "C": "Provision an Amazon API Gateway API. Connect the API to AWS Lambda to run the containers.",
      "D": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 worker nodes",
      "E": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes."
    },
    "correct_answer": "AC",
    "vote_percentage": "67%",
    "question_cn": "一家公司使用 Docker 容器构建了一个应用程序，需要在 AWS 云中运行该应用程序。该公司希望使用托管服务来托管该应用程序。该解决方案必须根据各个容器服务的需求进行适当的横向扩展和缩减。该解决方案还不能导致额外的运营开销或需要管理的架构。哪种解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate。",
      "B": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 AWS Fargate。",
      "C": "配置 Amazon API Gateway API。将 API 连接到 AWS Lambda 以运行容器。",
      "D": "使用 Amazon Elastic Container Service (Amazon ECS) 和 Amazon EC2 工作节点。",
      "E": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 Amazon EC2 工作节点。"
    },
    "tags": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon EKS",
      "Amazon EC2",
      "Docker",
      "AWS Lambda",
      "Amazon API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 67%），解析仅供参考。】\n\n考察使用托管容器服务的选型；与 ECS、EKS、EC2、Fargate 等服务的功能特性，以及无服务器架构的考量相关。",
      "why_correct": "Amazon ECS 与 AWS Fargate 结合使用，提供了完全托管的容器编排服务。Fargate 允许您在无需管理 EC2 实例的情况下运行容器，从而满足了对托管服务和减少运营开销的需求。ECS 负责容器的调度、横向扩展和缩减，而 Fargate 则负责底层基础设施的管理，非常适合此类场景。",
      "why_wrong": "B. 使用 EKS 和 Fargate 也是一种托管解决方案，但 EKS 本身比 ECS 更复杂，可能带来额外的管理开销。C. 使用 API Gateway 和 Lambda 虽然可以运行容器，但这种方法并不直接支持容器的编排和横向扩展，而且对容器的运行环境控制较弱，也并非主要针对容器的应用场景。D. 使用 ECS 和 EC2 工作节点，虽然提供了灵活性，但需要用户自己管理 EC2 实例，这增加了运营开销。E. 使用 EKS 和 EC2 工作节点，也需要用户管理 EC2 实例，同样增加了运营开销，并且 EKS 的复杂性也高于 ECS。"
    },
    "related_terms": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon EKS",
      "Amazon EC2",
      "Docker",
      "AWS Lambda",
      "Amazon API Gateway",
      "EC2",
      "EBS"
    ]
  },
  {
    "id": 773,
    "topic": "1",
    "question_en": "An ecommerce company is running a seasonal online sale. The company hosts its website on Amazon EC2 instances spanning multiple Availability Zones. The company wants its website to manage sudden trafic increases during the sale. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an Auto Scaling group that is large enough to handle peak trafic load. Stop half of the Amazon EC2 instances. Configure the Auto Scaling group to use the stopped instances to scale out when trafic increases.",
      "B": "Create an Auto Scaling group for the website. Set the minimum size of the Auto Scaling group so that it can handle high trafic volumes without the need to scale out.",
      "C": "Use Amazon CloudFront and Amazon ElastiCache to cache dynamic content with an Auto Scaling group set as the origin. Configure the Auto Scaling group with the instances necessary to populate CloudFront and ElastiCache. Scale in after the cache is fully populated.",
      "D": "Configure an Auto Scaling group to scale out as trafic increases. Create a launch template to start new instances from a preconfigured Amazon Machine Image (AMI)."
    },
    "correct_answer": "D",
    "vote_percentage": "80%",
    "question_cn": "一家电子商务公司正在进行季节性在线促销活动。该公司将其网站托管在跨多个可用区的 Amazon EC2 实例上。该公司希望其网站能够管理促销期间突然增加的流量。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "创建一个足够大的 Auto Scaling 组以处理峰值流量负载。停止一半的 Amazon EC2 实例。将 Auto Scaling 组配置为使用已停止的实例在流量增加时进行扩展。",
      "B": "为网站创建一个 Auto Scaling 组。将 Auto Scaling 组的最小大小设置为可以处理高流量，而无需横向扩展。",
      "C": "使用 Amazon CloudFront 和 Amazon ElastiCache 来缓存动态内容，并将 Auto Scaling 组设置为源。配置 Auto Scaling 组，其中包含填充 CloudFront 和 ElastiCache 所需的实例。在缓存完全填充后进行缩减。",
      "D": "配置一个 Auto Scaling 组，以便在流量增加时横向扩展。创建一个启动模板，从预配置的 Amazon Machine Image (AMI) 启动新实例。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 80%），解析仅供参考。】\n\n考查了如何使用 Auto Scaling 组以及其他 AWS 服务，以应对网站流量激增，并考虑成本效益。",
      "why_correct": "选项 D 提供了最具成本效益的解决方案。通过配置 Auto Scaling 组，系统可以根据流量增加自动添加 EC2 实例。使用启动模板和预配置的 Amazon Machine Image (AMI) 可以快速启动新实例，满足流量激增的需求，同时保持成本效益。",
      "why_wrong": "选项 A 的问题在于停止 EC2 实例会增加启动延迟，无法快速响应流量高峰。选项 B 无法横向扩展，无法有效应对促销期间的流量突增。选项 C 中 ElastiCache 缓存无法缓存所有动态内容，且缓存填充时间较长，无法及时响应，并且配置复杂，成本较高。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon Machine Image (AMI)",
      "Amazon CloudFront",
      "Amazon ElastiCache"
    ]
  },
  {
    "id": 774,
    "topic": "1",
    "question_en": "A solutions architect must provide an automated solution for a company's compliance policy that states security groups cannot include a rule that allows SSH from 0.0.0.0/0. The company needs to be notified if there is any breach in the policy. A solution is needed as soon as possible. What should the solutions architect do to meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Write an AWS Lambda script that monitors security groups for SSH being open to 0.0.0.0/0 addresses and creates a notification every time it finds one.",
      "B": "Enable the restricted-ssh AWS Config managed rule and generate an Amazon Simple Notification Service (Amazon SNS) notification when a noncompliant rule is created.",
      "C": "Create an IAM role with permissions to globally open security groups and network ACLs. Create an Amazon Simple Notification Service (Amazon SNS) topic to generate a notification every time the role is assumed by a user.",
      "D": "Configure a service control policy (SCP) that prevents non-administrative users from creating or editing security groups. Create a notification in the ticketing system when a user requests a rule that needs administrator permissions."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师必须为一家公司的合规策略提供一个自动化解决方案，该策略规定安全组不能包含允许来自 0.0.0.0/0 的 SSH 的规则。如果该策略有任何违规行为，公司需要收到通知。需要尽快提供一个解决方案。 解决方案架构师应该怎么做才能以最少的运营开销来满足这些要求？",
    "options_cn": {
      "A": "编写一个 AWS Lambda 脚本，该脚本监视安全组，以查找允许 SSH 到 0.0.0.0/0 地址的情况，并在每次发现此类情况时创建一个通知。",
      "B": "启用 restricted-ssh AWS Config 托管规则，并在创建不合规规则时生成一个 Amazon Simple Notification Service (Amazon SNS) 通知。",
      "C": "创建一个 IAM 角色，该角色具有全局打开安全组和网络 ACL 的权限。创建一个 Amazon Simple Notification Service (Amazon SNS) 主题，以便在用户每次承担该角色时生成通知。",
      "D": "配置一个服务控制策略 (SCP)，阻止非管理用户创建或编辑安全组。当用户请求需要管理员权限的规则时，在工单系统中创建一个通知。"
    },
    "tags": [
      "Security Groups",
      "IAM",
      "Amazon SNS",
      "AWS Config",
      "AWS Lambda",
      "SCP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察使用 AWS Config 及托管规则检测安全组配置违规，并利用 SNS 发送通知的能力。",
      "why_correct": "启用 restricted-ssh AWS Config 托管规则可以自动检测安全组中 SSH 访问 0.0.0.0/0 的不合规行为。当检测到违规时，Config 会生成事件，触发 SNS 通知，满足题目中对合规策略和通知的需求。这种方案具有低运营开销的特点，因为 AWS Config 和 SNS 均为托管服务。",
      "why_wrong": "选项 A 需要手动编写和维护 Lambda 脚本，增加了运营开销，并且可能存在延时。选项 C 描述的方案没有直接解决安全组违规问题，而是通过 IAM 角色权限变更来生成通知，无法有效检测安全组的配置。选项 D 描述了 SCP 策略和工单系统，主要用于控制权限，而非自动化检测和通知安全组违规情况，且增加了管理员手动处理的环节。"
    },
    "related_terms": [
      "AWS Lambda",
      "SSH",
      "0.0.0.0/0",
      "Amazon Simple Notification Service (Amazon SNS)",
      "restricted-ssh",
      "AWS Config",
      "IAM",
      "SCP",
      "security group",
      "network ACL"
    ]
  },
  {
    "id": 775,
    "topic": "1",
    "question_en": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 worker nodes. A company has deployed an application in an AWS account. The application consists of microservices that run on AWS Lambda and Amazon Elastic Kubernetes Service (Amazon EKS). A separate team supports each microservice. The company has multiple AWS accounts and wants to give each team its own account for its microservices. A solutions architect needs to design a solution that will provide service-to-service communication over HTTPS (port 443). The solution also must provide a service registry for service discovery. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Create an inspection VPC. Deploy an AWS Network Firewall firewall to the inspection VPC. Attach the inspection VPC to a new transit gateway. Route VPC-to-VPC trafic to the inspection VPC. Apply firewall rules to allow only HTTPS communication.",
      "B": "Create a VPC Lattice service network. Associate the microservices with the service network. Define HTTPS listeners for each service. Register microservice compute resources as targets. Identify VPCs that need to communicate with the services. Associate those VPCs with the service network.",
      "C": "Create a Network Load Balancer (NLB) with an HTTPS listener and target groups for each microservice. Create an AWS PrivateLink endpoint service for each microservice. Create an interface VPC endpoint in each VPC that needs to consume that microservice.",
      "D": "Create peering connections between VPCs that contain microservices. Create a prefix list for each service that requires a connection to a client. Create route tables to route trafic to the appropriate VPC. Create security groups to allow only HTTPS communication."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和 Amazon EC2 工作节点。一家公司已在一个 AWS 账户中部署了一个应用程序。该应用程序由在 AWS Lambda 和 Amazon Elastic Kubernetes Service (Amazon EKS) 上运行的微服务组成。每个微服务由一个单独的团队支持。该公司有多个 AWS 账户，并希望为每个团队提供自己的账户来运行其微服务。解决方案架构师需要设计一个解决方案，该方案将通过 HTTPS (443 端口) 提供服务到服务的通信。该解决方案还必须为服务发现提供服务注册表。哪种解决方案将以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "创建一个检查 VPC。将 AWS Network Firewall 防火墙部署到检查 VPC。将检查 VPC 附加到新的 transit gateway。将 VPC 到 VPC 的流量路由到检查 VPC。应用防火墙规则，仅允许 HTTPS 通信。",
      "B": "创建 VPC Lattice 服务网络。将微服务与服务网络关联。为每个服务定义 HTTPS 监听器。将微服务计算资源注册为目标。确定需要与服务通信的 VPC。将这些 VPC 与服务网络关联。",
      "C": "创建一个 Network Load Balancer (NLB)，其中包含 HTTPS 监听器和每个微服务的目标组。为每个微服务创建一个 AWS PrivateLink 终端节点服务。在每个需要使用该微服务的 VPC 中创建一个接口 VPC 终端节点。",
      "D": "在包含微服务的 VPC 之间创建对等连接。为每个需要与客户端建立连接的服务创建一个前缀列表。创建路由表以将流量路由到相应的 VPC。创建安全组以仅允许 HTTPS 通信。"
    },
    "tags": [
      "Amazon VPC",
      "AWS Network Firewall",
      "Transit Gateway",
      "HTTPS",
      "Service-to-service communication",
      "VPC Peering"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查在 EKS 环境下，使用服务发现和 HTTPS 通信实现服务间通信的解决方案，侧重于管理开销和 AWS 服务的选择。",
      "why_correct": "VPC Lattice 是一种完全托管的服务网格，专为简化服务间通信而设计。它能够实现服务发现、HTTPS 通信，并减少管理开销。 通过将微服务与 VPC Lattice 服务网络关联，并为每个服务定义 HTTPS 监听器，可以满足服务间 HTTPS 通信的需求。 将 VPC 与服务网络关联后，VPC 中的服务可以相互发现和通信，无需手动配置复杂的网络或服务发现机制。",
      "why_wrong": "A 选项涉及 AWS Network Firewall 和 Transit Gateway，增加了管理复杂性，且并非实现服务间通信的最简方案。C 选项需要配置 Network Load Balancer (NLB) 和 AWS PrivateLink，配置相对复杂，维护成本较高。D 选项使用 VPC 对等连接和前缀列表，在服务数量多时，配置和维护路由表、安全组的工作量较大，扩展性差，也难以实现服务发现，且管理开销较大。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon EC2",
      "AWS Lambda",
      "HTTPS",
      "VPC",
      "AWS Network Firewall",
      "transit gateway",
      "Network Load Balancer (NLB)",
      "AWS PrivateLink",
      "VPC Lattice"
    ]
  },
  {
    "id": 776,
    "topic": "1",
    "question_en": "A company has a mobile game that reads most of its metadata from an Amazon RDS DB instance. As the game increased in popularity, developers noticed slowdowns related to the game's metadata load times. Performance metrics indicate that simply scaling the database will not help. A solutions architect must explore all options that include capabilities for snapshots, replication, and sub-millisecond response times. What should the solutions architect recommend to solve these issues?",
    "options_en": {
      "A": "Migrate the database to Amazon Aurora with Aurora Replicas.",
      "B": "Migrate the database to Amazon DynamoDB with global tables.",
      "C": "Add an Amazon ElastiCache for Redis layer in front of the database.",
      "D": "Add an Amazon ElastiCache for Memcached layer in front of the database."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一款手机游戏，该游戏的大部分元数据从 Amazon RDS 数据库实例中读取。随着游戏的普及，开发人员注意到与游戏元数据加载时间相关的速度变慢。性能指标表明，简单地扩展数据库将无济于事。一位解决方案架构师必须探索所有选项，包括快照、复制和亚毫秒级响应时间的功能。 解决方案架构师应该推荐什么来解决这些问题？",
    "options_cn": {
      "A": "将数据库迁移到带有 Aurora Replicas 的 Amazon Aurora。",
      "B": "将数据库迁移到带有全局表的 Amazon DynamoDB。",
      "C": "在数据库前面添加一个 Amazon ElastiCache for Redis 层。",
      "D": "在数据库前面添加一个 Amazon ElastiCache for Memcached 层。"
    },
    "tags": [
      "Amazon RDS",
      "Amazon ElastiCache",
      "Memcached",
      "Database Performance",
      "Sub-millisecond latency"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用缓存优化数据库读取性能的场景，以及 ElastiCache 的选择。题目要求亚毫秒级响应时间，且需要考虑扩展性。",
      "why_correct": "Amazon ElastiCache for Redis 是一个高性能的内存数据存储服务，可以用于缓存数据库查询结果。通过在数据库前面添加 Redis 层，可以将频繁访问的元数据缓存在内存中，从而实现亚毫秒级的响应时间。Redis 支持数据复制和持久化，提供了高可用性和数据安全性。这满足了题目中对性能和扩展性的要求。",
      "why_wrong": "选项 A 迁移到 Amazon Aurora 是一种数据库的替换方案，虽然 Aurora 提供了更高的性能和可扩展性，但直接迁移数据库并不能直接解决元数据加载速度慢的问题，尤其是在原有数据库性能瓶颈并非完全在于计算能力时。选项 B 迁移到 DynamoDB 虽然可以提供高扩展性，但 DynamoDB 的数据模型与 RDS 数据库不同，迁移的复杂性较高，且 DynamoDB 并非为所有类型的元数据访问模式优化。选项 D 虽然 Memcached 也是一个缓存服务，但相比 Redis，Memcached 功能较少，例如不支持复杂的数据结构和持久化，在复杂元数据场景下，优势不如 Redis 明显，也无法满足题目中对亚毫秒级响应时间的要求。"
    },
    "related_terms": [
      "Amazon RDS",
      "Amazon Aurora",
      "Aurora Replicas",
      "Amazon DynamoDB",
      "Global Tables",
      "Amazon ElastiCache for Redis",
      "Amazon ElastiCache for Memcached"
    ]
  },
  {
    "id": 777,
    "topic": "1",
    "question_en": "A company uses AWS Organizations for its multi-account AWS setup. The security organizational unit (OU) of the company needs to share approved Amazon Machine Images (AMIs) with the development OU. The AMIs are created by using AWS Key Management Service (AWS KMS) encrypted snapshots. Which solution will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Add the development team's OU Amazon Resource Name (ARN) to the launch permission list for the AMIs.",
      "B": "Add the Organizations root Amazon Resource Name (ARN) to the launch permission list for the AMIs.",
      "C": "Update the key policy to allow the development team's OU to use the AWS KMS keys that are used to decrypt the snapshots.",
      "D": "Add the development team’s account Amazon Resource Name (ARN) to the launch permission list for the AMIs",
      "E": "Recreate the AWS KMS key. Add a key policy to allow the Organizations root Amazon Resource Name (ARN) to use the AWS KMS key."
    },
    "correct_answer": "AC",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS Organizations 设置其多账户 AWS。 该公司的安全组织单元 (OU) 需要与开发 OU 共享已批准的 Amazon Machine Images (AMI)。 AMI 是使用 AWS Key Management Service (AWS KMS) 加密快照创建的。 哪种解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将开发团队的 OU Amazon Resource Name (ARN) 添加到 AMI 的启动权限列表中。",
      "B": "将 Organizations 根 Amazon Resource Name (ARN) 添加到 AMI 的启动权限列表中。",
      "C": "更新密钥策略，允许开发团队的 OU 使用用于解密快照的 AWS KMS 密钥。",
      "D": "将开发团队的账户 Amazon Resource Name (ARN) 添加到 AMI 的启动权限列表中。",
      "E": "重新创建 AWS KMS 密钥。 添加一个密钥策略，允许 Organizations 根 Amazon Resource Name (ARN) 使用 AWS KMS 密钥。"
    },
    "tags": [
      "Amazon EC2",
      "AMI",
      "AWS KMS",
      "AWS Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 100%），解析仅供参考。】\n\n考查了跨账户共享 AMI 的方法，以及如何使用 AWS KMS 加密快照并控制访问。",
      "why_correct": "选项 A 正确，通过将开发团队的 OU ARN 添加到 AMI 的启动权限列表中，允许开发团队的 OU 启动使用该 AMI 的实例。选项 C 正确，因为 AMI 使用了 KMS 加密，需要授权开发 OU 使用 KMS 密钥解密快照。",
      "why_wrong": "选项 B 错误，将 Organizations 根 ARN 添加到 AMI 启动权限列表，会使根 OU 下的所有账户都能启动该 AMI，这违背了只允许开发 OU 使用的需求。选项 D 错误，虽然添加开发团队账户的 ARN 可以，但如果后续开发 OU 中添加了新的账户，则需要修改 AMI 的权限，增加了维护成本。选项 E 错误，重新创建 KMS 密钥并添加 Organizations 根 ARN，会使得安全 OU 之外的其他账户也能访问 KMS 密钥，这不符合题目需求，且会导致密钥管理复杂化。"
    },
    "related_terms": [
      "AWS Organizations",
      "OU (Organizational Unit)",
      "Amazon Machine Images (AMI)",
      "AWS Key Management Service (AWS KMS)",
      "Amazon Resource Name (ARN)",
      "KMS key"
    ]
  },
  {
    "id": 778,
    "topic": "1",
    "question_en": "A data analytics company has 80 ofices that are distributed globally. Each ofice hosts 1 PB of data and has between 1 and 2 Gbps of internet bandwidth. The company needs to perform a one-time migration of a large amount of data from its ofices to Amazon S3. The company must complete the migration within 4 weeks. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Establish a new 10 Gbps AWS Direct Connect connection to each ofice. Transfer the data to Amazon S3.",
      "B": "Use multiple AWS Snowball Edge storage-optimized devices to store and transfer the data to Amazon S3.",
      "C": "Use an AWS Snowmobile to store and transfer the data to Amazon S3.",
      "D": "Set up an AWS Storage Gateway Volume Gateway to transfer the data to Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "88%",
    "question_cn": "一家数据分析公司在全球分布着 80 个办事处。每个办事处托管 1 PB 的数据，并拥有 1 到 2 Gbps 的互联网带宽。该公司需要一次性将大量数据从其办事处迁移到 Amazon S3。该公司必须在 4 周内完成迁移。哪种解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "为每个办事处建立一个新的 10 Gbps AWS Direct Connect 连接。将数据传输到 Amazon S3。",
      "B": "使用多个 AWS Snowball Edge 存储优化设备来存储数据并将数据传输到 Amazon S3。",
      "C": "使用 AWS Snowmobile 来存储数据并将数据传输到 Amazon S3。",
      "D": "设置 AWS Storage Gateway Volume Gateway 以将数据传输到 Amazon S3。"
    },
    "tags": [
      "Amazon S3",
      "AWS Snowmobile",
      "Data Migration",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 88%），解析仅供参考。】\n\n考查大规模数据迁移的成本效益解决方案，需要考虑数据量、带宽限制和时间窗口。",
      "why_correct": "AWS Snowball Edge 存储优化设备是为大规模数据迁移设计的，尤其适用于边缘计算场景。它提供了数据存储、数据传输和预处理能力，可以满足办事处数据量和带宽限制的要求。多个 Snowball Edge 设备可以并行工作，加速数据传输，并在四周内完成迁移，具有成本效益。",
      "why_wrong": "A 选项，AWS Direct Connect 方案需要建立新的连接，成本高昂，且每个办事处仅有 1-2 Gbps 的带宽，建立 10 Gbps 的连接无法充分利用带宽。C 选项，AWS Snowmobile 适用于 PB 级别以上的数据迁移，但其部署成本远高于 Snowball Edge，且办事处数量众多，Snowmobile 无法满足分散部署的需求。D 选项，AWS Storage Gateway Volume Gateway 主要用于混合云环境的文件、卷和磁带存储，不适用于一次性大规模数据迁移的场景。它的传输速度受限于互联网带宽，难以在四周内完成迁移。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Snowball Edge",
      "AWS Direct Connect",
      "AWS Snowmobile",
      "AWS Storage Gateway Volume Gateway"
    ]
  },
  {
    "id": 779,
    "topic": "1",
    "question_en": "A company has an Amazon Elastic File System (Amazon EFS) file system that contains a reference dataset. The company has applications on Amazon EC2 instances that need to read the dataset. However, the applications must not be able to change the dataset. The company wants to use IAM access control to prevent the applications from being able to modify or delete the dataset. Which solution will meet these requirements?",
    "options_en": {
      "A": "Mount the EFS file system in read-only mode from within the EC2 instances.",
      "B": "Create a resource policy for the EFS file system that denies the elasticfilesystem:ClientWrite action to the IAM roles that are attached to the EC2 instances.",
      "C": "Create an identity policy for the EFS file system that denies the elasticfilesystem:ClientWrite action on the EFS file system.",
      "D": "Create an EFS access point for each application. Use Portable Operating System Interface (POSIX) file permissions to allow read-only access to files in the root directory."
    },
    "correct_answer": "B",
    "vote_percentage": "60%",
    "question_cn": "一家公司有一个包含参考数据集的 Amazon Elastic File System (Amazon EFS) 文件系统。该公司在 Amazon EC2 实例上有需要读取数据集的应用程序。但是，这些应用程序不能更改数据集。该公司希望使用 IAM 访问控制来阻止应用程序修改或删除数据集。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "从 EC2 实例中以只读模式挂载 EFS 文件系统。",
      "B": "为 EFS 文件系统创建一个资源策略，该策略拒绝 elasticfilesystem:ClientWrite 操作给附加到 EC2 实例的 IAM 角色。",
      "C": "为 EFS 文件系统创建一个身份策略，该策略拒绝在 EFS 文件系统上执行 elasticfilesystem:ClientWrite 操作。",
      "D": "为每个应用程序创建一个 EFS 访问点。使用 Portable Operating System Interface (POSIX) 文件权限来允许对根目录中的文件进行只读访问。"
    },
    "tags": [
      "Amazon EFS",
      "IAM",
      "EC2",
      "File System Permissions"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 60%），解析仅供参考。】\n\n考查通过 IAM 权限控制对 Amazon EFS 文件系统的访问，特别是如何阻止应用程序写入数据。",
      "why_correct": "选项 B 通过使用 EFS 的资源策略来限制 IAM 角色对文件系统的操作，能够精准控制用户权限。具体来说，资源策略可以拒绝特定 IAM 角色执行 `elasticfilesystem:ClientWrite` 操作，从而阻止应用程序修改或删除 EFS 中的数据。这种方式简单直接，满足了题目提出的需求。",
      "why_wrong": "选项 A 仅提供了挂载文件系统的模式，但没有明确阻止写入操作的 IAM 权限控制，无法满足题目要求。选项 C 使用身份策略 (Identity-based policy)，这通常用于控制用户或角色在 AWS 资源上的权限，而不是针对 EFS 文件系统本身。而且，身份策略不直接用于 EFS 文件系统的访问控制。选项 D 通过 EFS 访问点和 POSIX 文件权限来控制访问，但这种方式不如资源策略直接和有效。而且，POSIX 权限可能需要复杂的配置，不能保证完全满足题目要求，而且管理起来也比较复杂。"
    },
    "related_terms": [
      "Amazon EFS",
      "Amazon EC2",
      "IAM",
      "IAM role",
      "resource policy",
      "elasticfilesystem:ClientWrite",
      "POSIX",
      "EFS access point"
    ]
  },
  {
    "id": 780,
    "topic": "1",
    "question_en": "A company has hired an external vendor to perform work in the company’s AWS account. The vendor uses an automated tool that is hosted in an AWS account that the vendor owns. The vendor does not have IAM access to the company’s AWS account. The company needs to grant the vendor access to the company’s AWS account. Which solution will meet these requirements MOST securely?",
    "options_en": {
      "A": "Create an IAM role in the company’s account to delegate access to the vendor’s IAM role. Attach the appropriate IAM policies to the role for the permissions that the vendor requires.",
      "B": "Create an IAM user in the company’s account with a password that meets the password complexity requirements. Attach the appropriate IAM policies to the user for the permissions that the vendor requires.",
      "C": "Create an IAM group in the company’s account. Add the automated tool’s IAM user from the vendor account to the group. Attach the appropriate IAM policies to the group for the permissions that the vendor requires.",
      "D": "Create an IAM user in the company’s account that has a permission boundary that allows the vendor’s account. Attach the appropriate IAM policies to the user for the permissions that the vendor requires."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司聘请了一家外部供应商在其公司的 AWS 账户中执行工作。该供应商使用托管在其拥有的 AWS 账户中的自动化工具。该供应商无法访问该公司 AWS 账户的 IAM。该公司需要授予供应商对其公司 AWS 账户的访问权限。哪种解决方案最安全地满足这些要求？",
    "options_cn": {
      "A": "在公司的账户中创建一个 IAM 角色，以将访问权限委托给供应商的 IAM 角色。将适当的 IAM 策略附加到该角色，以获取供应商所需的权限。",
      "B": "在公司的账户中创建一个 IAM 用户，该用户的密码符合密码复杂性要求。将适当的 IAM 策略附加到该用户，以获取供应商所需的权限。",
      "C": "在公司的账户中创建一个 IAM 组。将供应商账户中的自动化工具的 IAM 用户添加到该组。将适当的 IAM 策略附加到该组，以获取供应商所需的权限。",
      "D": "在公司的账户中创建一个 IAM 用户，该用户具有一个允许供应商账户的权限边界。将适当的 IAM 策略附加到该用户，以获取供应商所需的权限。"
    },
    "tags": [
      "IAM",
      "IAM Role",
      "IAM User",
      "IAM Group",
      "Permissions Boundary"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查跨账户访问的安全性。主要考察 IAM 角色和 IAM 用户在授权访问方面的区别，以及权限边界的用途。同时考察委托（delegation）的概念。",
      "why_correct": "IAM 角色是实现跨账户访问最安全、最推荐的方式。通过创建 IAM 角色，公司账户可以允许供应商账户中的 IAM 角色代入该角色，从而获得对公司账户资源的访问权限。这种方式避免了共享长期凭证（如用户名密码），并提供了细粒度的访问控制，确保了最小权限原则。",
      "why_wrong": "B 选项使用 IAM 用户，这意味着供应商需要使用公司的用户名和密码进行身份验证。这违反了安全最佳实践，因为需要共享凭证，且难以管理密码的轮换和安全。C 选项尝试将供应商账户中的 IAM 用户添加到公司账户的 IAM 组，这实际上是不可行的，IAM 组只能管理公司账户内的用户。D 选项使用 IAM 用户并设置权限边界。虽然权限边界可以限制 IAM 用户拥有的最大权限，但它仍然需要共享凭证，并不能解决题干中供应商无法访问公司 IAM 的问题。而且，权限边界更多的是为了限制用户能够拥有的最大权限，而不是用于跨账户的授权。"
    },
    "related_terms": [
      "IAM",
      "AWS",
      "IAM Role",
      "IAM User",
      "IAM Group",
      "Permissions Boundary"
    ]
  },
  {
    "id": 781,
    "topic": "1",
    "question_en": "A company wants to run its experimental workloads in the AWS Cloud. The company has a budget for cloud spending. The company's CFO is concerned about cloud spending accountability for each department. The CFO wants to receive notification when the spending threshold reaches 60% of the budget. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use cost allocation tags on AWS resources to label owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget.",
      "B": "Use AWS Cost Explorer forecasts to determine resource owners. Use AWS Cost Anomaly Detection to create alert threshold notifications when spending exceeds 60% of the budget.",
      "C": "Use cost allocation tags on AWS resources to label owners. Use AWS Support API on AWS Trusted Advisor to create alert threshold notifications when spending exceeds 60% of the budget.",
      "D": "Use AWS Cost Explorer forecasts to determine resource owners. Create usage budgets in AWS Budgets. Add an alert threshold to receive notification when spending exceeds 60% of the budget."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在 AWS 云中运行其实验性工作负载。该公司有一个用于云支出的预算。该公司的首席财务官担心每个部门的云支出问责制。首席财务官希望在支出阈值达到预算的 60% 时收到通知。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS 资源上使用成本分配标签来标记所有者。在 AWS Budgets 中创建使用预算。添加一个警报阈值，以便在支出超过预算的 60% 时收到通知。",
      "B": "使用 AWS Cost Explorer 预测来确定资源所有者。使用 AWS Cost Anomaly Detection 在支出超过预算的 60% 时创建警报阈值通知。",
      "C": "在 AWS 资源上使用成本分配标签来标记所有者。使用 AWS Trusted Advisor 上的 AWS Support API 在支出超过预算的 60% 时创建警报阈值通知。",
      "D": "使用 AWS Cost Explorer 预测来确定资源所有者。在 AWS Budgets 中创建使用预算。添加一个警报阈值，以便在支出超过预算的 60% 时收到通知。"
    },
    "tags": [
      "AWS Budgets",
      "Cost Allocation Tags",
      "AWS Cost Explorer",
      "AWS Trusted Advisor",
      "AWS Cost Anomaly Detection"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查利用 AWS Budgets 和成本分配标签来实现云支出监控和预算控制；与预算管理、成本追踪、报警通知相关。",
      "why_correct": "选项 A 提供了完整的解决方案，满足了题目的需求。首先，使用成本分配标签（Cost Allocation Tags）标记资源所有者，以便追踪每个部门的支出。然后，在 AWS Budgets 中创建使用预算（Usage Budget），并设置警报阈值，当实际支出达到预算的 60% 时，触发通知。这种组合实现了预算跟踪、部门归属以及预警机制。",
      "why_wrong": "选项 B 错误，因为它使用了 AWS Cost Anomaly Detection 来实现报警。Cost Anomaly Detection 主要用于检测成本的异常波动，而不是基于预算的阈值报警。选项 C 错误，因为它提到了 AWS Trusted Advisor。Trusted Advisor 主要用于提供资源优化、安全、性能和成本方面的建议，不具备直接设置预算和报警的功能。选项 D 错误，因为它虽然使用了 AWS Budgets，但是没有提到使用成本分配标签，这使得它无法满足对每个部门的云支出进行问责的要求，因为无法区分不同部门的开支。"
    },
    "related_terms": [
      "AWS Budgets",
      "AWS Cost Explorer",
      "AWS Trusted Advisor",
      "Cost Allocation Tags",
      "AWS Cost Anomaly Detection"
    ]
  },
  {
    "id": 782,
    "topic": "1",
    "question_en": "A company wants to deploy an internal web application on AWS. The web application must be accessible only from the company's ofice. The company needs to download security patches for the web application from the internet. The company has created a VPC and has configured an AWS Site-to-Site VPN connection to the company's ofice. A solutions architect must design a secure architecture for the web application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy the web application on Amazon EC2 instances in public subnets behind a public Application Load Balancer (ALB). Attach an internet gateway to the VPC. Set the inbound source of the ALB's security group to 0.0.0.0/0.",
      "B": "Deploy the web application on Amazon EC2 instances in private subnets behind an internal Application Load Balancer (ALB). Deploy NAT gateways in public subnets. Attach an internet gateway to the VPC. Set the inbound source of the ALB's security group to the company's ofice network CIDR block.",
      "C": "Deploy the web application on Amazon EC2 instances in public subnets behind an internal Application Load Balancer (ALB). Deploy NAT gateways in private subnets. Attach an internet gateway to the VPSet the outbound destination of the ALB’s security group to the company's ofice network CIDR block.",
      "D": "Deploy the web application on Amazon EC2 instances in private subnets behind a public Application Load Balancer (ALB). Attach an internet gateway to the VPC. Set the outbound destination of the ALB’s security group to 0.0.0.0/0."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在 AWS 上部署一个内部 Web 应用程序。该 Web 应用程序必须仅从公司的办公室访问。该公司需要从互联网下载 Web 应用程序的安全补丁。该公司已创建了 VPC，并配置了到公司办公室的 AWS Site-to-Site VPN 连接。解决方案架构师必须为 Web 应用程序设计一个安全的架构。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 Web 应用程序部署在公有子网中的 Amazon EC2 实例上，位于公有 Application Load Balancer (ALB) 之后。将互联网网关附加到 VPC。将 ALB 的安全组的入站源设置为 0.0.0.0/0。",
      "B": "将 Web 应用程序部署在私有子网中的 Amazon EC2 实例上，位于内部 Application Load Balancer (ALB) 之后。在公有子网中部署 NAT 网关。将互联网网关附加到 VPC。将 ALB 的安全组的入站源设置为公司办公室网络 CIDR 块。",
      "C": "将 Web 应用程序部署在公有子网中的 Amazon EC2 实例上，位于内部 Application Load Balancer (ALB) 之后。在私有子网中部署 NAT 网关。将互联网网关附加到 VPC。将 ALB 的安全组的出站目标设置为公司办公室网络 CIDR 块。",
      "D": "将 Web 应用程序部署在私有子网中的 Amazon EC2 实例上，位于公有 Application Load Balancer (ALB) 之后。将互联网网关附加到 VPC。将 ALB 的安全组的出站目标设置为 0.0.0.0/0。"
    },
    "tags": [
      "Amazon EC2",
      "VPC",
      "Application Load Balancer (ALB)",
      "NAT Gateway",
      "Site-to-Site VPN",
      "Security Group",
      "Internet Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查在 VPC 环境中部署 Web 应用程序的安全架构，并结合 VPN 连接和安全补丁下载需求。与 VPC 结构、安全组配置、NAT Gateway 的作用相关。",
      "why_correct": "将 Web 应用程序部署在私有子网中的 Amazon EC2 实例上，由内部 Application Load Balancer (ALB) 负载均衡。在公有子网中部署 NAT 网关，使得 EC2 实例可以访问互联网下载补丁。安全组的入站规则配置为仅允许来自公司办公室网络 CIDR 块的流量访问 ALB，确保了访问的安全性。",
      "why_wrong": {
        "A": "将 Web 应用程序部署在公有子网，并使用公有 ALB，违反了题目中“仅从公司的办公室访问”的要求。安全组的入站源设置为 0.0.0.0/0 会允许来自任何地方的访问，不安全。",
        "C": "使用内部 ALB 虽然满足了访问限制，但将 Web 应用程序部署在公有子网是不必要的，且不安全。 NAT 网关应该部署在公有子网，才能让私有子网中的 EC2 实例访问互联网。 安全组的出站目标设置为公司办公室网络 CIDR 块是错误的，出站规则是针对响应流量的，而不是控制请求流量的。",
        "D": "将 Web 应用程序部署在私有子网中，符合安全要求，但是使用了公有 ALB，导致互联网访问应用程序。将 ALB 的安全组的出站目标设置为 0.0.0.0/0 是错误的，出站规则控制的是响应流量，而不是请求流量。"
      }
    },
    "related_terms": [
      "Amazon EC2",
      "VPC",
      "NAT Gateway",
      "Site-to-Site VPN",
      "Internet Gateway",
      "CIDR",
      "Application Load Balancer (ALB)",
      "Security Group"
    ]
  },
  {
    "id": 783,
    "topic": "1",
    "question_en": "A company maintains its accounting records in a custom application that runs on Amazon EC2 instances. The company needs to migrate the data to an AWS managed service for development and maintenance of the application data. The solution must require minimal operational support and provide immutable, cryptographically verifiable logs of data changes. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Copy the records from the application into an Amazon Redshift cluster.",
      "B": "Copy the records from the application into an Amazon Neptune cluster.",
      "C": "Copy the records from the application into an Amazon Timestream database.",
      "D": "Copy the records from the application into an Amazon Quantum Ledger Database (Amazon QLDB) ledger."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Amazon EC2 实例上运行的自定义应用程序中维护其会计记录。该公司需要将数据迁移到 AWS 托管服务，用于应用程序数据的开发和维护。该解决方案必须需要最少的运营支持，并提供对数据更改的不可变、经过密码学验证的日志。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将记录从应用程序复制到 Amazon Redshift 集群。",
      "B": "将记录从应用程序复制到 Amazon Neptune 集群。",
      "C": "将记录从应用程序复制到 Amazon Timestream 数据库。",
      "D": "将记录从应用程序复制到 Amazon Quantum Ledger Database (Amazon QLDB) 账本。"
    },
    "tags": [
      "Amazon QLDB",
      "Ledger Database",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查在满足数据不可变性和日志审计需求的前提下，以最具成本效益的方式选择合适的数据库服务。这涉及到对不同数据库服务特点的理解以及成本模型的比较。",
      "why_correct": "Amazon Quantum Ledger Database (Amazon QLDB) 是一个完全托管的账本数据库，专为提供高度安全、不可变的交易数据而设计。它特别适用于需要可验证的数据变更历史的应用场景，满足了题目中对数据不可变性和密码学验证日志的要求。QLDB 还提供了基于使用量的定价模式，在满足业务需求的同时，通常比其他数据库服务更具成本效益，尤其是针对写入频繁，读取不频繁的场景。",
      "why_wrong": "选项 A 错误，将记录复制到 Amazon Redshift 集群，Redshift 是一个数据仓库服务，主要用于大规模数据分析，而非设计用于维护不可变的账本。Redshift 的数据并非默认不可变，需要额外的配置和复杂的操作来模拟数据变更审计。选项 B 错误，将记录复制到 Amazon Neptune 集群，Neptune 是一个图数据库服务，适用于存储和处理高度互连的数据。Neptune 也不提供内置的、经过密码学验证的数据不可变日志功能，且在成本上可能不具备优势。选项 C 错误，将记录复制到 Amazon Timestream 数据库，Timestream 是一种时间序列数据库，适用于存储和分析时间序列数据。虽然 Timestream 提供了数据存储和版本控制功能，但其设计目标并非账本数据库，且在不可变性和密码学验证日志方面不如 QLDB 完善，且不一定在成本上有优势。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon Redshift",
      "Amazon Neptune",
      "Amazon Timestream",
      "QLDB",
      "Amazon QLDB"
    ]
  },
  {
    "id": 784,
    "topic": "1",
    "question_en": "A company's marketing data is uploaded from multiple sources to an Amazon S3 bucket. A series of data preparation jobs aggregate the data for reporting. The data preparation jobs need to run at regular intervals in parallel. A few jobs need to run in a specific order later. The company wants to remove the operational overhead of job error handling, retry logic, and state management. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an AWS Lambda function to process the data as soon as the data is uploaded to the S3 bucket. Invoke other Lambda functions at regularly scheduled intervals.",
      "B": "Use Amazon Athena to process the data. Use Amazon EventBridge Scheduler to invoke Athena on a regular internal.",
      "C": "Use AWS Glue DataBrew to process the data. Use an AWS Step Functions state machine to run the DataBrew data preparation jobs.",
      "D": "Use AWS Data Pipeline to process the data. Schedule Data Pipeline to process the data once at midnight."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司的营销数据从多个源上传到 Amazon S3 存储桶。一系列数据准备作业聚合数据用于报告。数据准备作业需要定期间隔并行运行。少数作业需要在稍后以特定顺序运行。该公司希望移除作业错误处理、重试逻辑和状态管理的操作开销。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Lambda 函数在数据上传到 S3 存储桶后立即处理数据。以定期间隔调用其他 Lambda 函数。",
      "B": "使用 Amazon Athena 处理数据。使用 Amazon EventBridge Scheduler 定期间隔调用 Athena。",
      "C": "使用 AWS Glue DataBrew 处理数据。使用 AWS Step Functions 状态机运行 DataBrew 数据准备作业。",
      "D": "使用 AWS Data Pipeline 处理数据。将 Data Pipeline 调度为在午夜处理数据一次。"
    },
    "tags": [
      "Amazon S3",
      "AWS Lambda",
      "AWS Glue DataBrew",
      "AWS Step Functions",
      "Amazon Athena",
      "Amazon EventBridge Scheduler",
      "AWS Data Pipeline"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查数据处理流程的编排和调度；与 AWS Glue DataBrew、AWS Step Functions、Lambda、Data Pipeline 和 EventBridge Scheduler 的选型和对比相关。",
      "why_correct": "AWS Glue DataBrew 提供了可视化的数据准备工具，可以简化数据转换和清洗流程。AWS Step Functions 是一种完全托管的编排服务，可以编排多个 AWS 服务，包括 DataBrew 作业，从而实现并行执行和顺序执行的流程。 通过Step Functions的状态机，可以管理作业的依赖关系、错误处理和重试逻辑，满足了题目的要求，减少了运营开销。",
      "why_wrong": "A.  使用 Lambda 函数处理数据，需要自己编写代码处理数据，并实现重试、状态管理等逻辑，这增加了操作开销，与题目要求不符。另外，需要自己实现 Lambda 函数的调度，不如 Step Functions 方便。\nB.  Amazon Athena 主要用于交互式查询，不适合作为数据准备的核心服务。虽然可以使用 EventBridge Scheduler 调度 Athena 查询，但是无法满足并行和特定顺序执行的需求，并且需要自行实现作业的依赖管理。\nD.  AWS Data Pipeline 相对复杂，配置和维护成本较高，而且提供的调度功能不如 Step Functions 灵活。题目的需求是需要并行和特定顺序的执行，Data Pipeline 在这方面支持不够好。Data Pipeline 在错误处理和重试方面的配置也需要额外的工作量，与题目的目标不符。"
    },
    "related_terms": [
      "Amazon S3",
      "AWS Lambda",
      "AWS Step Functions",
      "Amazon Athena",
      "AWS Glue DataBrew",
      "Amazon EventBridge Scheduler",
      "AWS Data Pipeline"
    ]
  },
  {
    "id": 785,
    "topic": "1",
    "question_en": "A solutions architect is designing a payment processing application that runs on AWS Lambda in private subnets across multiple Availability Zones. The application uses multiple Lambda functions and processes millions of transactions each day. The architecture must ensure that the application does not process duplicate payments. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon S3 bucket. Configure the S3 bucket with an event notification to invoke another Lambda function to process the due payments.",
      "B": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) queue. Configure another Lambda function to poll the SQS queue and to process the due payments.",
      "C": "Use Lambda to retrieve all due payments. Publish the due payments to an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Configure another Lambda function to poll the FIFO queue and to process the due payments.",
      "D": "Use Lambda to retrieve all due payments. Store the due payments in an Amazon DynamoDB table. Configure streams on the DynamoDB table to invoke another Lambda function to process the due payments."
    },
    "correct_answer": "C",
    "vote_percentage": "73%",
    "question_cn": "一位解决方案架构师正在设计一个支付处理应用程序，该应用程序在跨多个可用区的私有子网中的 AWS Lambda 上运行。该应用程序使用多个 Lambda 函数，并且每天处理数百万笔交易。该架构必须确保应用程序不会处理重复的付款。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Lambda 检索所有到期付款。将到期付款发布到 Amazon S3 存储桶。使用事件通知配置 S3 存储桶，以调用另一个 Lambda 函数来处理到期付款。",
      "B": "使用 Lambda 检索所有到期付款。将到期付款发布到 Amazon Simple Queue Service (Amazon SQS) 队列。配置另一个 Lambda 函数来轮询 SQS 队列并处理到期付款。",
      "C": "使用 Lambda 检索所有到期付款。将到期付款发布到 Amazon Simple Queue Service (Amazon SQS) FIFO 队列。配置另一个 Lambda 函数来轮询 FIFO 队列并处理到期付款。",
      "D": "使用 Lambda 检索所有到期付款。将到期付款存储在 Amazon DynamoDB 表中。在 DynamoDB 表上配置流以调用另一个 Lambda 函数来处理到期付款。"
    },
    "tags": [
      "AWS Lambda",
      "Amazon SQS",
      "SQS FIFO",
      "Payment Processing",
      "Concurrency"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 73%），解析仅供参考。】\n\n本题考查了在 AWS Lambda 中处理支付应用程序时，如何避免重复处理，以及消息队列的选择。与 Amazon SQS、SQS FIFO 队列的特性，以及 Lambda 函数的触发方式相关。",
      "why_correct": "选项 C 使用 Amazon SQS FIFO 队列，可以确保消息的顺序处理，从而避免重复的付款处理。FIFO 队列保证消息按照发送的顺序被处理，这对于需要严格顺序的支付处理场景至关重要。配置另一个 Lambda 函数来轮询 FIFO 队列并处理到期付款可以保证消息的消费顺序。",
      "why_wrong": "选项 A 使用 Amazon S3 存储桶和事件通知。S3 本身不提供消息的顺序保证，当并发触发 Lambda 函数时，无法确保付款按正确顺序处理，可能导致重复付款。选项 B 使用 Amazon SQS 标准队列。标准队列不保证消息的顺序，因此同样无法避免重复的付款处理。选项 D 使用 Amazon DynamoDB 表和 DynamoDB 流。虽然 DynamoDB 流可以触发 Lambda 函数，但流的触发顺序不保证与写入 DynamoDB 的顺序一致，因此无法避免重复付款。此外，直接依赖 DynamoDB 流处理海量支付数据，可能造成数据处理延迟，影响用户体验。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon SQS",
      "Amazon S3",
      "DynamoDB",
      "Lambda",
      "SQS",
      "SQS FIFO",
      "Payment Processing"
    ]
  },
  {
    "id": 786,
    "topic": "1",
    "question_en": "A company runs multiple workloads in its on-premises data center. The company's data center cannot scale fast enough to meet the company's expanding business needs. The company wants to collect usage and configuration data about the on-premises servers and workloads to plan a migration to AWS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set the home AWS Region in AWS Migration Hub. Use AWS Systems Manager to collect data about the on-premises servers.",
      "B": "Set the home AWS Region in AWS Migration Hub. Use AWS Application Discovery Service to collect data about the on-premises servers.",
      "C": "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Trusted Advisor to collect data about the on-premises servers.",
      "D": "Use the AWS Schema Conversion Tool (AWS SCT) to create the relevant templates. Use AWS Database Migration Service (AWS DMS) to collect data about the on-premises servers."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其本地数据中心运行多个工作负载。该公司的的数据中心无法快速扩展以满足公司不断增长的业务需求。该公司希望收集有关本地服务器和工作负载的用量和配置数据，以便规划向 AWS 的迁移。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Migration Hub 中设置 home AWS 区域。 使用 AWS Systems Manager 收集有关本地服务器的数据。",
      "B": "在 AWS Migration Hub 中设置 home AWS 区域。使用 AWS Application Discovery Service 收集有关本地服务器的数据。",
      "C": "使用 AWS Schema Conversion Tool (AWS SCT) 创建相关模板。 使用 AWS Trusted Advisor 收集有关本地服务器的数据。",
      "D": "使用 AWS Schema Conversion Tool (AWS SCT) 创建相关模板。使用 AWS Database Migration Service (AWS DMS) 收集有关本地服务器的数据。"
    },
    "tags": [
      "AWS Migration Hub",
      "AWS Application Discovery Service",
      "AWS Systems Manager",
      "AWS Schema Conversion Tool",
      "AWS Trusted Advisor",
      "AWS Database Migration Service",
      "migration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查的是 AWS 迁移规划的工具选择，特别是用于收集本地服务器数据的服务。与 AWS Migration Hub、AWS Application Discovery Service 等服务的适用场景和功能相关。",
      "why_correct": "选项 B 正确。AWS Application Discovery Service 旨在帮助您规划迁移到 AWS。它通过收集有关本地服务器的用量和配置数据，帮助评估您的环境。首先，在 AWS Migration Hub 中设置 home AWS 区域，然后使用 Application Discovery Service 收集本地服务器信息，满足题目需求，为迁移规划提供必要的依据。",
      "why_wrong": "选项 A 错误。AWS Systems Manager 更多用于管理 AWS 上的资源，虽然可以收集一些数据，但并非专门为迁移规划设计，功能不如 Application Discovery Service 完善，收集数据的方式和侧重点也不同。选项 C 错误。AWS Schema Conversion Tool (AWS SCT) 用于数据库模式转换，与收集服务器数据无关，且 AWS Trusted Advisor 主要用于优化 AWS 环境，并非用于收集本地服务器数据。选项 D 错误。AWS Schema Conversion Tool (AWS SCT) 用于数据库模式转换，而 AWS Database Migration Service (AWS DMS) 则是数据库迁移服务，两者都与收集本地服务器数据没有直接关系，不符合题目需求。"
    },
    "related_terms": [
      "AWS Migration Hub",
      "AWS Systems Manager",
      "AWS Trusted Advisor",
      "AWS Database Migration Service",
      "AWS",
      "EC2",
      "EBS",
      "AWS Application Discovery Service",
      "AWS Schema Conversion Tool"
    ]
  },
  {
    "id": 787,
    "topic": "1",
    "question_en": "A company has an organization in AWS Organizations that has all features enabled. The company requires that all API calls and logins in any existing or new AWS account must be audited. The company needs a managed solution to prevent additional work and to minimize costs. The company also needs to know when any AWS account is not compliant with the AWS Foundational Security Best Practices (FSBP) standard. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Deploy an AWS Control Tower environment in the Organizations management account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment.",
      "B": "Deploy an AWS Control Tower environment in a dedicated Organizations member account. Enable AWS Security Hub and AWS Control Tower Account Factory in the environment.",
      "C": "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision Amazon GuardDuty in the MALZ.",
      "D": "Use AWS Managed Services (AMS) Accelerate to build a multi-account landing zone (MALZ). Submit an RFC to self-service provision AWS Security Hub in the MALZ."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS Organizations 中拥有已启用所有功能的组织。该公司要求必须审核任何现有或新 AWS 账户中的所有 API 调用和登录。该公司需要一个托管解决方案，以防止额外的工作并最大限度地降低成本。该公司还需要知道何时任何 AWS 账户不符合 AWS 基础安全最佳实践 (FSBP) 标准。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 Organizations 管理账户中部署 AWS Control Tower 环境。在该环境中启用 AWS Security Hub 和 AWS Control Tower Account Factory。",
      "B": "在专用的 Organizations 成员账户中部署 AWS Control Tower 环境。在该环境中启用 AWS Security Hub 和 AWS Control Tower Account Factory。",
      "C": "使用 AWS Managed Services (AMS) Accelerate 构建一个多账户登陆区 (MALZ)。提交一个 RFC 以自助服务方式在 MALZ 中配置 Amazon GuardDuty。",
      "D": "使用 AWS Managed Services (AMS) Accelerate 构建一个多账户登陆区 (MALZ)。提交一个 RFC 以自助服务方式在 MALZ 中配置 AWS Security Hub。"
    },
    "tags": [
      "AWS Organizations",
      "AWS Control Tower",
      "AWS Security Hub",
      "AWS Account Factory",
      "Amazon GuardDuty",
      "AWS Managed Services (AMS)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 AWS Organizations 环境中账户安全审计和合规性的解决方案。涉及 AWS Control Tower、Security Hub、Account Factory 和成本优化，以及与 GuardDuty 和 AMS 的对比。",
      "why_correct": "在 Organizations 管理账户中部署 AWS Control Tower 环境是最佳实践。Control Tower 提供了一种简化和自动化多账户环境设置和管理的途径，包括账户创建、安全合规性和治理。通过启用 Security Hub 自动收集、组织和优先级排序安全警报，并使用 Account Factory 简化新账户的配置，可以满足题目中对 API 调用和登录审计、成本优化和 FSBP 合规性的要求。",
      "why_wrong": "选项 B 错误，因为在成员账户中部署 Control Tower 会导致管理复杂性增加，而非管理账户才是推荐的。选项 C 错误，虽然 AMS 可以用于构建多账户登陆区，但提交 RFC 并手动配置 GuardDuty 的流程不如 Control Tower 和 Security Hub 的自动化。此外，在账户创建和合规性方面，AMS 提供的自动化程度不如 Control Tower。选项 D 错误，虽然 AMS 可以用于构建多账户登陆区，但提交 RFC 并手动配置 Security Hub 的流程不如 Control Tower 和 Security Hub 的自动化。Control Tower 提供了更完善的账户生命周期管理，并可以与 Security Hub 紧密集成，实现最佳的安全实践。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS Control Tower",
      "AWS Security Hub",
      "Amazon GuardDuty",
      "API",
      "EC2",
      "EBS",
      "AWS Account Factory",
      "AWS Managed Services (AMS)",
      "FSBP"
    ]
  },
  {
    "id": 788,
    "topic": "1",
    "question_en": "A company has stored 10 TB of log files in Apache Parquet format in an Amazon S3 bucket. The company occasionally needs to use SQL to analyze the log files. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an Amazon Aurora MySQL database. Migrate the data from the S3 bucket into Aurora by using AWS Database Migration Service (AWS DMS). Issue SQL statements to the Aurora database.",
      "B": "Create an Amazon Redshift cluster. Use Redshift Spectrum to run SQL statements directly on the data in the S3 bucket.",
      "C": "Create an AWS Glue crawler to store and retrieve table metadata from the S3 bucket. Use Amazon Athena to run SQL statements directly on the data in the S3 bucket.",
      "D": "Create an Amazon EMR cluster. Use Apache Spark SQL to run SQL statements directly on the data in the S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司将 10 TB 的日志文件以 Apache Parquet 格式存储在 Amazon S3 存储桶中。该公司偶尔需要使用 SQL 分析日志文件。哪种解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Aurora MySQL 数据库。使用 AWS Database Migration Service (AWS DMS) 将数据从 S3 存储桶迁移到 Aurora。向 Aurora 数据库发出 SQL 语句。",
      "B": "创建一个 Amazon Redshift 集群。使用 Redshift Spectrum 直接在 S3 存储桶中的数据上运行 SQL 语句。",
      "C": "创建一个 AWS Glue 爬虫程序，以存储和检索 S3 存储桶中的表元数据。使用 Amazon Athena 直接在 S3 存储桶中的数据上运行 SQL 语句。",
      "D": "创建一个 Amazon EMR 集群。使用 Apache Spark SQL 直接在 S3 存储桶中的数据上运行 SQL 语句。"
    },
    "tags": [
      "Amazon S3",
      "Amazon Athena",
      "AWS Glue",
      "Redshift Spectrum",
      "Amazon EMR",
      "Apache Spark SQL",
      "Amazon Aurora",
      "AWS DMS",
      "Amazon Redshift"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查在 S3 上存储的 Parquet 格式日志文件的分析方案选择；与数据分析服务、成本效益相关。",
      "why_correct": "Amazon Athena 是一种无服务器查询服务，允许直接在 Amazon S3 中存储的数据上运行 SQL 查询。使用 Athena 不需要预置任何基础设施，按查询的数据量付费，非常适合偶尔的 SQL 分析需求，具有成本效益。通过 AWS Glue 爬虫程序可以自动发现并注册 S3 存储桶中 Parquet 文件的 schema，Athena 能够基于这些元数据来执行查询。",
      "why_wrong": {
        "A": "Amazon Aurora MySQL 数据库和 AWS DMS 方案涉及到将数据从 S3 迁移到 Aurora 的过程，这增加了额外的复杂性和成本。Aurora 数据库的维护成本高于 Athena，并且不适合仅仅用于偶尔的查询需求，这不具有成本效益。",
        "B": "Amazon Redshift 是一个数据仓库服务，虽然 Redshift Spectrum 允许直接查询 S3 中的数据，但 Redshift 的成本比 Athena 高，尤其是在数据量不大且查询频率不高的情况下。Redshift 的最低部署成本也高于 Athena。",
        "D": "Amazon EMR 和 Apache Spark SQL 方案需要创建一个 EMR 集群，这涉及创建和管理 EC2 实例，增加了管理和运维成本。 虽然 Spark SQL 可以直接查询 S3 数据，但是相比于 Athena 而言，维护成本更高，更适合需要复杂数据处理和更频繁查询的场景，而非偶尔的 SQL 分析。"
      }
    },
    "related_terms": [
      "Amazon S3",
      "SQL",
      "Amazon Athena",
      "AWS Glue",
      "Amazon Aurora",
      "MySQL",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon Redshift",
      "Redshift Spectrum",
      "Amazon EMR",
      "Apache Parquet",
      "Apache Spark SQL"
    ]
  },
  {
    "id": 789,
    "topic": "1",
    "question_en": "A company needs a solution to prevent AWS CloudFormation stacks from deploying AWS Identity and Access Management (IAM) resources that include an inline policy or “*” in the statement. The solution must also prohibit deployment of Amazon EC2 instances with public IP addresses. The company has AWS Control Tower enabled in its organization in AWS Organizations. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Control Tower proactive controls to block deployment of EC2 instances with public IP addresses and inline policies with elevated access or “*”.",
      "B": "Use AWS Control Tower detective controls to block deployment of EC2 instances with public IP addresses and inline policies with elevated access or “*”.",
      "C": "Use AWS Config to create rules for EC2 and IAM compliance. Configure the rules to run an AWS Systems Manager Session Manager automation to delete a resource when it is not compliant.",
      "D": "Use a service control policy (SCP) to block actions for the EC2 instances and IAM resources if the actions lead to noncompliance."
    },
    "correct_answer": "A",
    "vote_percentage": "64%",
    "question_cn": "一家公司需要一个解决方案，以防止 AWS CloudFormation 堆栈部署包含内联策略或语句中包含“*”的 AWS Identity and Access Management (IAM) 资源。该解决方案还必须禁止部署具有公共 IP 地址的 Amazon EC2 实例。该公司在其 AWS Organizations 组织中启用了 AWS Control Tower。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Control Tower 主动控制来阻止部署具有公共 IP 地址的 EC2 实例以及具有提升访问权限或“*”的内联策略。",
      "B": "使用 AWS Control Tower 侦探控制来阻止部署具有公共 IP 地址的 EC2 实例以及具有提升访问权限或“*”的内联策略。",
      "C": "使用 AWS Config 为 EC2 和 IAM 合规性创建规则。配置规则以运行 AWS Systems Manager Session Manager 自动化，以便在资源不合规时删除资源。",
      "D": "使用服务控制策略 (SCP) 来阻止 EC2 实例和 IAM 资源的动作，如果这些动作导致不合规。"
    },
    "tags": [
      "IAM",
      "SCP",
      "EC2",
      "AWS Control Tower",
      "AWS Config",
      "Systems Manager Session Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 64%），解析仅供参考。】\n\n考查 AWS Control Tower 的主动控制和侦探控制功能，以及如何通过配置规则和 SCP 来管理 IAM 和 EC2 资源的合规性。",
      "why_correct": "AWS Control Tower 的主动控制可以主动阻止不符合要求的资源部署，例如具有公共 IP 地址的 EC2 实例和包含通配符或提升权限的 IAM 策略。结合 Control Tower，该方案提供了所需的预防性控制，以满足题目的要求。",
      "why_wrong": "B 选项的侦探控制侧重于检测不合规行为，而非阻止部署，因此无法满足题目的需求。C 选项使用 AWS Config 和 Systems Manager Session Manager，虽然可以检测和修复不合规资源，但并非 Control Tower 的最佳实践，且配置和管理相对复杂。D 选项使用 SCP，SCP 可以限制账户内的操作，但无法防止 Control Tower 管理的账户部署不合规资源，因为 SCP 的应用范围不如 Control Tower 的主动控制广泛。"
    },
    "related_terms": [
      "AWS CloudFormation",
      "IAM",
      "EC2",
      "AWS Control Tower",
      "AWS Organizations",
      "AWS Config",
      "Systems Manager Session Manager",
      "SCP"
    ]
  },
  {
    "id": 790,
    "topic": "1",
    "question_en": "A company's web application that is hosted in the AWS Cloud recently increased in popularity. The web application currently exists on a single Amazon EC2 instance in a single public subnet. The web application has not been able to meet the demand of the increased web trafic. The company needs a solution that will provide high availability and scalability to meet the increased user demand without rewriting the web application. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Replace the EC2 instance with a larger compute optimized instance.",
      "B": "Configure Amazon EC2 Auto Scaling with multiple Availability Zones in private subnets.",
      "C": "Configure a NAT gateway in a public subnet to handle web requests.",
      "D": "Replace the EC2 instance with a larger memory optimized instanc",
      "E": "E. Configure an Application Load Balancer in a public subnet to distribute web trafic."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家公司托管在 AWS 云中的 Web 应用程序最近越来越受欢迎。该 Web 应用程序目前存在于单个公有子网中的单个 Amazon EC2 实例上。该 Web 应用程序一直无法满足不断增长的 Web 流量的需求。该公司需要一个解决方案，该方案将提供高可用性和可扩展性，以满足不断增长的用户需求，而无需重写 Web 应用程序。哪两种步骤的组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "用更大的计算优化实例替换 EC2 实例。",
      "B": "配置 Amazon EC2 Auto Scaling，在私有子网中使用多个可用区。",
      "C": "在公有子网中配置一个 NAT Gateway 来处理 Web 请求。",
      "D": "用更大的内存优化实例替换 EC2 实例。",
      "E": "在公有子网中配置一个 Application Load Balancer 来分发 Web 流量。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "Application Load Balancer",
      "Availability Zone",
      "NAT Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n考查 Web 应用程序的可用性、可扩展性解决方案；与 EC2 Auto Scaling、负载均衡器、可用区选型相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：配置 Amazon EC2 Auto Scaling (AS) 以实现高可用性和可扩展性。AS 自动根据负载变化增加或减少 EC2 实例的数量。在私有子网中使用多个可用区 (AZ) 确保了高可用性，因为即使一个 AZ 发生故障，流量也可以路由到其他 AZ 中的实例。这种架构满足了对 Web 应用程序的需求，无需重写应用程序。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A：使用更大的计算优化实例替换 EC2 实例虽然可以提升单个实例的性能，但无法提供高可用性和自动扩展功能。如果该实例发生故障，Web 应用程序将不可用。这没有解决高可用性和可扩展性问题。\n\n选项 C：在公有子网中配置 NAT Gateway 仅用于使私有子网中的实例能够访问 Internet，而不是用于处理 Web 流量。它与负载均衡和扩展性无关。因此，它无法满足增加的 Web 流量需求。\n\n选项 D：使用更大的内存优化实例替换 EC2 实例，类似于选项 A，只能提升单个实例的性能，而不能提供高可用性和自动扩展。如果该实例发生故障，Web 应用程序将不可用。不能解决高可用性和可扩展性问题。\n\n选项 E：在公有子网中配置 Application Load Balancer (ALB) 可以将 Web 流量分发到多个 EC2 实例。但是，单靠 ALB 不能提供高可用性和自动扩展。ALB 通常与 EC2 Auto Scaling 结合使用，以根据需求自动调整 EC2 实例的数量。单独使用 ALB 无法满足题目要求，它需要与其他组件配合使用才能实现解决方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 Auto Scaling",
      "Application Load Balancer",
      "Availability Zone",
      "NAT Gateway",
      "EC2"
    ]
  },
  {
    "id": 791,
    "topic": "1",
    "question_en": "A company has AWS Lambda functions that use environment variables. The company does not want its developers to see environment variables in plaintext. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy code to Amazon EC2 instances instead of using Lambda functions.",
      "B": "Configure SSL encryption on the Lambda functions to use AWS CloudHSM to store and encrypt the environment variables.",
      "C": "Create a certificate in AWS Certificate Manager (ACM). Configure the Lambda functions to use the certificate to encrypt the environment variables.",
      "D": "Create an AWS Key Management Service (AWS KMS) key. Enable encryption helpers on the Lambda functions to use the KMS key to store and encrypt the environment variables."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有使用环境变量的 AWS Lambda 函数。该公司不希望其开发人员以纯文本形式查看环境变量。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将代码部署到 Amazon EC2 实例而不是使用 Lambda 函数。",
      "B": "在 Lambda 函数上配置 SSL 加密，以使用 AWS CloudHSM 存储和加密环境变量。",
      "C": "在 AWS Certificate Manager (ACM) 中创建证书。配置 Lambda 函数使用该证书加密环境变量。",
      "D": "创建 AWS Key Management Service (AWS KMS) 密钥。在 Lambda 函数上启用加密助手，以使用 KMS 密钥存储和加密环境变量。"
    },
    "tags": [
      "AWS Lambda",
      "KMS",
      "Lambda Environment Variables",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查如何在 Lambda 函数中安全地管理环境变量，以及与 KMS 的集成。本题考察了环境变量的安全性，以及加密存储的实现方式，涉及了 Lambda、KMS 等服务的选型和使用。",
      "why_correct": "选择 D，创建 AWS KMS 密钥，然后在 Lambda 函数中使用 KMS 密钥对环境变量进行加密，这是满足安全要求的最佳实践。KMS 提供了密钥管理和加密服务，能够安全地存储和保护敏感数据。通过启用加密助手，Lambda 函数可以使用 KMS 密钥对环境变量进行加密和解密，从而避免了以明文形式存储敏感信息，增强了安全性。",
      "why_wrong": "A 选项，使用 Amazon EC2 实例代替 Lambda 函数，并不能直接解决环境变量的安全性问题。EC2 实例上的环境变量仍然可能以明文形式存储，除非额外配置加密措施。B 选项，在 Lambda 函数上配置 SSL 加密并使用 AWS CloudHSM 存储和加密环境变量，方案过于复杂。SSL 主要用于加密传输数据，而不是保护静态数据，且 CloudHSM 的配置和维护成本较高，不适用于简单场景。C 选项，使用 AWS Certificate Manager (ACM) 创建证书来加密环境变量，ACM 主要用于管理和部署 SSL/TLS 证书，与加密环境变量的功能无关，且不具备对环境变量的加密能力。这种方案无法解决题目的需求，即安全地存储和保护 Lambda 函数中的环境变量。"
    },
    "related_terms": [
      "AWS Lambda",
      "Lambda",
      "AWS KMS",
      "KMS",
      "Amazon EC2",
      "EC2",
      "SSL",
      "ACM",
      "Environment Variables",
      "Encryption",
      "AWS CloudHSM",
      "CloudHSM",
      "AWS Certificate Manager"
    ]
  },
  {
    "id": 792,
    "topic": "1",
    "question_en": "An analytics company uses Amazon VPC to run its multi-tier services. The company wants to use RESTful APIs to offer a web analytics service to millions of users. Users must be verified by using an authentication service to access the APIs. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Configure an Amazon Cognito user pool for user authentication. Implement Amazon API Gateway REST APIs with a Cognito authorizer.",
      "B": "Configure an Amazon Cognito identity pool for user authentication. Implement Amazon API Gateway HTTP APIs with a Cognito authorizer.",
      "C": "Configure an AWS Lambda function to handle user authentication. Implement Amazon API Gateway REST APIs with a Lambda authorizer.",
      "D": "Configure an IAM user to handle user authentication. Implement Amazon API Gateway HTTP APIs with an IAM authorizer."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家分析公司使用 Amazon VPC 运行其多层服务。该公司希望使用 RESTful APIs 向数百万用户提供 Web 分析服务。用户必须通过身份验证服务进行验证才能访问这些 APIs。哪个解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "配置一个 Amazon Cognito 用户池用于用户身份验证。使用 Cognito 授权器实现 Amazon API Gateway REST APIs。",
      "B": "配置一个 Amazon Cognito 身份池用于用户身份验证。使用 Cognito 授权器实现 Amazon API Gateway HTTP APIs。",
      "C": "配置一个 AWS Lambda 函数来处理用户身份验证。使用 Lambda 授权器实现 Amazon API Gateway REST APIs。",
      "D": "配置一个 IAM 用户来处理用户身份验证。使用 IAM 授权器实现 Amazon API Gateway HTTP APIs。"
    },
    "tags": [
      "Amazon API Gateway",
      "Amazon Cognito",
      "AWS Lambda",
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n考查在 Amazon VPC 中使用 RESTful APIs 向大量用户提供 Web 分析服务时，如何通过身份验证和授权以最高运营效率满足要求。",
      "why_correct": "选项 A 选择了 Amazon Cognito 用户池进行用户身份验证，并使用 Cognito 授权器实现 Amazon API Gateway REST APIs。Cognito 用户池是管理用户身份验证的理想选择，适用于大规模用户场景。Cognito 授权器与 API Gateway REST APIs 集成，可以简化身份验证流程，并提供高度的可扩展性和运营效率。",
      "why_wrong": "选项 B 错误，因为 Amazon Cognito 身份池主要用于临时访问 AWS 资源，而非用户身份验证。虽然它适用于身份验证，但不如用户池。另外，API Gateway HTTP APIs 目前无法直接使用 Cognito 授权器。选项 C 错误，虽然 Lambda 授权器可以实现自定义身份验证，但它增加了运营复杂性，效率不如 Cognito 授权器。选项 D 错误，IAM 用户主要用于管理 AWS 服务和资源的访问权限，不适用于直接的用户身份验证，并且 IAM 授权器与 HTTP APIs 集成不如 Cognito 方便。"
    },
    "related_terms": [
      "Amazon VPC",
      "RESTful APIs",
      "Amazon Cognito",
      "Cognito User Pool",
      "Cognito Identity Pool",
      "Amazon API Gateway",
      "REST APIs",
      "HTTP APIs",
      "Cognito Authorizer",
      "Lambda Authorizer",
      "AWS Lambda",
      "IAM",
      "IAM Authorizer"
    ]
  },
  {
    "id": 793,
    "topic": "1",
    "question_en": "A company has a mobile app for customers. The app’s data is sensitive and must be encrypted at rest. The company uses AWS Key Management Service (AWS KMS). The company needs a solution that prevents the accidental deletion of KMS keys. The solution must use Amazon Simple Notification Service (Amazon SNS) to send an email notification to administrators when a user attempts to delete a KMS key. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon EventBridge rule that reacts when a user tries to delete a KMS key. Configure an AWS Config rule that cancels any deletion of a KMS key. Add the AWS Config rule as a target of the EventBridge rule. Create an SNS topic that notifies the administrators.",
      "B": "Create an AWS Lambda function that has custom logic to prevent KMS key deletion. Create an Amazon CloudWatch alarm that is activated when a user tries to delete a KMS key. Create an Amazon EventBridge rule that invokes the Lambda function when the DeleteKey operation is performed. Create an SNS topic. Configure the EventBridge rule to publish an SNS message that notifies the administrators.",
      "C": "Create an Amazon EventBridge rule that reacts when the KMS DeleteKey operation is performed. Configure the rule to initiate an AWS Systems Manager Automation runbook. Configure the runbook to cancel the deletion of the KMS key. Create an SNS topic. Configure the EventBridge rule to publish an SNS message that notifies the administrators.",
      "D": "Create an AWS CloudTrail trail. Configure the trail to deliver logs to a new Amazon CloudWatch log group. Create a CloudWatch alarm based on the metric filter for the CloudWatch log group. Configure the alarm to use Amazon SNS to notify the administrators when the KMS DeleteKey operation is performed."
    },
    "correct_answer": "C",
    "vote_percentage": "80%",
    "question_cn": "一家公司为客户提供移动应用程序。该应用程序的数据很敏感，必须在静态时进行加密。该公司使用 AWS Key Management Service (AWS KMS)。该公司需要一个解决方案来防止意外删除 KMS 密钥。该解决方案必须使用 Amazon Simple Notification Service (Amazon SNS) 向管理员发送电子邮件通知，当用户尝试删除 KMS 密钥时。哪个解决方案能以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon EventBridge 规则，该规则在用户尝试删除 KMS 密钥时做出反应。配置一个 AWS Config 规则，取消任何 KMS 密钥的删除。将 AWS Config 规则添加为 EventBridge 规则的目标。创建一个 SNS 主题，通知管理员。",
      "B": "创建一个 AWS Lambda 函数，该函数具有自定义逻辑以防止 KMS 密钥被删除。创建一个 Amazon CloudWatch 警报，当用户尝试删除 KMS 密钥时激活。创建一个 Amazon EventBridge 规则，该规则在执行 DeleteKey 操作时调用 Lambda 函数。创建一个 SNS 主题。配置 EventBridge 规则以发布 SNS 消息，通知管理员。",
      "C": "创建一个 Amazon EventBridge 规则，该规则在 KMS DeleteKey 操作执行时做出反应。配置该规则以启动一个 AWS Systems Manager Automation runbook。配置 runbook 以取消 KMS 密钥的删除。创建一个 SNS 主题。配置 EventBridge 规则以发布 SNS 消息，通知管理员。",
      "D": "创建一个 AWS CloudTrail 追踪。配置追踪以将日志发送到一个新的 Amazon CloudWatch 日志组。基于 CloudWatch 日志组的指标过滤器创建一个 CloudWatch 警报。配置警报以使用 Amazon SNS 在执行 KMS DeleteKey 操作时通知管理员。"
    },
    "tags": [
      "AWS KMS",
      "Amazon SNS",
      "Amazon CloudWatch",
      "AWS CloudTrail",
      "CloudWatch Logs",
      "CloudWatch Metrics",
      "Amazon EventBridge",
      "AWS Config",
      "AWS Lambda",
      "AWS Systems Manager"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 80%），解析仅供参考。】\n\n考察使用 EventBridge、Systems Manager Automation 和 SNS 结合，以实现对 KMS 密钥删除操作的监控和阻止，并通过通知机制告知管理员。",
      "why_correct": "选项 C 提供了最直接且运营开销最低的解决方案。它使用 EventBridge 捕获 KMS 的 DeleteKey 操作，触发 Systems Manager Automation Runbook 来取消删除，有效地防止了密钥被意外删除。同时，它还配置了 SNS 主题，以便及时通知管理员，满足了题目的所有要求。",
      "why_wrong": "选项 A 依赖于 AWS Config 的规则，但无法直接阻止删除操作，且配置复杂。选项 B 使用 Lambda 函数，增加了额外的复杂性和运营成本，需要编写自定义逻辑来取消删除操作。选项 D 仅通过 CloudTrail 和 CloudWatch 监控删除操作，虽然能提供通知，但无法阻止密钥的删除，不符合题意要求。"
    },
    "related_terms": [
      "AWS Key Management Service (AWS KMS)",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon EventBridge",
      "AWS Config",
      "AWS Lambda",
      "Amazon CloudWatch",
      "AWS CloudTrail",
      "AWS Systems Manager Automation",
      "DeleteKey",
      "SNS"
    ]
  },
  {
    "id": 794,
    "topic": "1",
    "question_en": "A company wants to analyze and generate reports to track the usage of its mobile app. The app is popular and has a global user base. The company uses a custom report building program to analyze application usage. The program generates multiple reports during the last week of each month. The program takes less than 10 minutes to produce each report. The company rarely uses the program to generate reports outside of the last week of each month The company wants to generate reports in the least amount of time when the reports are requested. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Run the program by using Amazon EC2 On-Demand Instances. Create an Amazon EventBridge rule to start the EC2 instances when reports are requested. Run the EC2 instances continuously during the last week of each month.",
      "B": "Run the program in AWS Lambda. Create an Amazon EventBridge rule to run a Lambda function when reports are requested.",
      "C": "Run the program in Amazon Elastic Container Service (Amazon ECS). Schedule Amazon ECS to run the program when reports are requested.",
      "D": "Run the program by using Amazon EC2 Spot Instances. Create an Amazon EventBndge rule to start the EC2 instances when reports are requested. Run the EC2 instances continuously during the last week of each month."
    },
    "correct_answer": "B",
    "vote_percentage": "60%",
    "question_cn": "一家公司希望分析并生成报告，以跟踪其移动应用程序的使用情况。该应用程序很受欢迎，并且拥有全球用户群。该公司使用自定义报告构建程序来分析应用程序的使用情况。该程序在每个月的最后一周生成多个报告。该程序生成每个报告需要不到 10 分钟的时间。该公司很少在每个月最后一周之外使用该程序生成报告。该公司希望在报告被请求时，以最少的时间生成报告。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EC2 按需实例运行该程序。创建 Amazon EventBridge 规则以在请求报告时启动 EC2 实例。在每个月的最后一周持续运行 EC2 实例。",
      "B": "在 AWS Lambda 中运行该程序。创建 Amazon EventBridge 规则以在请求报告时运行 Lambda 函数。",
      "C": "在 Amazon Elastic Container Service (Amazon ECS) 中运行该程序。调度 Amazon ECS 在请求报告时运行该程序。",
      "D": "使用 Amazon EC2 Spot 实例运行该程序。创建 Amazon EventBridge 规则以在请求报告时启动 EC2 实例。在每个月的最后一周持续运行 EC2 实例。"
    },
    "tags": [
      "AWS Lambda",
      "Amazon EventBridge",
      "Amazon EC2",
      "Amazon ECS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 60%），解析仅供参考。】\n\n考查如何以最具成本效益的方式运行计算任务，并根据需求进行扩展。与 EC2 实例类型选择、Lambda 函数的适用场景相关。",
      "why_correct": "在 AWS Lambda 中运行该程序是最具成本效益的解决方案。Lambda 是一种按需计算服务，仅在函数被调用时才收费，这与该程序在每个月的最后一周才频繁运行，且很少在其他时间运行的需求相符。结合 Amazon EventBridge，可以轻松地按需触发 Lambda 函数的执行，从而实现按需报告生成。由于计算时间短（10 分钟内），Lambda 的启动时间和执行时间成本都非常适合此类场景。",
      "why_wrong": "A 选项使用 Amazon EC2 按需实例，虽然可以满足功能需求，但成本效率较低。即使只有在需要报告时启动 EC2 实例，按需实例的计费方式仍然会导致在非使用期间产生不必要的成本，尤其是在每月只有一周会频繁运行的情况下。C 选项使用 Amazon ECS 调度程序，相较于 Lambda，ECS 需要管理容器和基础设施，复杂度增加，且 ECS 的计费模式通常不如 Lambda 灵活，成本相对较高。D 选项使用 EC2 Spot 实例，虽然 Spot 实例的成本较低，但由于 Spot 实例的可用性是可变的，存在中断的风险，不适合对报告生成有及时性要求的场景。此外，Spot 实例的管理和维护也比 Lambda 复杂。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EventBridge",
      "AWS Lambda",
      "Amazon ECS",
      "EC2",
      "Spot instances"
    ]
  },
  {
    "id": 795,
    "topic": "1",
    "question_en": "A company is designing a tightly coupled high performance computing (HPC) environment in the AWS Cloud. The company needs to include features that will optimize the HPC environment for networking and storage. Which combination of solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an accelerator in AWS Global Accelerator. Configure custom routing for the accelerator.",
      "B": "Create an Amazon FSx for Lustre file system. Configure the file system with scratch storage.",
      "C": "Create an Amazon CloudFront distribution. Configure the viewer protocol policy to be HTTP and HTTPS.",
      "D": "Launch Amazon EC2 instances. Attach an Elastic Fabric Adapter (EFA) to the instances",
      "E": "Create an AWS Elastic Beanstalk deployment to manage the environment."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 AWS 云中设计一个紧密耦合的高性能计算 (HPC) 环境。公司需要包含可以优化 HPC 环境网络和存储的功能。哪种解决方案组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 AWS Global Accelerator 中创建一个加速器。为加速器配置自定义路由。",
      "B": "创建一个 Amazon FSx for Lustre 文件系统。使用临时存储配置文件系统。",
      "C": "创建一个 Amazon CloudFront 分发。将查看器协议策略配置为 HTTP 和 HTTPS。",
      "D": "启动 Amazon EC2 实例。将 Elastic Fabric Adapter (EFA) 附加到实例。",
      "E": "创建一个 AWS Elastic Beanstalk 部署来管理环境。"
    },
    "tags": [
      "HPC",
      "EC2",
      "EFA",
      "FSx for Lustre",
      "Global Accelerator",
      "CloudFront",
      "Elastic Beanstalk"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n此题考察了 HPC 环境的构建方案。EFA 提供了高性能的实例间通信能力，FSx for Lustre 提供了高性能、可扩展的共享存储。这两个组件的组合能够很好地满足 HPC 场景的需求。",
      "why_correct": "选项 B 提供了 Amazon FSx for Lustre 文件系统作为高性能存储，配合临时存储配置文件系统满足 HPC 对存储的需求；选项 D 提供了 EC2 实例和 EFA，满足 HPC 对网络的需求。",
      "why_wrong": "选项 A 介绍了 Global Accelerator，它主要用于提高应用程序的可用性和性能，不适用于 HPC 场景。选项 C 介绍了 CloudFront，它是一个内容分发网络，主要用于加速内容交付，不适用于 HPC 场景。选项 E 介绍了 Elastic Beanstalk，主要用于部署和管理 Web 应用程序，不适用于 HPC 场景。"
    },
    "related_terms": [
      "HPC",
      "Amazon FSx for Lustre",
      "Amazon EC2",
      "AWS Global Accelerator",
      "Amazon CloudFront",
      "AWS Elastic Beanstalk",
      "Elastic Fabric Adapter (EFA)"
    ]
  },
  {
    "id": 796,
    "topic": "1",
    "question_en": "A company needs a solution to prevent photos with unwanted content from being uploaded to the company's web application. The solution must not involve training a machine learning (ML) model. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create and deploy a model by using Amazon SageMaker Autopilot. Create a real-time endpoint that the web application invokes when new photos are uploaded.",
      "B": "Create an AWS Lambda function that uses Amazon Rekognition to detect unwanted content. Create a Lambda function URL that the web application invokes when new photos are uploaded.",
      "C": "Create an Amazon CloudFront function that uses Amazon Comprehend to detect unwanted content. Associate the function with the web application.",
      "D": "Create an AWS Lambda function that uses Amazon Rekognition Video to detect unwanted content. Create a Lambda function URL that the web application invokes when new photos are uploaded."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要一个解决方案，以防止包含不必要内容的照片上传到公司的 Web 应用程序。该解决方案不得涉及训练机器学习 (ML) 模型。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon SageMaker Autopilot 创建并部署一个模型。创建一个实时终端节点，当上传新照片时，Web 应用程序会调用该节点。",
      "B": "创建一个 AWS Lambda 函数，该函数使用 Amazon Rekognition 检测不需要的内容。创建一个 Lambda 函数 URL，当上传新照片时，Web 应用程序会调用该 URL。",
      "C": "创建一个 Amazon CloudFront 函数，该函数使用 Amazon Comprehend 检测不需要的内容。将该函数与 Web 应用程序关联。",
      "D": "创建一个 AWS Lambda 函数，该函数使用 Amazon Rekognition Video 检测不需要的内容。创建一个 Lambda 函数 URL，当上传新照片时，Web 应用程序会调用该 URL。"
    },
    "tags": [
      "Rekognition",
      "Lambda",
      "CloudFront",
      "Comprehend",
      "SageMaker"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查了防止不必要内容上传的解决方案。使用 Amazon Rekognition 可以检测图片中的内容，而无需训练 ML 模型。使用 Lambda 函数可以快速响应事件，实现与 Web 应用程序的集成。",
      "why_correct": "选项 B 使用 Amazon Rekognition 检测不需要的内容，并使用 Lambda 函数和 Lambda 函数 URL 响应 Web 应用程序的上传事件，满足题目的需求。",
      "why_wrong": "选项 A 涉及了使用 SageMaker Autopilot 训练模型，不符合题意，且 SageMaker 的部署和调用流程较为复杂。选项 C 介绍了 Comprehend，主要用于文本分析，不适用于图片内容检测。选项 D 使用了 Rekognition Video，但检测照片并不需要使用 Video，且 Lambda 函数 URL 可以满足需求。"
    },
    "related_terms": [
      "Amazon Rekognition",
      "AWS Lambda",
      "Amazon CloudFront",
      "Amazon Comprehend",
      "Amazon SageMaker",
      "ML"
    ]
  },
  {
    "id": 797,
    "topic": "1",
    "question_en": "A company uses AWS to run its ecommerce platform. The platform is critical to the company's operations and has a high volume of trafic and transactions. The company configures a multi-factor authentication (MFA) device to secure its AWS account root user credentials. The company wants to ensure that it will not lose access to the root user account if the MFA device is lost. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up a backup administrator account that the company can use to log in if the company loses the MFA device.",
      "B": "Add multiple MFA devices for the root user account to handle the disaster scenario.",
      "C": "Create a new administrator account when the company cannot access the root account.",
      "D": "Attach the administrator policy to another IAM user when the company cannot access the root account."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 AWS 运行其电子商务平台。该平台对公司的运营至关重要，并且拥有大量的流量和交易。该公司配置了多因素身份验证 (MFA) 设备来保护其 AWS 账户的根用户凭证。该公司希望确保如果 MFA 设备丢失，它将不会失去对根用户账户的访问权限。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置一个备用管理员账户，以便在公司丢失 MFA 设备时登录。",
      "B": "为根用户账户添加多个 MFA 设备以处理灾难场景。",
      "C": "当公司无法访问根账户时，创建一个新的管理员账户。",
      "D": "当公司无法访问根账户时，将管理员策略附加到另一个 IAM 用户。"
    },
    "tags": [
      "IAM",
      "MFA",
      "根用户"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n该题考察了根用户 MFA 保护策略。根用户是 AWS 账户的最高权限用户，启用 MFA 可以提高安全性。如果 MFA 设备丢失，需要有备用方案。",
      "why_correct": "选项 B 提供了为根用户账户添加多个 MFA 设备的方案，以应对 MFA 设备丢失的情况，保证账户的访问安全性。",
      "why_wrong": "选项 A 提供了备用管理员账户，但是不符合题目中关于根用户账户的 MFA 保护要求。选项 C 介绍了新建管理员账户，但在 MFA 设备丢失时，新建账户需要经过额外的安全验证，无法保证访问的连续性。选项 D 介绍了将管理员策略附加到另一个 IAM 用户，但是不能解决根用户账户的 MFA 保护问题。"
    },
    "related_terms": [
      "MFA",
      "IAM",
      "根用户"
    ]
  },
  {
    "id": 798,
    "topic": "1",
    "question_en": "A social media company is creating a rewards program website for its users. The company gives users points when users create and upload videos to the website. Users redeem their points for gifts or discounts from the company's afiliated partners. A unique ID identifies users. The partners refer to this ID to verify user eligibility for rewards. The partners want to receive notification of user IDs through an HTTP endpoint when the company gives users points. Hundreds of vendors are interested in becoming afiliated partners every day. The company wants to design an architecture that gives the website the ability to add partners rapidly in a scalable way. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Create an Amazon Timestream database to keep a list of afiliated partners. Implement an AWS Lambda function to read the list. Configure the Lambda function to send user IDs to each partner when the company gives users points.",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Choose an endpoint protocol. Subscribe the partners to the topic. Publish user IDs to the topic when the company gives users points.",
      "C": "Create an AWS Step Functions state machine. Create a task for every afiliated partner. Invoke the state machine with user IDs as input when the company gives users points.",
      "D": "Create a data stream in Amazon Kinesis Data Streams. Implement producer and consumer applications. Store a list of afiliated partners in the data stream. Send user IDs when the company gives users points."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家社交媒体公司正在为其用户创建一个奖励计划网站。当用户创建并将视频上传到网站时，该公司会向用户提供积分。用户可以用他们的积分兑换公司附属合作伙伴提供的礼物或折扣。唯一的 ID 标识用户。合作伙伴参考此 ID 来验证用户的奖励资格。合作伙伴希望在公司向用户提供积分时，通过 HTTP 端点接收用户 ID 的通知。每天都有数百家供应商有兴趣成为附属合作伙伴。该公司希望设计一个架构，使网站能够以可扩展的方式快速添加合作伙伴。哪种解决方案将以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Timestream 数据库来保存附属合作伙伴的列表。实施一个 AWS Lambda 函数来读取该列表。配置 Lambda 函数，在公司向用户提供积分时，将用户 ID 发送到每个合作伙伴。",
      "B": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题。选择一个端点协议。让合作伙伴订阅该主题。在公司向用户提供积分时，将用户 ID 发布到该主题。",
      "C": "创建一个 AWS Step Functions 状态机。为每个附属合作伙伴创建一个任务。在公司向用户提供积分时，使用用户 ID 作为输入来调用该状态机。",
      "D": "在 Amazon Kinesis Data Streams 中创建一个数据流。实施生产者和消费者应用程序。在数据流中存储附属合作伙伴的列表。在公司向用户提供积分时，发送用户 ID。"
    },
    "tags": [
      "SNS",
      "Lambda",
      "Step Functions",
      "Kinesis Data Streams",
      "Timestream"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何设计一个可扩展的架构，用于将用户ID通知给多个合作伙伴，并评估不同AWS服务的适用性。",
      "why_correct": "Amazon SNS主题是发布/订阅服务，非常适合将消息发送给大量接收者。合作伙伴可以订阅SNS主题，并接收用户ID通知。这种方法易于扩展，只需简单配置即可添加新的合作伙伴，且实施工作量最少。",
      "why_wrong": "A. Amazon Timestream用于时间序列数据的存储，与通知场景不符。使用Lambda函数发送通知，需要针对每个合作伙伴进行单独调用，可扩展性差。C. AWS Step Functions 用于编排任务流程，不适合简单的消息通知。为每个合作伙伴创建任务，会增加复杂性。D. Amazon Kinesis Data Streams用于实时数据流处理，增加了不必要的复杂性。维护合作伙伴列表也增加了额外的管理工作。"
    },
    "related_terms": [
      "Amazon Timestream",
      "AWS Lambda",
      "Amazon SNS",
      "AWS Step Functions",
      "Amazon Kinesis Data Streams",
      "HTTP"
    ]
  },
  {
    "id": 799,
    "topic": "1",
    "question_en": "A company needs to extract the names of ingredients from recipe records that are stored as text files in an Amazon S3 bucket. A web application will use the ingredient names to query an Amazon DynamoDB table and determine a nutrition score. The application can handle non-food records and errors. The company does not have any employees who have machine learning knowledge to develop this solution. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon Comprehend. Store the Amazon Comprehend output in the DynamoDB table.",
      "B": "Use an Amazon EventBridge rule to invoke an AWS Lambda function when PutObject requests occur. Program the Lambda function to analyze the object by using Amazon Forecast to extract the ingredient names. Store the Forecast output in the DynamoDB table.",
      "C": "Use S3 Event Notifications to invoke an AWS Lambda function when PutObject requests occur. Use Amazon Polly to create audio recordings of the recipe records. Save the audio files in the S3 bucket. Use Amazon Simple Notification Service (Amazon SNS) to send a URL as a message to employees. Instruct the employees to listen to the audio files and calculate the nutrition score. Store the ingredient names in the DynamoDB table.",
      "D": "Use an Amazon EventBridge rule to invoke an AWS Lambda function when a PutObject request occurs. Program the Lambda function to analyze the object and extract the ingredient names by using Amazon SageMaker. Store the inference output from the SageMaker endpoint in the DynamoDB table."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要从存储为文本文件的、位于 Amazon S3 存储桶中的食谱记录中提取配料名称。一个 Web 应用程序将使用配料名称来查询 Amazon DynamoDB 表并确定营养评分。该应用程序可以处理非食品记录和错误。该公司没有任何具备机器学习知识的员工来开发此解决方案。哪个解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 S3 事件通知在 PutObject 请求发生时调用 AWS Lambda 函数。对 Lambda 函数进行编程，以使用 Amazon Comprehend 分析对象并提取配料名称。将 Amazon Comprehend 输出存储在 DynamoDB 表中。",
      "B": "使用 Amazon EventBridge 规则在 PutObject 请求发生时调用 AWS Lambda 函数。对 Lambda 函数进行编程，以使用 Amazon Forecast 分析对象来提取配料名称。将 Forecast 输出存储在 DynamoDB 表中。",
      "C": "使用 S3 事件通知在 PutObject 请求发生时调用 AWS Lambda 函数。使用 Amazon Polly 创建食谱记录的录音。将音频文件保存在 S3 存储桶中。使用 Amazon Simple Notification Service (Amazon SNS) 将 URL 作为消息发送给员工。指示员工收听音频文件并计算营养评分。将配料名称存储在 DynamoDB 表中。",
      "D": "使用 Amazon EventBridge 规则在 PutObject 请求发生时调用 AWS Lambda 函数。对 Lambda 函数进行编程，以使用 Amazon SageMaker 分析对象并提取配料名称。将 SageMaker 终端节点的推理输出存储在 DynamoDB 表中。"
    },
    "tags": [
      "S3",
      "Lambda",
      "Comprehend",
      "Polly",
      "DynamoDB",
      "EventBridge",
      "SageMaker",
      "Forecast"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查利用 AWS 服务从 S3 中的文本文件提取数据，并存储到 DynamoDB 表中的方案，要求方案具有成本效益且无需机器学习专业知识。",
      "why_correct": "选项 A 使用 Amazon Comprehend 进行文本分析，提取配料名称。Comprehend 是一项 NLP 服务，无需机器学习专业知识即可使用。它能直接从文本文件中提取信息。S3 事件触发 Lambda 函数，将结果存储到 DynamoDB，满足题目要求且具有成本效益。",
      "why_wrong": "选项 B 错误，因为 Amazon Forecast 用于时间序列预测，不适用于从文本文件中提取信息。选项 C 错误，因为它使用 Polly 和 SNS 创建和分发音频文件，然后由人工进行处理，效率低下且成本高昂，不符合题意。选项 D 错误，因为它使用 SageMaker，需要专业的机器学习知识，而题目要求没有此类知识。SageMaker 也比 Comprehend 更复杂，成本更高。"
    },
    "related_terms": [
      "Amazon S3",
      "DynamoDB",
      "AWS Lambda",
      "Amazon Comprehend",
      "Amazon EventBridge",
      "Amazon Forecast",
      "Amazon Polly",
      "Amazon Simple Notification Service (Amazon SNS)",
      "Amazon SageMaker",
      "PutObject",
      "S3 event notification"
    ]
  },
  {
    "id": 800,
    "topic": "1",
    "question_en": "A company needs to create an AWS Lambda function that will run in a VPC in the company's primary AWS account. The Lambda function needs to access files that the company stores in an Amazon Elastic File System (Amazon EFS) file system. The EFS file system is located in a secondary AWS account. As the company adds files to the file system, the solution must scale to meet the demand. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a new EFS file system in the primary account. Use AWS DataSync to copy the contents of the original EFS file system to the new EFS file system.",
      "B": "Create a VPC peering connection between the VPCs that are in the primary account and the secondary account.",
      "C": "Create a second Lambda function in the secondary account that has a mount that is configured for the file system. Use the primary account's Lambda function to invoke the secondary account's Lambda function.",
      "D": "Move the contents of the file system to a Lambda layer. Configure the Lambda layer's permissions to allow the company's secondary account to use the Lambda layer."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要创建一个 AWS Lambda 函数，该函数将在公司主 AWS 账户的 VPC 中运行。 Lambda 函数需要访问公司存储在 Amazon EFS 文件系统中的文件。 EFS 文件系统位于辅助 AWS 账户中。 随着公司将文件添加到文件系统，解决方案必须扩展以满足需求。 哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "在主账户中创建一个新的 EFS 文件系统。 使用 AWS DataSync 将原始 EFS 文件系统的内容复制到新的 EFS 文件系统。",
      "B": "在主账户和辅助账户中的 VPC 之间创建 VPC 对等连接。",
      "C": "在辅助账户中创建第二个 Lambda 函数，该函数具有为文件系统配置的挂载。 使用主账户的 Lambda 函数来调用辅助账户的 Lambda 函数。",
      "D": "将文件系统的内容移动到 Lambda 层。 将 Lambda 层的权限配置为允许公司的辅助账户使用 Lambda 层。"
    },
    "tags": [
      "EFS",
      "Lambda",
      "VPC",
      "EFS",
      "DataSync"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查了跨账户访问 Amazon EFS 的 Lambda 函数的成本效益解决方案，以及对 VPC 对等连接和 Lambda 层的理解。",
      "why_correct": "选项 B 是最佳解决方案。 VPC 对等连接允许主账户中的 Lambda 函数通过私有 IP 地址访问辅助账户中的 EFS 文件系统，无需经过 Internet。 这保持了流量的私密性，并避免了额外的成本，同时满足了扩展需求，因为 EFS 可以根据需要扩展。",
      "why_wrong": "选项 A 不合适，因为它涉及数据复制，这会产生额外的成本和管理开销，并且会增加延迟。选项 C 需要跨账户的 Lambda 函数调用，这增加了复杂性，且可能产生额外的调用费用。选项 D 将文件存储在 Lambda 层中，这不适合存储大型文件，并且 Lambda 层的存储容量有限，无法满足扩展需求。"
    },
    "related_terms": [
      "AWS Lambda",
      "Amazon EFS",
      "VPC",
      "AWS DataSync",
      "VPC Peering Connection",
      "Lambda layer"
    ]
  },
  {
    "id": 801,
    "topic": "1",
    "question_en": "A financial company needs to handle highly sensitive data. The company will store the data in an Amazon S3 bucket. The company needs to ensure that the data is encrypted in transit and at rest. The company must manage the encryption keys outside the AWS Cloud. Which solution will meet these requirements?",
    "options_en": {
      "A": "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) customer managed key.",
      "B": "Encrypt the data in the S3 bucket with server-side encryption (SSE) that uses an AWS Key Management Service (AWS KMS) AWS managed key.",
      "C": "Encrypt the data in the S3 bucket with the default server-side encryption (SSE).",
      "D": "Encrypt the data at the company's data center before storing the data in the S3 bucket."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家金融公司需要处理高度敏感的数据。该公司将数据存储在 Amazon S3 存储桶中。该公司需要确保数据在传输中和静态时都已加密。该公司必须在 AWS 云之外管理加密密钥。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用服务器端加密 (SSE) 对 S3 存储桶中的数据进行加密，该加密使用 AWS Key Management Service (AWS KMS) 客户托管密钥。",
      "B": "使用服务器端加密 (SSE) 对 S3 存储桶中的数据进行加密，该加密使用 AWS Key Management Service (AWS KMS) AWS 托管密钥。",
      "C": "使用默认的服务器端加密 (SSE) 对 S3 存储桶中的数据进行加密。",
      "D": "在将数据存储在 S3 存储桶中之前，在公司的数据中心对数据进行加密。"
    },
    "tags": [
      "S3",
      "KMS",
      "SSE"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察使用在 AWS 云外管理的加密密钥对 S3 数据进行加密的方案。",
      "why_correct": "选项 D 满足了在云外管理密钥的要求。通过在公司数据中心加密数据，加密密钥由公司完全控制，数据在传输到 S3 之前就已经加密，并在静态时保持加密状态，即使存储在 S3 中。",
      "why_wrong": "选项 A 使用 AWS KMS 客户托管密钥，虽然密钥在 AWS 中，但仍属于客户管理，不满足在 AWS 云之外管理密钥的要求。选项 B 使用 AWS KMS AWS 托管密钥，密钥由 AWS 管理，不满足在云外管理密钥的要求。选项 C 使用默认的 SSE，没有指定加密密钥的管理方式，无法确定密钥是否在云外管理，且此方法通常使用 AWS 托管密钥，不满足题目要求。"
    },
    "related_terms": [
      "Amazon S3",
      "SSE",
      "AWS KMS",
      "AWS",
      "encryption"
    ]
  },
  {
    "id": 802,
    "topic": "1",
    "question_en": "A company wants to run its payment application on AWS. The application receives payment notifications from mobile devices. Payment notifications require a basic validation before they are sent for further processing. The backend processing application is long running and requires compute and memory to be adjusted. The company does not want to manage the infrastructure. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an Amazon EventBridge rule to receive payment notifications from mobile devices. Configure the rule to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Kubernetes Service (Amazon EKS) Anywhere. Create a standalone cluster.",
      "B": "Create an Amazon API Gateway API. Integrate the API with an AWS Step Functions state machine to receive payment notifications from mobile devices. Invoke the state machine to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure an EKS cluster with self-managed nodes.",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Integrate the queue with an Amazon EventBridge rule to receive payment notifications from mobile devices. Configure the rule to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon EC2 Spot Instances. Configure a Spot Fleet with a default allocation strategy.",
      "D": "Create an Amazon API Gateway API. Integrate the API with AWS Lambda to receive payment notifications from mobile devices. Invoke a Lambda function to validate payment notifications and send the notifications to the backend application. Deploy the backend application on Amazon Elastic Container Service (Amazon ECS). Configure Amazon ECS with an AWS Fargate launch type."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望在 AWS 上运行其支付应用程序。该应用程序接收来自移动设备的支付通知。支付通知在发送进行进一步处理之前需要进行基本的验证。后端处理应用程序是长时间运行的，需要调整计算和内存。该公司不希望管理基础设施。哪个解决方案能以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。将队列与 Amazon EventBridge 规则集成，以接收来自移动设备的支付通知。将该规则配置为验证支付通知并将通知发送到后端应用程序。将后端应用程序部署在 Amazon Elastic Kubernetes Service (Amazon EKS) Anywhere 上。创建一个独立的集群。",
      "B": "创建一个 Amazon API Gateway API。将 API 与 AWS Step Functions 状态机集成，以接收来自移动设备的支付通知。调用状态机来验证支付通知并将通知发送到后端应用程序。将后端应用程序部署在 Amazon Elastic Kubernetes Service (Amazon EKS) 上。配置一个具有自管理节点的 EKS 集群。",
      "C": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。将队列与 Amazon EventBridge 规则集成，以接收来自移动设备的支付通知。将该规则配置为验证支付通知并将通知发送到后端应用程序。将后端应用程序部署在 Amazon EC2 Spot Instances 上。使用默认分配策略配置一个 Spot Fleet。",
      "D": "创建一个 Amazon API Gateway API。将 API 与 AWS Lambda 集成，以接收来自移动设备的支付通知。调用 Lambda 函数来验证支付通知并将通知发送到后端应用程序。将后端应用程序部署在 Amazon Elastic Container Service (Amazon ECS) 上。使用 AWS Fargate 启动类型配置 Amazon ECS。"
    },
    "tags": [
      "SQS",
      "EventBridge",
      "API Gateway",
      "Lambda",
      "ECS",
      "Fargate",
      "EKS",
      "Spot Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查在无服务器环境下，如何利用 API Gateway、Lambda 和 ECS/Fargate 组合实现支付应用程序的接收、验证和后端处理，以满足低运维成本的需求。",
      "why_correct": "选项 D 提供了最佳的无服务器解决方案。API Gateway 接收支付通知。Lambda 函数处理通知验证。后端处理程序部署在 ECS 上，并使用 Fargate 作为启动类型，实现了完全托管的容器编排，无需管理底层服务器，符合最低运营开销的要求，并且能够动态调整计算资源以适应长时间运行的后端应用程序。",
      "why_wrong": "选项 A 使用 EKS Anywhere 和 EKS 集群，需要自行管理基础设施，运维开销过高，不符合要求。选项 B 虽然使用了 API Gateway 和 Step Functions，但依然使用 EKS 并需要管理集群，运维成本较高。选项 C 使用 EC2 Spot Instances，需要管理 EC2 实例，并且 Spot 实例的不可靠性可能导致问题，不符合不希望管理基础设施的要求。"
    },
    "related_terms": [
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon EventBridge",
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon API Gateway",
      "AWS Step Functions",
      "Amazon EC2 Spot Instances",
      "Spot Fleet",
      "AWS Lambda",
      "Amazon Elastic Container Service (Amazon ECS)",
      "AWS Fargate"
    ]
  },
  {
    "id": 803,
    "topic": "1",
    "question_en": "A solutions architect is designing a user authentication solution for a company. The solution must invoke two-factor authentication for users that log in from inconsistent geographical locations, IP addresses, or devices. The solution must also be able to scale up to accommodate millions of users. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure Amazon Cognito user pools for user authentication. Enable the risk-based adaptive authentication feature with multifactor authentication (MFA).",
      "B": "Configure Amazon Cognito identity pools for user authentication. Enable multi-factor authentication (MFA).",
      "C": "Configure AWS Identity and Access Management (IAM) users for user authentication. Attach an IAM policy that allows the AllowManageOwnUserMFA action.",
      "D": "Configure AWS IAM Identity Center (AWS Single Sign-On) authentication for user authentication. Configure the permission sets to require multi-factor authentication (MFA)."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在为一家公司设计用户身份验证解决方案。该解决方案必须为从不一致的地理位置、IP 地址或设备登录的用户调用双因素身份验证。该解决方案还必须能够扩展以容纳数百万用户。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Cognito 用户池进行用户身份验证。启用具有多因素身份验证 (MFA) 的基于风险的自适应身份验证功能。",
      "B": "配置 Amazon Cognito 身份池进行用户身份验证。启用多因素身份验证 (MFA)。",
      "C": "配置 AWS Identity and Access Management (IAM) 用户进行用户身份验证。附加一个允许 AllowManageOwnUserMFA 操作的 IAM 策略。",
      "D": "配置 AWS IAM Identity Center (AWS Single Sign-On) 身份验证进行用户身份验证。配置权限集以要求多因素身份验证 (MFA)。"
    },
    "tags": [
      "Cognito",
      "IAM",
      "MFA",
      "身份验证"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考察了 Amazon Cognito 用户池的自适应身份验证能力，以及其在应对用户登录安全方面的优势。",
      "why_correct": "Amazon Cognito 用户池提供了用户身份验证和授权服务。它支持基于风险的自适应身份验证，可以根据用户的登录行为（如地理位置、IP 地址、设备等）触发 MFA。Cognito 能够扩展以支持数百万用户，满足了题目的需求。",
      "why_wrong": "选项 B 错误，因为 Cognito 身份池主要用于临时访问 AWS 资源，而非用户身份验证和 MFA。选项 C 错误，IAM 用户主要用于管理 AWS 资源访问，不适用于用户身份验证和自适应 MFA。选项 D 错误，AWS IAM Identity Center (AWS SSO) 主要用于单点登录和集中管理，虽然支持 MFA，但其自适应身份验证能力不如 Cognito 用户池。"
    },
    "related_terms": [
      "Amazon Cognito",
      "Amazon Cognito User Pool",
      "MFA",
      "AWS IAM",
      "IAM User",
      "AWS IAM Identity Center (AWS SSO)",
      "Adaptive Authentication"
    ]
  },
  {
    "id": 804,
    "topic": "1",
    "question_en": "A company has an Amazon S3 data lake. The company needs a solution that transforms the data from the data lake and loads the data into a data warehouse every day. The data warehouse must have massively parallel processing (MPP) capabilities. Data analysts then need to create and train machine learning (ML) models by using SQL commands on the data. The solution must use serverless AWS services wherever possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Run a daily Amazon EMR job to transform the data and load the data into Amazon Redshift. Use Amazon Redshift ML to create and train the ML models.",
      "B": "Run a daily Amazon EMR job to transform the data and load the data into Amazon Aurora Serverless. Use Amazon Aurora ML to create and train the ML models.",
      "C": "Run a daily AWS Glue job to transform the data and load the data into Amazon Redshift Serverless. Use Amazon Redshift ML to create and train the ML models.",
      "D": "Run a daily AWS Glue job to transform the data and load the data into Amazon Athena tables. Use Amazon Athena ML to create and train the ML models."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个 Amazon S3 数据湖。该公司需要一个解决方案，每天转换来自数据湖的数据，并将数据加载到数据仓库中。数据仓库必须具有大规模并行处理 (MPP) 功能。然后，数据分析师需要使用 SQL 命令在数据上创建和训练机器学习 (ML) 模型。该解决方案必须尽可能使用无服务器 AWS 服务。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "运行每日 Amazon EMR 作业，以转换数据并将数据加载到 Amazon Redshift 中。使用 Amazon Redshift ML 创建和训练 ML 模型。",
      "B": "运行每日 Amazon EMR 作业，以转换数据并将数据加载到 Amazon Aurora Serverless 中。使用 Amazon Aurora ML 创建和训练 ML 模型。",
      "C": "运行每日 AWS Glue 作业，以转换数据并将数据加载到 Amazon Redshift Serverless 中。使用 Amazon Redshift ML 创建和训练 ML 模型。",
      "D": "运行每日 AWS Glue 作业，以转换数据并将数据加载到 Amazon Athena 表中。使用 Amazon Athena ML 创建和训练 ML 模型。"
    },
    "tags": [
      "EMR",
      "Redshift",
      "Athena",
      "Glue",
      "Aurora",
      "Redshift Serverless",
      "Redshift ML",
      "ML"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查利用无服务器服务实现数据湖数据转换、加载到数据仓库，并使用 SQL 创建和训练 ML 模型。",
      "why_correct": "选项 C 提供了最符合要求的无服务器解决方案。AWS Glue 用于数据转换和ETL，Amazon Redshift Serverless 提供了 MPP 功能的数据仓库，满足大规模数据处理需求，并且可以使用 Amazon Redshift ML，通过 SQL 语句创建和训练 ML 模型。",
      "why_wrong": "选项 A 采用了 EMR，不符合无服务器要求。选项 B 使用了 Aurora Serverless，而 Aurora 主要针对 OLTP 工作负载，并不擅长大规模数据仓库场景，且 Aurora ML 功能相对较弱。选项 D 使用了 Athena，Athena 适合查询数据湖中的数据，但不适合作为数据仓库，不具备 MPP 功能，且性能较差，不利于后续的 ML 模型训练。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon EMR",
      "Amazon Redshift",
      "Amazon Aurora Serverless",
      "AWS Glue",
      "Amazon Athena",
      "Amazon Redshift ML",
      "ML",
      "MPP",
      "SQL",
      "ETL"
    ]
  },
  {
    "id": 805,
    "topic": "1",
    "question_en": "A company runs containers in a Kubernetes environment in the company's local data center. The company wants to use Amazon Elastic Kubernetes Service (Amazon EKS) and other AWS managed services. Data must remain locally in the company's data center and cannot be stored in any remote site or cloud to maintain compliance. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy AWS Local Zones in the company's data center.",
      "B": "Use an AWS Snowmobile in the company's data center.",
      "C": "Install an AWS Outposts rack in the company's data center.",
      "D": "Install an AWS Snowball Edge Storage Optimized node in the data center."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其本地数据中心内的 Kubernetes 环境中运行容器。该公司希望使用 Amazon Elastic Kubernetes Service (Amazon EKS) 和其他 AWS 托管服务。为了保持合规性，数据必须保留在该公司的数据中心内，并且不能存储在任何远程站点或云中。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在该公司的数据中心部署 AWS Local Zones。",
      "B": "在该公司的数据中心使用 AWS Snowmobile。",
      "C": "在该公司的数据中心安装 AWS Outposts 机架。",
      "D": "在该数据中心安装 AWS Snowball Edge 存储优化设备。"
    },
    "tags": [
      "EKS",
      "Outposts",
      "Snowmobile",
      "Snowball Edge",
      "Local Zones"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察如何在本地数据中心部署 AWS 服务以满足数据驻留要求。",
      "why_correct": "AWS Outposts 机架是设计用于在客户数据中心运行 AWS 服务的物理基础设施。它允许在本地运行 EKS 和其他 AWS 服务，同时满足数据必须保留在本地的要求。这使得公司能够利用 AWS 的优势，同时保持对数据的控制。",
      "why_wrong": "AWS Local Zones 扩展了 AWS 的基础设施，但不是部署在客户的数据中心，因此不满足数据驻留要求。AWS Snowmobile 用于大规模数据迁移到 AWS 云，违背了数据不能存储在任何远程站点或云中的要求。AWS Snowball Edge 存储优化设备是边缘计算和数据迁移设备，但不是用于运行 EKS 的基础设施，且也无法满足数据必须完全驻留本地的需求。"
    },
    "related_terms": [
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "AWS Local Zones",
      "AWS Snowmobile",
      "AWS Outposts",
      "AWS Snowball Edge"
    ]
  },
  {
    "id": 806,
    "topic": "1",
    "question_en": "A social media company has workloads that collect and process data. The workloads store the data in on-premises NFS storage. The data store cannot scale fast enough to meet the company’s expanding business needs. The company wants to migrate the current data store to AWS. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Set up an AWS Storage Gateway Volume Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.",
      "B": "Set up an AWS Storage Gateway Amazon S3 File Gateway. Use an Amazon S3 Lifecycle policy to transition the data to the appropriate storage class.",
      "C": "Use the Amazon Elastic File System (Amazon EFS) Standard-Infrequent Access (Standard-IA) storage class. Activate the infrequent access lifecycle policy.",
      "D": "Use the Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA) storage class. Activate the infrequent access lifecycle policy."
    },
    "correct_answer": "B",
    "vote_percentage": "90%",
    "question_cn": "一家社交媒体公司有收集和处理数据的业务。这些业务将数据存储在本地 NFS 存储中。数据存储无法快速扩展以满足公司不断增长的业务需求。该公司希望将当前数据存储迁移到 AWS。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "设置 AWS Storage Gateway Volume Gateway。 使用 Amazon S3 生命周期策略将数据转换为适当的存储类别。",
      "B": "设置 AWS Storage Gateway Amazon S3 File Gateway。 使用 Amazon S3 生命周期策略将数据转换为适当的存储类别。",
      "C": "使用 Amazon EFS 标准-不频繁访问（标准-IA）存储类别。 激活不频繁访问生命周期策略。",
      "D": "使用 Amazon EFS 单区-不频繁访问（单区-IA）存储类别。 激活不频繁访问生命周期策略。"
    },
    "tags": [
      "Storage Gateway",
      "S3",
      "EFS",
      "IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 90%），解析仅供参考。】\n\n考查如何以最具成本效益的方式将本地 NFS 存储迁移到 AWS，并满足可扩展性需求，以及选择合适的 AWS 存储服务和生命周期策略。",
      "why_correct": "选项 B 使用 Storage Gateway 的 File Gateway 模式，可以无缝地将本地 NFS 文件存储迁移到 Amazon S3。File Gateway 提供了基于文件的协议访问 S3 的能力，并且通过 S3 的生命周期策略，可以根据访问频率自动将数据迁移到更具成本效益的存储类别，例如 S3 Intelligent-Tiering 或 S3 Glacier，从而满足成本效益需求。",
      "why_wrong": "选项 A 使用 Volume Gateway 不适用于基于文件的数据迁移，它主要用于块存储卷的备份和恢复，无法直接处理文件存储。选项 C 和 D 虽然使用 Amazon EFS，但 EFS 在成本方面通常不如 S3 具有成本效益，特别是针对不频繁访问的数据。EFS 的成本结构通常高于 S3 标准-IA 或更低级别的存储类别，尤其是在迁移大量数据时。选项 D 的 Single Zone 存储类型，虽然成本更低，但其可用性较低，不适用于关键业务数据。"
    },
    "related_terms": [
      "AWS Storage Gateway",
      "NFS",
      "Amazon S3",
      "File Gateway",
      "Volume Gateway",
      "Amazon EFS",
      "S3 Intelligent-Tiering",
      "S3 Glacier",
      "Standard-IA",
      "Single Zone"
    ]
  },
  {
    "id": 807,
    "topic": "1",
    "question_en": "A company uses high concurrency AWS Lambda functions to process a constantly increasing number of messages in a message queue during marketing events. The Lambda functions use CPU intensive code to process the messages. The company wants to reduce the compute costs and to maintain service latency for its customers. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure reserved concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.",
      "B": "Configure reserved concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations.",
      "C": "Configure provisioned concurrency for the Lambda functions. Decrease the memory allocated to the Lambda functions.",
      "D": "Configure provisioned concurrency for the Lambda functions. Increase the memory according to AWS Compute Optimizer recommendations."
    },
    "correct_answer": "D",
    "vote_percentage": "69%",
    "question_cn": "一家公司使用高并发的 AWS Lambda 函数来处理营销活动期间消息队列中不断增加的消息数量。Lambda 函数使用 CPU 密集型代码来处理消息。该公司希望降低计算成本并保持其客户的服务延迟。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Lambda 函数配置预留并发。减少分配给 Lambda 函数的内存。",
      "B": "为 Lambda 函数配置预留并发。根据 AWS Compute Optimizer 的建议增加内存。",
      "C": "为 Lambda 函数配置预置并发。减少分配给 Lambda 函数的内存。",
      "D": "为 Lambda 函数配置预置并发。根据 AWS Compute Optimizer 的建议增加内存。"
    },
    "tags": [
      "Lambda",
      "Concurrency",
      "Compute Optimizer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 69%），解析仅供参考。】\n\n考察 Lambda 函数的并发控制、计算成本优化策略，以及 AWS Compute Optimizer 的应用。",
      "why_correct": "预置并发 (Provisioned Concurrency) 保证了 Lambda 函数有可用的初始化实例，减少了冷启动带来的延迟，满足了保持服务延迟的需求。增加内存可以提升 CPU 密集型 Lambda 函数的性能，降低计算时间，从而降低计算成本。同时，使用 AWS Compute Optimizer 的建议能够更有效地调整内存配置。",
      "why_wrong": "选项 A 减少内存可能导致 Lambda 函数性能下降，增加计算时间，与降低计算成本的目标相悖。选项 B 虽然使用预留并发并且增加内存，但是缺少减少冷启动时间的预置并发。选项 C 使用预置并发，可以减少冷启动时间，但减少内存可能导致函数性能下降，与降低计算成本的目标不符。"
    },
    "related_terms": [
      "AWS Lambda",
      "Provisioned Concurrency",
      "Reserved Concurrency",
      "AWS Compute Optimizer",
      "CPU"
    ]
  },
  {
    "id": 808,
    "topic": "1",
    "question_en": "A company runs its workloads on Amazon Elastic Container Service (Amazon ECS). The container images that the ECS task definition uses need to be scanned for Common Vulnerabilities and Exposures (CVEs). New container images that are created also need to be scanned. Which solution will meet these requirements with the FEWEST changes to the workloads?",
    "options_en": {
      "A": "Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository to store the container images. Specify scan on push filters for the ECR basic scan.",
      "B": "Store the container images in an Amazon S3 bucket. Use Amazon Macie to scan the images. Use an S3 Event Notification to initiate a Macie scan for every event with an s3:ObjectCreated:Put event type.",
      "C": "Deploy the workloads to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon Elastic Container Registry (Amazon ECR) as a private image repository. Specify scan on push filters for the ECR enhanced scan.",
      "D": "Store the container images in an Amazon S3 bucket that has versioning enabled. Configure an S3 Event Notification for s3:ObjectCreated:* events to invoke an AWS Lambda function. Configure the Lambda function to initiate an Amazon Inspector scan."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Amazon Elastic Container Service (Amazon ECS) 上运行其工作负载。 ECS 任务定义使用的容器镜像需要扫描 Common Vulnerabilities and Exposures (CVE)。还需要扫描创建的新容器镜像。哪种解决方案能够以对工作负载影响最小的方式满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic Container Registry (Amazon ECR) 作为私有镜像仓库来存储容器镜像。为 ECR 基本扫描指定推送扫描过滤器。",
      "B": "将容器镜像存储在 Amazon S3 存储桶中。使用 Amazon Macie 扫描镜像。使用 S3 事件通知为每个具有 s3:ObjectCreated:Put 事件类型的事件启动 Macie 扫描。",
      "C": "将工作负载部署到 Amazon Elastic Kubernetes Service (Amazon EKS)。 使用 Amazon Elastic Container Registry (Amazon ECR) 作为私有镜像仓库。 为 ECR 增强扫描指定推送扫描过滤器。",
      "D": "将容器镜像存储在已启用版本控制的 Amazon S3 存储桶中。 配置 S3 事件通知，用于 s3:ObjectCreated:* 事件以调用 AWS Lambda 函数。 配置 Lambda 函数以启动 Amazon Inspector 扫描。"
    },
    "tags": [
      "ECS",
      "ECR",
      "Inspector",
      "EKS",
      "Macie"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 ECS 上容器镜像的 CVE 扫描方案，需关注对工作负载的影响。考察 Amazon ECR 的推送扫描能力。",
      "why_correct": "Amazon ECR 提供了镜像扫描功能，可以在镜像推送到 ECR 时自动扫描。 使用 ECR 的推送扫描过滤器可以配置扫描范围，并在镜像推送到 ECR 后立即触发扫描。 这满足了扫描新容器镜像的需求，且对工作负载影响最小。",
      "why_wrong": "选项 B 使用 Amazon Macie 扫描存储在 S3 中的镜像，这不适用于容器镜像的 CVE 扫描，Macie 主要用于数据安全。 选项 C 引入了 EKS，增加了工作负载的复杂性，且 ECR 的增强扫描非题干要求。 选项 D 使用 S3 触发 Lambda 调用 Amazon Inspector 扫描，相比于 ECR 的原生扫描方案，增加了复杂性，且延迟更高。"
    },
    "related_terms": [
      "Amazon Elastic Container Service (Amazon ECS)",
      "Common Vulnerabilities and Exposures (CVE)",
      "Amazon Elastic Container Registry (Amazon ECR)",
      "Amazon S3",
      "Amazon Macie",
      "s3:ObjectCreated:Put",
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "s3:ObjectCreated:*",
      "AWS Lambda",
      "Amazon Inspector"
    ]
  },
  {
    "id": 809,
    "topic": "1",
    "question_en": "A company uses an AWS Batch job to run its end-of-day sales process. The company needs a serverless solution that will invoke a third-party reporting application when the AWS Batch job is successful. The reporting application has an HTTP API interface that uses username and password authentication. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an Amazon EventBridge rule to match incoming AWS Batch job SUCCEEDED events. Configure the third-party API as an EventBridge API destination with a username and password. Set the API destination as the EventBridge rule target.",
      "B": "Configure Amazon EventBridge Scheduler to match incoming AWS Batch job SUCCEEDED events. Configure an AWS Lambda function to invoke the third-party API by using a username and password. Set the Lambda function as the EventBridge rule target.",
      "C": "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure an HTTP proxy integration on the API Gateway REST API to invoke the third-party API by using a username and password.",
      "D": "Configure an AWS Batch job to publish job SUCCEEDED events to an Amazon API Gateway REST API. Configure a proxy integration on the API Gateway REST API to an AWS Lambda function. Configure the Lambda function to invoke the third-party API by using a username and password."
    },
    "correct_answer": "A",
    "vote_percentage": "61%",
    "question_cn": "一家公司使用 AWS Batch 作业来运行其每日销售流程。该公司需要一个无服务器解决方案，以便在 AWS Batch 作业成功时调用第三方报告应用程序。该报告应用程序具有使用用户名和密码身份验证的 HTTP API 接口。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon EventBridge 规则以匹配传入的 AWS Batch 作业 SUCCEEDED 事件。将第三方 API 配置为带有用户名和密码的 EventBridge API 目标。将 API 目标设置为 EventBridge 规则目标。",
      "B": "配置 Amazon EventBridge Scheduler 以匹配传入的 AWS Batch 作业 SUCCEEDED 事件。配置一个 AWS Lambda 函数，使用用户名和密码调用第三方 API。将 Lambda 函数设置为 EventBridge 规则目标。",
      "C": "配置 AWS Batch 作业以将作业 SUCCEEDED 事件发布到 Amazon API Gateway REST API。在 API Gateway REST API 上配置 HTTP 代理集成，以便使用用户名和密码调用第三方 API。",
      "D": "配置 AWS Batch 作业以将作业 SUCCEEDED 事件发布到 Amazon API Gateway REST API。在 API Gateway REST API 上配置代理集成到 AWS Lambda 函数。配置 Lambda 函数以使用用户名和密码调用第三方 API。"
    },
    "tags": [
      "Batch",
      "EventBridge",
      "API Gateway",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 61%），解析仅供参考。】\n\n考察了 AWS Batch 作业成功后触发第三方 API 调用的无服务器解决方案，以及 EventBridge 的使用和 API 身份验证。",
      "why_correct": "Amazon EventBridge 可以检测 AWS Batch 作业的 SUCCEEDED 事件。EventBridge 支持直接调用 HTTP API 目标，允许在目标配置中设置用户名和密码进行身份验证。这种配置方式无需使用额外的组件，符合题目要求的无服务器解决方案。",
      "why_wrong": "选项 B 引入了额外的 Lambda 函数，增加了复杂性，且 EventBridge 能够直接调用 API。选项 C 和 D 引入了 API Gateway，增加了复杂性，且 API Gateway 的使用场景更多是用于提供 API 服务，而非直接调用第三方 API。选项 C 无法直接配置 HTTP 代理的用户名和密码。选项 D 也需要额外的 Lambda 函数，增加了复杂性。"
    },
    "related_terms": [
      "AWS Batch",
      "Amazon EventBridge",
      "HTTP API",
      "Lambda",
      "API Gateway",
      "EventBridge API target",
      "SUCCEEDED event"
    ]
  },
  {
    "id": 810,
    "topic": "1",
    "question_en": "A company collects and processes data from a vendor. The vendor stores its data in an Amazon RDS for MySQL database in the vendor's own AWS account. The company’s VPC does not have an internet gateway, an AWS Direct Connect connection, or an AWS Site-to-Site VPN connection. The company needs to access the data that is in the vendor database. Which solution will meet this requirement?",
    "options_en": {
      "A": "Instruct the vendor to sign up for the AWS Hosted Connection Direct Connect Program. Use VPC peering to connect the company's VPC and the vendor's VPC.",
      "B": "Configure a client VPN connection between the company's VPC and the vendor's VPC. Use VPC peering to connect the company's VPC and the vendor's VPC.",
      "C": "Instruct the vendor to create a Network Load Balancer (NLB). Place the NLB in front of the Amazon RDS for MySQL database. Use AWS PrivateLink to integrate the company's VPC and the vendor's VPC.",
      "D": "Use AWS Transit Gateway to integrate the company's VPC and the vendor's VPC. Use VPC peering to connect the company’s VPC and the vendor's VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "93%",
    "question_cn": "一家公司从供应商处收集和处理数据。供应商将其数据存储在供应商自己的 AWS 账户中的 Amazon RDS for MySQL 数据库中。该公司的 VPC 没有互联网网关、AWS Direct Connect 连接或 AWS Site-to-Site VPN 连接。该公司需要访问供应商数据库中的数据。哪种解决方案将满足此要求？",
    "options_cn": {
      "A": "指示供应商注册 AWS Hosted Connection Direct Connect Program。使用 VPC 对等互连来连接公司的 VPC 和供应商的 VPC。",
      "B": "在公司的 VPC 和供应商的 VPC 之间配置客户端 VPN 连接。使用 VPC 对等互连来连接公司的 VPC 和供应商的 VPC。",
      "C": "指示供应商创建一个 Network Load Balancer (NLB)。将 NLB 放置在 Amazon RDS for MySQL 数据库的前面。使用 AWS PrivateLink 来集成公司的 VPC 和供应商的 VPC。",
      "D": "使用 AWS Transit Gateway 来集成公司的 VPC 和供应商的 VPC。使用 VPC 对等互连来连接公司的 VPC 和供应商的 VPC。"
    },
    "tags": [
      "VPC",
      "Direct Connect",
      "VPC Peering",
      "RDS",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 93%），解析仅供参考。】\n\n考察如何在没有互联网网关、Direct Connect 和 Site-to-Site VPN 的情况下，安全地访问另一个 AWS 账户中的 RDS for MySQL 数据库中的数据。",
      "why_correct": "选项 C 提供了最直接且安全的方法。Network Load Balancer (NLB) 可以作为 RDS 数据库的前端，并提供一个稳定的访问点。AWS PrivateLink 允许在不暴露流量到公网的情况下，通过私有连接访问供应商 VPC 中的 NLB，从而实现与 RDS 数据库的通信。",
      "why_wrong": "选项 A 依赖于 Direct Connect，而题目明确指出没有 Direct Connect 连接，因此无法满足需求。选项 B 建议使用 Client VPN，这需要互联网连接才能建立 VPN 连接，与题目的限制相悖。选项 D 使用 Transit Gateway 和 VPC 对等互连，Transit Gateway 设计用于连接多个 VPC 和 on-premises 网络，对于这种简单场景来说过于复杂，且 VPC 对等互连不能满足私有连接的需求，Public IP 暴露风险高，无法满足安全需求。"
    },
    "related_terms": [
      "Amazon RDS for MySQL",
      "VPC",
      "Internet Gateway",
      "AWS Direct Connect",
      "AWS Site-to-Site VPN",
      "Network Load Balancer (NLB)",
      "AWS PrivateLink",
      "Client VPN",
      "Transit Gateway",
      "VPC peering",
      "AWS Hosted Connection Direct Connect Program"
    ]
  },
  {
    "id": 811,
    "topic": "1",
    "question_en": "A company wants to set up Amazon Managed Grafana as its visualization tool. The company wants to visualize data from its Amazon RDS database as one data source. The company needs a secure solution that will not expose the data over the internet. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon Managed Grafana workspace without a VPC. Create a public endpoint for the RDS database. Configure the public endpoint as a data source in Amazon Managed Grafana.",
      "B": "Create an Amazon Managed Grafana workspace in a VPC. Create a private endpoint for the RDS database. Configure the private endpoint as a data source in Amazon Managed Grafana.",
      "C": "Create an Amazon Managed Grafana workspace without a VPCreate an AWS PrivateLink endpoint to establish a connection between Amazon Managed Grafana and Amazon RDS. Set up Amazon RDS as a data source in Amazon Managed Grafana.",
      "D": "Create an Amazon Managed Grafana workspace in a VPC. Create a public endpoint for the RDS database. Configure the public endpoint as a data source in Amazon Managed Grafana."
    },
    "correct_answer": "B",
    "vote_percentage": "67%",
    "question_cn": "一家公司希望将 Amazon Managed Grafana 设置为其可视化工具。该公司希望将来自 Amazon RDS 数据库的数据可视化为一个数据源。该公司需要一个安全的解决方案，该解决方案不会通过互联网公开数据。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在没有 VPC 的情况下创建 Amazon Managed Grafana 工作区。为 RDS 数据库创建一个公共端点。在 Amazon Managed Grafana 中将公共端点配置为数据源。",
      "B": "在 VPC 中创建 Amazon Managed Grafana 工作区。为 RDS 数据库创建一个私有端点。在 Amazon Managed Grafana 中将私有端点配置为数据源。",
      "C": "在没有 VPC 的情况下创建 Amazon Managed Grafana 工作区。创建一个 AWS PrivateLink 端点以在 Amazon Managed Grafana 和 Amazon RDS 之间建立连接。在 Amazon Managed Grafana 中将 Amazon RDS 设置为数据源。",
      "D": "在 VPC 中创建 Amazon Managed Grafana 工作区。为 RDS 数据库创建一个公共端点。在 Amazon Managed Grafana 中将公共端点配置为数据源。"
    },
    "tags": [
      "Grafana",
      "RDS",
      "VPC",
      "PrivateLink"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 67%），解析仅供参考。】\n\n此题考察如何安全地将 Amazon Managed Grafana 连接到 Amazon RDS 数据库。通过使用 VPC 和私有端点，可以确保数据流量不会暴露在公共互联网上。AWS PrivateLink 提供了一种安全的方式，用于在 VPC 和其他 AWS 服务之间建立私有连接。",
      "why_correct": "选项 B 正确，因为在 VPC 中创建 Amazon Managed Grafana 工作区，并使用 RDS 数据库的私有端点，可以确保数据流量在 VPC 内部，不会暴露在公共互联网上。",
      "why_wrong": "选项 A 错误，因为在没有 VPC 的情况下创建 Grafana 工作区，以及使用公共端点，将导致数据通过公共互联网传输。选项 C 错误，因为 AWS PrivateLink 无法直接将 Grafana 与 RDS 连接，并且需要在 Grafana 工作区中配置 RDS 数据源。选项 D 错误，因为使用公共端点将数据暴露在互联网上。"
    },
    "related_terms": [
      "Amazon Managed Grafana",
      "RDS",
      "VPC",
      "AWS PrivateLink"
    ]
  },
  {
    "id": 812,
    "topic": "1",
    "question_en": "A company hosts a data lake on Amazon S3. The data lake ingests data in Apache Parquet format from various data sources. The company uses multiple transformation steps to prepare the ingested data. The steps include filtering of anomalies, normalizing of data to standard date and time values, and generation of aggregates for analyses. The company must store the transformed data in S3 buckets that data analysts access. The company needs a prebuilt solution for data transformation that does not require code. The solution must provide data lineage and data profiling. The company needs to share the data transformation steps with employees throughout the company. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure an AWS Glue Studio visual canvas to transform the data. Share the transformation steps with employees by using AWS Glue jobs.",
      "B": "Configure Amazon EMR Serverless to transform the data. Share the transformation steps with employees by using EMR Serverless jobs.",
      "C": "Configure AWS Glue DataBrew to transform the data. Share the transformation steps with employees by using DataBrew recipes.",
      "D": "Create Amazon Athena tables for the data. Write Athena SQL queries to transform the data. Share the Athena SQL queries with employees."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 Amazon S3 上托管数据湖。数据湖以 Apache Parquet 格式从各种数据 源 摄取数据。该公司使用多个转换步骤来准备摄取的数据。这些步骤包括过滤异常值、将数据标准化为标准日期和时间值，以及生成用于分析的聚合。该公司必须将转换后的数据存储在数据分析师访问的 S3 存储桶中。该公司需要一个无需代码即可进行数据转换的预构建解决方案。该解决方案必须提供数据沿袭和数据分析。该公司需要与整个公司的员工共享数据转换步骤。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 AWS Glue Studio 可视画布来转换数据。通过使用 AWS Glue 作业与员工共享转换步骤。",
      "B": "配置 Amazon EMR Serverless 来转换数据。通过使用 EMR Serverless 作业与员工共享转换步骤。",
      "C": "配置 AWS Glue DataBrew 来转换数据。通过使用 DataBrew 配方与员工共享转换步骤。",
      "D": "为数据创建 Amazon Athena 表。编写 Athena SQL 查询来转换数据。与员工共享 Athena SQL 查询。"
    },
    "tags": [
      "S3",
      "Glue",
      "DataBrew",
      "EMR Serverless",
      "Athena"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察使用无代码数据转换工具、数据沿袭、数据分析以及共享转换步骤的能力，并要求能够满足 S3 数据湖场景。",
      "why_correct": "AWS Glue DataBrew 是一种无代码的数据准备服务，非常适合数据转换。DataBrew 配方提供了数据沿袭和数据分析功能。DataBrew 配方可以轻松地与公司员工共享，满足了题目中所有要求。",
      "why_wrong": "选项 A，AWS Glue Studio 虽然提供了可视化转换功能，但与 DataBrew 相比，在无代码数据准备和共享转换步骤方面不如 DataBrew 方便。选项 B，Amazon EMR Serverless 是一种计算服务，不具备开箱即用的数据准备和共享转换步骤的功能。选项 D，Amazon Athena 是一种查询服务，需要编写 SQL 查询进行数据转换，不满足无需代码的要求，并且共享 SQL 查询不如 DataBrew 配方方便。"
    },
    "related_terms": [
      "Amazon S3",
      "Apache Parquet",
      "AWS Glue Studio",
      "AWS Glue",
      "Amazon EMR Serverless",
      "AWS Glue DataBrew",
      "DataBrew",
      "Athena",
      "SQL"
    ]
  },
  {
    "id": 813,
    "topic": "1",
    "question_en": "A solutions architect runs a web application on multiple Amazon EC2 instances that are in individual target groups behind an Application Load Balancer (ALB). Users can reach the application through a public website. The solutions architect wants to allow engineers to use a development version of the website to access one specific development EC2 instance to test new features for the application. The solutions architect wants to use an Amazon Route 53 hosted zone to give the engineers access to the development instance. The solution must automatically route to the development instance even if the development instance is replaced. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an A Record for the development website that has the value set to the ALB. Create a listener rule on the ALB that forwards requests for the development website to the target group that contains the development instance.",
      "B": "Recreate the development instance with a public IP address. Create an A Record for the development website that has the value set to the public IP address of the development instance.",
      "C": "Create an A Record for the development website that has the value set to the ALB. Create a listener rule on the ALB to redirect requests for the development website to the public IP address of the development instance.",
      "D": "Place all the instances in the same target group. Create an A Record for the development website. Set the value to the ALB. Create a listener rule on the ALB that forwards requests for the development website to the target group."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师在多个 Amazon EC2 实例上运行一个 Web 应用程序，这些实例位于 Application Load Balancer (ALB) 之后的单独目标组中。用户可以通过公共网站访问该应用程序。解决方案架构师希望允许工程师使用网站的开发版本来访问一个特定的开发 EC2 实例，以测试该应用程序的新功能。解决方案架构师希望使用 Amazon Route 53 托管区域为工程师提供对开发实例的访问权限。该解决方案必须自动路由到开发实例，即使开发实例被替换。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为开发网站创建一个 A 记录，其值设置为 ALB。在 ALB 上创建一个侦听器规则，将开发网站的请求转发到包含开发实例的目标组。",
      "B": "使用公共 IP 地址重新创建开发实例。为开发网站创建一个 A 记录，其值设置为开发实例的公共 IP 地址。",
      "C": "为开发网站创建一个 A 记录，其值设置为 ALB。在 ALB 上创建一个侦听器规则，将开发网站的请求重定向到开发实例的公共 IP 地址。",
      "D": "将所有实例放在同一个目标组中。为开发网站创建一个 A 记录。将值设置为 ALB。在 ALB 上创建一个侦听器规则，将开发网站的请求转发到目标组。"
    },
    "tags": [
      "Route 53",
      "ALB",
      "EC2",
      "DNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查利用 Amazon Route 53、Application Load Balancer (ALB) 和目标组实现开发环境的灵活访问。要求通过 Route 53 访问特定 EC2 实例，并具备自动路由能力。",
      "why_correct": "选项 A 提供了最适合的解决方案。它利用 ALB 的侦听器规则将特定域名或路径的流量路由到包含开发实例的目标组。通过将 Route 53 A 记录指向 ALB，确保了流量首先到达 ALB，再由 ALB 根据规则转发。即使开发实例被替换，ALB 也能通过目标组自动将流量导向新的实例，满足了自动路由的需求。",
      "why_wrong": "选项 B 错误，因为它依赖于公共 IP 地址，这不具备动态性和可维护性，实例的 IP 发生变化后，就需要更新 DNS 记录。选项 C 错误，因为 ALB 不能重定向到 IP 地址，它只能转发到目标组。选项 D 错误，虽然 ALB 和目标组能够满足路由需求，但将所有实例放入同一个目标组不符合题目要求，因为需要将流量导向特定的开发实例，而不是所有的实例。"
    },
    "related_terms": [
      "Amazon EC2",
      "Application Load Balancer (ALB)",
      "Route 53",
      "A record",
      "Target Group"
    ]
  },
  {
    "id": 814,
    "topic": "1",
    "question_en": "A company runs a container application on a Kubernetes cluster in the company's data center. The application uses Advanced Message Queuing Protocol (AMQP) to communicate with a message queue. The data center cannot scale fast enough to meet the company’s expanding business needs. The company wants to migrate the workloads to AWS. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Migrate the container application to Amazon Elastic Container Service (Amazon ECS). Use Amazon Simple Queue Service (Amazon SQS) to retrieve the messages.",
      "B": "Migrate the container application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon MQ to retrieve the messages.",
      "C": "Use highly available Amazon EC2 instances to run the application. Use Amazon MQ to retrieve the messages.",
      "D": "Use AWS Lambda functions to run the application. Use Amazon Simple Queue Service (Amazon SQS) to retrieve the messages."
    },
    "correct_answer": "B",
    "vote_percentage": "93%",
    "question_cn": "一家公司在其数据中心的 Kubernetes 集群上运行一个容器应用程序。该应用程序使用高级消息队列协议 (AMQP) 与消息队列通信。数据中心无法快速扩展以满足公司不断增长的业务需求。该公司希望将工作负载迁移到 AWS。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "将容器应用程序迁移到 Amazon Elastic Container Service (Amazon ECS)。使用 Amazon Simple Queue Service (Amazon SQS) 检索消息。",
      "B": "将容器应用程序迁移到 Amazon Elastic Kubernetes Service (Amazon EKS)。使用 Amazon MQ 检索消息。",
      "C": "使用高可用 Amazon EC2 实例来运行应用程序。使用 Amazon MQ 检索消息。",
      "D": "使用 AWS Lambda 函数来运行应用程序。使用 Amazon Simple Queue Service (Amazon SQS) 检索消息。"
    },
    "tags": [
      "ECS",
      "EKS",
      "SQS",
      "MQ",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 93%），解析仅供参考。】\n\n考察了 Kubernetes 集群的迁移方案，以及如何使用 AWS 服务满足 AMQP 消息队列的需求，并关注运营开销。",
      "why_correct": "Amazon EKS 提供了与 Kubernetes 集群的兼容性，方便应用程序迁移。Amazon MQ 支持 AMQP 协议，与现有应用程序的消息队列通信方式匹配。这种方案可以最大程度地减少代码更改，并降低运营开销。",
      "why_wrong": "A 选项将容器应用迁移到 Amazon ECS，但需要将 AMQP 协议的消息队列替换为 Amazon SQS，可能需要更改应用程序代码，并且 Amazon SQS 不直接支持 AMQP。C 选项使用 Amazon EC2 实例，增加了管理和维护的复杂性，运营开销较高。D 选项使用 AWS Lambda 函数，但 Lambda 函数与现有容器应用程序的兼容性较差，且 AWS Lambda 不支持直接使用 AMQP，需要对应用程序架构进行大规模修改。"
    },
    "related_terms": [
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Elastic Kubernetes Service (Amazon EKS)",
      "Amazon MQ",
      "Amazon EC2",
      "AWS Lambda",
      "Kubernetes",
      "AMQP"
    ]
  },
  {
    "id": 815,
    "topic": "1",
    "question_en": "An online gaming company hosts its platform on Amazon EC2 instances behind Network Load Balancers (NLBs) across multiple AWS Regions. The NLBs can route requests to targets over the internet. The company wants to improve the customer playing experience by reducing end-to- end load time for its global customer base. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create Application Load Balancers (ALBs) in each Region to replace the existing NLBs. Register the existing EC2 instances as targets for the ALBs in each Region.",
      "B": "Configure Amazon Route 53 to route equally weighted trafic to the NLBs in each Region.",
      "C": "Create additional NLBs and EC2 instances in other Regions where the company has large customer bases.",
      "D": "Create a standard accelerator in AWS Global Accelerator. Configure the existing NLBs as target endpoints."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家在线游戏公司在其平台上的 Amazon EC2 实例上托管平台，这些实例位于多个 AWS 区域内的网络负载均衡器 (NLB) 之后。 NLB 可以通过互联网将请求路由到目标。该公司希望通过减少其全球客户群的端到端加载时间来改善客户的游戏体验。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在每个区域创建 Application Load Balancer (ALB) 以替换现有的 NLB。将现有的 EC2 实例注册为每个区域中 ALB 的目标。",
      "B": "配置 Amazon Route 53 以将流量平均路由到每个区域中的 NLB。",
      "C": "在公司拥有大量客户群的其他区域中创建额外的 NLB 和 EC2 实例。",
      "D": "在 AWS Global Accelerator 中创建一个标准加速器。将现有的 NLB 配置为目标端点。"
    },
    "tags": [
      "NLB",
      "ALB",
      "EC2",
      "Route 53",
      "Global Accelerator"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察使用 AWS Global Accelerator 优化全球客户的网络延迟，从而提升用户体验。",
      "why_correct": "AWS Global Accelerator 通过利用 AWS 的全球网络基础设施，将用户的流量引导到最近的边缘站点，从而减少网络延迟。题目要求减少全球客户的端到端加载时间，Global Accelerator 正好满足此需求。将现有的 NLB 配置为 Global Accelerator 的目标端点，无需更改现有的基础设施即可实现加速。",
      "why_wrong": "A 选项，使用 ALB 替换 NLB 并不能解决全球客户的延迟问题，反而增加了复杂性，ALB 专注于应用层流量的管理。B 选项，Route 53 仅提供 DNS 解析和流量路由功能，无法直接加速流量，只是将流量平均分配到不同区域的 NLB。C 选项，在其他区域部署 NLB 和 EC2 实例可以一定程度上缓解延迟，但没有从根本上解决问题，且增加了管理成本和复杂性。"
    },
    "related_terms": [
      "Amazon EC2",
      "Network Load Balancer (NLB)",
      "Application Load Balancer (ALB)",
      "Amazon Route 53",
      "AWS Global Accelerator"
    ]
  },
  {
    "id": 816,
    "topic": "1",
    "question_en": "A company has an on-premises application that uses SFTP to collect financial data from multiple vendors. The company is migrating to the AWS Cloud. The company has created an application that uses Amazon S3 APIs to upload files from vendors. Some vendors run their systems on legacy applications that do not support S3 APIs. The vendors want to continue to use SFTP-based applications to upload data. The company wants to use managed services for the needs of the vendors that use legacy applications. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an AWS Database Migration Service (AWS DMS) instance to replicate data from the storage of the vendors that use legacy applications to Amazon S3. Provide the vendors with the credentials to access the AWS DMS instance.",
      "B": "Create an AWS Transfer Family endpoint for vendors that use legacy applications.",
      "C": "Configure an Amazon EC2 instance to run an SFTP server. Instruct the vendors that use legacy applications to use the SFTP server to upload data.",
      "D": "Configure an Amazon S3 File Gateway for vendors that use legacy applications to upload files to an SMB file share."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个本地应用程序，该应用程序使用 SFTP 从多个供应商处收集财务数据。该公司正在迁移到 AWS 云。该公司创建了一个使用 Amazon S3 API 从供应商处上传文件的应用程序。一些供应商在其不支持 S3 API 的旧版应用程序上运行其系统。供应商希望继续使用基于 SFTP 的应用程序来上传数据。该公司希望使用托管服务来满足使用旧版应用程序的供应商的需求。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Database Migration Service (AWS DMS) 实例，以将数据从使用旧版应用程序的供应商的存储复制到 Amazon S3。向供应商提供访问 AWS DMS 实例的凭证。",
      "B": "为使用旧版应用程序的供应商创建一个 AWS Transfer Family 端点。",
      "C": "配置一个 Amazon EC2 实例来运行 SFTP 服务器。指示使用旧版应用程序的供应商使用 SFTP 服务器上传数据。",
      "D": "为使用旧版应用程序的供应商配置一个 Amazon S3 File Gateway，以便将文件上传到 SMB 文件共享。"
    },
    "tags": [
      "SFTP",
      "S3",
      "Transfer Family",
      "DMS",
      "EC2",
      "File Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察如何满足使用旧版应用程序的供应商的需求，同时迁移到 AWS 云。AWS Transfer Family 是一项托管服务，支持 SFTP，FTPS，FTP 协议，并直接与 Amazon S3 集成。其他方案需要更多的配置和管理。",
      "why_correct": "选项 B 正确，因为 AWS Transfer Family 可以直接支持 SFTP 协议，满足使用旧版应用程序的供应商的需求。",
      "why_wrong": "选项 A 错误，因为 AWS DMS 不支持 SFTP 协议。选项 C 错误，因为需要创建和维护 EC2 实例，增加了运营开销。选项 D 错误，因为 Amazon S3 File Gateway 无法支持 SFTP。"
    },
    "related_terms": [
      "SFTP",
      "S3",
      "Transfer Family",
      "DMS",
      "EC2",
      "File Gateway"
    ]
  },
  {
    "id": 817,
    "topic": "1",
    "question_en": "A marketing team wants to build a campaign for an upcoming multi-sport event. The team has news reports from the past five years in PDF format. The team needs a solution to extract insights about the content and the sentiment of the news reports. The solution must use Amazon Textract to process the news reports. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Provide the extracted insights to Amazon Athena for analysis. Store the extracted insights and analysis in an Amazon S3 bucket.",
      "B": "Store the extracted insights in an Amazon DynamoDB table. Use Amazon SageMaker to build a sentiment model.",
      "C": "Provide the extracted insights to Amazon Comprehend for analysis. Save the analysis to an Amazon S3 bucket.",
      "D": "Store the extracted insights in an Amazon S3 bucket. Use Amazon QuickSight to visualize and analyze the data."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一个市场营销团队想要为即将到来的多项体育赛事建立一个宣传活动。该团队有过去五年的新闻报道，格式为 PDF。该团队需要一个解决方案来提取关于新闻报道内容和情感的见解。该解决方案必须使用 Amazon Textract 来处理新闻报道。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将提取的见解提供给 Amazon Athena 进行分析。将提取的见解和分析存储在 Amazon S3 存储桶中。",
      "B": "将提取的见解存储在 Amazon DynamoDB 表中。使用 Amazon SageMaker 构建情感模型。",
      "C": "将提取的见解提供给 Amazon Comprehend 进行分析。将分析结果保存到 Amazon S3 存储桶。",
      "D": "将提取的见解存储在 Amazon S3 存储桶中。使用 Amazon QuickSight 可视化和分析数据。"
    },
    "tags": [
      "Textract",
      "S3",
      "Athena",
      "Comprehend",
      "SageMaker",
      "QuickSight",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查利用 Amazon Textract 提取数据后，使用其他 AWS 服务进行分析，并选择运营开销最低的方案。",
      "why_correct": "选项 C 提供了使用 Amazon Comprehend 进行情感分析的解决方案，符合题干对情感分析的需求。Amazon Comprehend 是一种完全托管的服务，可以自动分析文本，提取情感、关键词等信息，并减少运营开销。将结果保存到 Amazon S3 也满足了存储需求。",
      "why_wrong": "选项 A 使用 Amazon Athena 分析数据，Athena 主要用于查询，无法直接进行情感分析。选项 B 使用 Amazon SageMaker 构建情感模型，增加了构建和维护模型的复杂性，运营开销较高，不符合最少的要求。选项 D 虽然使用了 Amazon QuickSight 进行可视化和分析，但缺乏情感分析的部分，且QuickSight不能直接分析Textract的输出结果， 需要额外的处理，不符合需求。"
    },
    "related_terms": [
      "Amazon Textract",
      "Amazon Athena",
      "Amazon S3",
      "Amazon DynamoDB",
      "Amazon SageMaker",
      "Amazon Comprehend",
      "Amazon QuickSight"
    ]
  },
  {
    "id": 818,
    "topic": "1",
    "question_en": "A company's application runs on Amazon EC2 instances that are in multiple Availability Zones. The application needs to ingest real-time data from third-party applications. The company needs a data ingestion solution that places the ingested raw data in an Amazon S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create Amazon Kinesis data streams for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume the Kinesis data streams. Specify the S3 bucket as the destination of the delivery streams.",
      "B": "Create database migration tasks in AWS Database Migration Service (AWS DMS). Specify replication instances of the EC2 instances as the source endpoints. Specify the S3 bucket as the target endpoint. Set the migration type to migrate existing data and replicate ongoing changes.",
      "C": "Create and configure AWS DataSync agents on the EC2 instances. Configure DataSync tasks to transfer data from the EC2 instances to the S3 bucket.",
      "D": "Create an AWS Direct Connect connection to the application for data ingestion. Create Amazon Kinesis Data Firehose delivery streams to consume direct PUT operations from the application. Specify the S3 bucket as the destination of the delivery streams."
    },
    "correct_answer": "A",
    "vote_percentage": "60%",
    "question_cn": "一家公司的应用程序运行在多个可用区中的 Amazon EC2 实例上。该应用程序需要从第三方应用程序提取实时数据。该公司需要一个数据提取解决方案，该解决方案将提取的原始数据放置在 Amazon S3 存储桶中。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为数据提取创建 Amazon Kinesis 数据流。创建 Amazon Kinesis Data Firehose 交付流以使用 Kinesis 数据流。将 S3 存储桶指定为交付流的目标。",
      "B": "在 AWS Database Migration Service (AWS DMS) 中创建数据库迁移任务。将 EC2 实例的复制实例指定为源终端节点。将 S3 存储桶指定为目标终端节点。将迁移类型设置为迁移现有数据并复制正在进行的更改。",
      "C": "在 EC2 实例上创建并配置 AWS DataSync 代理。配置 DataSync 任务，以将数据从 EC2 实例传输到 S3 存储桶。",
      "D": "创建与应用程序的 AWS Direct Connect 连接以进行数据提取。创建 Amazon Kinesis Data Firehose 交付流以使用来自应用程序的直接 PUT 操作。将 S3 存储桶指定为交付流的目标。"
    },
    "tags": [
      "Kinesis",
      "S3",
      "DMS",
      "DataSync",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 60%），解析仅供参考。】\n\n此题考察如何从第三方应用程序提取实时数据并存储在 Amazon S3 存储桶中。Amazon Kinesis Data Firehose 旨在将数据流式传输到 Amazon S3。AWS DMS 用于数据库迁移。AWS DataSync 用于数据传输和同步，需要安装在 EC2 上。Direct Connect 只是网络连接，不能提取数据。",
      "why_correct": "选项 A 正确，因为使用 Kinesis 数据流和 Kinesis Data Firehose 可以实现实时数据提取，并将数据流式传输到 S3。",
      "why_wrong": "选项 B 错误，因为 AWS DMS 用于数据库迁移，而不是数据提取。选项 C 错误，因为 DataSync 无法直接从第三方应用程序提取数据。选项 D 错误，因为 Direct Connect 不能用于数据提取。"
    },
    "related_terms": [
      "Kinesis",
      "S3",
      "DMS",
      "DataSync",
      "Direct Connect"
    ]
  },
  {
    "id": 819,
    "topic": "1",
    "question_en": "A company’s application is receiving data from multiple data sources. The size of the data varies and is expected to increase over time. The current maximum size is 700 KB. The data volume and data size continue to grow as more data sources are added. The company decides to use Amazon DynamoDB as the primary database for the application. A solutions architect needs to identify a solution that handles the large data sizes. Which solution will meet these requirements in the MOST operationally eficient way?",
    "options_en": {
      "A": "Create an AWS Lambda function to filter the data that exceeds DynamoDB item size limits. Store the larger data in an Amazon DocumentDB (with MongoDB compatibility) database.",
      "B": "Store the large data as objects in an Amazon S3 bucket. In a DynamoDB table, create an item that has an attribute that points to the S3 URL of the data.",
      "C": "Split all incoming large data into a collection of items that have the same partition key. Write the data to a DynamoDB table in a single operation by using the BatchWriteItem API operation.",
      "D": "Create an AWS Lambda function that uses gzip compression to compress the large objects as they are written to a DynamoDB table."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司的应用程序正在接收来自多个数据源的数据。数据大小各不相同，预计会随着时间的推移而增加。当前最大大小为 700 KB。随着添加更多数据源，数据量和数据大小会持续增长。该公司决定使用 Amazon DynamoDB 作为应用程序的主要数据库。解决方案架构师需要确定一个可以处理大型数据大小的解决方案。哪个解决方案能以最具运营效率的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数来筛选超出 DynamoDB 项目大小限制的数据。将较大的数据存储在 Amazon DocumentDB（与 MongoDB 兼容）数据库中。",
      "B": "将大型数据作为对象存储在 Amazon S3 存储桶中。在 DynamoDB 表中，创建一个具有指向数据 S3 URL 的属性的项目。",
      "C": "将所有传入的大型数据拆分为具有相同分区键的项目集合。使用 BatchWriteItem API 操作将数据写入 DynamoDB 表中的单个操作。",
      "D": "创建一个 AWS Lambda 函数，该函数使用 gzip 压缩来压缩写入 DynamoDB 表时的大型对象。"
    },
    "tags": [
      "DynamoDB",
      "S3",
      "Lambda",
      "DocumentDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考察如何使用 DynamoDB 存储大型数据。需要考虑数据大小限制和运营效率。",
      "why_correct": "选项 B 是最佳实践，可以高效地处理大型数据。将大型数据存储在 Amazon S3 中，并在 DynamoDB 中存储指向 S3 对象的 URL，有效绕过了 DynamoDB 的项目大小限制，同时也便于数据管理和检索。",
      "why_wrong": "选项 A 引入了额外的数据库 Amazon DocumentDB，增加了架构复杂性和运营成本，并且 Lambda 函数进行数据筛选也增加了延迟。选项 C 将大型数据拆分为多个项目，增加了复杂性，并且使用 BatchWriteItem 操作可能会受到吞吐量限制。选项 D 使用 gzip 压缩无法根本解决 DynamoDB 的项目大小限制，并且会增加 CPU 负载，影响性能。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "AWS Lambda",
      "Amazon DocumentDB",
      "MongoDB",
      "Amazon S3",
      "BatchWriteItem API",
      "gzip"
    ]
  },
  {
    "id": 820,
    "topic": "1",
    "question_en": "A company is migrating a legacy application from an on-premises data center to AWS. The application relies on hundreds of cron jobs that run between 1 and 20 minutes on different recurring schedules throughout the day. The company wants a solution to schedule and run the cron jobs on AWS with minimal refactoring. The solution must support running the cron jobs in response to an event in the future. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a recurring schedule. Run the cron job tasks as AWS Lambda functions.",
      "B": "Create a container image for the cron jobs. Use AWS Batch on Amazon Elastic Container Service (Amazon ECS) with a scheduling policy to run the cron jobs.",
      "C": "Create a container image for the cron jobs. Use Amazon EventBridge Scheduler to create a recurring schedule. Run the cron job tasks on AWS Fargate.",
      "D": "Create a container image for the cron jobs. Create a workfiow in AWS Step Functions that uses a Wait state to run the cron jobs at a specified time. Use the RunTask action to run the cron job tasks on AWS Fargate."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其遗留应用程序从本地数据中心迁移到 AWS。该应用程序依赖于数百个 cron 作业，这些作业每天在不同的重复时间表上运行 1 到 20 分钟。该公司希望在 AWS 上安排和运行 cron 作业的解决方案，并尽量减少重构。该解决方案必须支持响应未来发生的事件来运行 cron 作业。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 cron 作业创建容器镜像。使用 Amazon EventBridge Scheduler 创建重复计划。将 cron 作业任务作为 AWS Lambda 函数运行。",
      "B": "为 cron 作业创建容器镜像。在 Amazon Elastic Container Service (Amazon ECS) 上使用 AWS Batch 和调度策略来运行 cron 作业。",
      "C": "为 cron 作业创建容器镜像。使用 Amazon EventBridge Scheduler 创建重复计划。在 AWS Fargate 上运行 cron 作业任务。",
      "D": "为 cron 作业创建容器镜像。在 AWS Step Functions 中创建一个工作流，该工作流使用 Wait 状态在指定时间运行 cron 作业。使用 RunTask 操作在 AWS Fargate 上运行 cron 作业任务。"
    },
    "tags": [
      "EventBridge Scheduler",
      "Lambda",
      "ECS",
      "Fargate",
      "Step Functions"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何调度 cron 作业并最小化重构。EventBridge Scheduler 和 AWS Fargate 是无服务器解决方案。AWS Batch 需要更多的配置，而 Step Functions 并不适用于重复执行的作业。",
      "why_correct": "选项 C 正确，因为使用 EventBridge Scheduler 创建重复计划，并在 AWS Fargate 上运行 cron 作业任务，可以最小化重构，并满足未来事件的需求。",
      "why_wrong": "选项 A 错误，因为使用 AWS Lambda 函数比在 Fargate 上运行任务运营开销高。选项 B 错误，因为 AWS Batch 不适合最小化重构。选项 D 错误，因为 Step Functions 不适合用于调度任务。"
    },
    "related_terms": [
      "Lambda",
      "ECS",
      "Fargate",
      "Step Functions",
      "EventBridge Scheduler"
    ]
  },
  {
    "id": 821,
    "topic": "1",
    "question_en": "A company uses Salesforce. The company needs to load existing data and ongoing data changes from Salesforce to Amazon Redshift for analysis. The company does not want the data to travel over the public internet. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Establish a VPN connection from the VPC to Salesforce. Use AWS Glue DataBrew to transfer data.",
      "B": "Establish an AWS Direct Connect connection from the VPC to Salesforce. Use AWS Glue DataBrew to transfer data.",
      "C": "Create an AWS PrivateLink connection in the VPC to Salesforce. Use Amazon AppFlow to transfer data.",
      "D": "Create a VPC peering connection to Salesforce. Use Amazon AppFlow to transfer data."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Salesforce。该公司需要将现有数据和持续的数据更改从 Salesforce 加载到 Amazon Redshift 中进行分析。该公司不希望数据通过公共互联网传输。哪个解决方案将以最少的开发工作量满足这些要求？",
    "options_cn": {
      "A": "建立从 VPC 到 Salesforce 的 VPN 连接。使用 AWS Glue DataBrew 传输数据。",
      "B": "建立从 VPC 到 Salesforce 的 AWS Direct Connect 连接。使用 AWS Glue DataBrew 传输数据。",
      "C": "在 VPC 中创建到 Salesforce 的 AWS PrivateLink 连接。使用 Amazon AppFlow 传输数据。",
      "D": "创建到 Salesforce 的 VPC 对等连接。使用 Amazon AppFlow 传输数据。"
    },
    "tags": [
      "Salesforce",
      "Redshift",
      "VPC",
      "PrivateLink",
      "AppFlow",
      "DataBrew",
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察如何安全地将数据从 Salesforce 加载到 Amazon Redshift 中。AWS PrivateLink 提供了一种安全的方式，用于在 VPC 和其他 AWS 服务之间建立私有连接。Amazon AppFlow 是一项完全托管的服务，可以用于将数据从 Salesforce 传输到 Redshift。",
      "why_correct": "选项 C 正确，因为在 VPC 中创建到 Salesforce 的 AWS PrivateLink 连接，并使用 Amazon AppFlow 传输数据，可以确保数据不通过公共互联网传输。",
      "why_wrong": "选项 A 错误，因为 AWS Glue DataBrew 无法与 Salesforce 建立 VPN 连接。选项 B 错误，因为 AWS Glue DataBrew 无法与 Salesforce 建立 Direct Connect。选项 D 错误，因为 VPC 对等连接不安全，不符合要求。"
    },
    "related_terms": [
      "Redshift",
      "VPC",
      "PrivateLink",
      "Direct Connect",
      "Salesforce",
      "AppFlow",
      "DataBrew"
    ]
  },
  {
    "id": 822,
    "topic": "1",
    "question_en": "A company recently migrated its application to AWS. The application runs on Amazon EC2 Linux instances in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon Elastic File System (Amazon EFS) file system that uses EFS Standard- Infrequent Access storage. The application indexes the company's files. The index is stored in an Amazon RDS database. The company needs to optimize storage costs with some application and services changes. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an Amazon S3 bucket that uses an Intelligent-Tiering lifecycle policy. Copy all files to the S3 bucket. Update the application to use Amazon S3 API to store and retrieve files.",
      "B": "Deploy Amazon FSx for Windows File Server file shares. Update the application to use CIFS protocol to store and retrieve files.",
      "C": "Deploy Amazon FSx for OpenZFS file system shares. Update the application to use the new mount point to store and retrieve files.",
      "D": "Create an Amazon S3 bucket that uses S3 Glacier Flexible Retrieval. Copy all files to the S3 bucket. Update the application to use Amazon S3 API to store and retrieve files as standard retrievals."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司最近将其应用程序迁移到了 AWS。该应用程序在跨多个可用区的 Auto Scaling 组中的 Amazon EC2 Linux 实例上运行。该应用程序将数据存储在 Amazon Elastic File System (Amazon EFS) 文件系统中，该文件系统使用 EFS 标准-不频繁访问存储。该应用程序为公司的文件建立索引。索引存储在 Amazon RDS 数据库中。公司需要通过一些应用程序和服务更改来优化存储成本。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个使用智能分层生命周期策略的 Amazon S3 存储桶。将所有文件复制到 S3 存储桶。更新应用程序以使用 Amazon S3 API 存储和检索文件。",
      "B": "部署 Amazon FSx for Windows 文件服务器文件共享。更新应用程序以使用 CIFS 协议存储和检索文件。",
      "C": "部署 Amazon FSx for OpenZFS 文件系统共享。更新应用程序以使用新的挂载点来存储和检索文件。",
      "D": "创建一个使用 S3 Glacier Flexible Retrieval 的 Amazon S3 存储桶。将所有文件复制到 S3 存储桶。更新应用程序以使用 Amazon S3 API 将文件作为标准检索方式检索。"
    },
    "tags": [
      "EFS",
      "S3",
      "FSx",
      "Glacier",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察如何优化存储成本。S3 存储桶配合智能分层可以降低存储成本。FSx 和 Glacier 成本更高。S3 Glacier Flexible Retrieval 检索速度慢，也不适用。",
      "why_correct": "选项 A 正确，因为创建使用智能分层生命周期策略的 Amazon S3 存储桶，并将文件复制到 S3 存储桶，可以降低存储成本。",
      "why_wrong": "选项 B 错误，因为 Amazon FSx for Windows 文件服务器成本较高。选项 C 错误，因为 Amazon FSx for OpenZFS 文件系统共享成本较高。选项 D 错误，因为 S3 Glacier Flexible Retrieval 检索速度慢，不适用此场景。"
    },
    "related_terms": [
      "EFS",
      "S3",
      "FSx",
      "Glacier",
      "Cost Optimization"
    ]
  },
  {
    "id": 823,
    "topic": "1",
    "question_en": "A robotics company is designing a solution for medical surgery. The robots will use advanced sensors, cameras, and AI algorithms to perceive their environment and to complete surgeries. The company needs a public load balancer in the AWS Cloud that will ensure seamless communication with backend services. The load balancer must be capable of routing trafic based on the query strings to different target groups. The trafic must also be encrypted. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use a Network Load Balancer with a certificate attached from AWS Certificate Manager (ACM). Use query parameter-based routing.",
      "B": "Use a Gateway Load Balancer. Import a generated certificate in AWS Identity and Access Management (IAM). Attach the certificate to the load balancer. Use HTTP path-based routing.",
      "C": "Use an Application Load Balancer with a certificate attached from AWS Certificate Manager (ACM). Use query parameter-based routing.",
      "D": "Use a Network Load Balancer. Import a generated certificate in AWS Identity and Access Management (IAM). Attach the certificate to the load balancer. Use query parameter-based routing."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家机器人公司正在为医疗手术设计一个解决方案。机器人将使用先进的传感器、摄像头和 AI 算法来感知其环境并完成手术。该公司需要在 AWS Cloud 中使用一个公共负载均衡器，以确保与后端服务的无缝通信。该负载均衡器必须能够根据查询字符串将流量路由到不同的目标组。流量也必须被加密。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Network Load Balancer，并附带来自 AWS Certificate Manager (ACM) 的证书。使用基于查询参数的路由。",
      "B": "使用 Gateway Load Balancer。在 AWS Identity and Access Management (IAM) 中导入一个生成的证书。将该证书附加到负载均衡器。使用基于 HTTP 路径的路由。",
      "C": "使用 Application Load Balancer，并附带来自 AWS Certificate Manager (ACM) 的证书。使用基于查询参数的路由。",
      "D": "使用 Network Load Balancer。在 AWS Identity and Access Management (IAM) 中导入一个生成的证书。将该证书附加到负载均衡器。使用基于查询参数的路由。"
    },
    "tags": [
      "ALB",
      "NLB",
      "ACM",
      "Gateway Load Balancer",
      "HTTP",
      "HTTPS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察负载均衡器的选择和配置。Application Load Balancer (ALB) 支持基于内容的路由，例如基于查询参数的路由。使用 ALB 需要使用 HTTPS 协议，并使用 ACM 证书来实现加密。Network Load Balancer (NLB) 和 Gateway Load Balancer 都不支持基于内容的路由。",
      "why_correct": "选项 C 正确，因为 ALB 支持基于查询参数的路由，并且可以使用来自 ACM 的证书来实现加密。",
      "why_wrong": "选项 A 错误，因为 Network Load Balancer (NLB) 不支持基于查询参数的路由。选项 B 错误，因为 Gateway Load Balancer 不支持基于查询参数的路由。选项 D 错误，因为 Network Load Balancer (NLB) 不支持基于查询参数的路由。"
    },
    "related_terms": [
      "ALB",
      "NLB",
      "ACM",
      "HTTP",
      "HTTPS",
      "Gateway Load Balancer"
    ]
  },
  {
    "id": 824,
    "topic": "1",
    "question_en": "A company has an application that runs on a single Amazon EC2 instance. The application uses a MySQL database that runs on the same EC2 instance. The company needs a highly available and automatically scalable solution to handle increased trafic. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application Load Balancer. Create an Amazon Redshift cluster that has multiple MySQL-compatible nodes.",
      "B": "Deploy the application to EC2 instances that are configured as a target group behind an Application Load Balancer. Create an Amazon RDS for MySQL cluster that has multiple instances.",
      "C": "Deploy the application to EC2 instances that run in an Auto Scaling group behind an Application Load Balancer. Create an Amazon Aurora Serverless MySQL cluster for the database layer.",
      "D": "Deploy the application to EC2 instances that are configured as a target group behind an Application Load Balancer. Create an Amazon ElastiCache for Redis cluster that uses the MySQL connector."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个在单个 Amazon EC2 实例上运行的应用程序。该应用程序使用一个在同一 EC2 实例上运行的 MySQL 数据库。该公司需要一个高可用性且自动可扩展的解决方案来处理增加的流量。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将应用程序部署到在 Application Load Balancer 后运行在 Auto Scaling 组中的 EC2 实例。创建一个具有多个 MySQL 兼容节点的 Amazon Redshift 集群。",
      "B": "将应用程序部署到配置为 Application Load Balancer 后面的目标组的 EC2 实例。创建一个具有多个实例的 Amazon RDS for MySQL 集群。",
      "C": "将应用程序部署到在 Application Load Balancer 后运行在 Auto Scaling 组中的 EC2 实例。为数据库层创建一个 Amazon Aurora Serverless MySQL 集群。",
      "D": "将应用程序部署到配置为 Application Load Balancer 后面的目标组的 EC2 实例。创建一个使用 MySQL 连接器的 Amazon ElastiCache for Redis 集群。"
    },
    "tags": [
      "EC2",
      "ALB",
      "Auto Scaling",
      "RDS",
      "MySQL",
      "Aurora",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查构建高可用性、可扩展的应用程序架构，包括负载均衡、弹性伸缩和数据库解决方案的组合。",
      "why_correct": "选项 C 提供了满足高可用性和自动可扩展性的解决方案。将应用程序置于 Auto Scaling 组和 Application Load Balancer 之后，可以自动处理流量变化。Aurora Serverless MySQL 集群可以根据需要自动扩展数据库容量，满足数据库层面的弹性需求。",
      "why_wrong": "选项 A 错误，因为 Amazon Redshift 主要用于数据仓库，不适用于应用程序的在线事务处理 (OLTP) 数据库需求。选项 B 错误，虽然 Amazon RDS for MySQL 可以提供高可用性，但其扩展不如 Aurora Serverless MySQL 灵活。选项 D 错误，Amazon ElastiCache for Redis 主要用于缓存，不适合用作应用程序的数据库，且使用 Redis 作为主要数据存储，架构设计不合理。"
    },
    "related_terms": [
      "Amazon EC2",
      "MySQL",
      "Application Load Balancer",
      "Auto Scaling",
      "Amazon Redshift",
      "Amazon RDS for MySQL",
      "Amazon Aurora Serverless MySQL",
      "Amazon ElastiCache for Redis",
      "EC2 instance",
      "target group"
    ]
  },
  {
    "id": 825,
    "topic": "1",
    "question_en": "A company is planning to migrate data to an Amazon S3 bucket. The data must be encrypted at rest within the S3 bucket. The encryption key must be rotated automatically every year. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Migrate the data to the S3 bucket. Use server-side encryption with Amazon S3 managed keys (SSE-S3). Use the built-in key rotation behavior of SSE-S3 encryption keys.",
      "B": "Create an AWS Key Management Service (AWS KMS) customer managed key. Enable automatic key rotation. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Migrate the data to the S3 bucket.",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Set the S3 bucket's default encryption behavior to use the customer managed KMS key. Migrate the data to the S3 bucket. Manually rotate the KMS key every year.",
      "D": "Use customer key material to encrypt the data. Migrate the data to the S3 bucket. Create an AWS Key Management Service (AWS KMS) key without key material. Import the customer key material into the KMS key. Enable automatic key rotation."
    },
    "correct_answer": "B",
    "vote_percentage": "60%",
    "question_cn": "一家公司计划将数据迁移到 Amazon S3 存储桶。 数据必须在 S3 存储桶内静态加密。 加密密钥必须每年自动轮换。 哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将数据迁移到 S3 存储桶。 使用具有 Amazon S3 托管密钥 (SSE-S3) 的服务器端加密。 使用 SSE-S3 加密密钥的内置密钥轮换行为。",
      "B": "创建一个 AWS Key Management Service (AWS KMS) 客户托管密钥。 启用自动密钥轮换。 将 S3 存储桶的默认加密行为设置为使用客户托管 KMS 密钥。 将数据迁移到 S3 存储桶。",
      "C": "创建一个 AWS Key Management Service (AWS KMS) 客户托管密钥。 将 S3 存储桶的默认加密行为设置为使用客户托管 KMS 密钥。 将数据迁移到 S3 存储桶。 每年手动轮换 KMS 密钥。",
      "D": "使用客户密钥材料对数据进行加密。 将数据迁移到 S3 存储桶。 创建一个 AWS Key Management Service (AWS KMS) 密钥，不带密钥材料。 将客户密钥材料导入 KMS 密钥。 启用自动密钥轮换。"
    },
    "tags": [
      "S3",
      "SSE-S3",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 60%），解析仅供参考。】\n\n考查在 Amazon S3 中实现静态数据加密，并满足密钥自动轮换需求的最优方案。",
      "why_correct": "选项 B 提供了最简洁、运维开销最低的解决方案。它利用 AWS KMS 客户托管密钥，配置自动密钥轮换，并将 S3 存储桶默认加密设置为使用该 KMS 密钥，确保数据在上传到 S3 时自动加密，并满足了每年密钥轮换的需求。",
      "why_wrong": "选项 A 错误在于 SSE-S3 虽然提供了服务器端加密，但没有提供密钥轮换功能，无法满足题目的需求。选项 C 需要手动轮换 KMS 密钥，这增加了运营开销，与题目要求的“最少的运营开销”相悖。选项 D 的方案较为复杂，涉及密钥材料的导入，增加了不必要的步骤，且没有必要。此外，这种方案会产生额外的费用，增加了运营成本，同样不符合题目的要求。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "SSE-S3",
      "AWS Key Management Service",
      "AWS KMS",
      "KMS"
    ]
  },
  {
    "id": 826,
    "topic": "1",
    "question_en": "A company is migrating applications from an on-premises Microsoft Active Directory that the company manages to AWS. The company deploys the applications in multiple AWS accounts. The company uses AWS Organizations to manage the accounts centrally. The company's security team needs a single sign-on solution across all the company's AWS accounts. The company must continue to manage users and groups that are in the on-premises Active Directory. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Enterprise Edition Active Directory in AWS Directory Service for Microsoft Active Directory. Configure the Active Directory to be the identity source for AWS IAM Identity Center.",
      "B": "Enable AWS IAM Identity Center. Configure a two-way forest trust relationship to connect the company's self-managed Active Directory with IAM Identity Center by using AWS Directory Service for Microsoft Active Directory.",
      "C": "Use AWS Directory Service and create a two-way trust relationship with the company's self-managed Active Directory.",
      "D": "Deploy an identity provider (IdP) on Amazon EC2. Link the IdP as an identity source within AWS IAM Identity Center."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其应用程序从公司管理的本地 Microsoft Active Directory 迁移到 AWS。该公司在多个 AWS 账户中部署了这些应用程序。该公司使用 AWS Organizations 集中管理这些账户。该公司的安全团队需要一个跨所有公司 AWS 账户的单点登录解决方案。该公司必须继续管理位于本地 Active Directory 中的用户和组。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 AWS Directory Service for Microsoft Active Directory 中创建一个 Enterprise Edition Active Directory。将 Active Directory 配置为 AWS IAM Identity Center 的身份源。",
      "B": "打开 AWS IAM Identity Center。使用 AWS Directory Service for Microsoft Active Directory 配置双向林信任关系，以将公司的自管理 Active Directory 与 IAM Identity Center 连接起来。",
      "C": "使用 AWS Directory Service，并与公司的自管理 Active Directory 创建双向信任关系。",
      "D": "在 Amazon EC2 上部署一个身份提供商 (IdP)。将 IdP 链接为 AWS IAM Identity Center 中的身份源。"
    },
    "tags": [
      "IAM Identity Center",
      "AWS Directory Service",
      "Active Directory",
      "AWS Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查了如何整合本地 Active Directory 与 AWS IAM Identity Center 实现单点登录。通过配置 AWS Directory Service for Microsoft Active Directory 和双向林信任关系，可以实现用户和组的同步以及单点登录。选项 B 正确地描述了这种配置方式，能满足题目的要求。",
      "why_correct": "选项 B 描述了通过双向林信任关系将自管理 Active Directory 与 IAM Identity Center 连接起来的方法，实现了单点登录，并且保留了本地 Active Directory 的用户和组管理。",
      "why_wrong": "选项 A 错误，因为配置 Enterprise Edition Active Directory 后，它不是用来整合本地 Active Directory。选项 C 错误，因为仅使用 AWS Directory Service 无法实现单点登录。选项 D 错误，IdP 无法完全替代 Active Directory 的功能。"
    },
    "related_terms": [
      "AWS IAM Identity Center",
      "AWS Directory Service",
      "Active Directory",
      "AWS Organizations",
      "IdP",
      "Microsoft Active Directory"
    ]
  },
  {
    "id": 827,
    "topic": "1",
    "question_en": "A company is planning to deploy its application on an Amazon Aurora PostgreSQL Serverless v2 cluster. The application will receive large amounts of trafic. The company wants to optimize the storage performance of the cluster as the load on the application increases. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure the cluster to use the Aurora Standard storage configuration.",
      "B": "Configure the cluster storage type as Provisioned IOPS.",
      "C": "Configure the cluster storage type as General Purpose.",
      "D": "Configure the cluster to use the Aurora I/O-Optimized storage configuration."
    },
    "correct_answer": "D",
    "vote_percentage": "85%",
    "question_cn": "一家公司计划在其 Amazon Aurora PostgreSQL Serverless v2 集群上部署其应用程序。该应用程序将收到大量流量。该公司希望在应用程序的负载增加时优化集群的存储性能。哪个解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "将集群配置为使用 Aurora Standard 存储配置。",
      "B": "将集群存储类型配置为 Provisioned IOPS。",
      "C": "将集群存储类型配置为 General Purpose。",
      "D": "将集群配置为使用 Aurora I/O-Optimized 存储配置。"
    },
    "tags": [
      "Aurora PostgreSQL",
      "Serverless v2",
      "存储性能"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 85%），解析仅供参考。】\n\n本题考查了 Amazon Aurora Serverless v2 的存储优化策略，以及在面对高流量时如何以最具成本效益的方式提升存储性能。",
      "why_correct": "Aurora I/O-Optimized 存储配置专为 Aurora Serverless v2 设计，在负载增加时，能够根据需求自动扩展存储 I/O 性能，从而优化应用程序的性能。该配置可提供更好的 I/O 性能，同时实现成本优化，使其成为满足高流量要求的最佳选择。",
      "why_wrong": "选项 A，Aurora Standard 存储配置并非针对 Aurora Serverless v2 优化，无法满足动态扩展的需求。选项 B，Provisioned IOPS 适用于有固定 IOPS 需求的数据库，与 Aurora Serverless v2 的弹性特性不符，且成本相对较高。选项 C，General Purpose 存储虽然可用，但性能不如 I/O-Optimized，且无法充分发挥 Aurora Serverless v2 的优势。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL Serverless v2",
      "Aurora Standard",
      "Provisioned IOPS",
      "General Purpose",
      "Aurora I/O-Optimized"
    ]
  },
  {
    "id": 828,
    "topic": "1",
    "question_en": "A financial services company that runs on AWS has designed its security controls to meet industry standards. The industry standards include the National Institute of Standards and Technology (NIST) and the Payment Card Industry Data Security Standard (PCI DSS). The company's third-party auditors need proof that the designed controls have been implemented and are functioning correctly. The company has hundreds of AWS accounts in a single organization in AWS Organizations. The company needs to monitor the current state of the controls across accounts. Which solution will meet these requirements?",
    "options_en": {
      "A": "Designate one account as the Amazon Inspector delegated administrator account from the Organizations management account. Integrate Inspector with Organizations to discover and scan resources across all AWS accounts. Enable Inspector industry standards for NIST and PCI DSS.",
      "B": "Designate one account as the Amazon GuardDuty delegated administrator account from the Organizations management account. In the designated GuardDuty administrator account, enable GuardDuty to protect all member accounts. Enable GuardDuty industry standards for NIST and PCI DSS.",
      "C": "Configure an AWS CloudTrail organization trail in the Organizations management account. Designate one account as the compliance account. Enable CloudTrail security standards for NIST and PCI DSS in the compliance account.",
      "D": "Designate one account as the AWS Security Hub delegated administrator account from the Organizations management account. In the designated Security Hub administrator account, enable Security Hub for all member accounts. Enable Security Hub standards for NIST and PCI DSS."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家在 AWS 上运营的金融服务公司已设计其安全控制措施以满足行业标准。行业标准包括国家标准与技术研究院 (NIST) 和支付卡行业数据安全标准 (PCI DSS)。该公司的第三方审计员需要证明已实施所设计的控制措施并正常运行。该公司在 AWS Organizations 的单个组织中拥有数百个 AWS 账户。该公司需要监控所有账户的当前控制状态。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "从 Organizations 管理账户中指定一个账户作为 Amazon Inspector 委托管理员账户。将 Inspector 与 Organizations 集成，以发现和扫描所有 AWS 账户中的资源。为 NIST 和 PCI DSS 启用 Inspector 行业标准。",
      "B": "从 Organizations 管理账户中指定一个账户作为 Amazon GuardDuty 委托管理员账户。在指定的 GuardDuty 管理员账户中，启用 GuardDuty 以保护所有成员账户。为 NIST 和 PCI DSS 启用 GuardDuty 行业标准。",
      "C": "在 Organizations 管理账户中配置一个 AWS CloudTrail 组织追踪。将一个账户指定为合规账户。在合规账户中为 NIST 和 PCI DSS 启用 CloudTrail 安全标准。",
      "D": "从 Organizations 管理账户中指定一个账户作为 AWS Security Hub 委托管理员账户。在指定的 Security Hub 管理员账户中，为所有成员账户启用 Security Hub。为 NIST 和 PCI DSS 启用 Security Hub 标准。"
    },
    "tags": [
      "Security Hub",
      "GuardDuty",
      "Organizations",
      "Inspector",
      "NIST",
      "PCI DSS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考察 AWS Security Hub、GuardDuty 和 Inspector 的综合使用，以满足合规性要求。Security Hub 提供集中式安全状态视图，可以聚合多个账户的安全发现结果。通过与 AWS Organizations 集成，可以实现跨组织账户的合规性监控。 GuardDuty 和 Inspector 则侧重于威胁检测和漏洞扫描。",
      "why_correct": "Security Hub 可以通过指定委托管理员账户，集中管理多个账户的安全状况，并针对 NIST 和 PCI DSS 标准进行合规性检查，因此满足了题目要求。",
      "why_wrong": "选项 A 使用 Inspector 扫描，但无法满足集中监控多个账户的当前控制状态的要求。选项 B 使用 GuardDuty 进行安全检测，而不是安全状态的监控。选项 C 使用 CloudTrail 进行审计，没有提供安全状态的集中视图，无法满足要求。"
    },
    "related_terms": [
      "GuardDuty",
      "Inspector",
      "PCI DSS",
      "AWS CloudTrail",
      "Security Hub",
      "Organizations",
      "NIST"
    ]
  },
  {
    "id": 829,
    "topic": "1",
    "question_en": "A company uses an Amazon S3 bucket as its data lake storage platform. The S3 bucket contains a massive amount of data that is accessed randomly by multiple teams and hundreds of applications. The company wants to reduce the S3 storage costs and provide immediate availability for frequently accessed objects. What is the MOST operationally eficient solution that meets these requirements?",
    "options_en": {
      "A": "Create an S3 Lifecycle rule to transition objects to the S3 Intelligent-Tiering storage class.",
      "B": "Store objects in Amazon S3 Glacier. Use S3 Select to provide applications with access to the data.",
      "C": "Use data from S3 storage class analysis to create S3 Lifecycle rules to automatically transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class.",
      "D": "Transition objects to the S3 Standard-Infrequent Access (S3 Standard-IA) storage class. Create an AWS Lambda function to transition objects to the S3 Standard storage class when they are accessed by an application."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon S3 存储桶作为其数据湖存储平台。 S3 存储桶包含大量数据，这些数据由多个团队和数百个应用程序随机访问。该公司希望降低 S3 存储成本，并为经常访问的对象提供即时可用性。哪种解决方案在运营上最有效，可以满足这些要求？",
    "options_cn": {
      "A": "创建 S3 生命周期规则，将对象转换为 S3 Intelligent-Tiering 存储类。",
      "B": "将对象存储在 Amazon S3 Glacier 中。使用 S3 Select 为应用程序提供对数据的访问。",
      "C": "使用来自 S3 存储类分析的数据，创建 S3 生命周期规则，以自动将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA) 存储类。",
      "D": "将对象转换为 S3 Standard-Infrequent Access (S3 Standard-IA) 存储类。创建一个 AWS Lambda 函数，以便在应用程序访问对象时将对象转换为 S3 Standard 存储类。"
    },
    "tags": [
      "S3",
      "Intelligent-Tiering",
      "Glacier",
      "S3 Standard-IA",
      "Lifecycle"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考察了降低 S3 存储成本和提供即时可用性的方法。 S3 Intelligent-Tiering 根据访问频率自动将对象移动到不同的存储层，因此最有效。 Glacier 和 S3 Standard-IA 虽然成本较低，但访问速度较慢。",
      "why_correct": "S3 Intelligent-Tiering 存储类可以根据访问模式自动调整存储层，满足降低成本和即时可用性的要求，是运营上最有效的方案。",
      "why_wrong": "选项 B 将对象存储在 Glacier 中，访问速度慢，不满足即时可用性的要求。选项 C 涉及 S3 Standard-IA，访问频率不高，并且需要创建规则，不具备即时可用性。 选项 D 将对象转换为 S3 Standard-IA 并使用 Lambda 函数，增加了复杂性，不具备运营效率。"
    },
    "related_terms": [
      "S3",
      "Intelligent-Tiering",
      "Glacier",
      "S3 Standard-IA",
      "AWS Lambda",
      "Lifecycle"
    ]
  },
  {
    "id": 830,
    "topic": "1",
    "question_en": "A company has 5 TB of datasets. The datasets consist of 1 million user profiles and 10 million connections. The user profiles have connections as many-to-many relationships. The company needs a performance eficient way to find mutual connections up to five levels. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an Amazon S3 bucket to store the datasets. Use Amazon Athena to perform SQL JOIN queries to find connections.",
      "B": "Use Amazon Neptune to store the datasets with edges and vertices. Query the data to find connections.",
      "C": "Use an Amazon S3 bucket to store the datasets. Use Amazon QuickSight to visualize connections.",
      "D": "Use Amazon RDS to store the datasets with multiple tables. Perform SQL JOIN queries to find connections."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司有 5 TB 的数据集。数据集由 100 万个用户配置文件和 1000 万个连接组成。用户配置文件具有多对多的连接关系。该公司需要一种高性能的方式来查找多达五个级别的共同连接。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 存储桶存储数据集。使用 Amazon Athena 执行 SQL JOIN 查询来查找连接。",
      "B": "使用 Amazon Neptune 存储带有边和顶点的的数据集。查询数据以查找连接。",
      "C": "使用 Amazon S3 存储桶存储数据集。使用 Amazon QuickSight 可视化连接。",
      "D": "使用 Amazon RDS 存储带有多个表的数据集。执行 SQL JOIN 查询来查找连接。"
    },
    "tags": [
      "Neptune",
      "S3",
      "Athena",
      "QuickSight",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n该题考察了使用哪种数据库存储图形数据，以便查找共同连接。 Amazon Neptune 是专门为图形数据库设计的，可以高效地处理多层连接查询。 S3、Athena、QuickSight 和 RDS 不适合处理这种类型的查询。",
      "why_correct": "Amazon Neptune 是专门为图形数据库设计的服务，非常适合处理具有多对多连接关系的数据集，能够高效地查找多层共同连接。",
      "why_wrong": "选项 A 使用 Amazon S3 存储桶和 Athena，Athena 并不适合此类复杂连接查询。选项 C 使用 Amazon S3 存储桶和 QuickSight 进行可视化，不具备查询能力。选项 D 使用 Amazon RDS，RDS 在处理这种复杂连接查询时性能较差。"
    },
    "related_terms": [
      "S3",
      "Athena",
      "QuickSight",
      "RDS",
      "Neptune"
    ]
  },
  {
    "id": 831,
    "topic": "1",
    "question_en": "A company needs a secure connection between its on-premises environment and AWS. This connection does not need high bandwidth and will handle a small amount of trafic. The connection should be set up quickly. What is the MOST cost-effective method to establish this type of connection?",
    "options_en": {
      "A": "Implement a client VPN.",
      "B": "Implement AWS Direct Connect.",
      "C": "Implement a bastion host on Amazon EC2.",
      "D": "Implement an AWS Site-to-Site VPN connection."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司需要在其本地环境和 AWS 之间建立安全连接。此连接不需要高带宽，并且将处理少量流量。该连接应快速建立。建立此类连接的最具成本效益的方法是什么？",
    "options_cn": {
      "A": "实施客户端 VPN。",
      "B": "实施 AWS Direct Connect。",
      "C": "在 Amazon EC2 上实施堡垒主机。",
      "D": "实施 AWS 站点到站点 VPN 连接。"
    },
    "tags": [
      "VPN",
      "Direct Connect",
      "EC2",
      "SFTP",
      "Site-to-Site VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考查在本地环境和 AWS 之间建立安全连接的成本效益。对于不需要高带宽且处理少量流量的场景，Site-to-Site VPN 是最具成本效益的解决方案。其他选项的成本或建立时间都较高。",
      "why_correct": "Site-to-Site VPN 提供了加密的安全连接，适用于少量流量且不需要高带宽的场景，连接建立快速且成本较低。",
      "why_wrong": "客户端 VPN 成本较高。 Direct Connect 成本高于 VPN 且建立时间较长。在 EC2 上实施堡垒主机无法提供安全连接。"
    },
    "related_terms": [
      "VPN",
      "Direct Connect",
      "EC2",
      "SFTP",
      "Site-to-Site VPN",
      "AWS"
    ]
  },
  {
    "id": 832,
    "topic": "1",
    "question_en": "A company has an on-premises SFTP file transfer solution. The company is migrating to the AWS Cloud to scale the file transfer solution and to optimize costs by using Amazon S3. The company's employees will use their credentials for the on-premises Microsoft Active Directory (AD) to access the new solution. The company wants to keep the current authentication and file access mechanisms. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure an S3 File Gateway. Create SMB file shares on the file gateway that use the existing Active Directory to authenticate.",
      "B": "Configure an Auto Scaling group with Amazon EC2 instances to run an SFTP solution. Configure the group to scale up at 60% CPU utilization.",
      "C": "Create an AWS Transfer Family server with SFTP endpoints. Choose the AWS Directory Service option as the identity provider. Use AD Connector to connect the on-premises Active Directory.",
      "D": "Create an AWS Transfer Family SFTP endpoint. Configure the endpoint to use the AWS Directory Service option as the identity provider to connect to the existing Active Directory."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个本地 SFTP 文件传输解决方案。该公司正在迁移到 AWS 云，以扩展文件传输解决方案，并通过使用 Amazon S3 来优化成本。该公司的员工将使用其本地 Microsoft Active Directory (AD) 的凭证来访问新解决方案。该公司希望保留当前的身份验证和文件访问机制。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "配置 S3 File Gateway。在 File Gateway 上创建 SMB 文件共享，使用现有的 Active Directory 进行身份验证。",
      "B": "配置一个包含 Amazon EC2 实例的 Auto Scaling 组以运行 SFTP 解决方案。将该组配置为在 60% CPU 利用率时进行扩展。",
      "C": "使用 SFTP 端点创建一个 AWS Transfer Family 服务器。选择 AWS Directory Service 选项作为身份提供商。使用 AD Connector 连接本地 Active Directory。",
      "D": "创建一个 AWS Transfer Family SFTP 端点。将端点配置为使用 AWS Directory Service 选项作为身份提供商以连接到现有的 Active Directory。"
    },
    "tags": [
      "SFTP",
      "Transfer Family",
      "Directory Service",
      "Active Directory"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查了 SFTP 解决方案的迁移到 AWS 并与现有 Active Directory 集成的最佳实践。 Transfer Family 提供了对 SFTP 协议的支持，并支持与 AWS Directory Service 集成，实现用户身份验证。选项 C 正确地描述了这种整合方案，且满足最少运营开销的要求。",
      "why_correct": "选项 C 使用 AWS Transfer Family 服务器的 SFTP 端点，并通过 AD Connector 连接到本地 Active Directory 进行身份验证，保留了现有身份验证机制。",
      "why_wrong": "选项 A 使用 S3 File Gateway，File Gateway 不支持 SFTP 协议。选项 B 涉及 EC2 实例，增加了运营开销。选项 D 虽然使用 Transfer Family，但没有连接到 AD Connector，无法满足要求。"
    },
    "related_terms": [
      "SFTP",
      "Transfer Family",
      "Active Directory",
      "AD Connector",
      "Directory Service",
      "S3 File Gateway"
    ]
  },
  {
    "id": 833,
    "topic": "1",
    "question_en": "A company is designing an event-driven order processing system. Each order requires multiple validation steps after the order is created. An idempotent AWS Lambda function performs each validation step. Each validation step is independent from the other validation steps. Individual validation steps need only a subset of the order event information. The company wants to ensure that each validation step Lambda function has access to only the information from the order event that the function requires. The components of the order processing system should be loosely coupled to accommodate future business changes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue for each validation step. Create a new Lambda function to transform the order data to the format that each validation step requires and to publish the messages to the appropriate SQS queues. Subscribe each validation step Lambda function to its corresponding SQS queue.",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the validation step Lambda functions to the SNS topic. Use message body filtering to send only the required data to each subscribed Lambda function.",
      "C": "Create an Amazon EventBridge event bus. Create an event rule for each validation step. Configure the input transformer to send only the required data to each target validation step Lambda function.",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a new Lambda function to subscribe to the SQS queue and to transform the order data to the format that each validation step requires. Use the new Lambda function to perform synchronous invocations of the validation step Lambda functions in parallel on separate threads."
    },
    "correct_answer": "C",
    "vote_percentage": "89%",
    "question_cn": "一家公司正在设计一个事件驱动的订单处理系统。每个订单在创建后都需要多个验证步骤。一个幂等 AWS Lambda 函数执行每个验证步骤。每个验证步骤与其他验证步骤无关。各个验证步骤只需要订单事件信息的一个子集。该公司希望确保每个验证步骤 Lambda 函数只能访问该函数所需的订单事件中的信息。订单处理系统的组件应松散耦合，以适应未来的业务变更。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为每个验证步骤创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。创建一个新的 Lambda 函数来转换订单数据以满足每个验证步骤的要求，并将消息发布到相应的 SQS 队列。将每个验证步骤 Lambda 函数订阅到其相应的 SQS 队列。",
      "B": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题。将验证步骤 Lambda 函数订阅到 SNS 主题。使用消息正文过滤仅将所需数据发送到每个已订阅的 Lambda 函数。",
      "C": "创建一个 Amazon EventBridge 事件总线。为每个验证步骤创建一个事件规则。配置输入转换器以仅将所需数据发送到每个目标验证步骤 Lambda 函数。",
      "D": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列。创建一个新的 Lambda 函数来订阅 SQS 队列，并将订单数据转换为每个验证步骤所需的格式。使用新的 Lambda 函数在单独的线程上并行执行验证步骤 Lambda 函数的同步调用。"
    },
    "tags": [
      "SQS",
      "SNS",
      "EventBridge",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 89%），解析仅供参考。】\n\n此题考察了事件驱动架构的设计。 Amazon EventBridge 可以作为事件总线，将事件路由到不同的 Lambda 函数。 使用输入转换器可以只发送 Lambda 函数需要的数据，满足松散耦合和信息隔离的要求。",
      "why_correct": "选项 C 使用 EventBridge 和输入转换器，可以确保每个验证步骤的 Lambda 函数只接收到其所需的订单事件信息，满足了松散耦合和信息隔离的要求。",
      "why_wrong": "选项 A 使用 SQS 和新的 Lambda 函数转换数据，增加了额外的开销，耦合性也较高。选项 B 使用 SNS 和消息正文过滤，可能导致传递不必要的冗余数据。选项 D 使用 SQS，需要一个新的 Lambda 函数，并且执行同步调用，耦合性较高，效率也较差。"
    },
    "related_terms": [
      "SQS",
      "SNS",
      "EventBridge",
      "Lambda",
      "Amazon SQS"
    ]
  },
  {
    "id": 834,
    "topic": "1",
    "question_en": "A company is migrating a three-tier application to AWS. The application requires a MySQL database. In the past, the application users reported poor application performance when creating new entries. These performance issues were caused by users generating different real- time reports from the application during working hours. Which solution will improve the performance of the application when it is moved to AWS?",
    "options_en": {
      "A": "Import the data into an Amazon DynamoDB table with provisioned capacity. Refactor the application to use DynamoDB for reports.",
      "B": "Create the database on a compute optimized Amazon EC2 instance. Ensure compute resources exceed the on-premises database.",
      "C": "Create an Amazon Aurora MySQL Multi-AZ DB cluster with multiple read replicas. Configure the application to use the reader endpoint for reports.",
      "D": "Create an Amazon Aurora MySQL Multi-AZ DB cluster. Configure the application to use the backup instance of the cluster as an endpoint for the reports."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将三层应用程序迁移到 AWS。该应用程序需要一个 MySQL 数据库。过去，当创建新条目时，应用程序用户报告了较差的应用程序性能。这些性能问题是由用户在工作时间从应用程序生成不同的实时报告引起的。将应用程序移动到 AWS 时，哪种解决方案将提高应用程序的性能？",
    "options_cn": {
      "A": "将数据导入具有预置容量的 Amazon DynamoDB 表。重构应用程序以使用 DynamoDB 进行报告。",
      "B": "在计算优化的 Amazon EC2 实例上创建数据库。确保计算资源超过本地数据库。",
      "C": "使用多个只读副本创建 Amazon Aurora MySQL 多可用区数据库集群。配置应用程序以将读取器端点用于报告。",
      "D": "创建 Amazon Aurora MySQL 多可用区数据库集群。配置应用程序以使用集群的备份实例作为报告的端点。"
    },
    "tags": [
      "Aurora MySQL",
      "Multi-AZ",
      "Read Replicas",
      "DynamoDB",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察了提高应用程序性能的策略。 在应用程序产生大量报告负载的情况下，利用只读副本将报告流量分流可以有效地提高性能。 Aurora MySQL 多可用区集群支持只读副本，满足需求。",
      "why_correct": "选项 C 使用 Amazon Aurora MySQL 多可用区数据库集群并配置应用程序将读取器端点用于报告，可将报告负载转移到只读副本，提高主数据库的性能。",
      "why_wrong": "选项 A 使用 DynamoDB，需要重构应用程序，不是最佳实践。选项 B 在计算优化的 EC2 实例上创建数据库，无法解决报告负载造成的性能问题。选项 D 使用备份实例作为报告端点，无法充分利用只读副本的优势。"
    },
    "related_terms": [
      "Aurora MySQL",
      "Multi-AZ",
      "DynamoDB",
      "EC2",
      "Read Replicas"
    ]
  },
  {
    "id": 835,
    "topic": "1",
    "question_en": "A company is expanding a secure on-premises network to the AWS Cloud by using an AWS Direct Connect connection. The on-premises network has no direct internet access. An application that runs on the on-premises network needs to use an Amazon S3 bucket. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a public virtual interface (VIF). Route the AWS trafic over the public VIF.",
      "B": "Create a VPC and a NAT gateway. Route the AWS trafic from the on-premises network to the NAT gateway.",
      "C": "Create a VPC and an Amazon S3 interface endpoint. Route the AWS trafic from the on-premises network to the S3 interface endpoint.",
      "D": "Create a VPC peering connection between the on-premises network and Direct Connect. Route the AWS trafic over the peering connection."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在使用 AWS Direct Connect 连接将其安全的本地网络扩展到 AWS Cloud。本地网络没有直接的互联网访问。一个在本地网络上运行的应用程序需要使用 Amazon S3 存储桶。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个公共虚拟接口（VIF）。通过公共 VIF 路由 AWS 流量。",
      "B": "创建一个 VPC 和一个 NAT 网关。将来自本地网络的 AWS 流量路由到 NAT 网关。",
      "C": "创建一个 VPC 和一个 Amazon S3 接口终端节点。将来自本地网络的 AWS 流量路由到 S3 接口终端节点。",
      "D": "在本地网络和 Direct Connect 之间创建一个 VPC 对等连接。通过对等连接路由 AWS 流量。"
    },
    "tags": [
      "Direct Connect",
      "VPC",
      "NAT",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了使用 Direct Connect 连接到 AWS 时访问 S3 的最佳实践。对于没有互联网访问的本地网络，使用 S3 接口终端节点是最具成本效益的。通过接口终端节点，流量无需经过互联网，从而提高性能和安全性。",
      "why_correct": "选项 C 使用 S3 接口终端节点，可以将来自本地网络的 AWS 流量直接路由到 S3，无需经过互联网，满足了成本效益和安全性的要求。",
      "why_wrong": "选项 A 创建公共虚拟接口，无法满足本地网络没有直接互联网访问的要求。 选项 B 使用 NAT 网关，会产生额外的费用且增加了复杂性。选项 D 使用 VPC 对等连接，无法直接连接到 S3，不符合要求。"
    },
    "related_terms": [
      "Direct Connect",
      "VPC",
      "NAT",
      "S3",
      "Transit Gateway",
      "VPC endpoint"
    ]
  },
  {
    "id": 836,
    "topic": "1",
    "question_en": "A company serves its website by using an Auto Scaling group of Amazon EC2 instances in a single AWS Region. The website does not require a database. The company is expanding, and the company's engineering team deploys the website to a second Region. The company wants to distribute trafic across both Regions to accommodate growth and for disaster recovery purposes. The solution should not serve trafic from a Region in which the website is unhealthy. Which policy or resource should the company use to meet these requirements?",
    "options_en": {
      "A": "An Amazon Route 53 simple routing policy",
      "B": "An Amazon Route 53 multivalue answer routing policy",
      "C": "An Application Load Balancer in one Region with a target group that specifies the EC2 instance IDs from both Regions",
      "D": "An Application Load Balancer in one Region with a target group that specifies the IP addresses of the EC2 instances from both Regions"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用单个 AWS 区域中的 Amazon EC2 实例的 Auto Scaling 组来提供其网站服务。该网站不需要数据库。该公司正在扩展，并且该公司的工程团队将该网站部署到第二个区域。该公司希望跨两个区域分配流量以适应增长和灾难恢复。该解决方案不应从网站运行状况不佳的区域提供流量。该公司应使用哪种策略或资源来满足这些要求？",
    "options_cn": {
      "A": "Amazon Route 53 简单路由策略",
      "B": "Amazon Route 53 多值应答路由策略",
      "C": "一个区域中的 Application Load Balancer，其目标组指定来自两个区域的 EC2 实例 ID",
      "D": "一个区域中的 Application Load Balancer，其目标组指定来自两个区域的 EC2 实例的 IP 地址"
    },
    "tags": [
      "Route 53",
      "ALB",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考察了跨多个区域分配流量以实现高可用性和灾难恢复的策略。 多值应答路由策略可以在多个健康端点之间分配流量，并且如果某个端点不健康，则停止将流量发送到该端点。",
      "why_correct": "选项 B，使用 Route 53 多值应答路由策略，可以跨多个区域分配流量，并且可以根据健康检查状态停止将流量发送到不健康的区域，从而满足要求。",
      "why_wrong": "选项 A 的简单路由策略无法实现负载均衡。选项 C 和 D 使用一个区域的 ALB，无法实现跨区域的流量分配，并且没有实现灾备。 "
    },
    "related_terms": [
      "Route 53",
      "ALB",
      "Multi-AZ",
      "Elastic Load Balancer"
    ]
  },
  {
    "id": 837,
    "topic": "1",
    "question_en": "A company runs its applications on Amazon EC2 instances that are backed by Amazon Elastic Block Store (Amazon EBS). The EC2 instances run the most recent Amazon Linux release. The applications are experiencing availability issues when the company's employees store and retrieve files that are 25 GB or larger. The company needs a solution that does not require the company to transfer files between EC2 instances. The files must be available across many EC2 instances and across multiple Availability Zones. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate all the files to an Amazon S3 bucket. Instruct the employees to access the files from the S3 bucket.",
      "B": "Take a snapshot of the existing EBS volume. Mount the snapshot as an EBS volume across the EC2 instances. Instruct the employees to access the files from the EC2 instances.",
      "C": "Mount an Amazon Elastic File System (Amazon EFS) file system across all the EC2 instances. Instruct the employees to access the files from the EC2 instances.",
      "D": "Create an Amazon Machine Image (AMI) from the EC2 instances. Configure new EC2 instances from the AMI that use an instance store volume. Instruct the employees to access the files from the EC2 instances."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其由 Amazon Elastic Block Store (Amazon EBS) 支持的 Amazon EC2 实例上运行其应用程序。 EC2 实例运行最新的 Amazon Linux 版本。 当公司的员工存储和检索 25 GB 或更大的文件时，应用程序遇到了可用性问题。 公司需要一个不需要公司在 EC2 实例之间传输文件的解决方案。 文件必须在多个 EC2 实例和多个可用区中可用。 哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将所有文件迁移到 Amazon S3 存储桶。 指示员工从 S3 存储桶访问文件。",
      "B": "拍摄现有 EBS 卷的快照。 将快照作为 EBS 卷挂载在 EC2 实例中。 指示员工从 EC2 实例访问文件。",
      "C": "在所有 EC2 实例中挂载一个 Amazon Elastic File System (Amazon EFS) 文件系统。 指示员工从 EC2 实例访问文件。",
      "D": "从 EC2 实例创建 Amazon Machine Image (AMI)。 从使用实例存储卷的 AMI 配置新的 EC2 实例。 指示员工从 EC2 实例访问文件。"
    },
    "tags": [
      "EFS",
      "EBS",
      "S3",
      "EC2",
      "AMI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考察了共享文件存储的需求。 EFS 提供了一个共享文件系统，可以被多个 EC2 实例访问，并且文件存储在多个可用区中，保证了高可用性。 EFS 满足了题目中对文件可用性、以及无需在 EC2 实例之间传输文件的要求。",
      "why_correct": "选项 C 使用 EFS，EFS 允许多个 EC2 实例访问相同的文件系统，满足了多个 EC2 实例访问和高可用性的要求，无需在 EC2 实例之间传输文件。",
      "why_wrong": "选项 A 将文件迁移到 S3，员工需要从 S3 访问文件，不满足题意。 选项 B 使用快照，每个 EC2 实例都有一个独立的文件系统，不满足多个实例访问的需求。 选项 D 使用 AMI，每个 EC2 实例都有独立的文件系统，不满足多个实例访问的需求。"
    },
    "related_terms": [
      "EFS",
      "EBS",
      "S3",
      "EC2",
      "AMI",
      "Amazon Elastic Block Store",
      "Elastic File System"
    ]
  },
  {
    "id": 838,
    "topic": "1",
    "question_en": "A company is running a highly sensitive application on Amazon EC2 backed by an Amazon RDS database. Compliance regulations mandate that all personally identifiable information (PII) be encrypted at rest. Which solution should a solutions architect recommend to meet this requirement with the LEAST amount of changes to the infrastructure?",
    "options_en": {
      "A": "Deploy AWS Certificate Manager to generate certificates. Use the certificates to encrypt the database volume.",
      "B": "Deploy AWS CloudHSM, generate encryption keys, and use the keys to encrypt database volumes.",
      "C": "Configure SSL encryption using AWS Key Management Service (AWS KMS) keys to encrypt database volumes.",
      "D": "Configure Amazon Elastic Block Store (Amazon EBS) encryption and Amazon RDS encryption with AWS Key Management Service (AWS KMS) keys to encrypt instance and database volumes."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在 Amazon EC2 上运行一个高度敏感的应用程序，该应用程序由 Amazon RDS 数据库支持。合规性法规要求所有个人身份信息 (PII) 都必须进行静态加密。解决方案架构师应该推荐哪个解决方案以最少的对基础设施的更改来满足此要求？",
    "options_cn": {
      "A": "部署 AWS Certificate Manager 以生成证书。使用这些证书对数据库卷进行加密。",
      "B": "部署 AWS CloudHSM，生成加密密钥，并使用这些密钥对数据库卷进行加密。",
      "C": "使用 AWS Key Management Service (AWS KMS) 密钥配置 SSL 加密以对数据库卷进行加密。",
      "D": "配置 Amazon Elastic Block Store (Amazon EBS) 加密和 Amazon RDS 加密，使用 AWS Key Management Service (AWS KMS) 密钥对实例和数据库卷进行加密。"
    },
    "tags": [
      "EBS",
      "RDS",
      "KMS",
      "SSL/TLS",
      "CloudHSM",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n该题考察了对 RDS 数据库进行静态加密的最佳实践。 最佳实践是同时配置 EBS 加密和 RDS 加密，并使用 KMS 密钥。 这可以最大限度地保护数据，而对基础设施的改动最少。其它选项需要更多的配置。",
      "why_correct": "选项 D 同时配置了 EBS 加密和 RDS 加密，并使用 KMS 密钥，提供了对数据库卷的静态加密，满足了合规性要求，并且对基础设施的改动最少。",
      "why_wrong": "选项 A 涉及 ACM，用于生成证书，不能用于数据库卷的加密。 选项 B 使用 CloudHSM，需要额外的硬件和配置，增加了复杂性。 选项 C 使用 SSL 加密，SSL 加密的是传输过程中的数据，而不是静态数据。"
    },
    "related_terms": [
      "EBS",
      "RDS",
      "KMS",
      "SSL/TLS",
      "ACM",
      "Amazon RDS",
      "Amazon EBS",
      "AWS KMS",
      "SSL",
      "CloudHSM"
    ]
  },
  {
    "id": 839,
    "topic": "1",
    "question_en": "A company runs an AWS Lambda function in private subnets in a VPC. The subnets have a default route to the internet through an Amazon EC2 NAT instance. The Lambda function processes input data and saves its output as an object to Amazon S3. Intermittently, the Lambda function times out while trying to upload the object because of saturated trafic on the NAT instance's network. The company wants to access Amazon S3 without traversing the internet. Which solution will meet these requirements?",
    "options_en": {
      "A": "Replace the EC2 NAT instance with an AWS managed NAT gateway.",
      "B": "Increase the size of the EC2 NAT instance in the VPC to a network optimized instance type.",
      "C": "Provision a gateway endpoint for Amazon S3 in the VPUpdate the route tables of the subnets accordingly.",
      "D": "Provision a transit gateway. Place transit gateway attachments in the private subnets where the Lambda function is running."
    },
    "correct_answer": "C",
    "vote_percentage": "88%",
    "question_cn": "一家公司在 VPC 的私有子网中运行 AWS Lambda 函数。这些子网通过 Amazon EC2 NAT 实例具有通往互联网的默认路由。Lambda 函数处理输入数据，并将其输出保存为 Amazon S3 中的一个对象。间歇性地，Lambda 函数在尝试上传对象时超时，原因是 NAT 实例的网络流量饱和。该公司希望访问 Amazon S3 而无需遍历互联网。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 EC2 NAT 实例替换为 AWS 托管的 NAT 网关。",
      "B": "将 VPC 中 EC2 NAT 实例的大小增加到网络优化实例类型。",
      "C": "在 VPC 中配置 Amazon S3 的网关 VPC endpoint，并相应地更新子网的路由表。",
      "D": "配置一个 Transit Gateway。将 Transit Gateway 附件放置在 Lambda 函数运行的私有子网中。"
    },
    "tags": [
      "Lambda",
      "VPC",
      "S3",
      "NAT",
      "VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 88%），解析仅供参考。】\n\n本题考察了 Lambda 函数访问 S3 的网络配置优化。 当 Lambda 函数位于 VPC 的私有子网中，并通过 NAT 实例访问互联网时，访问 S3 流量也会通过 NAT 实例。  使用 S3 网关 VPC 终端节点，流量可以无需经过互联网，从而提高性能和避免 NAT 实例的网络流量饱和问题。",
      "why_correct": "选项 C 使用 S3 的网关 VPC endpoint，可以使 Lambda 函数访问 S3 的流量通过 VPC 内部，避免了使用 NAT 实例，解决了网络流量饱和问题。",
      "why_wrong": "选项 A 将 EC2 NAT 实例替换为 AWS 托管的 NAT 网关，无法根本解决 NAT 实例的网络流量饱和问题。 选项 B 增大 EC2 NAT 实例的大小，只能缓解，不能根本解决问题。 选项 D 使用 Transit Gateway，会增加复杂性，也无法解决问题。"
    },
    "related_terms": [
      "Lambda",
      "VPC",
      "S3",
      "NAT",
      "NAT Gateway",
      "Transit Gateway",
      "VPC endpoint"
    ]
  },
  {
    "id": 840,
    "topic": "1",
    "question_en": "A news company that has reporters all over the world is hosting its broadcast system on AWS. The reporters send live broadcasts to the broadcast system. The reporters use software on their phones to send live streams through the Real Time Messaging Protocol (RTMP). A solutions architect must design a solution that gives the reporters the ability to send the highest quality streams. The solution must provide accelerated TCP connections back to the broadcast system. What should the solutions architect use to meet these requirements?",
    "options_en": {
      "A": "Amazon CloudFront",
      "B": "AWS Global Accelerator",
      "C": "AWS Client VPN",
      "D": "Amazon EC2 instances and AWS Elastic IP addresses"
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家新闻公司在全球各地都设有记者，该公司正在 AWS 上托管其广播系统。记者将实时广播发送到广播系统。记者使用手机上的软件通过实时消息传递协议 (RTMP) 发送实时流。解决方案架构师必须设计一个解决方案，使记者能够发送最高质量的流。该解决方案必须提供加速的 TCP 连接以返回到广播系统。解决方案架构师应该使用什么来满足这些要求？",
    "options_cn": {
      "A": "Amazon CloudFront",
      "B": "AWS Global Accelerator",
      "C": "AWS Client VPN",
      "D": "Amazon EC2 实例和 AWS Elastic IP 地址"
    },
    "tags": [
      "Amazon CloudFront",
      "S3 Transfer Acceleration",
      "RTMP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查利用加速 TCP 连接传输实时流的解决方案，需要选择合适的 AWS 服务来满足加速传输和高可用性需求。",
      "why_correct": "AWS Global Accelerator 能够利用 AWS 的全球网络基础设施来优化 TCP 和 UDP 流量的性能。它通过将流量路由到最佳终端节点（例如位于不同 AWS 区域的 EC2 实例或负载均衡器），从而降低延迟并提高性能。Global Accelerator 支持加速 TCP 连接，非常适合需要加速实时流媒体传输的场景。",
      "why_wrong": "Amazon CloudFront 是内容分发网络（CDN），主要用于缓存静态和动态内容，虽然可以提高内容的分发速度，但其侧重点并非加速 TCP 连接。AWS Client VPN 主要用于建立 VPN 连接，提供安全访问 AWS 资源的通道，与加速 TCP 连接的需求不符。使用 Amazon EC2 实例和 AWS Elastic IP 地址虽然可以部署流媒体服务器，但无法提供 Global Accelerator 提供的加速 TCP 连接和全球网络优化功能。"
    },
    "related_terms": [
      "AWS Global Accelerator",
      "RTMP",
      "TCP",
      "UDP",
      "Amazon CloudFront",
      "AWS Client VPN",
      "Amazon EC2",
      "AWS Elastic IP"
    ]
  },
  {
    "id": 841,
    "topic": "1",
    "question_en": "A company uses Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) to run its self-managed database. The company has 350 TB of data spread across all EBS volumes. The company takes daily EBS snapshots and keeps the snapshots for 1 month. The daily change rate is 5% of the EBS volumes. Because of new regulations, the company needs to keep the monthly snapshots for 7 years. The company needs to change its backup strategy to comply with the new regulations and to ensure that data is available with minimal administrative effort. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Copy the monthly snapshot to Amazon S3 Glacier Deep Archive with a 7-year retention period.",
      "B": "Continue with the current EBS snapshot policy. Add a new policy to move the monthly snapshot to Amazon EBS Snapshots Archive with a 7-year retention period.",
      "C": "Keep the daily snapshot in the EBS snapshot standard tier for 1 month. Keep the monthly snapshot in the standard tier for 7 years. Use incremental snapshots.",
      "D": "Keep the daily snapshot in the EBS snapshot standard tier. Use EBS direct APIs to take snapshots of all the EBS volumes every month. Store the snapshots in an Amazon S3 bucket in the Infrequent Access tier for 7 years."
    },
    "correct_answer": "B",
    "vote_percentage": "45%",
    "question_cn": "一家公司使用 Amazon EC2 实例和 Amazon Elastic Block Store (Amazon EBS) 来运行其自管理的数据库。该公司有 350 TB 的数据分布在所有 EBS 卷中。该公司每天拍摄 EBS 快照，并将快照保留 1 个月。每日变化率为 EBS 卷的 5%。由于新的法规，该公司需要将每月快照保留 7 年。该公司需要更改其备份策略以符合新法规，并确保数据可用且管理工作量最少。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "将每日快照保留在 EBS 快照标准层中 1 个月。将每月快照复制到 Amazon S3 Glacier Deep Archive，保留期为 7 年。",
      "B": "继续执行当前的 EBS 快照策略。添加一项新策略，将每月快照移动到 Amazon EBS Snapshots Archive，保留期为 7 年。",
      "C": "将每日快照保留在 EBS 快照标准层中 1 个月。将每月快照保留在标准层中 7 年。使用增量快照。",
      "D": "将每日快照保留在 EBS 快照标准层中。使用 EBS 直接 API 每月对所有 EBS 卷进行快照。将快照存储在 Amazon S3 存储桶的非频繁访问层中 7 年。"
    },
    "tags": [
      "Amazon EBS",
      "EBS Snapshots",
      "Amazon S3 Glacier Deep Archive",
      "Amazon S3",
      "Cost Optimization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 45%），解析仅供参考。】\n\n考查如何以最具成本效益的方式在 AWS 上长期保留 EBS 快照，同时满足法规遵从性要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 提供了最经济高效的解决方案。通过利用 Amazon EBS Snapshots Archive，可以将每月快照以较低的成本存储 7 年。该方案在满足数据保留要求的同时，最大程度地降低了存储成本，并且保持了现有的每日快照策略，避免了额外的复杂性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 的问题在于，将每月快照复制到 Amazon S3 Glacier Deep Archive 增加了数据检索的延迟和成本，且并不比 Snapshots Archive 更具成本效益。选项 C 错误地将所有快照（包括每日快照）保留在标准层中 7 年，这会产生高昂的存储费用。选项 D 涉及使用 EBS Direct API 进行快照，并将其存储在 S3 的非频繁访问层，这增加了管理复杂性，且存储成本高于 EBS Snapshots Archive，并且该方案并非针对快照存储进行优化。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon EBS",
      "EBS Snapshots",
      "Amazon S3 Glacier Deep Archive",
      "EBS Snapshots Archive",
      "EBS Direct API"
    ]
  },
  {
    "id": 842,
    "topic": "1",
    "question_en": "A company runs an application on several Amazon EC2 instances that store persistent data on an Amazon Elastic File System (Amazon EFS) file system. The company needs to replicate the data to another AWS Region by using an AWS managed service solution. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use the EFS-to-EFS backup solution to replicate the data to an EFS file system in another Region.",
      "B": "Run a nightly script to copy data from the EFS file system to an Amazon S3 bucket. Enable S3 Cross-Region Replication on the S3 bucket.",
      "C": "Create a VPC in another Region. Establish a cross-Region VPC peer. Run a nightly rsync to copy data from the original Region to the new Region.",
      "D": "Use AWS Backup to create a backup plan with a rule that takes a daily backup and replicates it to another Region. Assign the EFS file system resource to the backup plan."
    },
    "correct_answer": "D",
    "vote_percentage": "71%",
    "question_cn": "一家公司在其几个 Amazon EC2 实例上运行一个应用程序，这些实例将持久性数据存储在 Amazon Elastic File System (Amazon EFS) 文件系统中。该公司需要使用 AWS 托管服务解决方案将数据复制到另一个 AWS 区域。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 EFS 到 EFS 备份解决方案将数据复制到另一个区域中的 EFS 文件系统。",
      "B": "运行一个夜间脚本，将数据从 EFS 文件系统复制到 Amazon S3 存储桶。在 S3 存储桶上打开目标 S3 存储桶的S3 跨区域复制。",
      "C": "在另一个区域中创建一个 VPC。建立跨区域 VPC 对等。运行一个夜间 rsync 以将数据从原始区域复制到新区域。",
      "D": "使用 AWS Backup 创建一个备份计划，其中包含一个规则，该规则会进行每日备份并将其复制到另一个区域。将 EFS 文件系统资源分配给备份计划。"
    },
    "tags": [
      "Amazon EFS",
      "AWS Backup",
      "Cross-Region Replication",
      "Amazon S3",
      "Amazon EC2",
      "VPC",
      "rsync",
      "EFS to EFS backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 71%），解析仅供参考。】\n\n本题考查 EFS 跨区域数据复制的成本效益方案，涉及 EFS、S3、AWS Backup、VPC 对等、rsync 等服务的选型和对比。需要在满足数据复制需求的同时，考虑成本、维护工作量和自动化程度。",
      "why_correct": "AWS Backup 是一个完全托管的备份服务，可以自动执行跨区域备份。 通过创建备份计划并指定跨区域复制规则，可以定期备份 EFS 数据，并将备份复制到另一个区域的 AWS 账户。 这种方法提供了一种简单、可靠且具有成本效益的 EFS 跨区域备份和恢复解决方案，无需手动配置和管理复制过程，简化了运维。",
      "why_wrong": "选项 A，EFS 到 EFS 备份方案通常用于在同一区域内进行备份和恢复，而非跨区域复制，并且其跨区域复制的功能和成本不如 AWS Backup。 选项 B，将数据复制到 S3 再开启跨区域复制，涉及手动脚本编写，增加了维护的复杂性，并且需要将数据从 EFS 复制到 S3，增加了存储成本和数据传输成本。 选项 C，使用 rsync 结合 VPC 对等进行数据复制，需要手动管理 EC2 实例、VPC 连接、rsync 脚本的执行，以及监控和故障处理等，维护成本和复杂性较高，也增加了运维负担。此外，rsync 是一种基于文件级别的复制工具，对于大型 EFS 文件系统，复制时间可能较长，影响效率。"
    },
    "related_terms": [
      "Amazon EFS",
      "EFS",
      "Amazon EC2",
      "AWS Backup",
      "Amazon S3",
      "S3",
      "VPC",
      "rsync",
      "Cross-Region Replication",
      "EFS to EFS backup"
    ]
  },
  {
    "id": 843,
    "topic": "1",
    "question_en": "An ecommerce company is migrating its on-premises workload to the AWS Cloud. The workload currently consists of a web application and a backend Microsoft SQL database for storage. The company expects a high volume of customers during a promotional event. The new infrastructure in the AWS Cloud must be highly available and scalable. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Migrate the web application to two Amazon EC2 instances across two Availability Zones behind an Application Load Balancer. Migrate the database to Amazon RDS for Microsoft SQL Server with read replicas in both Availability Zones.",
      "B": "Migrate the web application to an Amazon EC2 instance that runs in an Auto Scaling group across two Availability Zones behind an Application Load Balancer. Migrate the database to two EC2 instances across separate AWS Regions with database replication.",
      "C": "Migrate the web application to Amazon EC2 instances that run in an Auto Scaling group across two Availability Zones behind an Application Load Balancer. Migrate the database to Amazon RDS with Multi-AZ deployment.",
      "D": "Migrate the web application to three Amazon EC2 instances across three Availability Zones behind an Application Load Balancer. Migrate the database to three EC2 instances across three Availability Zones."
    },
    "correct_answer": "C",
    "vote_percentage": "88%",
    "question_cn": "一家电子商务公司正在将其本地工作负载迁移到 AWS 云。该工作负载目前包含一个 Web 应用程序和一个用于存储的后端 Microsoft SQL 数据库。该公司预计在促销活动期间将有大量的客户。AWS 云中的新基础设施必须具有高可用性和可扩展性。哪种解决方案将以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "将 Web 应用程序迁移到两个 Amazon EC2 实例，这些实例跨越两个可用区，位于 Application Load Balancer 之后。将数据库迁移到 Amazon RDS for Microsoft SQL Server，并在两个可用区中设置读取副本。",
      "B": "将 Web 应用程序迁移到在跨越两个可用区的 Auto Scaling 组中运行的 Amazon EC2 实例，位于 Application Load Balancer 之后。将数据库迁移到跨越不同 AWS 区域的两个 EC2 实例，并进行数据库复制。",
      "C": "将 Web 应用程序迁移到在跨越两个可用区的 Auto Scaling 组中运行的 Amazon EC2 实例，位于 Application Load Balancer 之后。将数据库迁移到具有 Multi-AZ 部署的 Amazon RDS。",
      "D": "将 Web 应用程序迁移到三个 Amazon EC2 实例，这些实例跨越三个可用区，位于 Application Load Balancer 之后。将数据库迁移到跨越三个可用区的三个 EC2 实例。"
    },
    "tags": [
      "Amazon EC2",
      "Application Load Balancer",
      "Amazon RDS",
      "Auto Scaling",
      "Multi-AZ",
      "High Availability",
      "Scalability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 88%），解析仅供参考。】\n\n考查在 AWS 云上构建高可用性（High Availability）和可扩展性（Scalability）架构，并降低管理开销。主要涉及 Web 应用程序和数据库的部署方案，以及不同 AWS 服务在满足这些需求方面的选择和比较。",
      "why_correct": "选项 C 提供了满足高可用性、可扩展性和低管理开销的解决方案。Web 应用程序部署在 Auto Scaling 组中的 EC2 实例上，并位于 Application Load Balancer 之后，这提供了可扩展性和负载均衡。数据库使用 Amazon RDS，并启用了 Multi-AZ 部署，确保了高可用性和故障转移能力。Amazon RDS 简化了数据库管理，降低了管理开销。",
      "why_wrong": "选项 A 同样考虑了 Web 应用程序的高可用性，但数据库使用读取副本，这不直接提供高可用性，且管理开销较大。选项 B 将数据库复制到不同的 AWS 区域，增加了复杂性，且跨区域复制的延迟可能不满足需求。选项 D 使用了三个 EC2 实例，这增加了不必要的成本和管理复杂度；数据库没有使用 Amazon RDS，而是手动管理，增加了管理开销，并且三个可用区并不一定能提供比 Multi-AZ RDS 更高的可用性，除非配置得当。这些选项都未能像选项 C 一样，在满足需求的同时最大程度降低管理开销。"
    },
    "related_terms": [
      "Amazon EC2",
      "Application Load Balancer",
      "Amazon RDS",
      "Auto Scaling",
      "Multi-AZ",
      "Microsoft SQL Server",
      "Availability Zone",
      "Web Application"
    ]
  },
  {
    "id": 844,
    "topic": "1",
    "question_en": "A company has an on-premises business application that generates hundreds of files each day. These files are stored on an SMB file share and require a low-latency connection to the application servers. A new company policy states all application-generated files must be copied to AWS. There is already a VPN connection to AWS. The application development team does not have time to make the necessary code modifications to move the application to AWS. Which service should a solutions architect recommend to allow the application to copy files to AWS?",
    "options_en": {
      "A": "Amazon Elastic File System (Amazon EFS)",
      "B": "Amazon FSx for Windows File Server",
      "C": "AWS Snowball",
      "D": "AWS Storage Gateway"
    },
    "correct_answer": "B",
    "vote_percentage": "50%",
    "question_cn": "一家公司有一个本地业务应用程序，该应用程序每天生成数百个文件。这些文件存储在 SMB 文件共享上，并且需要与应用程序服务器建立低延迟连接。一项新的公司策略规定，所有应用程序生成的文件都必须复制到 AWS。已经有一个到 AWS 的 VPN 连接。应用程序开发团队没有时间对代码进行必要的修改以将应用程序移至 AWS。解决方案架构师应该推荐哪种服务来允许应用程序将文件复制到 AWS？",
    "options_cn": {
      "A": "Amazon Elastic File System (Amazon EFS)",
      "B": "Amazon FSx for Windows File Server",
      "C": "AWS Snowball",
      "D": "AWS Storage Gateway"
    },
    "tags": [
      "AWS Storage Gateway",
      "File Gateway",
      "VPN",
      "SMB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 50%），解析仅供参考。】\n\n考察本地 SMB 文件共享文件向 AWS 迁移的场景，以及如何通过现有 VPN 连接实现。重点在于选择能够与本地 SMB 兼容且易于部署的服务。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：Amazon FSx for Windows File Server 提供了与本地 SMB 兼容的完全托管的 Windows 文件服务器。它通过 VPN 连接允许应用程序直接写入文件到 FSx。由于题目要求低延迟连接和无需修改应用程序，FSx for Windows File Server 能够满足这些需求，因为它模拟了 SMB 文件共享的功能。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nAmazon EFS 虽然也是文件存储服务，但它与本地 SMB 不兼容，无法直接使用。AWS Snowball 适用于大量数据的离线迁移，不适用于每天数百个文件的小量数据增量迁移，并且会引入额外的时延。AWS Storage Gateway 可以作为文件网关连接到 AWS，但不如 FSx for Windows File Server 直接，且可能引入额外的延迟，并且也无法与应用程序无缝集成。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Elastic File System (Amazon EFS)",
      "Amazon FSx for Windows File Server",
      "AWS Snowball",
      "AWS Storage Gateway",
      "SMB",
      "VPN",
      "AWS"
    ]
  },
  {
    "id": 845,
    "topic": "1",
    "question_en": "A company has 15 employees. The company stores employee start dates in an Amazon DynamoDB table. The company wants to send an email message to each employee on the day of the employee's work anniversary. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create a script that scans the DynamoDB table and uses Amazon Simple Notification Service (Amazon SNS) to send email messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2 instance.",
      "B": "Create a script that scans the DynamoDB table and uses Amazon Simple Queue Service (Amazon SQS) to send email messages to employees when necessary. Use a cron job to run this script every day on an Amazon EC2 instance.",
      "C": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple Notification Service (Amazon SNS) to send email messages to employees when necessary. Schedule this Lambda function to run every day.",
      "D": "Create an AWS Lambda function that scans the DynamoDB table and uses Amazon Simple Queue Service (Amazon SQS) to send email messages to employees when necessary. Schedule this Lambda function to run every day."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司有 15 名员工。该公司将员工开始日期存储在 Amazon DynamoDB 表中。该公司希望在员工的周年纪念日向每位员工发送一封电子邮件。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "创建一个脚本，扫描 DynamoDB 表，并在必要时使用 Amazon Simple Notification Service (Amazon SNS) 向员工发送电子邮件。使用 cron 作业每天在 Amazon EC2 实例上运行此脚本。",
      "B": "创建一个脚本，扫描 DynamoDB 表，并在必要时使用 Amazon Simple Queue Service (Amazon SQS) 向员工发送电子邮件。使用 cron 作业每天在 Amazon EC2 实例上运行此脚本。",
      "C": "创建一个 AWS Lambda 函数，扫描 DynamoDB 表，并在必要时使用 Amazon Simple Notification Service (Amazon SNS) 向员工发送电子邮件。将此 Lambda 函数安排为每天运行。",
      "D": "创建一个 AWS Lambda 函数，扫描 DynamoDB 表，并在必要时使用 Amazon Simple Queue Service (Amazon SQS) 向员工发送电子邮件。将此 Lambda 函数安排为每天运行。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon SNS",
      "AWS Lambda",
      "cron",
      "Amazon EC2",
      "Amazon SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考查如何使用 AWS 服务，以最有效的方式实现根据 DynamoDB 表中的数据，定时向员工发送邮件的功能，涉及 Lambda 函数的调度、SNS 的使用以及与 EC2、SQS 的对比。",
      "why_correct": "选项 C 提供了最有效率的解决方案。它使用 AWS Lambda 函数来扫描 DynamoDB 表，并利用 Amazon SNS 发送邮件。通过将 Lambda 函数设置为每天运行，可以自动处理周年纪念日的邮件发送。Lambda 函数的无服务器特性使其无需管理服务器，并按需付费，从而实现最高的运营效率。",
      "why_wrong": "选项 A 和 B 都涉及在 EC2 实例上运行脚本，这增加了管理开销，包括服务器的配置、维护和监控，与题目要求的运营效率最高相悖。选项 A 使用 SNS 发送邮件，这没问题，但 EC2 实例的维护成本较高。选项 B 使用 SQS 发送邮件，虽然 SQS 可以提供异步处理，但对于简单的邮件发送任务来说，引入 SQS 增加了复杂性，且同样需要 EC2 实例来处理。选项 D 使用 Lambda 和 SQS，虽然 Lambda 简化了处理流程，但引入 SQS 增加了不必要的复杂性，对于简单的通知场景而言是过度的。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon SNS",
      "AWS Lambda",
      "Amazon EC2",
      "Amazon SQS",
      "cron"
    ]
  },
  {
    "id": 846,
    "topic": "1",
    "question_en": "A company’s application is running on Amazon EC2 instances within an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. Based on the application's history, the company anticipates a spike in trafic during a holiday each year. A solutions architect must design a strategy to ensure that the Auto Scaling group proactively increases capacity to minimize any performance impact on application users. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon CloudWatch alarm to scale up the EC2 instances when CPU utilization exceeds 90%.",
      "B": "Create a recurring scheduled action to scale up the Auto Scaling group before the expected period of peak demand.",
      "C": "Increase the minimum and maximum number of EC2 instances in the Auto Scaling group during the peak demand period.",
      "D": "Configure an Amazon Simple Notification Service (Amazon SNS) notification to send alerts when there are autoscaling:EC2_INSTANCE_LAUNCH events."
    },
    "correct_answer": "B",
    "vote_percentage": "91%",
    "question_cn": "一家公司的应用程序在弹性负载平衡 (ELB) 负载均衡器后面的 Auto Scaling 组中的 Amazon EC2 实例上运行。根据应用程序的历史记录，该公司预计每年假期期间流量会激增。解决方案架构师必须设计一种策略，以确保 Auto Scaling 组主动增加容量，以最大限度地减少对应用程序用户的任何性能影响。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon CloudWatch 警报，当 CPU 利用率超过 90% 时，扩大 EC2 实例的规模。",
      "B": "创建一个定期计划的操作，在预计的峰值需求时段之前扩大 Auto Scaling 组的规模。",
      "C": "在峰值需求期间增加 Auto Scaling 组中 EC2 实例的最小和最大数量。",
      "D": "配置一个 Amazon Simple Notification Service (Amazon SNS) 通知，以在发生 autoscaling:EC2_INSTANCE_LAUNCH 事件时发送警报。"
    },
    "tags": [
      "Auto Scaling",
      "EC2",
      "CloudWatch",
      "ELB",
      "Amazon SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 91%），解析仅供参考。】\n\n考察 Auto Scaling 组的容量规划及应对流量峰值的策略，涉及对预期的流量高峰进行主动扩容。与 CloudWatch 监控、定时任务、以及 Auto Scaling 的配置相关。",
      "why_correct": "选项 B 创建了一个定期计划的操作，在预计的峰值需求时段之前扩大 Auto Scaling 组的规模。这种方法允许在流量激增发生之前主动扩展容量，从而减少对应用程序用户的性能影响。通过预先扩展，可以确保在峰值到来时有足够的 EC2 实例来处理增加的负载。",
      "why_wrong": "选项 A 通过 CloudWatch 警报响应 CPU 利用率，属于被动响应，无法提前扩容，在流量激增的开始时可能已经对用户造成影响。选项 C 虽然调整了 Auto Scaling 组的最小和最大实例数量，但不会主动触发扩容，它只是限制了 Auto Scaling 的伸缩范围，并不能保证在峰值到来之前有足够的容量。选项 D 配置 Amazon SNS 通知，仅在 EC2 实例启动时发送通知，这对于提前扩容没有帮助，并且它也不会主动扩大 Auto Scaling 组的规模，只是一种事件通知机制。"
    },
    "related_terms": [
      "Auto Scaling",
      "EC2",
      "CloudWatch",
      "ELB",
      "Amazon SNS",
      "CPU utilization",
      "Auto Scaling Group"
    ]
  },
  {
    "id": 847,
    "topic": "1",
    "question_en": "A company uses Amazon RDS for PostgreSQL databases for its data tier. The company must implement password rotation for the databases. Which solution meets this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the password in AWS Secrets Manager. Enable automatic rotation on the secret.",
      "B": "Store the password in AWS Systems Manager Parameter Store. Enable automatic rotation on the parameter.",
      "C": "Store the password in AWS Systems Manager Parameter Store. Write an AWS Lambda function that rotates the password.",
      "D": "Store the password in AWS Key Management Service (AWS KMS). Enable automatic rotation on the AWS KMS key."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon RDS for PostgreSQL 数据库作为其数据层。该公司必须为数据库实施密码轮换。哪个解决方案以最少的运营开销满足此要求？",
    "options_cn": {
      "A": "将密码存储在 AWS Secrets Manager 中。在密钥上启用自动轮换。",
      "B": "将密码存储在 AWS Systems Manager Parameter Store 中。在参数上启用自动轮换。",
      "C": "将密码存储在 AWS Systems Manager Parameter Store 中。编写一个 AWS Lambda 函数来轮换密码。",
      "D": "将密码存储在 AWS Key Management Service (AWS KMS) 中。在 AWS KMS 密钥上启用自动轮换。"
    },
    "tags": [
      "Amazon RDS",
      "PostgreSQL",
      "AWS Secrets Manager",
      "Secrets Manager",
      "Automatic rotation",
      "AWS Systems Manager Parameter Store",
      "Parameter Store",
      "AWS Lambda",
      "Lambda",
      "AWS KMS",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 RDS 数据库密码轮换的解决方案选择；与 Secrets Manager 和 Parameter Store 的特性对比、以及 Lambda 函数的定制化实现相关。",
      "why_correct": "AWS Secrets Manager 专门设计用于安全存储和管理密码等敏感信息。Secrets Manager 提供了自动轮换密钥的功能，可以定期更改密码。这种方法简化了密码管理，减少了运营开销，并确保了安全性。",
      "why_wrong": "选项 B 错误，因为 Parameter Store 虽然可以存储秘密，但其自动轮换功能不如 Secrets Manager 完善，更侧重于配置参数的存储。选项 C 错误，因为使用 Parameter Store 并结合 Lambda 函数轮换密码虽然可行，但增加了额外的复杂性，增加了运营负担。选项 D 错误，因为 AWS KMS 主要用于加密密钥的管理，不直接支持密码的存储和自动轮换，这需要 Secrets Manager 的配合才能实现，不如直接使用 Secrets Manager 方便。"
    },
    "related_terms": [
      "Amazon RDS",
      "PostgreSQL",
      "AWS Secrets Manager",
      "Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "Parameter Store",
      "AWS Lambda",
      "Lambda",
      "AWS KMS",
      "KMS",
      "Automatic rotation"
    ]
  },
  {
    "id": 848,
    "topic": "1",
    "question_en": "A company runs its application on Oracle Database Enterprise Edition. The company needs to migrate the application and the database to AWS. The company can use the Bring Your Own License (BYOL) model while migrating to AWS. The application uses third-party database features that require privileged access. A solutions architect must design a solution for the database migration. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Migrate the database to Amazon RDS for Oracle by using native tools. Replace the third-party features with AWS Lambda.",
      "B": "Migrate the database to Amazon RDS Custom for Oracle by using native tools. Customize the new database settings to support the third-party features.",
      "C": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS). Customize the new database settings to support the third-party features.",
      "D": "Migrate the database to Amazon RDS for PostgreSQL by using AWS Database Migration Service (AWS DMS). Rewrite the application code to remove the dependency on third-party features."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司在其 Oracle Database Enterprise Edition 上运行其应用程序。该公司需要将应用程序和数据库迁移到 AWS。该公司可以在迁移到 AWS 时使用自带许可证 (BYOL) 模型。该应用程序使用需要特权访问的第三方数据库功能。解决方案架构师必须为数据库迁移设计一个解决方案。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用原生工具将数据库迁移到 Amazon RDS for Oracle。用 AWS Lambda 替换第三方功能。",
      "B": "使用原生工具将数据库迁移到 Amazon RDS Custom for Oracle。自定义新的数据库设置以支持第三方功能。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 将数据库迁移到 Amazon DynamoDB。自定义新的数据库设置以支持第三方功能。",
      "D": "使用 AWS Database Migration Service (AWS DMS) 将数据库迁移到 Amazon RDS for PostgreSQL。重写应用程序代码以移除对第三方功能的依赖。"
    },
    "tags": [
      "Amazon RDS Custom for Oracle",
      "BYOL",
      "Database Migration",
      "Oracle Database",
      "AWS Lambda",
      "Amazon RDS for Oracle",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon DynamoDB",
      "Amazon RDS for PostgreSQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n本题考查数据库迁移策略，特别是针对 Oracle 数据库的 BYOL（Bring Your Own License）场景。 题目要求在保证功能需求（第三方数据库功能）的前提下，选择最具成本效益的迁移方案。这需要对不同 RDS 选项、AWS DMS 和 Lambda 的适用场景有清晰的认识，并与 BYOL 模式结合考虑。",
      "why_correct": "Amazon RDS Custom for Oracle 允许用户在 Amazon EC2 实例上运行 Oracle 数据库，并完全控制数据库引擎和操作系统。这使得用户可以使用自带许可证 (BYOL) 模型，并支持第三方数据库功能。 通过自定义数据库设置，可以在 RDS Custom for Oracle 上保留和使用这些第三方功能，满足题目中对特权访问和数据库迁移的要求，同时避免了应用程序重构。",
      "why_wrong": "选项 A 错误，因为 Amazon RDS for Oracle 不支持第三方数据库功能，并且通常不兼容 BYOL 模式，除非选择特定的许可选项。 选项 C 错误，因为将 Oracle 数据库迁移到 Amazon DynamoDB 不适用，Amazon DynamoDB 是一种 NoSQL 数据库，不兼容 Oracle 的 SQL 数据库。此外，迁移也需要应用程序的重大重构。选项 D 错误，因为将 Oracle 数据库迁移到 Amazon RDS for PostgreSQL 意味着数据库引擎的完全更改，需要应用程序代码的重写，这增加了迁移的复杂性和成本，与成本效益的考量相悖，并且无法保留第三方数据库功能。"
    },
    "related_terms": [
      "Amazon RDS for Oracle",
      "AWS Lambda",
      "AWS Database Migration Service (AWS DMS)",
      "Amazon DynamoDB",
      "Amazon RDS for PostgreSQL",
      "Amazon EC2",
      "Oracle Database",
      "Amazon RDS Custom for Oracle",
      "BYOL"
    ]
  },
  {
    "id": 849,
    "topic": "1",
    "question_en": "A large international university has deployed all of its compute services in the AWS Cloud. These services include Amazon EC2, Amazon RDS, and Amazon DynamoDB. The university currently relies on many custom scripts to back up its infrastructure. However, the university wants to centralize management and automate data backups as much as possible by using AWS native options. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use third-party backup software with an AWS Storage Gateway tape gateway virtual tape library.",
      "B": "Use AWS Backup to configure and monitor all backups for the services in use.",
      "C": "Use AWS Config to set lifecycle management to take snapshots of all data sources on a schedule.",
      "D": "Use AWS Systems Manager State Manager to manage the configuration and monitoring of backup tasks."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一所大型国际大学已在 AWS 云中部署了其所有计算服务。这些服务包括 Amazon EC2、Amazon RDS 和 Amazon DynamoDB。该大学目前依赖许多自定义脚本来备份其基础设施。但是，该大学希望通过使用 AWS 原生选项来集中管理并尽可能多地自动化数据备份。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用第三方备份软件与 AWS Storage Gateway 磁带网关虚拟磁带库。",
      "B": "使用 AWS Backup 为正在使用的服务配置和监控所有备份。",
      "C": "使用 AWS Config 设置生命周期管理，以按计划拍摄所有数据源的快照。",
      "D": "使用 AWS Systems Manager State Manager 来管理备份任务的配置和监控。"
    },
    "tags": [
      "AWS Backup",
      "Amazon EC2",
      "Amazon RDS",
      "Amazon DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何使用 AWS 原生服务集中管理和自动化数据备份，重点在于 AWS Backup 的功能与适用场景。这与备份策略、数据保护、以及不同 AWS 服务（如 EC2、RDS、DynamoDB）的备份方式相关。",
      "why_correct": "AWS Backup 是一款集中式备份服务，支持跨 AWS 服务的备份和恢复，包括 EC2、RDS 和 DynamoDB。它允许用户定义备份计划、设置保留期、监控备份状态，并提供合规性报告。AWS Backup 完全符合题目的需求，即使用 AWS 原生服务来集中管理和自动化备份任务。",
      "why_wrong": "选项 A 使用第三方备份软件和 AWS Storage Gateway 磁带网关。虽然 Storage Gateway 可以用于备份，但引入第三方软件增加了复杂性，且并非 AWS 原生解决方案。选项 C 使用 AWS Config，虽然可以追踪资源配置变更，但其主要功能并非备份，而是配置审计和合规性管理。 选项 D 使用 AWS Systems Manager State Manager，这主要用于配置管理和自动化任务，而不是专门用于数据备份。它虽然可以用于执行备份任务，但功能不如 AWS Backup 强大，且不够集中和自动化，不能完全满足题目对备份的需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Amazon DynamoDB",
      "AWS Backup",
      "AWS Storage Gateway",
      "AWS Config",
      "AWS Systems Manager State Manager"
    ]
  },
  {
    "id": 850,
    "topic": "1",
    "question_en": "A company wants to build a map of its IT infrastructure to identify and enforce policies on resources that pose security risks. The company's security team must be able to query data in the IT infrastructure map and quickly identify security risks. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon RDS to store the data. Use SQL to query the data to identify security risks.",
      "B": "Use Amazon Neptune to store the data. Use SPARQL to query the data to identify security risks.",
      "C": "Use Amazon Redshift to store the data. Use SQL to query the data to identify security risks.",
      "D": "Use Amazon DynamoDB to store the data. Use PartiQL to query the data to identify security risks."
    },
    "correct_answer": "B",
    "vote_percentage": "86%",
    "question_cn": "一家公司希望构建其 IT 基础设施的地图，以识别和执行对构成安全风险的资源的策略。该公司的安全团队必须能够查询 IT 基础设施地图中的数据并快速识别安全风险。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon RDS 存储数据。使用 SQL 查询数据以识别安全风险。",
      "B": "使用 Amazon Neptune 存储数据。使用 SPARQL 查询数据以识别安全风险。",
      "C": "使用 Amazon Redshift 存储数据。使用 SQL 查询数据以识别安全风险。",
      "D": "使用 Amazon DynamoDB 存储数据。使用 PartiQL 查询数据以识别安全风险。"
    },
    "tags": [
      "Amazon Neptune",
      "SPARQL",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 86%），解析仅供参考。】\n\n考查构建 IT 基础设施地图并快速识别安全风险的场景下，如何选择存储方案。这与数据结构、查询语言和运营开销密切相关，需要考虑图数据库的优势。",
      "why_correct": "Amazon Neptune 是一种图数据库服务，非常适合存储和查询高度互连的数据，例如 IT 基础设施的地图。SPARQL 是 Neptune 支持的查询语言，用于有效地查询图数据并识别安全风险。由于 Neptune 专门为图数据设计，因此它能够提供比关系数据库、数据仓库或 NoSQL 数据库更优的查询性能和更适合安全分析的查询功能。",
      "why_wrong": "选项 A 使用 Amazon RDS 存储数据，虽然 RDS 能够存储数据并使用 SQL 查询，但关系数据库通常不适合存储互连复杂的 IT 基础设施图数据。选项 C 使用 Amazon Redshift 存储数据，Redshift 是一种数据仓库服务，主要用于分析大型数据集，但它也不是为图数据设计的。选项 D 使用 Amazon DynamoDB 存储数据，并使用 PartiQL 查询，DynamoDB 是一种 NoSQL 数据库，虽然可以存储数据，但其不擅长处理图数据，使用 PartiQL 查询进行安全风险识别的效率不如图数据库。"
    },
    "related_terms": [
      "Amazon RDS",
      "SQL",
      "Amazon Neptune",
      "SPARQL",
      "Amazon Redshift",
      "Amazon DynamoDB",
      "PartiQL"
    ]
  },
  {
    "id": 851,
    "topic": "1",
    "question_en": "A large company wants to provide its globally located developers separate, limited size, managed PostgreSQL databases for development purposes. The databases will be low volume. The developers need the databases only when they are actively working. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Give the developers the ability to launch separate Amazon Aurora instances. Set up a process to shut down Aurora instances at the end of the workday and to start Aurora instances at the beginning of the next workday.",
      "B": "Develop an AWS Service Catalog product that enforces size restrictions for launching Amazon Aurora instances. Give the developers access to launch the product when they need a development database.",
      "C": "Create an Amazon Aurora Serverless cluster. Develop an AWS Service Catalog product to launch databases in the cluster with the default capacity settings. Grant the developers access to the product.",
      "D": "Monitor AWS Trusted Advisor checks for idle Amazon RDS databases. Create a process to terminate identified idle RDS databases."
    },
    "correct_answer": "C",
    "vote_percentage": "75%",
    "question_cn": "一家大型公司希望为其全球的开发人员提供独立的、大小受限的、托管的 PostgreSQL 数据库，用于开发目的。这些数据库的用量很低。开发人员仅在他们积极工作时才需要这些数据库。哪种解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "让开发人员能够启动独立的 Amazon Aurora 实例。设置一个流程，在工作日结束时关闭 Aurora 实例，并在下一个工作日开始时启动 Aurora 实例。",
      "B": "开发一个 AWS Service Catalog 产品，该产品强制执行启动 Amazon Aurora 实例的大小限制。让开发人员在需要开发数据库时访问启动该产品。",
      "C": "创建一个 Amazon Aurora Serverless 集群。开发一个 AWS Service Catalog 产品，以默认容量设置在集群中启动数据库。授予开发人员访问该产品的权限。",
      "D": "监控 AWS Trusted Advisor 检查，检查闲置的 Amazon RDS 数据库。创建一个流程来终止已识别的闲置 RDS 数据库。"
    },
    "tags": [
      "Amazon Aurora",
      "Aurora Serverless",
      "AWS Service Catalog",
      "Amazon RDS",
      "AWS Trusted Advisor"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 75%），解析仅供参考。】\n\n本题考察 Aurora Serverless 的适用场景，以及 Service Catalog 的使用。与 Aurora 的不同部署模式（如 Provisioned、Multi-AZ）以及 RDS 的闲置资源管理相关。",
      "why_correct": "Amazon Aurora Serverless 集群可以根据数据库活动自动启动、关闭和调整容量，非常适合间歇性使用、负载不稳定的场景。通过 AWS Service Catalog，可以为开发人员提供受控的数据库创建流程，限制数据库的配置，并确保数据库大小符合公司的要求。这种组合方案能够以最具成本效益的方式满足题目中对低负载、按需使用的开发数据库的需求。",
      "why_wrong": {
        "A": "让开发人员手动启动和关闭 Aurora 实例虽然可行，但管理成本较高。人为的启动/关闭流程容易出错，且无法完全匹配 Aurora Serverless 的自动伸缩特性。并且，如果关闭时间控制不准确，也会造成资源浪费。",
        "B": "使用 Service Catalog 可以限制 Aurora 实例的大小，但手动管理 Aurora 实例的启动和关闭仍然增加了运营开销。此外，Aurora 的 Provisioned 模式仍然存在资源预留，即使不使用，也会产生一定的费用，不如 Aurora Serverless 灵活。",
        "D": "监控 AWS Trusted Advisor 的检查以识别并终止闲置的 RDS 数据库，适用于优化现有 RDS 数据库资源。然而，这并不能主动满足按需创建和使用的需求，开发人员还需要等待 RDS 数据库的创建，且 RDS 实例没有自动伸缩特性，不利于快速响应开发需求。 此外，Trusted Advisor 仅能提供检查和建议，需要手动或额外流程去处理。"
      }
    },
    "related_terms": [
      "Amazon Aurora",
      "Aurora Serverless",
      "AWS Service Catalog",
      "Amazon RDS",
      "Aurora",
      "Multi-AZ",
      "Trusted Advisor",
      "Provisioned"
    ]
  },
  {
    "id": 852,
    "topic": "1",
    "question_en": "A company is building a web application that serves a content management system. The content management system runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. Users are constantly adding and updating files, blogs, and other website assets in the content management system. A solutions architect must implement a solution in which all the EC2 instances share up-to-date website content with the least possible lag time. Which solution meets these requirements?",
    "options_en": {
      "A": "Update the EC2 user data in the Auto Scaling group lifecycle policy to copy the website assets from the EC2 instance that was launched most recently. Configure the ALB to make changes to the website assets only in the newest EC2 instance.",
      "B": "Copy the website assets to an Amazon Elastic File System (Amazon EFS) file system. Configure each EC2 instance to mount the EFS file system locally. Configure the website hosting application to reference the website assets that are stored in the EFS file system.",
      "C": "Copy the website assets to an Amazon S3 bucket. Ensure that each EC2 instance downloads the website assets from the S3 bucket to the attached Amazon Elastic Block Store (Amazon EBS) volume. Run the S3 sync command once each hour to keep files up to date.",
      "D": "Restore an Amazon Elastic Block Store (Amazon EBS) snapshot with the website assets. Attach the EBS snapshot as a secondary EBS volume when a new EC2 instance is launched. Configure the website hosting application to reference the website assets that are stored in the secondary EBS volume."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在构建一个为内容管理系统提供服务的 Web 应用程序。内容管理系统在应用程序负载均衡器 (ALB) 后的 Amazon EC2 实例上运行。EC2 实例在多个可用区中的 Auto Scaling 组中运行。用户不断在内容管理系统中添加和更新文件、博客和其他网站资产。解决方案架构师必须实施一种解决方案，其中所有 EC2 实例共享最新的网站内容，且滞后时间最短。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "更新 Auto Scaling 组生命周期策略中的 EC2 用户数据，以从最近启动的 EC2 实例复制网站资产。配置 ALB 仅在新 EC2 实例中更改网站资产。",
      "B": "将网站资产复制到 Amazon Elastic File System (Amazon EFS) 文件系统。将每个 EC2 实例配置为在本地挂载 EFS 文件系统。配置网站托管应用程序以引用存储在 EFS 文件系统中的网站资产。",
      "C": "将网站资产复制到 Amazon S3 存储桶。确保每个 EC2 实例从 S3 存储桶下载网站资产到连接的 Amazon Elastic Block Store (Amazon EBS) 卷。定期间隔运行 S3 同步命令以保持文件最新。",
      "D": "使用网站资产还原 Amazon Elastic Block Store (Amazon EBS) 快照。在启动新 EC2 实例时，将 EBS 快照作为辅助 EBS 卷连接。配置网站托管应用程序以引用存储在辅助 EBS 卷中的网站资产。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon EFS",
      "Amazon EBS",
      "Amazon S3",
      "Application Load Balancer (ALB)"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n考查如何在 Auto Scaling 环境下，实现多个 EC2 实例共享网站资产，并保证最低延迟。涉及 EFS、EBS、S3 的选型与对比。",
      "why_correct": "将网站资产复制到 Amazon EFS 文件系统，并挂载到每个 EC2 实例是最佳解决方案。EFS 提供了一个可横向扩展、高度可用且持久的文件系统，非常适合需要在多个 EC2 实例之间共享文件的场景。通过挂载 EFS 文件系统，所有 EC2 实例都能实时访问和更新网站资产，从而实现最低延迟，且无需额外的同步机制。",
      "why_wrong": "A 方案中，使用 EC2 用户数据来复制资产，存在诸多问题。首先，用户数据运行于实例启动时，如果资产很大，会导致启动时间变长，影响扩展速度。其次，ALB 仅在新实例中更改网站资产，这无法保证所有实例拥有最新的资产版本，因为旧实例可能未及时更新。C 方案中，虽然使用 Amazon S3 存储桶可以存储资产，但每个实例需要定期从 S3 下载，增加了延迟，且需要维护 S3 同步命令。D 方案中，使用 EBS 快照作为辅助卷，问题在于快照并非实时更新，当有更新时，需要重新创建快照，并连接到新实例，这会带来延迟，且在更新时，需要复杂的管理流程，无法满足题目的低延迟需求。此外，EBS 卷的容量限制和单实例的访问特性，也不适合多实例共享文件。使用 EBS 快照会使数据在实例之间同步的开销远高于 EFS。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon EFS",
      "Amazon EBS",
      "Amazon S3",
      "EC2",
      "EFS",
      "EBS",
      "S3",
      "ALB",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 853,
    "topic": "1",
    "question_en": "A company's web application consists of multiple Amazon EC2 instances that run behind an Application Load Balancer in a VPC. An Amazon RDS for MySQL DB instance contains the data. The company needs the ability to automatically detect and respond to suspicious or unexpected behavior in its AWS environment. The company already has added AWS WAF to its architecture. What should a solutions architect do next to protect against threats?",
    "options_en": {
      "A": "Use Amazon GuardDuty to perform threat detection. Configure Amazon EventBridge to filter for GuardDuty findings and to invoke an AWS Lambda function to adjust the AWS WAF rules.",
      "B": "Use AWS Firewall Manager to perform threat detection. Configure Amazon EventBridge to filter for Firewall Manager findings and to invoke an AWS Lambda function to adjust the AWS WAF web ACL.",
      "C": "Use Amazon Inspector to perform threat detection and to update the AWS WAF rules. Create a VPC network ACL to limit access to the web application.",
      "D": "Use Amazon Macie to perform threat detection and to update the AWS WAF rules. Create a VPC network ACL to limit access to the web application."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司的 Web 应用程序由多个在 VPC 中的 Application Load Balancer 后面运行的 Amazon EC2 实例组成。Amazon RDS for MySQL 数据库实例包含数据。该公司需要能够自动检测和响应其 AWS 环境中可疑或意外行为的能力。该公司已在其架构中添加了 AWS WAF。解决方案架构师接下来应该做什么来防范威胁？",
    "options_cn": {
      "A": "使用 Amazon GuardDuty 执行威胁检测。配置 Amazon EventBridge 以筛选 GuardDuty 结果并调用 AWS Lambda 函数以调整 AWS WAF 规则。",
      "B": "使用 AWS Firewall Manager 执行威胁检测。配置 Amazon EventBridge 以筛选 Firewall Manager 结果并调用 AWS Lambda 函数以调整 AWS WAF Web ACL。",
      "C": "使用 Amazon Inspector 执行威胁检测并更新 AWS WAF 规则。创建 VPC 网络 ACL 以限制对 Web 应用程序的访问。",
      "D": "使用 Amazon Macie 执行威胁检测并更新 AWS WAF 规则。创建 VPC 网络 ACL 以限制对 Web 应用程序的访问。"
    },
    "tags": [
      "Amazon GuardDuty",
      "Amazon EventBridge",
      "AWS Lambda",
      "AWS WAF",
      "Threat Detection",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在现有架构中部署威胁检测与自动响应。题目要求在已部署 AWS WAF 的环境中增强安全防护，重点关注威胁检测与响应的自动化。这与 GuardDuty、EventBridge 和 Lambda 的结合使用有关。",
      "why_correct": "Amazon GuardDuty 是一种威胁检测服务，可以持续监控恶意活动，并提供安全问题警报。结合 Amazon EventBridge，可以根据 GuardDuty 的结果触发操作，例如调用 Lambda 函数。Lambda 函数可以动态地调整 AWS WAF 规则，以应对检测到的威胁，实现自动化的安全响应。",
      "why_wrong": "B. AWS Firewall Manager 主要用于集中管理和部署 AWS WAF 规则，并非直接用于威胁检测。虽然它能管理 WAF 规则，但本身不具备威胁检测能力。C. Amazon Inspector 专注于 EC2 实例的漏洞评估和合规性检查，而非针对网络威胁的检测。同时，创建 VPC 网络 ACL 用于限制访问，虽然是安全措施之一，但它不直接与检测威胁和自动响应相关联，也无法动态调整 WAF 规则。D. Amazon Macie 用于发现和保护敏感数据，不适用于检测网络威胁。创建 VPC 网络 ACL 的方式同 C，无法实现自动响应和 WAF 规则的动态调整。"
    },
    "related_terms": [
      "Amazon GuardDuty",
      "Amazon EventBridge",
      "AWS Lambda",
      "AWS WAF",
      "Amazon Inspector",
      "Amazon Macie",
      "VPC",
      "EC2",
      "Application Load Balancer",
      "AWS Firewall Manager",
      "RDS for MySQL"
    ]
  },
  {
    "id": 854,
    "topic": "1",
    "question_en": "A company is planning to run a group of Amazon EC2 instances that connect to an Amazon Aurora database. The company has built an AWS CloudFormation template to deploy the EC2 instances and the Aurora DB cluster. The company wants to allow the instances to authenticate to the database in a secure way. The company does not want to maintain static database credentials. Which solution meets these requirements with the LEAST operational effort?",
    "options_en": {
      "A": "Create a database user with a user name and password. Add parameters for the database user name and password to the CloudFormation template. Pass the parameters to the EC2 instances when the instances are launched.",
      "B": "Create a database user with a user name and password. Store the user name and password in AWS Systems Manager Parameter Store. Configure the EC2 instances to retrieve the database credentials from Parameter Store.",
      "C": "Configure the DB cluster to use IAM database authentication. Create a database user to use with IAM authentication. Associate a role with the EC2 instances to allow applications on the instances to access the database.",
      "D": "Configure the DB cluster to use IAM database authentication with an IAM user. Create a database user that has a name that matches the IAM user. Associate the IAM user with the EC2 instances to allow applications on the instances to access the database."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划运行一组连接到 Amazon Aurora 数据库的 Amazon EC2 实例。该公司构建了一个 AWS CloudFormation 模板来部署 EC2 实例和 Aurora 数据库集群。该公司希望允许实例以安全的方式向数据库进行身份验证。该公司不想维护静态数据库凭证。哪种解决方案以最少的运营工作量满足这些要求？",
    "options_cn": {
      "A": "使用用户名和密码创建数据库用户。将数据库用户名和密码的参数添加到 CloudFormation 模板。在启动实例时将参数传递给 EC2 实例。",
      "B": "使用用户名和密码创建数据库用户。将用户名和密码存储在 AWS Systems Manager Parameter Store 中。配置 EC2 实例从 Parameter Store 检索数据库凭证。",
      "C": "配置数据库集群以使用 IAM 数据库身份验证。创建一个数据库用户以与 IAM 身份验证一起使用。将一个角色与 EC2 实例关联，以允许实例上的应用程序访问数据库。",
      "D": "配置数据库集群以使用 IAM 数据库身份验证以及 IAM 用户。创建一个与 IAM 用户名匹配的数据库用户。将 IAM 用户与 EC2 实例关联，以允许实例上的应用程序访问数据库。"
    },
    "tags": [
      "Amazon Aurora",
      "EC2",
      "IAM",
      "IAM database authentication",
      "AWS CloudFormation",
      "AWS Systems Manager Parameter Store"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n本题考查 EC2 实例访问 Aurora 数据库的身份验证方式，重点关注 IAM 数据库身份验证以及减少运营工作量。需要理解 IAM 身份验证的原理、EC2 实例与 IAM 角色之间的关系，以及对比不同方案的运维成本和安全性。",
      "why_correct": "选项 C 正确。该方案配置 Aurora 数据库集群使用 IAM 数据库身份验证，并通过 IAM 角色将权限授予 EC2 实例。这样，EC2 实例上的应用程序可以通过 IAM 角色安全地访问数据库，无需存储或管理静态凭证。这种方法提供了更高的安全性，并减少了维护凭证的运营开销，与题干中“不想维护静态数据库凭证”的要求相符。",
      "why_wrong": "选项 A 错误。将数据库用户名和密码作为 CloudFormation 模板的参数传递给 EC2 实例，会使凭证暴露在模板中，安全性较差，且不符合“不想维护静态数据库凭证”的要求。 选项 B 错误。虽然使用 AWS Systems Manager Parameter Store 存储凭证比直接在 CloudFormation 模板中硬编码要好，但仍然涉及凭证的管理和轮换，增加了运营负担，且不如 IAM 数据库身份验证安全。选项 D 错误。使用 IAM 用户而不是 IAM 角色与 EC2 实例关联，无法满足 EC2 实例以安全的方式访问数据库的需求。IAM 用户直接与 EC2 实例关联不符合最佳实践，且增加了管理复杂性。"
    },
    "related_terms": [
      "Amazon Aurora",
      "EC2",
      "IAM",
      "AWS CloudFormation",
      "AWS Systems Manager Parameter Store",
      "IAM database authentication"
    ]
  },
  {
    "id": 855,
    "topic": "1",
    "question_en": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. The company does not want to use the default domain name for the distribution. Instead, the company wants to use a different domain name for the distribution. Which solution will deploy the certificate without incurring any additional costs?",
    "options_en": {
      "A": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-east-1 Region.",
      "B": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-west-1 Region.",
      "C": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region.",
      "D": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-west-1 Region."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望配置其 Amazon CloudFront 分配以使用 SSL/TLS 证书。该公司不想使用该分配的默认域名。相反，该公司希望使用该分配的不同域名。哪种解决方案将部署证书而不会产生任何额外费用？",
    "options_cn": {
      "A": "从 us-east-1 区域的 AWS Certificate Manager (ACM) 请求 Amazon 颁发的私有证书。",
      "B": "从 us-west-1 区域的 AWS Certificate Manager (ACM) 请求 Amazon 颁发的私有证书。",
      "C": "从 us-east-1 区域的 AWS Certificate Manager (ACM) 请求 Amazon 颁发的公共证书。",
      "D": "从 us-west-1 区域的 AWS Certificate Manager (ACM) 请求 Amazon 颁发的公共证书。"
    },
    "tags": [
      "CloudFront",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n考察 CloudFront 分配配置 SSL/TLS 证书的解决方案，以及 ACM 证书的可用性和区域限制。",
      "why_correct": "选项 C 正确。要使用自定义域名，CloudFront 需要一个 SSL/TLS 证书。使用 ACM 颁发的公共证书是免费的，并且可以与 CloudFront 完美集成。由于 CloudFront 是一个全局服务，它使用 us-east-1 区域的 ACM 证书。",
      "why_wrong": "选项 A 错误。ACM 颁发的私有证书无法免费使用。选项 B 错误。ACM 颁发的私有证书无法免费使用，并且 ACM 的区域与 CloudFront 关联性并不影响私有证书的付费使用。选项 D 错误。使用 ACM 颁发的公共证书是正确的，但 ACM 公共证书的区域应该是 us-east-1，而不是 us-west-1。"
    },
    "related_terms": [
      "CloudFront",
      "SSL/TLS",
      "ACM",
      "us-east-1",
      "us-west-1",
      "Amazon"
    ]
  },
  {
    "id": 856,
    "topic": "1",
    "question_en": "A company creates operations data and stores the data in an Amazon S3 bucket. For the company's annual audit, an external consultant needs to access an annual report that is stored in the S3 bucket. The external consultant needs to access the report for 7 days. The company must implement a solution to allow the external consultant access to only the report. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create a new S3 bucket that is configured to host a public static website. Migrate the operations data to the new S3 bucket. Share the S3 website URL with the external consultant.",
      "B": "Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the external consultant completes the audit.",
      "C": "Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to the external consultant. Revoke the access keys after 7 days.",
      "D": "Generate a presigned URL that has the required access to the location of the report on the S3 bucket. Share the presigned URL with the external consultant."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司创建运营数据并将数据存储在 Amazon S3 存储桶中。对于公司的年度审计，外部顾问需要访问存储在 S3 存储桶中的年度报告。外部顾问需要访问该报告 7 天。该公司必须实施一个解决方案，以允许外部顾问仅访问该报告。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 S3 存储桶，该存储桶配置为托管公共静态网站。将运营数据迁移到新的 S3 存储桶。与外部顾问共享 S3 网站 URL。",
      "B": "打开对 S3 存储桶的公共访问权限 7 天。当外部顾问完成审计时，移除对 S3 存储桶的访问权限。",
      "C": "创建一个新的 IAM 用户，该用户有权访问 S3 存储桶中的报告。向外部顾问提供访问密钥。7 天后撤销访问密钥。",
      "D": "生成一个预签名 URL，该 URL 具有对 S3 存储桶中报告位置的必要访问权限。与外部顾问共享预签名 URL。"
    },
    "tags": [
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察 S3 存储桶的访问控制和预签名 URL 的应用。预签名 URL 允许对 S3 中的特定对象进行临时访问，是提供访问的有效方法。",
      "why_correct": "选项 D 正确，预签名 URL 提供了对 S3 存储桶中特定对象的临时访问，满足了需求。",
      "why_wrong": "选项 A 错误，创建新的 S3 存储桶并配置为公共网站，违反了最小权限原则，操作效率低。选项 B 错误，直接开放 S3 存储桶的公共访问权限，存在安全风险。选项 C 错误，创建 IAM 用户并提供访问密钥，增加了管理复杂性，不够灵活。"
    },
    "related_terms": [
      "S3",
      "IAM",
      "预签名 URL"
    ]
  },
  {
    "id": 857,
    "topic": "1",
    "question_en": "A company plans to run a high performance computing (HPC) workload on Amazon EC2 Instances. The workload requires low-latency network performance and high network throughput with tightly coupled node-to-node communication. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the EC2 instances to be part of a cluster placement group.",
      "B": "Launch the EC2 instances with Dedicated Instance tenancy.",
      "C": "Launch the EC2 instances as Spot Instances.",
      "D": "Configure an On-Demand Capacity Reservation when the EC2 instances are launched."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司计划在 Amazon EC2 实例上运行高性能计算 (HPC) 工作负载。该工作负载需要低延迟的网络性能和高网络吞吐量，并具有紧密耦合的节点间通信。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 EC2 实例配置为集群放置组的一部分。",
      "B": "使用 Dedicated Instance 租赁启动 EC2 实例。",
      "C": "将 EC2 实例作为 Spot Instances 启动。",
      "D": "在启动 EC2 实例时配置按需容量预留。"
    },
    "tags": [
      "EC2",
      "HPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查 EC2 实例集群放置组（Cluster Placement Group）的适用场景和优势，以及其他 EC2 实例启动选项的特性。",
      "why_correct": "配置 EC2 实例为集群放置组的一部分可以满足低延迟、高吞吐量和紧密耦合节点间通信的需求。集群放置组将实例放置在单个可用区内的逻辑上彼此相邻的位置，从而最大程度地减少节点之间的网络延迟，并提供更高的网络性能。这非常适合 HPC 工作负载。",
      "why_wrong": "使用 Dedicated Instance 租赁主要提供硬件隔离，但并不直接优化网络性能。启动 Spot Instances 可能会因实例中断而导致中断，不适合需要稳定性和可靠性的 HPC 工作负载。按需容量预留确保在特定可用区和实例类型中的容量，但不会直接影响网络性能或节点间的紧密耦合，无法满足低延迟需求。"
    },
    "related_terms": [
      "Amazon EC2",
      "EC2 instance",
      "HPC",
      "Cluster Placement Group",
      "Dedicated Instance",
      "Spot Instances",
      "Availability Zone",
      "On-Demand Capacity Reservation"
    ]
  },
  {
    "id": 858,
    "topic": "1",
    "question_en": "A company has primary and secondary data centers that are 500 miles (804.7 km) apart and interconnected with high-speed fiber-optic cable. The company needs a highly available and secure network connection between its data centers and a VPC on AWS for a mission-critical workload. A solutions architect must choose a connection solution that provides maximum resiliency. Which solution meets these requirements?",
    "options_en": {
      "A": "Two AWS Direct Connect connections from the primary data center terminating at two Direct Connect locations on two separate devices",
      "B": "A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct Connect location on the same device",
      "C": "Two AWS Direct Connect connections from each of the primary and secondary data centers terminating at two Direct Connect locations on two separate devices",
      "D": "A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct Connect location on two separate devices"
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有主数据中心和辅助数据中心，这两个数据中心相距 500 英里（804.7 公里），并通过高速光纤电缆互连。该公司需要在其数据中心和 AWS 上的 VPC 之间建立一个高可用性和安全的网络连接，以用于关键任务工作负载。解决方案架构师必须选择一个能提供最大弹性的连接解决方案。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "从主数据中心到两个 Direct Connect 位置的两个 AWS Direct Connect 连接，终止于两个独立设备",
      "B": "从主数据中心和辅助数据中心各自到同一个 Direct Connect 位置的一个 AWS Direct Connect 连接，终止于同一设备",
      "C": "从主数据中心和辅助数据中心各自到两个 Direct Connect 位置的两个 AWS Direct Connect 连接，终止于两个独立设备",
      "D": "从主数据中心和辅助数据中心各自到同一个 Direct Connect 位置的一个 AWS Direct Connect 连接，终止于两个独立设备"
    },
    "tags": [
      "Direct Connect"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考察 Direct Connect 的高可用性设计。为了实现高可用性和冗余，应该从每个数据中心连接到多个 Direct Connect 位置，并终止于独立的设备。",
      "why_correct": "选项 C 正确，从主数据中心和辅助数据中心各自到两个 Direct Connect 位置的两个 Direct Connect 连接，终止于两个独立设备，提供了最高级别的冗余和可用性。",
      "why_wrong": "选项 A 错误，从一个数据中心连接到两个 Direct Connect 位置，如果主数据中心故障，则辅助数据中心无法使用。选项 B 错误，虽然可以建立连接，但是只有一个连接位置，如果该位置发生故障，将导致服务中断。选项 D 错误，从每个数据中心到同一个 Direct Connect 位置，没有高可用性。"
    },
    "related_terms": [
      "Direct Connect",
      "VPC"
    ]
  },
  {
    "id": 859,
    "topic": "1",
    "question_en": "A company runs several Amazon RDS for Oracle On-Demand DB instances that have high utilization. The RDS DB instances run in member accounts that are in an organization in AWS Organizations. The company's finance team has access to the organization's management account and member accounts. The finance team wants to find ways to optimize costs by using AWS Trusted Advisor. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use the Trusted Advisor recommendations in the management account.",
      "B": "Use the Trusted Advisor recommendations in the member accounts where the RDS DB instances are running.",
      "C": "Review the Trusted Advisor checks for Amazon RDS Reserved Instance Optimization.",
      "D": "Review the Trusted Advisor checks for Amazon RDS Idle DB Instances",
      "E": "Review the Trusted Advisor checks for compute optimization. Crosscheck the results by using AWS Compute Optimizer."
    },
    "correct_answer": "AC",
    "vote_percentage": "62%",
    "question_cn": "一家公司运行几个 Amazon RDS for Oracle 按需数据库实例，这些实例具有高利用率。RDS 数据库实例在 AWS Organizations 中的组织中的成员账户中运行。该公司的财务团队有权访问该组织的管理账户和成员账户。财务团队希望通过使用 AWS Trusted Advisor 找到优化成本的方法。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用管理账户中的 Trusted Advisor 建议。",
      "B": "使用 RDS 数据库实例运行所在的成员账户中的 Trusted Advisor 建议。",
      "C": "查看 Amazon RDS 保留实例优化方面的 Trusted Advisor 检查。",
      "D": "查看 Amazon RDS 空闲数据库实例的 Trusted Advisor 检查。",
      "E": "查看计算优化方面的 Trusted Advisor 检查。使用 AWS Compute Optimizer 交叉检查结果。"
    },
    "tags": [
      "RDS",
      "Trusted Advisor",
      "Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 62%），解析仅供参考。】\n\n考察使用 AWS Trusted Advisor 优化 Amazon RDS for Oracle 数据库的成本。需要确定财务团队能够访问 Trusted Advisor 信息的配置，以及 Trusted Advisor 的哪些检查可以帮助优化 RDS 成本。",
      "why_correct": "A. 财务团队有权访问管理账户，因此可以使用管理账户中的 Trusted Advisor。Trusted Advisor 在管理账户中可以提供组织级别的信息。\nC. Trusted Advisor 提供了关于 RDS 保留实例优化的检查，可以帮助财务团队评估和优化 RDS 成本，例如是否应该购买保留实例以降低成本。",
      "why_wrong": "B. 虽然可以使用成员账户中的 Trusted Advisor 建议，但题干要求同时使用管理账户。因此仅使用成员账户无法满足需求。\nD. 查看 Amazon RDS 空闲数据库实例的 Trusted Advisor 检查有助于优化成本，但题目要求选择两个正确的步骤，而只选择此选项无法涵盖其他成本优化措施。\nE. 查看计算优化方面的 Trusted Advisor 检查，以及使用 AWS Compute Optimizer 交叉检查结果，虽然在某些情况下可以提高效率，但不是针对 RDS 数据库实例成本优化的主要手段，与题目侧重点不符。"
    },
    "related_terms": [
      "Amazon RDS for Oracle",
      "AWS Trusted Advisor",
      "AWS Organizations",
      "RDS",
      "Reserved Instances",
      "AWS Compute Optimizer"
    ]
  },
  {
    "id": 860,
    "topic": "1",
    "question_en": "A solutions architect is creating an application. The application will run on Amazon EC2 instances in private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently access large files that contain confidential information. These files are stored in Amazon S3 buckets for processing. The solutions architect must optimize the network architecture to minimize data transfer costs. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets, add an entry for the gateway endpoint.",
      "B": "Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a default route that points to the NAT gateway.",
      "C": "Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPIn the route tables for the private subnets, add an entry for the interface endpoint.",
      "D": "Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables for the private subnets, add a default route that points to the NAT gateway in the same Availability Zone."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在创建一个应用程序。该应用程序将在VPC中跨多个可用区内的私有子网中的Amazon EC2实例上运行。EC2实例将频繁访问包含机密信息的大型文件。这些文件存储在Amazon S3存储桶中进行处理。解决方案架构师必须优化网络架构以最大限度地降低数据传输成本。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "在VPC中为Amazon S3创建一个网关endpoint。在私有子网的路由表中，为网关endpoint添加一个条目。",
      "B": "在公共子网中创建一个NAT gateway。在私有子网的路由表中，添加一个指向NAT gateway的默认路由。",
      "C": "在VPC中为Amazon S3创建一个AWS PrivateLink interface endpoint。在私有子网的路由表中，为interface endpoint添加一个条目。",
      "D": "在公共子网中为每个可用区创建一个NAT gateway。在每个私有子网的路由表中，添加一个指向同一可用区中NAT gateway的默认路由。"
    },
    "tags": [
      "VPC",
      "S3",
      "endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查了在 VPC 中访问 S3 的网络架构优化，以降低数据传输成本。重点在于选择合适的 endpoint 类型及路由配置。",
      "why_correct": "创建一个 S3 网关 endpoint 允许 EC2 实例通过 VPC 内部网络直接访问 S3，避免了数据通过 Internet 出入 VPC，从而减少了数据传输成本。在私有子网的路由表中配置指向 S3 网关 endpoint 的路由，确保流量被正确导向。",
      "why_wrong": "B 选项使用 NAT gateway，EC2 实例访问 S3 仍需要通过 Internet，这增加了数据传输成本，且引入了额外的管理和配置负担。C 选项使用 PrivateLink interface endpoint，增加了复杂性和成本，通常用于访问其他 VPC 或 AWS 服务。D 选项也使用 NAT gateway，且为每个可用区创建，成本更高，同样增加了数据传输成本。"
    },
    "related_terms": [
      "VPC",
      "Amazon EC2",
      "Amazon S3",
      "endpoint",
      "NAT gateway",
      "AWS PrivateLink",
      "interface endpoint",
      "subnet",
      "route table",
      "availability zone"
    ]
  },
  {
    "id": 861,
    "topic": "1",
    "question_en": "A company wants to relocate its on-premises MySQL database to AWS. The database accepts regular imports from a client-facing application, which causes a high volume of write operations. The company is concerned that the amount of trafic might be causing performance issues within the application. How should a solutions architect design the architecture on AWS?",
    "options_en": {
      "A": "Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor write operation metrics by using Amazon CloudWatch. Adjust the provisioned IOPS if necessary.",
      "B": "Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an Amazon ElastiCache cluster in front of the DB instance. Configure the application to query ElastiCache instead.",
      "C": "Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory optimized instance type. Monitor Amazon CloudWatch for performance-related issues. Change the instance class if necessary.",
      "D": "Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose performance mode. Monitor Amazon CloudWatch for IOPS bottlenecks. Change to Provisioned Throughput performance mode if necessary."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司希望将其本地 MySQL 数据库迁移到 AWS。该数据库接受来自面向客户端应用程序的定期导入，这导致了大量的写操作。该公司担心流量的增加可能会导致应用程序内的性能问题。解决方案架构师应该如何在 AWS 上设计架构？",
    "options_cn": {
      "A": "预置一个带有 Provisioned IOPS SSD 存储的 Amazon RDS for MySQL 数据库实例。使用 Amazon CloudWatch 监控写操作指标。如有必要，调整已预置的 IOPS。",
      "B": "预置一个带有 General Purpose SSD 存储的 Amazon RDS for MySQL 数据库实例。在数据库实例前面放置一个 Amazon ElastiCache 集群。配置应用程序查询 ElastiCache。",
      "C": "预置一个带有内存优化实例类型的 Amazon DocumentDB（兼容 MongoDB）实例。监控 Amazon CloudWatch 以查找与性能相关的问题。如有必要，更改实例类。",
      "D": "在 General Purpose 性能模式下预置一个 Amazon Elastic File System (Amazon EFS) 文件系统。监控 Amazon CloudWatch 以查找 IOPS 瓶颈。如有必要，更改为 Provisioned Throughput 性能模式。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "ElastiCache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n此题考察 RDS MySQL 的性能优化策略。当数据库写入量大时，使用预置 IOPS 存储可以提升性能。ElastiCache 集群可以缓存查询结果，减少数据库负载，但并不能解决写操作瓶颈。",
      "why_correct": "选项 A 正确，预置 IOPS SSD 存储可以提供更快的写入性能，并可以通过 CloudWatch 监控指标进行调整。",
      "why_wrong": "选项 B 错误，ElastiCache 缓存无法解决写入量大的问题。选项 C 错误，DocumentDB 数据库本身并非专门为高性能写入而设计。选项 D 错误，EFS 文件系统不适合用于数据库。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "ElastiCache",
      "CloudWatch",
      "Provisioned IOPS"
    ]
  },
  {
    "id": 862,
    "topic": "1",
    "question_en": "A company runs an application in the AWS Cloud that generates sensitive archival data files. The company wants to rearchitect the application's data storage. The company wants to encrypt the data files and to ensure that third parties do not have access to the data before the data is encrypted and sent to AWS. The company has already created an Amazon S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption key. Configure the application to use the S3 bucket to store the archival files.",
      "B": "Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.",
      "C": "Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.",
      "D": "Configure the application to use client-side encryption with a key stored in AWS Key Management Service (AWS KMS). Configure the application to store the archival files in the S3 bucket."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 云中运行一个应用程序，该应用程序生成敏感的归档数据文件。该公司希望重新设计应用程序的数据存储。该公司希望对数据文件进行加密，并确保第三方在数据加密并发送到 AWS 之前无法访问这些数据。该公司已经创建了一个 Amazon S3 存储桶。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 S3 存储桶配置为使用客户端加密和 Amazon S3 托管加密密钥。将应用程序配置为使用 S3 存储桶存储归档文件。",
      "B": "将 S3 存储桶配置为使用具有 AWS KMS 密钥的服务器端加密 (SSE-KMS)。将应用程序配置为使用 S3 存储桶存储归档文件。",
      "C": "将 S3 存储桶配置为使用具有 AWS KMS 密钥的双层服务器端加密 (SSE-KMS)。将应用程序配置为使用 S3 存储桶存储归档文件。",
      "D": "将应用程序配置为使用客户端加密，使用存储在 AWS Key Management Service (AWS KMS) 中的密钥。将应用程序配置为将归档文件存储在 S3 存储桶中。"
    },
    "tags": [
      "S3",
      "加密",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察 S3 存储桶的加密方式和客户端加密的应用。为了确保第三方无法在数据加密前访问，需要使用客户端加密，并结合 KMS 密钥来保护数据。",
      "why_correct": "选项 D 正确，应用程序使用客户端加密，并使用 AWS KMS 中的密钥，确保了第三方无法在数据加密前访问。",
      "why_wrong": "选项 A 错误，使用 S3 托管密钥的服务器端加密，密钥由 AWS 管理。选项 B 错误，使用 SSE-KMS 的服务器端加密，密钥由 AWS KMS 管理。选项 C 错误，双层服务器端加密 (SSE-KMS) 也使用 AWS KMS 的密钥，密钥由 AWS 管理。"
    },
    "related_terms": [
      "S3",
      "加密",
      "KMS",
      "SSE-KMS",
      "客户端加密"
    ]
  },
  {
    "id": 863,
    "topic": "1",
    "question_en": "A company uses Amazon RDS with default backup settings for its database tier. The company needs to make a daily backup of the database to meet regulatory requirements. The company must retain the backups for 30 days. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Write an AWS Lambda function to create an RDS snapshot every day.",
      "B": "Modify the RDS database to have a retention period of 30 days for automated backups.",
      "C": "Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.",
      "D": "Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention period."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon RDS 及其数据库层的默认备份设置。该公司需要每天备份数据库以满足监管要求。该公司必须将备份保留 30 天。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "编写一个 AWS Lambda 函数，每天创建 RDS 快照。",
      "B": "修改 RDS 数据库，使其自动备份的保留期为 30 天。",
      "C": "使用 AWS Systems Manager Maintenance Windows 修改 RDS 备份保留期。",
      "D": "每天使用 AWS CLI 创建手动快照。修改 RDS 备份保留期。"
    },
    "tags": [
      "RDS",
      "备份"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 RDS 数据库的备份策略。RDS 提供了自动备份功能，可以满足日常备份需求。修改备份保留期是最直接的方案。",
      "why_correct": "选项 B 正确，通过修改 RDS 数据库的自动备份保留期为 30 天，可以满足监管要求。",
      "why_wrong": "选项 A 错误，使用 Lambda 函数创建 RDS 快照会增加管理复杂性。选项 C 错误，使用 Systems Manager Maintenance Windows 修改 RDS 备份保留期不是标准的方法。选项 D 错误，使用 AWS CLI 创建手动快照会增加管理成本。"
    },
    "related_terms": [
      "RDS",
      "备份",
      "Lambda",
      "Systems Manager"
    ]
  },
  {
    "id": 864,
    "topic": "1",
    "question_en": "A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. During peak usage hours when multiple users access and read the data, the monitoring system shows degradation of database performance for the write queries. The company wants to increase the scalability of the application to meet peak usage demands. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a second Aurora DB cluster. Configure a copy job to replicate the users’ data to the new database. Update the application to use the second database to read the data.",
      "B": "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB cluster. Update the application to use the DAX cluster for read-only queries. Write data directly to the Aurora DB cluster.",
      "C": "Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the replica endpoint for read-only queries and to use the cluster endpoint for write queries.",
      "D": "Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the application to connect to the Redshift cluster and to perform read-only queries on the Redshift cluster."
    },
    "correct_answer": "C",
    "vote_percentage": "80%",
    "question_cn": "一家在 AWS 上运行其应用程序的公司使用 Amazon Aurora 数据库集群作为其数据库。 在多个用户访问和读取数据的峰值使用时段，监控系统显示写入查询的数据库性能下降。该公司希望提高应用程序的可扩展性以满足峰值使用需求。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建第二个 Aurora 数据库集群。 配置一个复制作业以将用户的数据复制到新数据库。 更新应用程序以使用第二个数据库读取数据。",
      "B": "在现有的 Aurora 数据库集群前面创建一个 Amazon DynamoDB Accelerator (DAX) 集群。更新应用程序以使用 DAX 集群进行只读查询。 直接将数据写入 Aurora 数据库集群。",
      "C": "在现有的 Aurora 数据库集群中创建一个 Aurora 只读副本。更新应用程序以使用副本终端节点进行只读查询，并使用集群终端节点进行写入查询。",
      "D": "创建一个 Amazon Redshift 集群。将用户的数据复制到 Redshift 集群。更新应用程序以连接到 Redshift 集群，并在 Redshift 集群上执行只读查询。"
    },
    "tags": [
      "Aurora",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 80%），解析仅供参考。】\n\n此题考察 Aurora 数据库的扩展性解决方案。Aurora 只读副本可以提高读取性能，是一种成本效益高的方式。",
      "why_correct": "选项 C 正确，创建 Aurora 只读副本，应用程序可以将读请求分发到副本，从而提高读取性能。",
      "why_wrong": "选项 A 错误，创建新的 Aurora 集群增加了成本，且数据复制不是最有效的方式。选项 B 错误，DAX 无法解决写入查询的性能问题。选项 D 错误，Redshift 集群是数据仓库，不适合作为数据库，且增加了复杂性。"
    },
    "related_terms": [
      "Aurora",
      "RDS",
      "DAX",
      "Redshift"
    ]
  },
  {
    "id": 865,
    "topic": "1",
    "question_en": "A company's near-real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Choose two.)",
    "options_en": {
      "A": "Use Amazon Kinesis Data Firehose to ingest the data.",
      "B": "Use AWS Lambda with AWS Step Functions to process the data.",
      "C": "Use AWS Database Migration Service (AWS DMS) to ingest the data.",
      "D": "Use Amazon EC2 instances in an Auto Scaling group to process the data",
      "E": "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data."
    },
    "correct_answer": "AE",
    "vote_percentage": "100%",
    "question_cn": "一家公司的近实时流应用程序正在 AWS 上运行。当数据被摄取时，一个作业在数据上运行，需要 30 分钟才能完成。由于大量传入数据，工作负载经常遇到高延迟。一位解决方案架构师需要设计一个可扩展的、无服务器的解决方案来提高性能。解决方案架构师应该采取哪些组合步骤？（选择两个。）",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Firehose 摄取数据。",
      "B": "使用 AWS Lambda 和 AWS Step Functions 处理数据。",
      "C": "使用 AWS Database Migration Service (AWS DMS) 摄取数据。",
      "D": "使用 Auto Scaling 组中的 Amazon EC2 实例来处理数据。",
      "E": "使用 AWS Fargate 和 Amazon Elastic Container Service (Amazon ECS) 来处理数据。"
    },
    "tags": [
      "Kinesis",
      "Lambda",
      "ECS",
      "无服务器"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 100%），解析仅供参考。】\n\n本题考查无服务器流数据处理架构的设计，重点关注如何通过选择合适的 AWS 服务来提升处理性能和可扩展性。",
      "why_correct": "选项 A，Amazon Kinesis Data Firehose 能够将流数据摄取到各种目的地，例如 Amazon S3、Amazon Redshift 和 Amazon Elasticsearch Service。它简化了数据加载，并支持批量处理，从而提高了效率。选项 E，AWS Fargate 和 Amazon ECS 提供了无服务器容器化计算环境，可以自动扩展以处理传入数据流。Fargate 负责管理底层基础设施，使您可以专注于应用程序逻辑。",
      "why_wrong": "选项 B，虽然 AWS Lambda 和 AWS Step Functions 适用于构建无服务器工作流程，但单独使用它们处理大规模的流数据作业可能导致成本较高，并且在长时间运行的作业（30 分钟）上效率较低。选项 C，AWS Database Migration Service (AWS DMS) 专门用于数据库迁移，不适合直接摄取和处理流数据。选项 D，使用 Auto Scaling 组中的 Amazon EC2 实例虽然可以扩展，但需要管理服务器，这与无服务器的目标相悖，并且手动管理 EC2 实例增加了运维复杂性。"
    },
    "related_terms": [
      "Amazon Kinesis Data Firehose",
      "AWS Lambda",
      "AWS Step Functions",
      "AWS Database Migration Service (AWS DMS)",
      "Auto Scaling",
      "Amazon EC2",
      "AWS Fargate",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon S3",
      "Amazon Redshift",
      "Amazon Elasticsearch Service"
    ]
  },
  {
    "id": 866,
    "topic": "1",
    "question_en": "A company runs a web application on multiple Amazon EC2 instances in a VPC. The application needs to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public internet. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the endpoint.",
      "B": "Create an internal Network Load Balancer that has the S3 bucket as the target.",
      "C": "Deploy the S3 bucket inside the VPCreate a route in the VPC route table to the bucket.",
      "D": "Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 VPC 中的多个 Amazon EC2 实例上运行一个 Web 应用程序。该应用程序需要将敏感数据写入 Amazon S3 存储桶。数据不能通过公共互联网发送。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Amazon S3 创建一个网关 VPC endpoint。在 VPC 路由表中创建指向该 endpoint 的路由。",
      "B": "创建一个内部 Network Load Balancer，该负载均衡器将 S3 存储桶作为目标。",
      "C": "在 VPC 内部部署 S3 存储桶。在 VPC 路由表中创建指向该存储桶的路由。",
      "D": "在 VPC 和 S3 区域 endpoint 之间创建 AWS Direct Connect 连接。"
    },
    "tags": [
      "VPC",
      "S3",
      "endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察 VPC 中访问 S3 的安全性。网关 VPC endpoint 允许从 VPC 内安全地访问 S3，数据不会通过公共互联网传输。",
      "why_correct": "选项 A 正确，创建网关 VPC endpoint，并在 VPC 路由表中添加指向该 endpoint 的路由，可以确保 VPC 内的 EC2 实例安全地访问 S3。",
      "why_wrong": "选项 B 错误，内部 Network Load Balancer 无法指向 S3 存储桶。选项 C 错误，VPC 内部署 S3 存储桶是错误的，S3 存储桶位于 AWS 后端。选项 D 错误，Direct Connect 连接不是必须的，并且增加了复杂性。"
    },
    "related_terms": [
      "VPC",
      "S3",
      "Direct Connect",
      "endpoint"
    ]
  },
  {
    "id": 867,
    "topic": "1",
    "question_en": "A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block Store (Amazon EBS) volumes. A solutions architect needs to analyze the current EBS volume cost and to recommend optimizations. The recommendations need to include estimated monthly saving opportunities. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.",
      "B": "Use AWS Systems Manager reporting to determine EBS volume recommendations for optimization.",
      "C": "Use Amazon CloudWatch metrics reporting to determine EBS volume recommendations for optimization.",
      "D": "Use AWS Compute Optimizer to generate EBS volume recommendations for optimization."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司在其 Amazon EC2 实例上运行生产工作负载，这些实例使用 Amazon Elastic Block Store (Amazon EBS) 卷。一个解决方案架构师需要分析当前的 EBS 卷成本并推荐优化措施。这些建议需要包括估计的月度节省机会。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Inspector 报告生成 EBS 卷优化建议。",
      "B": "使用 AWS Systems Manager 报告来确定 EBS 卷优化建议。",
      "C": "使用 Amazon CloudWatch 指标报告来确定 EBS 卷优化建议。",
      "D": "使用 AWS Compute Optimizer 生成 EBS 卷优化建议。"
    },
    "tags": [
      "EC2",
      "EBS",
      "Compute Optimizer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察如何优化 EC2 实例的 EBS 卷成本。AWS Compute Optimizer 提供了 EBS 卷优化建议，包括预估的月度节省机会。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确，使用 AWS Compute Optimizer 可以生成 EBS 卷优化建议，并提供预估的月度节省机会。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，Amazon Inspector 报告不提供 EBS 卷优化建议。选项 B 错误，AWS Systems Manager 报告不提供 EBS 卷优化建议。选项 C 错误，Amazon CloudWatch 指标报告不能直接用于生成 EBS 卷优化建议。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "EBS",
      "Inspector",
      "CloudWatch",
      "Systems Manager",
      "Compute Optimizer"
    ]
  },
  {
    "id": 868,
    "topic": "1",
    "question_en": "A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled. Which solution will meet these requirements?",
    "options_en": {
      "B": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.",
      "C": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.",
      "D": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家全球公司在 AWS 上运行其工作负载。 公司的应用程序使用跨 AWS 区域的 Amazon S3 存储桶来存储和分析敏感数据。该公司每天在多个 S3 存储桶中存储数百万个对象。该公司希望识别所有未启用版本控制的 S3 存储桶。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 AWS Lambda 函数，该函数定期检查所有 S3 存储桶，以确定哪些存储桶未启用版本控制。",
      "B": "使用 Amazon S3 Storage Lens 识别跨区域所有未启用版本控制的 S3 存储桶。",
      "C": "为 S3 启用 IAM Access Analyzer 以识别跨区域所有未启用版本控制的 S3 存储桶。",
      "D": "创建一个 S3 多区域访问点来识别跨区域所有未启用版本控制的 S3 存储桶。"
    },
    "tags": [
      "S3",
      "Storage Lens",
      "版本控制"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察如何识别未启用版本控制的 S3 存储桶。S3 Storage Lens 提供了全局可视性，可以用来识别哪些存储桶未启用版本控制。",
      "why_correct": "选项 B 正确，S3 Storage Lens 可以用来识别所有未启用版本控制的 S3 存储桶。",
      "why_wrong": "选项 A 错误，Lambda 函数需要编写代码进行检查。选项 C 错误，IAM Access Analyzer 主要用于分析 IAM 权限。选项 D 错误，S3 多区域访问点不提供该功能。"
    },
    "related_terms": [
      "S3",
      "Storage Lens",
      "IAM Access Analyzer"
    ]
  },
  {
    "id": 889,
    "topic": "",
    "question_en": "On the other hand S3 Storage Lens: You can use the Versioning-enabled bucket count metric to see which buckets use S3 Versioning. Then, you can take action in the S3 console to enable S3 Versioning for other buckets. So",
    "options_en": {},
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "另一方面，S3 Storage Lens：您可以使用已启用版本控制的存储桶计数指标来查看哪些存储桶使用 S3 版本控制。然后，您可以在 S3 控制台中采取措施，为其他存储桶打开 S3 版本控制。所以",
    "options_cn": {
      "A": "S3 存储桶现在启用了版本控制。",
      "B": "版本控制已在所有 S3 存储桶中启用。",
      "C": "您可以识别当前未启用版本控制的 S3 存储桶。",
      "D": "您正在创建 S3 跨区域复制。"
    },
    "tags": [
      "S3",
      "Storage Lens",
      "版本控制"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察对 S3 Storage Lens 的理解。Storage Lens 可以显示已启用版本控制的存储桶的计数，从而帮助识别未启用版本控制的存储桶。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，Storage Lens 可以用于识别未启用版本控制的 S3 存储桶。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，题干并没有提到 S3 存储桶现在启用了版本控制。选项 C 错误，Storage Lens 能够用来识别哪些存储桶未启用版本控制。选项 D 错误，S3 跨区域复制和 Storage Lens 无关。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "Storage Lens"
    ]
  },
  {
    "id": 869,
    "topic": "1",
    "question_en": "A company wants to enhance its ecommerce order-processing application that is deployed on AWS. The application must process each order exactly once without affecting the customer experience during unpredictable trafic surges. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the SQS queue. Configure an AWS Lambda function as the target to process the orders.",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the orders to the SNS standard topic. Configure the application as a notification target.",
      "C": "Create a fiow by using Amazon AppFlow. Send the orders to the fiow. Configure an AWS Lambda function as the target to process the orders.",
      "D": "Configure AWS X-Ray in the application to track the order requests. Configure the application to process the orders by pulling the orders from Amazon CloudWatch."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望增强其部署在 AWS 上的电子商务订单处理应用程序。该应用程序必须准确地处理每个订单一次，且在不可预测的流量激增期间不影响客户体验。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建 Amazon Simple Queue Service (Amazon SQS) FIFO 队列。将所有订单放入 SQS 队列中。将 AWS Lambda 函数配置为目标以处理订单。",
      "B": "创建一个 Amazon Simple Notification Service (Amazon SNS) 标准主题。将所有订单发布到 SNS 标准主题。将应用程序配置为通知目标。",
      "C": "使用 Amazon AppFlow 创建一个流程。将订单发送到该流程。将 AWS Lambda 函数配置为目标以处理订单。",
      "D": "在应用程序中配置 AWS X-Ray 以跟踪订单请求。配置应用程序，通过从 Amazon CloudWatch 中提取订单来处理这些订单。"
    },
    "tags": [
      "SQS",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察如何构建可靠的、可扩展的订单处理系统。使用 SQS FIFO 队列确保订单按顺序处理且仅处理一次，再结合 Lambda 函数处理，可以应对流量峰值。",
      "why_correct": "选项 A 使用 SQS FIFO 队列保证顺序处理和精确的“一次且仅一次”处理语义，Lambda 函数处理订单，满足了题目的要求。",
      "why_wrong": "选项 B 使用 SNS 标准主题，无法保证消息的顺序和“一次且仅一次”的处理；选项 C 使用 AppFlow，它不适用于处理订单；选项 D 使用 X-Ray 和 CloudWatch，与题目需求的功能不符。"
    },
    "related_terms": [
      "Amazon SQS",
      "Lambda",
      "SNS",
      "X-Ray",
      "CloudWatch",
      "AppFlow"
    ]
  },
  {
    "id": 870,
    "topic": "1",
    "question_en": "A company has two AWS accounts: Production and Development. The company needs to push code changes in the Development account to the Production account. In the alpha phase, only two senior developers on the development team need access to the Production account. In the beta phase, more developers will need access to perform testing. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create two policy documents by using the AWS Management Console in each account. Assign the policy to developers who need access.",
      "B": "Create an IAM role in the Development account. Grant the IAM role access to the Production account. Allow developers to assume the role.",
      "C": "Create an IAM role in the Production account. Define a trust policy that specifies the Development account. Allow developers to assume the role.",
      "D": "Create an IAM group in the Production account. Add the group as a principal in a trust policy that specifies the Production account. Add developers to the group."
    },
    "correct_answer": "C",
    "vote_percentage": "55%",
    "question_cn": "一家公司有两个 AWS 账户：生产环境和开发环境。该公司需要在开发账户中推送代码更改到生产环境账户。在 alpha 阶段，只有开发团队中的两名高级开发人员需要访问生产环境账户。在 beta 阶段，将有更多开发人员需要访问以执行测试。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用每个账户中的 AWS 管理控制台创建两个策略文档。将策略分配给需要访问的开发人员。",
      "B": "在开发账户中创建一个 IAM 角色。授予 IAM 角色访问生产环境账户的权限。允许开发人员担任该角色。",
      "C": "在生产环境账户中创建一个 IAM 角色。定义一个信任策略，指定开发环境账户。允许开发人员担任该角色。",
      "D": "在生产环境账户中创建一个 IAM 组。将该组添加为指定生产环境账户的信任策略中的委托人。将开发人员添加到该组。"
    },
    "tags": [
      "IAM",
      "IAM Role"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 55%），解析仅供参考。】\n\n此题考查跨账户的访问控制。使用 IAM 角色是实现跨账户访问的最佳实践，可以避免硬编码凭据，提高安全性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 在生产账户中创建一个 IAM 角色，定义一个信任策略，允许开发环境账户担任该角色。这种方法安全可靠，满足题目要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 通过创建 IAM 策略并分配给开发人员，这种方法不安全，不利于权限的集中管理；选项 B 在开发账户中创建 IAM 角色不符合跨账户访问的要求；选项 D 在生产环境账户中创建 IAM 组，然后将开发人员添加到组中的方法不够灵活，不推荐。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM",
      "IAM Role",
      "IAM Group"
    ]
  },
  {
    "id": 871,
    "topic": "1",
    "question_en": "A company wants to restrict access to the content of its web application. The company needs to protect the content by using authorization techniques that are available on AWS. The company also wants to implement a serverless architecture for authorization and authentication that has low login latency. The solution must integrate with the web application and serve web content globally. The application currently has a small user base, but the company expects the application's user base to increase. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization. Configure Amazon CloudFront to serve the web application globally.",
      "B": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.",
      "C": "Configure Amazon Cognito for authentication. Implement AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.",
      "D": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望限制对其 Web 应用程序内容的访问。该公司需要使用 AWS 上可用的授权技术来保护内容。该公司还希望为授权和身份验证实施一个无服务器架构，该架构具有低登录延迟。该解决方案必须与 Web 应用程序集成并全球范围地提供 Web 内容。该应用程序目前的用户群很小，但该公司预计该应用程序的用户群会增加。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Cognito 用于身份验证。实施 Lambda@Edge 用于授权。配置 Amazon CloudFront 以在全球范围内提供 Web 应用程序。",
      "B": "配置 AWS Directory Service for Microsoft Active Directory 用于身份验证。实施 AWS Lambda 用于授权。使用 Application Load Balancer 以在全球范围内提供 Web 应用程序。",
      "C": "配置 Amazon Cognito 用于身份验证。实施 AWS Lambda 用于授权。使用 Amazon S3 Transfer Acceleration 以在全球范围内提供 Web 应用程序。",
      "D": "配置 AWS Directory Service for Microsoft Active Directory 用于身份验证。实施 Lambda@Edge 用于授权。使用 AWS Elastic Beanstalk 以在全球范围内提供 Web 应用程序。"
    },
    "tags": [
      "Cognito",
      "CloudFront",
      "Lambda@Edge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查无服务器架构下的身份验证和授权。结合 Cognito、Lambda@Edge 和 CloudFront，可以构建一个安全且低延迟的 Web 应用程序。",
      "why_correct": "选项 A 结合了 Cognito 用于身份验证，Lambda@Edge 用于授权，CloudFront 用于全球内容分发，满足所有需求。",
      "why_wrong": "选项 B 使用 AWS Directory Service for Microsoft Active Directory，增加了复杂性和延迟；选项 C 使用 S3 Transfer Acceleration，不能实现授权功能；选项 D 使用 AWS Elastic Beanstalk 部署 Web 应用程序，增加了复杂性。"
    },
    "related_terms": [
      "Cognito",
      "Lambda@Edge",
      "CloudFront",
      "Lambda",
      "S3 Transfer Acceleration",
      "Elastic Beanstalk",
      "Directory Service for Microsoft Active Directory"
    ]
  },
  {
    "id": 872,
    "topic": "1",
    "question_en": "A development team uses multiple AWS accounts for its development, staging, and production environments. Team members have been launching large Amazon EC2 instances that are underutilized. A solutions architect must prevent large instances from being launched in all accounts. How can the solutions architect meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users.",
      "B": "Define a resource in AWS Resource Access Manager that prevents the launch of large EC2 instances.",
      "C": "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role.",
      "D": "Create an organization in AWS Organizations in the management account with the default policy. Create a service control policy (SCP) that denies the launch of large EC2 instances, and apply it to the AWS accounts."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "开发团队为其开发、预发布和生产环境使用多个 AWS 账户。团队成员一直在启动未充分利用的大型 Amazon EC2 实例。解决方案架构师必须阻止在所有账户中启动大型实例。解决方案架构师如何以最小的运营开销满足此要求？",
    "options_cn": {
      "A": "更新 IAM 策略以拒绝启动大型 EC2 实例。将策略应用于所有用户。",
      "B": "在 AWS Resource Access Manager 中定义一个资源，以阻止启动大型 EC2 实例。",
      "C": "在每个账户中创建一个 IAM 角色，以拒绝启动大型 EC2 实例。授予开发人员 IAM 组访问该角色的权限。",
      "D": "在管理账户的 AWS Organizations 中使用默认策略创建一个组织。创建一个服务控制策略 (SCP) 以拒绝启动大型 EC2 实例，并将其应用于 AWS 账户。"
    },
    "tags": [
      "IAM",
      "Organizations",
      "SCP"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考察如何限制 AWS 账户中的资源启动。使用 SCP 是在 AWS Organizations 中控制资源启动的有效方法。",
      "why_correct": "选项 D 使用 AWS Organizations 和 SCP 限制启动大型 EC2 实例，这是管理多个 AWS 账户的最佳实践。",
      "why_wrong": "选项 A 通过更新 IAM 策略来实现，这无法保证在所有账户中的一致性；选项 B 使用 AWS Resource Access Manager 无法实现账户级别的资源限制；选项 C 需要在每个账户中单独配置，运营开销大。"
    },
    "related_terms": [
      "IAM",
      "SCP",
      "EC2",
      "AWS Resource Access Manager",
      "Organizations"
    ]
  },
  {
    "id": 873,
    "topic": "1",
    "question_en": "A company has migrated a fieet of hundreds of on-premises virtual machines (VMs) to Amazon EC2 instances. The instances run a diverse fieet of Windows Server versions along with several Linux distributions. The company wants a solution that will automate inventory and updates of the operating systems. The company also needs a summary of common vulnerabilities of each instance for regular monthly reviews. What should a solutions architect recommend to meet these requirements?",
    "options_en": {
      "A": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Configure AWS Security Hub to produce monthly reports.",
      "B": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Deploy Amazon Inspector, and configure monthly reports.",
      "C": "Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate patch installations on the EC2 instances.",
      "D": "Set up Amazon GuardDuty in the account to monitor all EC2 instances. Deploy AWS Config to automate patch installations on the EC2 instances."
    },
    "correct_answer": "B",
    "vote_percentage": "88%",
    "question_cn": "一家公司已将其数百台本地虚拟机 (VM) 迁移到 Amazon EC2 实例。 这些实例运行各种 Windows Server 版本以及多个 Linux 发行版。该公司希望一个可以自动进行操作系统清点和更新的解决方案。该公司还需要每个实例的常见漏洞摘要，以供定期的月度审查。解决方案架构师应该推荐什么来满足这些要求？",
    "options_cn": {
      "A": "设置 AWS Systems Manager Patch Manager 以管理所有 EC2 实例。配置 AWS Security Hub 以生成月度报告。",
      "B": "设置 AWS Systems Manager Patch Manager 以管理所有 EC2 实例。部署 Amazon Inspector，并配置月度报告。",
      "C": "设置 AWS Shield Advanced，并配置月度报告。部署 AWS Config 以自动在 EC2 实例上安装补丁程序。",
      "D": "在账户中设置 Amazon GuardDuty 以监控所有 EC2 实例。部署 AWS Config 以自动在 EC2 实例上安装补丁程序。"
    },
    "tags": [
      "Systems Manager",
      "Security Hub"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 88%），解析仅供参考。】\n\n考查利用 AWS Systems Manager 和 Amazon Inspector 管理 EC2 实例的补丁和安全漏洞评估，以满足操作系统清点、更新和漏洞摘要的需求。",
      "why_correct": "AWS Systems Manager Patch Manager 能够自动执行对 EC2 实例的操作系统补丁和更新。 Amazon Inspector 用于持续评估 EC2 实例是否存在安全漏洞。结合使用两者，能够满足题目的所有要求，包括操作系统清点和更新，以及每个实例的漏洞摘要，并生成月度报告。",
      "why_wrong": "选项 A 缺少对漏洞的全面评估，仅使用 Systems Manager 的补丁管理功能无法满足漏洞摘要的要求。选项 C 错误地使用了 AWS Shield Advanced，该服务主要用于抵御 DDoS 攻击，与题目需求无关。 选项 D 中的 Amazon GuardDuty 主要用于威胁检测，无法满足操作系统更新和漏洞评估的要求；AWS Config 自动安装补丁程序的方式不可靠，且无法生成漏洞摘要。"
    },
    "related_terms": [
      "AWS Systems Manager Patch Manager",
      "Amazon EC2",
      "Windows Server",
      "Linux",
      "AWS Security Hub",
      "Amazon Inspector",
      "AWS Shield Advanced",
      "AWS Config",
      "Amazon GuardDuty"
    ]
  },
  {
    "id": 874,
    "topic": "1",
    "question_en": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances in an Auto Scaling group behind an Elastic Load Balancing (ELB) load balancer. The application connects to an Amazon DynamoDB table. For disaster recovery (DR) purposes, the company wants to ensure that the application is available from another AWS Region with minimal downtime. Which solution will meet these requirements with the LEAST downtime?",
    "options_en": {
      "A": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new DR Region's ELB.",
      "B": "Create an AWS CloudFormation template to create EC2 instances, ELBs, and DynamoDB tables to be launched when necessary. Configure DNS failover to point to the new DR Region's ELB.",
      "C": "Create an AWS CloudFormation template to create EC2 instances and an ELB to be launched when necessary. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new DR Region's ELB.",
      "D": "Create an Auto Scaling group and an ELB in the DR Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm with an evaluation period of 10 minutes to invoke an AWS Lambda function that updates Amazon Route 53 to point to the DR Region's ELB."
    },
    "correct_answer": "A",
    "vote_percentage": "85%",
    "question_cn": "一家公司在 AWS 云中托管其应用程序。该应用程序在 Elastic Load Balancing (ELB) 负载均衡器后面的 Auto Scaling 组中的 Amazon EC2 实例上运行。该应用程序连接到 Amazon DynamoDB 表。为了灾难恢复 (DR) 的目的，该公司希望确保该应用程序可以从另一个 AWS 区域使用，并且停机时间最短。哪种解决方案能以最少的停机时间满足这些要求？",
    "options_cn": {
      "A": "在 DR 区域创建 Auto Scaling 组和 ELB。将 DynamoDB 表配置为全局表。配置 DNS 故障转移以指向新的 DR 区域的 ELB。",
      "B": "创建 AWS CloudFormation 模板以创建 EC2 实例、ELB 和 DynamoDB 表，以便在必要时启动。配置 DNS 故障转移以指向新的 DR 区域的 ELB。",
      "C": "创建 AWS CloudFormation 模板以创建 EC2 实例和 ELB，以便在必要时启动。将 DynamoDB 表配置为全局表。配置 DNS 故障转移以指向新的 DR 区域的 ELB。",
      "D": "在 DR 区域创建 Auto Scaling 组和 ELB。将 DynamoDB 表配置为全局表。创建一个 Amazon CloudWatch 警报，评估周期为 10 分钟，以调用一个 AWS Lambda 函数，该函数更新 Amazon Route 53 以指向 DR 区域的 ELB。"
    },
    "tags": [
      "DynamoDB",
      "Route 53",
      "Auto Scaling",
      "ELB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 85%），解析仅供参考。】\n\n此题考查如何实现应用程序的灾难恢复。使用全局表和 DNS 故障转移可以最大程度地减少停机时间。",
      "why_correct": "选项 A 在 DR 区域创建 Auto Scaling 组和 ELB，将 DynamoDB 表配置为全局表，并配置 DNS 故障转移，可以实现最快的恢复。",
      "why_wrong": "选项 B 和 C 使用 CloudFormation 模板，增加了恢复时间；选项 D 增加了不必要的复杂性，使用 CloudWatch 警报调用 Lambda 函数更新 Route 53，不如直接使用 DNS 故障转移。"
    },
    "related_terms": [
      "DynamoDB",
      "Route 53",
      "Auto Scaling",
      "ELB",
      "CloudFormation",
      "CloudWatch",
      "Lambda"
    ]
  },
  {
    "id": 875,
    "topic": "1",
    "question_en": "A company runs an application on Amazon EC2 instances in a private subnet. The application needs to store and retrieve data in Amazon S3 buckets. According to regulatory requirements, the data must not travel across the public internet. What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Deploy a NAT gateway to access the S3 buckets.",
      "B": "Deploy AWS Storage Gateway to access the S3 buckets.",
      "C": "Deploy an S3 interface endpoint to access the S3 buckets.",
      "D": "Deploy an S3 gateway endpoint to access the S3 buckets."
    },
    "correct_answer": "D",
    "vote_percentage": "85%",
    "question_cn": "一家公司在私有子网中的 Amazon EC2 实例上运行应用程序。 该应用程序需要在 Amazon S3 存储桶中存储和检索数据。 根据法规要求，数据不得通过公共互联网传输。 解决方案架构师应该怎么做才能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "部署 NAT Gateway 以访问 S3 存储桶。",
      "B": "部署 AWS Storage Gateway 以访问 S3 存储桶。",
      "C": "部署 S3 接口 endpoint 以访问 S3 存储桶。",
      "D": "部署 S3 网关 endpoint 以访问 S3 存储桶。"
    },
    "tags": [
      "S3",
      "Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 85%），解析仅供参考。】\n\n此题考察如何在私有子网中访问 S3。使用 S3 endpoint 可以确保数据不通过公共互联网传输。",
      "why_correct": "选项 D 使用 S3 网关 endpoint 访问 S3 存储桶，满足数据不出公网的需求，且最具成本效益。",
      "why_wrong": "选项 A 使用 NAT Gateway，增加了成本；选项 B 使用 AWS Storage Gateway，增加了复杂性；选项 C 使用 S3 接口 endpoint，无法满足数据不出公网的需求。"
    },
    "related_terms": [
      "S3",
      "NAT Gateway",
      "Storage Gateway",
      "Endpoint"
    ]
  },
  {
    "id": 876,
    "topic": "1",
    "question_en": "A company hosts an application on Amazon EC2 instances that run in a single Availability Zone. The application is accessible by using the transport layer of the Open Systems Interconnection (OSI) model. The company needs the application architecture to have high availability. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route trafic to all instances.",
      "B": "Configure a Network Load Balancer in front of the EC2 instances.",
      "C": "Configure a Network Load Balancer for TCP trafic to the instances. Configure an Application Load Balancer for HTTP and HTTPS trafic to the instances.",
      "D": "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use multiple Availability Zones. Configure the Auto Scaling group to run application health checks on the instances",
      "E": "Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that transition to a stopped state."
    },
    "correct_answer": "BD",
    "vote_percentage": "100%",
    "question_cn": "一家公司在单个可用区中运行的 Amazon EC2 实例上托管应用程序。该应用程序可以通过使用开放系统互连 (OSI) 模型的传输层进行访问。该公司需要应用程序架构具有高可用性。哪种步骤组合将以最具成本效益的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在不同的可用区中配置新的 EC2 实例。使用 Amazon Route 53 将流量路由到所有实例。",
      "B": "在 EC2 实例前面配置一个 Network Load Balancer。",
      "C": "为 TCP 流量配置一个 Network Load Balancer。为 HTTP 和 HTTPS 流量配置一个 Application Load Balancer。",
      "D": "为 EC2 实例创建一个 Auto Scaling 组。将 Auto Scaling 组配置为使用多个可用区。将 Auto Scaling 组配置为在实例上运行应用程序运行状况检查。",
      "E": "创建一个 Amazon CloudWatch 告警。将告警配置为重新启动转换为停止状态的 EC2 实例。"
    },
    "tags": [
      "ELB",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 100%），解析仅供参考。】\n\n考察如何通过使用负载均衡器和 Auto Scaling 组来实现 EC2 实例的高可用性，并兼顾成本效益。",
      "why_correct": "选项 B 正确，因为 Network Load Balancer (NLB) 擅长处理传输层（TCP）流量，并能实现高可用性。选项 D 正确，通过使用 Auto Scaling 组跨多个可用区部署 EC2 实例，并结合健康检查，可以自动扩展和替换不健康的实例，从而提高应用程序的可用性。",
      "why_wrong": "选项 A 错误，虽然 Route 53 可以实现流量路由，但没有解决单可用区的问题，并且仅仅依靠手动创建实例并不能自动实现高可用性。选项 C 错误，同时使用 NLB 和 Application Load Balancer (ALB) 增加了成本，并且题目已经指定使用传输层，ALB 并不合适。选项 E 错误，CloudWatch 告警并不能完全保证高可用性，重启实例需要人工干预或配置，且无法自动解决单可用区故障的问题。"
    },
    "related_terms": [
      "Amazon EC2",
      "OSI model",
      "Network Load Balancer (NLB)",
      "Amazon Route 53",
      "Application Load Balancer (ALB)",
      "Auto Scaling",
      "Amazon CloudWatch"
    ]
  },
  {
    "id": 877,
    "topic": "1",
    "question_en": "A company uses Amazon S3 to host its static website. The company wants to add a contact form to the webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company expects fewer than 100 site visits each month. The contact form must notify the company by email when a customer fills out the form. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Host the dynamic contact form in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to a third-party email provider.",
      "B": "Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda function. Configure another Lambda function on the API Gateway to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "C": "Host the website by using AWS Amplify Hosting for static content and dynamic content. Use server-side scripting to build the contact form. Configure Amazon Simple Queue Service (Amazon SQS) to deliver the message to the company.",
      "D": "Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use Internet Information Services (IIS) for Windows Server to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Amazon S3 托管其静态网站。该公司希望向网页添加一个联系表单。联系表单将具有动态服务器端组件，供用户输入他们的姓名、电子邮件地址、电话号码和用户消息。该公司预计每月访问量少于 100 次。当客户填写表单时，联系表单必须通过电子邮件通知该公司。哪种解决方案最经济高效地满足这些要求？",
    "options_cn": {
      "A": "在 Amazon Elastic Container Service (Amazon ECS) 中托管动态联系表单。设置 Amazon Simple Email Service (Amazon SES) 以连接到第三方电子邮件提供商。",
      "B": "创建一个 Amazon API Gateway 端点，该端点从 AWS Lambda 函数返回联系表单。在 API Gateway 上配置另一个 Lambda 函数，以将消息发布到 Amazon Simple Notification Service (Amazon SNS) 主题。",
      "C": "使用 AWS Amplify Hosting 托管静态内容和动态内容的网站。使用服务器端脚本构建联系表单。配置 Amazon Simple Queue Service (Amazon SQS) 将消息传递给公司。",
      "D": "将网站从 Amazon S3 迁移到运行 Windows Server 的 Amazon EC2 实例。使用 Internet Information Services (IIS) for Windows Server 托管网页。使用客户端脚本构建联系表单。将表单与 Amazon WorkMail 集成。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "SNS",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查如何构建一个经济高效的网站，包括动态的联系表单。使用 API Gateway、Lambda 和 SNS 可以实现此需求。",
      "why_correct": "选项 B 使用 API Gateway、Lambda 和 SNS，实现联系表单和邮件通知，经济高效。",
      "why_wrong": "选项 A 使用 ECS 和 SES，增加了复杂性；选项 C 使用 Amplify Hosting 和 SQS，不适用动态联系表单；选项 D 使用 EC2 和 IIS，成本较高。"
    },
    "related_terms": [
      "API Gateway",
      "Lambda",
      "SNS",
      "S3",
      "Elastic Container Service",
      "SES",
      "SQS",
      "EC2",
      "IIS",
      "Amplify Hosting"
    ]
  },
  {
    "id": 878,
    "topic": "1",
    "question_en": "A company creates dedicated AWS accounts in AWS Organizations for its business units. Recently, an important notification was sent to the root user email address of a business unit account instead of the assigned account owner. The company wants to ensure that all future notifications can be sent to different employees based on the notification categories of billing, operations, or security. Which solution will meet these requirements MOST securely?",
    "options_en": {
      "A": "Configure each AWS account to use a single email address that the company manages. Ensure that all account owners can access the email account to receive notifications. Configure alternate contacts for each AWS account with corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.",
      "B": "Configure each AWS account to use a different email distribution list for each business unit that the company manages. Configure each distribution list with administrator email addresses that can respond to alerts. Configure alternate contacts for each AWS account with corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.",
      "C": "Configure each AWS account root user email address to be the individual company managed email address of one person from each business unit. Configure alternate contacts for each AWS account with corresponding distribution lists for the billing team, the security team, and the operations team for each business unit.",
      "D": "Configure each AWS account root user to use email aliases that go to a centralized mailbox. Configure alternate contacts for each account by using a single business managed email distribution list each for the billing team, the security team, and the operations team."
    },
    "correct_answer": "D",
    "vote_percentage": "54%",
    "question_cn": "一家公司在 AWS Organizations 中为其业务部门创建了专用的 AWS 账户。最近，一条重要通知被发送到了某个业务部门账户的根用户电子邮件地址，而不是指定的账户所有者。该公司希望确保所有未来的通知可以根据账单、运营或安全通知类别发送给不同的员工。哪种解决方案将最安全地满足这些要求？",
    "options_cn": {
      "A": "将每个 AWS 账户配置为使用公司管理的单个电子邮件地址。确保所有账户所有者都可以访问该电子邮件账户以接收通知。为每个 AWS 账户配置备用联系人，并为每个业务部门配置相应的账单团队、安全团队和运营团队的通讯组列表。",
      "B": "将每个 AWS 账户配置为使用公司管理的每个业务部门的不同电子邮件通讯组列表。为每个通讯组列表配置可以响应警报的管理员电子邮件地址。为每个 AWS 账户配置备用联系人，并为每个业务部门配置相应的账单团队、安全团队和运营团队的通讯组列表。",
      "C": "将每个 AWS 账户的根用户电子邮件地址配置为来自每个业务部门的一个人的个人公司管理的电子邮件地址。为每个 AWS 账户配置备用联系人，并为每个业务部门配置相应的账单团队、安全团队和运营团队的通讯组列表。",
      "D": "配置每个 AWS 账户根用户使用指向集中邮箱的电子邮件别名。通过为每个账户使用一个业务管理的电子邮件通讯组列表（分别用于账单团队、安全团队和运营团队）来配置备用联系人。"
    },
    "tags": [
      "Organizations",
      "Email",
      "SNS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 54%），解析仅供参考。】\n\n考查如何通过配置通知类别和电子邮件别名，安全地将 AWS 账户的通知路由到指定接收者。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 提供了最安全的解决方案，通过使用电子邮件别名指向集中邮箱，避免了直接暴露根用户的电子邮件地址。同时，为每个账户配置针对不同通知类别的业务管理电子邮件通讯组列表，确保了通知能够被正确地分发到相应的团队，满足了题目对账单、运营和安全通知类别分发的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 存在安全隐患，所有账户所有者共享一个电子邮件账户，不利于账户安全管理。选项 B 相对来说更安全，但仍不如 D 集中化管理。选项 C 使用个人电子邮件地址作为根用户邮箱，会分散管理，并且难以维护和更新，也增加了安全风险。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Organizations",
      "AWS Account",
      "Root user",
      "Billing",
      "Operations",
      "Security",
      "Email alias",
      "Email distribution list",
      "Alternate contacts"
    ]
  },
  {
    "id": 879,
    "topic": "1",
    "question_en": "A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases and store the purchase details in an Amazon Aurora PostgreSQL DB cluster. Customers are experiencing application timeouts during times of peak usage. A solutions architect needs to rearchitect the application so that the application can scale to meet peak usage demands. Which combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing is complete. Update the applications to connect to the DB cluster by using Amazon RDS Proxy.",
      "B": "Configure the application to use an Amazon ElastiCache cluster in front of the Aurora PostgreSQL DB cluster.",
      "C": "Update the application to send the purchase requests to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an Auto Scaling group of new EC2 instances that read from the SQS queue.",
      "D": "Configure an AWS Lambda function to retry the ticket purchases until the processing is complet",
      "E": "E. Configure an Amazon AP! Gateway REST API with a usage plan."
    },
    "correct_answer": "AC",
    "vote_percentage": "80%",
    "question_cn": "一家公司在 AWS 上运行电子商务应用程序。Amazon EC2 实例处理购买并将其详细信息存储在 Amazon Aurora PostgreSQL 数据库集群中。客户在高峰使用期间遇到应用程序超时问题。解决方案架构师需要重新设计应用程序，以便应用程序可以扩展以满足高峰使用需求。哪两种操作的组合将以最具成本效益的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "配置新的 EC2 实例的 Auto Scaling 组，以重试购买，直到处理完成。更新应用程序以使用 Amazon RDS Proxy 连接到数据库集群。",
      "B": "配置应用程序以在 Aurora PostgreSQL 数据库集群前面使用 Amazon ElastiCache 集群。",
      "C": "更新应用程序以将购买请求发送到 Amazon Simple Queue Service (Amazon SQS) 队列。配置从 SQS 队列读取的新 EC2 实例的 Auto Scaling 组。",
      "D": "配置一个 AWS Lambda 函数以重试门票购买，直到处理完成。",
      "E": "配置带有使用计划的 Amazon API 网关 REST API。"
    },
    "tags": [
      "Aurora",
      "EC2",
      "Auto Scaling",
      "SQS",
      "RDS Proxy"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 80%），解析仅供参考。】\n\n此题考察如何优化应用程序以应对高峰负载。使用 SQS 和 RDS Proxy 可以提高应用程序的可扩展性和弹性。",
      "why_correct": "选项 A 使用 RDS Proxy 连接数据库集群，并且使用 Auto Scaling 组，可以提高应用程序的性能和可伸缩性。",
      "why_wrong": "选项 B 仅配置 ElastiCache，无法解决数据库负载问题；选项 C 使用 SQS，但是没有配合 RDS Proxy 和 Auto Scaling 组；选项 D 仅使用 Lambda，无法有效处理高峰负载；选项 E 使用 API 网关，无法解决数据库负载问题。"
    },
    "related_terms": [
      "Aurora",
      "EC2",
      "Auto Scaling",
      "SQS",
      "RDS Proxy",
      "ElastiCache",
      "API Gateway"
    ]
  },
  {
    "id": 880,
    "topic": "1",
    "question_en": "A company that uses AWS Organizations runs 150 applications across 30 different AWS accounts. The company used AWS Cost and Usage Report to create a new report in the management account. The report is delivered to an Amazon S3 bucket that is replicated to a bucket in the data collection account. The company’s senior leadership wants to view a custom dashboard that provides NAT gateway costs each day starting at the beginning of the current month. Which solution will meet these requirements?",
    "options_en": {
      "A": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use AWS DataSync to query the new report.",
      "B": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use Amazon Athena to query the new report.",
      "C": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use AWS DataSync to query the new report.",
      "D": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use Amazon Athena to query the new report."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家使用 AWS Organizations 的公司在 30 个不同的 AWS 账户上运行 150 个应用程序。该公司使用 AWS Cost and Usage Report 在管理账户中创建了一个新报告。该报告被交付到一个 Amazon S3 存储桶，该存储桶被复制到数据收集账户中的一个存储桶。公司的高级管理层希望查看一个自定义仪表板，该仪表板每天提供 NAT gateway 的成本，从当月月初开始。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "共享一个包含所需表格视图的 Amazon QuickSight 仪表板。配置 QuickSight 使用 AWS DataSync 来查询新报告。",
      "B": "共享一个包含所需表格视图的 Amazon QuickSight 仪表板。配置 QuickSight 使用 Amazon Athena 来查询新报告。",
      "C": "共享一个包含所需表格视图的 Amazon CloudWatch 仪表板。配置 CloudWatch 使用 AWS DataSync 来查询新报告。",
      "D": "共享一个包含所需表格视图的 Amazon CloudWatch 仪表板。配置 CloudWatch 使用 Amazon Athena 来查询新报告。"
    },
    "tags": [
      "QuickSight",
      "Athena",
      "CUR"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查如何创建自定义仪表板来分析 AWS 成本。使用 QuickSight 和 Athena 是最佳实践。",
      "why_correct": "选项 B 使用 QuickSight 和 Athena 来创建和查询自定义仪表板，可以满足要求。",
      "why_wrong": "选项 A 使用 QuickSight 和 DataSync，不适用查询 CUR 报告；选项 C 使用 CloudWatch 和 DataSync，不适用分析 CUR 报告；选项 D 使用 CloudWatch，不适用分析 CUR 报告。"
    },
    "related_terms": [
      "QuickSight",
      "Athena",
      "DataSync",
      "CloudWatch",
      "CUR"
    ]
  },
  {
    "id": 881,
    "topic": "1",
    "question_en": "A company is hosting a high-trafic static website on Amazon S3 with an Amazon CloudFront distribution that has a default TTL of 0 seconds. The company wants to implement caching to improve performance for the website. However, the company also wants to ensure that stale content is not served for more than a few minutes after a deployment. Which combination of caching methods should a solutions architect implement to meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Set the CloudFront default TTL to 2 minutes.",
      "B": "Set a default TTL of 2 minutes on the S3 bucket.",
      "C": "Add a Cache-Control private directive to the objects in Amazon S3.",
      "D": "Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure the function to run on viewer respons",
      "E": "E. Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment, create a CloudFront invalidation to clear any changed files from edge caches."
    },
    "correct_answer": "AE",
    "vote_percentage": "47%",
    "question_cn": "一家公司正在 Amazon S3 上托管一个高流量静态网站，该网站使用 Amazon CloudFront 分发，其默认 TTL 为 0 秒。该公司希望实现缓存以提高网站的性能。但是，该公司还希望确保在部署后几分钟内不会提供过时内容。解决方案架构师应该实施哪种缓存方法的组合来满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "将 CloudFront 默认 TTL 设置为 2 分钟。",
      "B": "在 S3 存储桶上设置 2 分钟的默认 TTL。",
      "C": "将 Cache-Control private 指令添加到 Amazon S3 中的对象。",
      "D": "创建一个 AWS Lambda@Edge 函数，以将 Expires 标头添加到 HTTP 响应中。将该函数配置为在查看器响应时运行。",
      "E": "将 Cache-Control max-age 指令设置为 Amazon S3 中对象的 24 小时。在部署时，创建一个 CloudFront 无效化，以从边缘缓存中清除任何已更改的文件。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "Cache"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AE（社区 47%），解析仅供参考。】\n\n此题考查如何使用 CloudFront 缓存静态网站内容。结合 CloudFront 默认 TTL 和 Expires 标头可以实现最佳缓存效果。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AE。理由简述：选项 A 将 CloudFront 默认 TTL 设置为 2 分钟，确保内容的及时更新。选项 E 创建一个 CloudFront 无效化，可以更及时更新缓存。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 设置 S3 存储桶 TTL，对 CloudFront 无效；选项 C 添加 Cache-Control private 指令，阻止缓存；选项 D 创建 Lambda@Edge 函数，增加复杂性，也并不适用于解决此问题。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "Lambda@Edge",
      "Cache"
    ]
  },
  {
    "id": 882,
    "topic": "1",
    "question_en": "A company runs its application by using Amazon EC2 instances and AWS Lambda functions. The EC2 instances run in private subnets of a VPC. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for 1 year. The number of Lambda functions that the application uses will increase during the 1-year period. The company must minimize costs on all application resources. Which solution will meet these requirements?",
    "options_en": {
      "A": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances.",
      "B": "Purchase an EC2 Instance Savings Plan. Connect the Lambda functions to new public subnets in the same VPC where the EC2 instances run.",
      "C": "Purchase a Compute Savings Plan. Connect the Lambda functions to the private subnets that contain the EC2 instances.",
      "D": "Purchase a Compute Savings Plan. Keep the Lambda functions in the Lambda service VPC."
    },
    "correct_answer": "C",
    "vote_percentage": "63%",
    "question_cn": "一家公司使用 Amazon EC2 实例和 AWS Lambda 函数运行其应用程序。EC2 实例在 VPC 的私有子网中运行。Lambda 函数需要直接网络访问 EC2 实例才能使应用程序工作。该应用程序将运行 1 年。应用程序使用的 Lambda 函数数量将在 1 年期间增加。公司必须最大限度地减少所有应用程序资源的成本。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "购买 EC2 实例储蓄计划。将 Lambda 函数连接到包含 EC2 实例的私有子网。",
      "B": "购买 EC2 实例储蓄计划。将 Lambda 函数连接到与 EC2 实例运行在同一 VPC 中的新公共子网。",
      "C": "购买计算储蓄计划。将 Lambda 函数连接到包含 EC2 实例的私有子网。",
      "D": "购买计算储蓄计划。将 Lambda 函数保留在 Lambda 服务 VPC 中。"
    },
    "tags": [
      "Lambda",
      "EC2",
      "Savings Plans"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 63%），解析仅供参考。】\n\n此题考查 Lambda 函数访问 EC2 实例的网络配置和成本优化。使用计算储蓄计划可以降低成本。",
      "why_correct": "选项 C 使用计算储蓄计划，同时将 Lambda 函数连接到包含 EC2 实例的私有子网，满足了成本优化和网络访问的需求。",
      "why_wrong": "选项 A 和 B 使用 EC2 实例储蓄计划，但无法优化 Lambda 函数的成本；选项 D 无法访问 VPC。"
    },
    "related_terms": [
      "Lambda",
      "EC2",
      "VPC",
      "Savings Plans"
    ]
  },
  {
    "id": 883,
    "topic": "1",
    "question_en": "A company has deployed a multi-account strategy on AWS by using AWS Control Tower. The company has provided individual AWS accounts to each of its developers. The company wants to implement controls to limit AWS resource costs that the developers incur. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Instruct each developer to tag all their resources with a tag that has a key of CostCenter and a value of the developer's name. Use the required-tags AWS Config managed rule to check for the tag. Create an AWS Lambda function to terminate resources that do not have the tag. Configure AWS Cost Explorer to send a daily report to each developer to monitor their spending.",
      "B": "Use AWS Budgets to establish budgets for each developer account. Set up budget alerts for actual and forecast values to notify developers when they exceed or expect to exceed their assigned budget. Use AWS Budgets actions to apply a DenyAll policy to the developer's IAM role to prevent additional resources from being launched when the assigned budget is reached.",
      "C": "Use AWS Cost Explorer to monitor and report on costs for each developer account. Configure Cost Explorer to send a daily report to each developer to monitor their spending. Use AWS Cost Anomaly Detection to detect anomalous spending and provide alerts.",
      "D": "Use AWS Service Catalog to allow developers to launch resources within a limited cost range. Create AWS Lambda functions in each AWS account to stop running resources at the end of each work day. Configure the Lambda functions to resume the resources at the start of each work day."
    },
    "correct_answer": "B",
    "vote_percentage": "75%",
    "question_cn": "一家公司通过使用 AWS Control Tower 在 AWS 上部署了多账户策略。该公司已为其每个开发人员提供了单独的 AWS 账户。该公司希望实施控制措施来限制开发人员产生的 AWS 资源成本。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "指示每个开发人员使用一个名为 CostCenter 的标签标记其所有资源，其值为开发人员的姓名。使用 required-tags AWS Config 托管规则来检查该标签。创建一个 AWS Lambda 函数以终止没有该标签的资源。配置 AWS Cost Explorer 每天向每个开发人员发送一份报告，以监控他们的支出。",
      "B": "使用 AWS Budgets 为每个开发人员账户建立预算。设置预算警报，用于实际值和预测值，以便在开发人员超出或预计超出其分配的预算时通知他们。使用 AWS Budgets 操作将 DenyAll 策略应用于开发人员的 IAM 角色，以防止在达到分配的预算时启动其他资源。",
      "C": "使用 AWS Cost Explorer 监控和报告每个开发人员账户的成本。配置 Cost Explorer 每天向每个开发人员发送一份报告，以监控他们的支出。使用 AWS Cost Anomaly Detection 检测异常支出并提供警报。",
      "D": "使用 AWS Service Catalog 允许开发人员在有限的成本范围内启动资源。在每个 AWS 账户中创建 AWS Lambda 函数，以在每个工作日结束时停止运行的资源。配置 Lambda 函数以在每个工作日开始时恢复资源。"
    },
    "tags": [
      "Budgets",
      "Cost Explorer",
      "Control Tower"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 75%），解析仅供参考。】\n\n此题考查如何控制开发人员的 AWS 资源成本。使用 AWS Budgets 可以有效地控制成本。",
      "why_correct": "选项 B 使用 AWS Budgets，可以设定预算、设置警报，并在超出预算时采取行动，以最少的运营开销满足要求。",
      "why_wrong": "选项 A 使用标签，但无法有效控制成本；选项 C 仅使用 Cost Explorer，无法控制成本；选项 D 使用 Service Catalog 和 Lambda，增加运营开销，不能有效控制成本。"
    },
    "related_terms": [
      "Lambda",
      "Budgets",
      "Cost Explorer",
      "Control Tower",
      "Service Catalog"
    ]
  },
  {
    "id": 884,
    "topic": "1",
    "question_en": "A solutions architect is designing a three-tier web application. The architecture consists of an internet-facing Application Load Balancer (ALB) and a web tier that is hosted on Amazon EC2 instances in private subnets. The application tier with the business logic runs on EC2 instances in private subnets. The database tier consists of Microsoft SQL Server that runs on EC2 instances in private subnets. Security is a high priority for the company. Which combination of security group configurations should the solutions architect use? (Choose three.)",
    "options_en": {
      "A": "Configure the security group for the web tier to allow inbound HTTPS trafic from the security group for the ALB.",
      "B": "Configure the security group for the web tier to allow outbound HTTPS trafic to 0.0.0.0/0.",
      "C": "Configure the security group for the database tier to allow inbound Microsoft SQL Server trafic from the security group for the application tier.",
      "D": "Configure the security group for the database tier to allow outbound HTTPS trafic and Microsoft SQL Server trafic to the security group for the web tier",
      "E": "Configure the security group for the application tier to allow inbound HTTPS trafic from the security group for the web tier",
      "F": "Configure the security group for the application tier to allow outbound HTTPS trafic and Microsoft SQL Server trafic to the security group for the web tier."
    },
    "correct_answer": "ACE",
    "vote_percentage": "",
    "question_cn": "一个解决方案架构师正在设计一个三层 Web 应用程序。该架构由一个面向互联网的 Application Load Balancer (ALB) 和一个托管在私有子网中的 Amazon EC2 实例上的 Web 层组成。带有业务逻辑的应用程序层在私有子网中的 EC2 实例上运行。数据库层由在私有子网中的 EC2 实例上运行的 Microsoft SQL Server 组成。安全性是该公司的高度优先事项。解决方案架构师应该使用哪种安全组配置组合？（选择三个。）",
    "options_cn": {
      "A": "将 Web 层的安全组配置为允许来自 ALB 安全组的入站 HTTPS 流量。",
      "B": "将 Web 层的安全组配置为允许到 0.0.0.0/0 的出站 HTTPS 流量。",
      "C": "将数据库层的安全组配置为允许来自应用程序层安全组的入站 Microsoft SQL Server 流量。",
      "D": "将数据库层的安全组配置为允许到 Web 层安全组的出站 HTTPS 流量和 Microsoft SQL Server 流量。",
      "E": "将应用程序层的安全组配置为允许来自 Web 层安全组的入站 HTTPS 流量。",
      "F": "将应用程序层的安全组配置为允许到 Web 层安全组的出站 HTTPS 流量和 Microsoft SQL Server 流量。"
    },
    "tags": [
      "ALB",
      "EC2",
      "Security Groups"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACE（社区 —），解析仅供参考。】\n\n该题考查了三层架构安全组的配置。需要配置安全组来控制入站和出站流量，实现最小权限原则，保障安全性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACE。理由简述：选项 A 正确，Web 层需要允许来自 ALB 的 HTTPS 流量以接收来自用户的请求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，Web 层出站 HTTPS 流量不应允许到 0.0.0.0/0，应该限制到特定服务或资源。选项 C 错误，数据库层需要允许来自应用程序层的 Microsoft SQL Server 流量，但方向相反。选项 D 错误，数据库层出站流量应控制，不应该允许到 Web 层。选项 E、F 错误，应用程序层不应允许 HTTPS 流量，应由 Web 层处理。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ALB",
      "HTTPS",
      "EC2",
      "Microsoft SQL Server",
      "Security Groups"
    ]
  },
  {
    "id": 885,
    "topic": "1",
    "question_en": "A company has released a new version of its production application. The company's workload uses Amazon EC2, AWS Lambda, AWS Fargate, and Amazon SageMaker. The company wants to cost optimize the workload now that usage is at a steady state. The company wants to cover the most services with the fewest savings plans. Which combination of savings plans will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and SageMaker.",
      "B": "Purchase a Compute Savings Plan for Amazon EC2, Lambda, and SageMaker.",
      "C": "Purchase a SageMaker Savings Plan.",
      "D": "Purchase a Compute Savings Plan for Lambda, Fargate, and Amazon EC2",
      "E": "Purchase an EC2 Instance Savings Plan for Amazon EC2 and Fargate."
    },
    "correct_answer": "CD",
    "vote_percentage": "100%",
    "question_cn": "一家公司发布了其生产应用程序的新版本。该公司的负载使用 Amazon EC2、AWS Lambda、AWS Fargate 和 Amazon SageMaker。该公司希望在用量达到稳定状态后对工作负载进行成本优化。该公司希望用最少的节省计划涵盖最多的服务。哪种节省计划组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "为 Amazon EC2 和 SageMaker 购买 EC2 实例节省计划。",
      "B": "为 Amazon EC2、Lambda 和 SageMaker 购买计算节省计划。",
      "C": "购买 SageMaker 节省计划。",
      "D": "为 Lambda、Fargate 和 Amazon EC2 购买计算节省计划。",
      "E": "为 Amazon EC2 和 Fargate 购买 EC2 实例节省计划。"
    },
    "tags": [
      "EC2",
      "Lambda",
      "SageMaker",
      "Fargate",
      "Savings Plans"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 100%），解析仅供参考。】\n\n考察 AWS 节省计划的覆盖范围和适用性，以及如何最大程度地利用节省计划来优化成本。",
      "why_correct": "选项 C 正确，因为 SageMaker 节省计划专门针对 SageMaker 实例。选项 D 正确，因为计算节省计划可以应用于 Lambda、Fargate 和 Amazon EC2。这组合提供了对多个服务的成本优化，满足了题目中“用最少的节省计划涵盖最多的服务”的要求。",
      "why_wrong": "选项 A 错误，EC2 实例节省计划仅涵盖 EC2 和 SageMaker，缺少对 Lambda 和 Fargate 的覆盖。选项 B 错误，计算节省计划可以覆盖 EC2、Lambda 和 SageMaker，但无法覆盖 Fargate。选项 E 错误，EC2 实例节省计划仅涵盖 EC2 和 Fargate，缺少对 Lambda 和 SageMaker 的覆盖，覆盖范围有限。"
    },
    "related_terms": [
      "Amazon EC2",
      "AWS Lambda",
      "AWS Fargate",
      "Amazon SageMaker",
      "EC2 Instance Savings Plans",
      "Compute Savings Plans"
    ]
  },
  {
    "id": 886,
    "topic": "1",
    "question_en": "A company uses a Microsoft SQL Server database. The company's applications are connected to the database. The company wants to migrate to an Amazon Aurora PostgreSQL database with minimal changes to the application code. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use the AWS Schema Conversion Tool (AWS SCT) to rewrite the SQL queries in the applications.",
      "B": "Enable Babelfish on Aurora PostgreSQL to run the SQL queries from the applications.",
      "C": "Migrate the database schema and data by using the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS).",
      "D": "Use Amazon RDS Proxy to connect the applications to Aurora PostgreSQL",
      "E": "Use AWS Database Migration Service (AWS DMS) to rewrite the SQL queries in the applications."
    },
    "correct_answer": "BC",
    "vote_percentage": "100%",
    "question_cn": "一家公司使用 Microsoft SQL Server 数据库。该公司的应用程序连接到该数据库。该公司希望以最少的应用程序代码更改迁移到 Amazon Aurora PostgreSQL 数据库。哪些步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Schema Conversion Tool (AWS SCT) 重写应用程序中的 SQL 查询。",
      "B": "在 Aurora PostgreSQL 上启用 Babelfish 以运行来自应用程序的 SQL 查询。",
      "C": "使用 AWS Schema Conversion Tool (AWS SCT) 和 AWS Database Migration Service (AWS DMS) 迁移数据库模式和数据。",
      "D": "使用 Amazon RDS Proxy 将应用程序连接到 Aurora PostgreSQL。",
      "E": "使用 AWS Database Migration Service (AWS DMS) 重写应用程序中的 SQL 查询。"
    },
    "tags": [
      "AWS SCT",
      "AWS DMS",
      "Aurora PostgreSQL",
      "Babelfish"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 100%），解析仅供参考。】\n\n考查迁移 Microsoft SQL Server 数据库到 Amazon Aurora PostgreSQL 的方法，以及如何最小化应用程序代码更改。",
      "why_correct": "选项 B 正确，因为 Babelfish for Aurora PostgreSQL 允许 Aurora PostgreSQL 理解 SQL Server 的 T-SQL 语法，从而减少应用程序代码更改的需求。选项 C 正确，AWS SCT 用于将数据库模式转换到 Aurora PostgreSQL，AWS DMS 用于迁移数据，是完成数据库迁移的关键步骤。",
      "why_wrong": "选项 A 错误，重写应用程序中的 SQL 查询会增加代码更改量，违背了题目要求。选项 D 错误，Amazon RDS Proxy 主要用于提高数据库连接效率和可用性，不能解决数据库语法兼容性问题，无法满足代码更改最小化的要求。选项 E 错误，使用 AWS DMS 重写应用程序中的 SQL 查询是不必要的，且会产生额外的代码变更工作量，不符合题目要求。"
    },
    "related_terms": [
      "Amazon Aurora PostgreSQL",
      "Microsoft SQL Server",
      "AWS Schema Conversion Tool (AWS SCT)",
      "AWS Database Migration Service (AWS DMS)",
      "Babelfish",
      "T-SQL",
      "Amazon RDS Proxy",
      "PostgreSQL"
    ]
  },
  {
    "id": 887,
    "topic": "1",
    "question_en": "A company plans to rehost an application to Amazon EC2 instances that use Amazon Elastic Block Store (Amazon EBS) as the attached storage. A solutions architect must design a solution to ensure that all newly created Amazon EBS volumes are encrypted by default. The solution must also prevent the creation of unencrypted EBS volumes. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the EC2 account attributes to always encrypt new EBS volumes.",
      "B": "Use AWS Config. Configure the encrypted-volumes identifier. Apply the default AWS Key Management Service (AWS KMS) key.",
      "C": "Configure AWS Systems Manager to create encrypted copies of the EBS volumes. Reconfigure the EC2 instances to use the encrypted volumes.",
      "D": "Create a customer managed key in AWS Key Management Service (AWS KMS). Configure AWS Migration Hub to use the key when the company migrates workloads."
    },
    "correct_answer": "A",
    "vote_percentage": "67%",
    "question_cn": "一家公司计划将应用程序重新托管到使用 Amazon Elastic Block Store (Amazon EBS) 作为附加存储的 Amazon EC2 实例。一位解决方案架构师必须设计一个解决方案，以确保所有新创建的 Amazon EBS 卷都默认加密。该解决方案还必须防止创建未加密的 EBS 卷。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 EC2 账户属性以始终加密新的 EBS 卷。",
      "B": "使用 AWS Config。配置 encrypted-volumes 标识符。应用默认的 AWS Key Management Service (AWS KMS) 密钥。",
      "C": "配置 AWS Systems Manager 以创建 EBS 卷的加密副本。重新配置 EC2 实例以使用加密卷。",
      "D": "在 AWS Key Management Service (AWS KMS) 中创建客户托管密钥。配置 AWS Migration Hub 在公司迁移工作负载时使用该密钥。"
    },
    "tags": [
      "EBS",
      "Encryption",
      "KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 67%），解析仅供参考。】\n\n此题考查 EBS 卷的默认加密设置。通过配置账户属性，可以确保新创建的 EBS 卷默认加密。",
      "why_correct": "选项 A 正确，在 EC2 账户属性中设置始终加密新的 EBS 卷，可以实现默认加密并防止未加密卷的创建。",
      "why_wrong": "选项 B 错误，AWS Config 只能用来检测，不能强制加密。选项 C 错误，AWS Systems Manager 不能用来创建 EBS 卷。选项 D 错误，AWS Migration Hub 不能创建加密卷。"
    },
    "related_terms": [
      "EBS",
      "KMS",
      "AWS Config",
      "AWS Systems Manager",
      "AWS Migration Hub",
      "Encryption"
    ]
  },
  {
    "id": 888,
    "topic": "1",
    "question_en": "An ecommerce company wants to collect user clickstream data from the company's website for real-time analysis. The website experiences fiuctuating trafic patterns throughout the day. The company needs a scalable solution that can adapt to varying levels of trafic. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use a data stream in Amazon Kinesis Data Streams in on-demand mode to capture the clickstream data. Use AWS Lambda to process the data in real time.",
      "B": "Use Amazon Kinesis Data Firehose to capture the clickstream data. Use AWS Glue to process the data in real time.",
      "C": "Use Amazon Kinesis Video Streams to capture the clickstream data. Use AWS Glue to process the data in real time.",
      "D": "Use Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) to capture the clickstream data. Use AWS Lambda to process the data in real time."
    },
    "correct_answer": "A",
    "vote_percentage": "88%",
    "question_cn": "一家电子商务公司希望从该公司的网站收集用户点击流数据，以进行实时分析。该网站在一天中会经历波动的流量模式。该公司需要一个可扩展的解决方案，该解决方案可以适应不同级别的流量。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Kinesis Data Streams 中的数据流以按需模式捕获点击流数据。使用 AWS Lambda 实时处理数据。",
      "B": "使用 Amazon Kinesis Data Firehose 捕获点击流数据。使用 AWS Glue 实时处理数据。",
      "C": "使用 Amazon Kinesis Video Streams 捕获点击流数据。使用 AWS Glue 实时处理数据。",
      "D": "使用 Amazon Managed Service for Apache Flink（以前称为 Amazon Kinesis Data Analytics）来捕获点击流数据。使用 AWS Lambda 实时处理数据。"
    },
    "tags": [
      "Kinesis Data Streams",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 88%），解析仅供参考。】\n\n该题考查了实时数据流处理的架构设计。Kinesis Data Streams 结合 Lambda 可以实现可扩展的实时数据处理。",
      "why_correct": "选项 A 正确，使用 Kinesis Data Streams 捕获点击流数据，Lambda 实时处理数据，可以根据流量动态扩展。",
      "why_wrong": "选项 B 错误，Kinesis Data Firehose 不支持按需模式。选项 C 错误，Kinesis Video Streams 用于视频流。选项 D 错误，Kinesis Data Analytics 无法捕获点击流。"
    },
    "related_terms": [
      "Kinesis Data Streams",
      "Lambda",
      "Kinesis Data Firehose",
      "Kinesis Video Streams",
      "Kinesis Data Analytics"
    ]
  },
  {
    "id": 889,
    "topic": "1",
    "question_en": "A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up an AWS CloudTrail event that has a rule to identify all S3 buckets that are not versioning-enabled across Regions.",
      "B": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.",
      "C": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.",
      "D": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家全球性公司在 AWS 上运行其工作负载。该公司的应用程序使用 Amazon S3 存储桶在不同的 AWS 区域中存储和分析敏感数据。该公司每天在多个 S3 存储桶中存储数百万个对象。该公司希望识别所有未启用版本控制的 S3 存储桶。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置一个 AWS CloudTrail 事件，该事件有一个规则来识别跨区域的所有未启用版本控制的 S3 存储桶。",
      "B": "使用 Amazon S3 Storage Lens 来识别跨区域的所有未启用版本控制的 S3 存储桶。",
      "C": "为 S3 启用 IAM Access Analyzer 以识别跨区域的所有未启用版本控制的 S3 存储桶。",
      "D": "创建一个 S3 多区域访问点来识别所有未启用版本控制的 S3 存储桶。"
    },
    "tags": [
      "S3",
      "Storage Lens",
      "Version Control"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查了 S3 存储桶的版本控制检查。S3 Storage Lens 可以用于分析和识别没有启用版本控制的 S3 存储桶。",
      "why_correct": "选项 B 正确，S3 Storage Lens 可以识别未启用版本控制的 S3 存储桶。",
      "why_wrong": "选项 A 错误，CloudTrail 不提供对存储桶版本控制的检测。选项 C 错误，IAM Access Analyzer 用于识别访问权限问题。选项 D 错误，S3 多区域访问点不提供版本控制检查功能。"
    },
    "related_terms": [
      "S3",
      "CloudTrail",
      "S3 多区域访问点",
      "Storage Lens",
      "IAM Access Analyzer",
      "Version Control"
    ]
  },
  {
    "id": 890,
    "topic": "1",
    "question_en": "A company needs to optimize its Amazon S3 storage costs for an application that generates many files that cannot be recreated. Each file is approximately 5 MB and is stored in Amazon S3 Standard storage. The company must store the files for 4 years before the files can be deleted. The files must be immediately accessible. The files are frequently accessed in the first 30 days of object creation, but they are rarely accessed after the first 30 days. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an S3 Lifecycle policy to move the files to S3 Glacier Instant Retrieval 30 days after object creation. Delete the files 4 years after object creation.",
      "B": "Create an S3 Lifecycle policy to move the files to S3 One Zone-Infrequent Access (S3 One Zone-IA) 30 days after object creation. Delete the files 4 years after object creation.",
      "C": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Delete the files 4 years after object creation.",
      "D": "Create an S3 Lifecycle policy to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) 30 days after object creation. Move the files to S3 Glacier Flexible Retrieval 4 years after object creation."
    },
    "correct_answer": "C",
    "vote_percentage": "50%",
    "question_cn": "一家公司需要优化其 Amazon S3 存储成本，该应用程序生成大量无法重新创建的文件。每个文件大约为 5 MB，存储在 Amazon S3 Standard 存储中。该公司必须将文件存储 4 年才能删除这些文件。必须立即访问这些文件。这些文件在对象创建后的前 30 天内经常被访问，但在前 30 天后很少被访问。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "创建一个 S3 生命周期策略，在对象创建 30 天后将文件移动到 S3 Glacier Instant Retrieval。在对象创建 4 年后删除这些文件。",
      "B": "创建一个 S3 生命周期策略，在对象创建 30 天后将文件移动到 S3 One Zone-Infrequent Access (S3 One Zone-IA)。在对象创建 4 年后删除这些文件。",
      "C": "创建一个 S3 生命周期策略，在对象创建 30 天后将文件移动到 S3 Standard-Infrequent Access (S3 Standard-IA)。在对象创建 4 年后删除这些文件。",
      "D": "创建一个 S3 生命周期策略，在对象创建 30 天后将文件移动到 S3 Standard-Infrequent Access (S3 Standard-IA)。在对象创建 4 年后将文件移动到 S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "S3",
      "Lifecycle Policies",
      "Storage Classes"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 50%），解析仅供参考。】\n\n考查 S3 存储类优化，根据访问频率和存储时长的不同，选择合适的 S3 生命周期策略以降低成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：S3 Standard-IA 存储类专为不经常访问但需要快速访问的数据设计。题目中文件在前 30 天后访问频率降低，满足了 S3 Standard-IA 的适用场景。S3 Standard-IA 的存储成本低于 S3 Standard，符合成本效益要求。4 年的存储时间可以通过生命周期策略配置来实现。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A，S3 Glacier Instant Retrieval 虽然可以立即访问，但其存储成本较高，不适合主要在创建后 30 天内访问的场景。选项 B，S3 One Zone-IA 虽然成本更低，但其数据冗余在单个可用区内，不适合无法重新创建的、需要高可用性的数据。选项 D，S3 Glacier Flexible Retrieval 适用于长期存储，但其检索时间较长，不符合需要立即访问的要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier Instant Retrieval",
      "S3 One Zone-IA",
      "S3 Glacier Flexible Retrieval",
      "S3 lifecycle policy"
    ]
  },
  {
    "id": 891,
    "topic": "1",
    "question_en": "A company runs its critical storage application in the AWS Cloud. The application uses Amazon S3 in two AWS Regions. The company wants the application to send remote user data to the nearest S3 bucket with no public network congestion. The company also wants the application to fail over with the least amount of management of Amazon S3. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement an active-active design between the two Regions. Configure the application to use the regional S3 endpoints closest to the user.",
      "B": "Use an active-passive configuration with S3 Multi-Region Access Points. Create a global endpoint for each of the Regions.",
      "C": "Send user data to the regional S3 endpoints closest to the user. Configure an S3 cross-account replication rule to keep the S3 buckets synchronized.",
      "D": "Set up Amazon S3 to use Multi-Region Access Points in an active-active configuration with a single global endpoint. Configure S3 Cross- Region Replication."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 云中运行其关键存储应用程序。该应用程序在两个 AWS 区域中使用 Amazon S3。公司希望应用程序将远程用户数据发送到最近的 S3 存储桶，而不会出现公共网络拥塞。公司还希望该应用程序以最少的 Amazon S3 管理方式进行故障转移。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在两个区域之间实施主动-主动设计。配置应用程序以使用最靠近用户的区域 S3 终端节点。",
      "B": "使用带有 S3 多区域访问点的活动-被动配置。为每个区域创建一个全局终端节点。",
      "C": "将用户数据发送到最靠近用户的区域 S3 终端节点。配置 S3 跨区域复制规则以保持 S3 存储桶同步。",
      "D": "设置 Amazon S3 以使用多区域访问点，采用主动-主动配置，使用单个全局终端节点。配置 S3 跨区域复制。"
    },
    "tags": [
      "S3",
      "Multi-Region Access Points"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考查了如何利用 S3 的多区域访问点（Multi-Region Access Point）和跨区域复制（Cross-Region Replication）来实现用户数据就近访问和故障转移，并减少管理成本。",
      "why_correct": "选项 D 提供了最佳解决方案。通过使用 S3 Multi-Region Access Point，可以为用户提供单个全局终端节点，自动将用户请求路由到最近的 S3 存储桶，实现就近访问。同时，配置 S3 跨区域复制，可以在不同区域之间同步数据，实现应用程序的故障转移。",
      "why_wrong": "选项 A 的问题在于，主动-主动设计虽然可以实现就近访问，但需要应用程序自行判断和选择区域终端节点，增加了应用程序的复杂性。选项 B 描述的多区域访问点配置不完整，未能说明如何通过全局终端节点实现就近访问。选项 C 方案也依赖于应用程序判断和选择区域终端节点，且只描述了 S3 跨区域复制，缺少就近访问的实现。并且没有利用 S3 Multi-Region Access Point 的优势，增加了应用程序管理的复杂性。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Multi-Region Access Point",
      "Cross-Region Replication",
      "AWS"
    ]
  },
  {
    "id": 892,
    "topic": "1",
    "question_en": "A company is migrating a data center from its on-premises location to AWS. The company has several legacy applications that are hosted on individual virtual servers. Changes to the application designs cannot be made. Each individual virtual server currently runs as its own EC2 instance. A solutions architect needs to ensure that the applications are reliable and fault tolerant after migration to AWS. The applications will run on Amazon EC2 instances. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Auto Scaling group that has a minimum of one and a maximum of one. Create an Amazon Machine Image (AMI) of each application instance. Use the AMI to create EC2 instances in the Auto Scaling group Configure an Application Load Balancer in front of the Auto Scaling group.",
      "B": "Use AWS Backup to create an hourly backup of the EC2 instance that hosts each application. Store the backup in Amazon S3 in a separate Availability Zone. Configure a disaster recovery process to restore the EC2 instance for each application from its most recent backup.",
      "C": "Create an Amazon Machine Image (AMI) of each application instance. Launch two new EC2 instances from the AMI. Place each EC2 instance in a separate Availability Zone. Configure a Network Load Balancer that has the EC2 instances as targets.",
      "D": "Use AWS Mitigation Hub Refactor Spaces to migrate each application off the EC2 instance. Break down functionality from each application into individual components. Host each application on Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type."
    },
    "correct_answer": "A",
    "vote_percentage": "50%",
    "question_cn": "一家公司正在将其数据中心从本地位置迁移到 AWS。该公司有几个托管在独立虚拟服务器上的旧版应用程序。无法对应用程序设计进行更改。每个独立的虚拟服务器当前都作为其自己的 EC2 实例运行。解决方案架构师需要确保在迁移到 AWS 后，应用程序具有可靠性和容错性。应用程序将在 Amazon EC2 实例上运行。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Auto Scaling 组，该组的最小值为 1，最大值为 1。创建每个应用程序实例的 Amazon Machine Image (AMI)。使用 AMI 在 Auto Scaling 组中创建 EC2 实例。在 Auto Scaling 组的前面配置一个 Application Load Balancer。",
      "B": "使用 AWS Backup 为托管每个应用程序的 EC2 实例创建每小时备份。将备份存储在不同可用区的 Amazon S3 中。配置灾难恢复流程，以从其最近的备份中恢复每个应用程序的 EC2 实例。",
      "C": "创建每个应用程序实例的 Amazon Machine Image (AMI)。从 AMI 启动两个新的 EC2 实例。将每个 EC2 实例放置在不同的可用区中。配置一个 Network Load Balancer，该负载均衡器将 EC2 实例作为目标。",
      "D": "使用 AWS Mitigation Hub Refactor Spaces 从 EC2 实例中迁移每个应用程序。将每个应用程序的功能分解为各个组件。将每个应用程序托管在 Amazon Elastic Container Service (Amazon ECS) 上，并使用 AWS Fargate 启动类型。"
    },
    "tags": [
      "EC2",
      "Load Balancer",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 50%），解析仅供参考。】\n\n考查在迁移旧版应用程序时，如何通过 Auto Scaling、AMI 和 Application Load Balancer 来提高可靠性和容错性，同时满足应用无法修改的设计限制。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 提供了针对旧版应用程序的可靠且容错的解决方案，这些应用程序无法进行设计更改。使用 Auto Scaling 组确保至少运行一个实例，即使实例出现故障也能自动替换。AMI 允许创建一致的实例。Application Load Balancer 将流量分发到 EC2 实例，从而提高了可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 侧重于备份和恢复，而非主动的可靠性和容错性。虽然备份很重要，但它无法解决实例故障或高负载下的问题。选项 C 仅提供了冗余，但没有自动化的故障转移和弹性伸缩能力。此外，如果单个实例过载，Network Load Balancer 也无法提供负载均衡。选项 D 建议使用 Refactor Spaces 和 ECS，这需要对应用程序进行大量更改，而题目明确指出无法对应用程序进行更改。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon Machine Image (AMI)",
      "Application Load Balancer",
      "AWS Backup",
      "Amazon S3",
      "Network Load Balancer",
      "AWS Mitigation Hub Refactor Spaces",
      "Amazon Elastic Container Service (Amazon ECS)",
      "AWS Fargate"
    ]
  },
  {
    "id": 893,
    "topic": "1",
    "question_en": "A company wants to isolate its workloads by creating an AWS account for each workload. The company needs a solution that centrally manages networking components for the workloads. The solution also must create accounts with automatic security controls (guardrails). Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS Control Tower to deploy accounts. Create a networking account that has a VPC with private subnets and public subnets. Use AWS Resource Access Manager (AWS RAM) to share the subnets with the workload accounts.",
      "B": "Use AWS Organizations to deploy accounts. Create a networking account that has a VPC with private subnets and public subnets. Use AWS Resource Access Manager (AWS RAM) to share the subnets with the workload accounts.",
      "C": "Use AWS Control Tower to deploy accounts. Deploy a VPC in each workload account. Configure each VPC to route through an inspection VPC by using a transit gateway attachment.",
      "D": "Use AWS Organizations to deploy accounts. Deploy a VPC in each workload account. Configure each VPC to route through an inspection VPC by using a transit gateway attachment."
    },
    "correct_answer": "A",
    "vote_percentage": "67%",
    "question_cn": "一家公司希望通过为每个工作负载创建 AWS 账户来隔离其工作负载。该公司需要一个解决方案来集中管理工作负载的网络组件。该解决方案还必须创建具有自动安全控制（护栏）的账户。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Control Tower 部署账户。创建一个网络账户，该账户有一个带有私有子网和公有子网的 VPC。使用 AWS Resource Access Manager (AWS RAM) 与工作负载账户共享子网。",
      "B": "使用 AWS Organizations 部署账户。创建一个网络账户，该账户有一个带有私有子网和公有子网的 VPC。使用 AWS Resource Access Manager (AWS RAM) 与工作负载账户共享子网。",
      "C": "使用 AWS Control Tower 部署账户。在每个工作负载账户中部署一个 VPC。配置每个 VPC，通过使用转接网关附件路由到检查 VPC。",
      "D": "使用 AWS Organizations 部署账户。在每个工作负载账户中部署一个 VPC。配置每个 VPC，通过使用转接网关附件路由到检查 VPC。"
    },
    "tags": [
      "AWS Control Tower",
      "AWS Organizations",
      "VPC",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 67%），解析仅供参考。】\n\n此题考查了多账户环境的网络隔离和安全控制。使用 Control Tower 可以集中管理和部署账户，并实施安全护栏。",
      "why_correct": "选项 A 正确，使用 Control Tower 部署账户，创建网络账户，共享子网可以实现账户隔离和集中管理。",
      "why_wrong": "选项 B 错误，AWS Organizations 无法实现安全护栏。选项 C 错误，在每个账户中部署 VPC 增加了管理复杂性。选项 D 错误，在每个账户中部署 VPC 增加了管理复杂性。"
    },
    "related_terms": [
      "AWS Control Tower",
      "AWS Organizations",
      "VPC",
      "AWS RAM",
      "Transit Gateway"
    ]
  },
  {
    "id": 894,
    "topic": "1",
    "question_en": "A company hosts a website on Amazon EC2 instances behind an Application Load Balancer (ALB). The website serves static content. Website trafic is increasing. The company wants to minimize the website hosting costs. Which solution will meet these requirements?",
    "options_en": {
      "A": "Move the website to an Amazon S3 bucket. Configure an Amazon CloudFront distribution for the S3 bucket.",
      "B": "Move the website to an Amazon S3 bucket. Configure an Amazon ElastiCache cluster for the S3 bucket.",
      "C": "Move the website to AWS Amplify. Configure an ALB to resolve to the Amplify website.",
      "D": "Move the website to AWS Amplify. Configure EC2 instances to cache the website."
    },
    "correct_answer": "A",
    "vote_percentage": "78%",
    "question_cn": "一家公司在其Application Load Balancer (ALB) 之后，在Amazon EC2实例上托管一个网站。该网站提供静态内容。网站流量正在增加。该公司希望最大限度地减少网站托管成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将网站迁移到Amazon S3 存储桶。为S3 存储桶配置Amazon CloudFront 分发。",
      "B": "将网站迁移到Amazon S3 存储桶。为S3 存储桶配置Amazon ElastiCache 集群。",
      "C": "将网站迁移到AWS Amplify。配置ALB以解析到Amplify网站。",
      "D": "将网站迁移到AWS Amplify。配置EC2实例来缓存网站。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "Websites"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 78%），解析仅供参考。】\n\n此题考查了网站托管的成本优化。将静态网站托管在 S3 上，并使用 CloudFront 可以实现低成本和高可用性。",
      "why_correct": "选项 A 正确，将网站迁移到 S3 并使用 CloudFront，可以降低成本，提高性能。",
      "why_wrong": "选项 B 错误，ElastiCache 主要用于缓存，不适合托管网站。选项 C 错误，Amplify 与 ALB 配合使用不适用于静态网站。选项 D 错误，配置 EC2 实例用于缓存网站增加了复杂性。"
    },
    "related_terms": [
      "S3",
      "CloudFront",
      "ElastiCache",
      "AWS Amplify",
      "ALB"
    ]
  },
  {
    "id": 895,
    "topic": "1",
    "question_en": "A company is implementing a shared storage solution for a media application that the company hosts on AWS. The company needs the ability to use SMB clients to access stored data. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Create an AWS Storage Gateway Volume Gateway. Create a file share that uses the required client protocol. Connect the application server to the file share.",
      "B": "Create an AWS Storage Gateway Tape Gateway. Configure tapes to use Amazon S3. Connect the application server to the Tape Gateway.",
      "C": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the instance. Connect the application server to the file share.",
      "D": "Create an Amazon FSx for Windows File Server file system. Connect the application server to the file system."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为其在 AWS 上托管的媒体应用程序实施共享存储解决方案。该公司需要使用 SMB 客户端访问存储数据的能力。哪种解决方案以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "创建 AWS Storage Gateway Volume Gateway。创建一个使用所需客户端协议的文件共享。将应用程序服务器连接到文件共享。",
      "B": "创建 AWS Storage Gateway Tape Gateway。配置磁带以使用 Amazon S3。将应用程序服务器连接到 Tape Gateway。",
      "C": "创建 Amazon EC2 Windows 实例。在该实例上安装并配置 Windows 文件共享角色。将应用程序服务器连接到文件共享。",
      "D": "创建 Amazon FSx for Windows File Server 文件系统。将应用程序服务器连接到文件系统。"
    },
    "tags": [
      "Storage Gateway",
      "FSx for Windows File Server",
      "SMB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n此题考查了使用 SMB 协议的共享存储方案。FSx for Windows File Server 提供了最简单的解决方案。",
      "why_correct": "选项 D 正确，FSx for Windows File Server 支持 SMB 协议，提供共享存储。",
      "why_wrong": "选项 A 错误，Volume Gateway 提供的是 iSCSI 块存储。选项 B 错误，Tape Gateway 提供的是磁带存储。选项 C 错误，在 EC2 实例上搭建 Windows 文件共享角色增加了管理开销。"
    },
    "related_terms": [
      "Storage Gateway",
      "FSx for Windows File Server",
      "SMB",
      "Volume Gateway",
      "Tape Gateway"
    ]
  },
  {
    "id": 896,
    "topic": "1",
    "question_en": "A company is designing its production application's disaster recovery (DR) strategy. The application is backed by a MySQL database on an Amazon Aurora cluster in the us-east-1 Region. The company has chosen the us-west-1 Region as its DR Region. The company's target recovery point objective (RPO) is 5 minutes and the target recovery time objective (RTO) is 20 minutes. The company wants to minimize configuration changes. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create an Aurora read replica in us-west-1 similar in size to the production application's Aurora MySQL cluster writer instance.",
      "B": "Convert the Aurora cluster to an Aurora global database. Configure managed failover.",
      "C": "Create a new Aurora cluster in us-west-1 that has Cross-Region Replication.",
      "D": "Create a new Aurora cluster in us-west-1. Use AWS Database Migration Service (AWS DMS) to sync both clusters."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在为其生产应用程序设计灾难恢复 (DR) 策略。该应用程序由 us-east-1 区域中 Amazon Aurora 集群上的 MySQL 数据库提供支持。该公司已选择 us-west-1 区域作为其 DR 区域。该公司的目标恢复点目标 (RPO) 为 5 分钟，目标恢复时间目标 (RTO) 为 20 分钟。该公司希望尽量减少配置更改。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "在 us-west-1 中创建一个 Aurora 只读副本，其大小与生产应用程序的 Aurora MySQL 集群写入器实例相似。",
      "B": "将 Aurora 集群转换为 Aurora 全局数据库。配置托管故障转移。",
      "C": "在 us-west-1 中创建一个新的 Aurora 集群，该集群具有跨区域复制。",
      "D": "在 us-west-1 中创建一个新的 Aurora 集群。使用 AWS Database Migration Service (AWS DMS) 同步这两个集群。"
    },
    "tags": [
      "Aurora",
      "Disaster Recovery"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考查了 Aurora 数据库的灾难恢复策略。使用 Aurora 全局数据库可以实现 RPO 和 RTO 的目标。",
      "why_correct": "选项 B 正确，Aurora 全局数据库可以实现快速故障转移，满足 RPO 和 RTO 要求。",
      "why_wrong": "选项 A 错误，只读副本不具备故障转移能力。选项 C 错误，跨区域复制可能导致 RPO 不满足要求。选项 D 错误，DMS 无法达到 5 分钟的 RPO。"
    },
    "related_terms": [
      "Aurora",
      "RPO",
      "RTO",
      "AWS DMS",
      "Disaster Recovery",
      "Aurora 全局数据库"
    ]
  },
  {
    "id": 897,
    "topic": "1",
    "question_en": "A company runs a critical data analysis job each week before the first day of the work week. The job requires at least 1 hour to complete the analysis. The job is stateful and cannot tolerate interruptions. The company needs a solution to run the job on AWS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a container for the job. Schedule the job to run as an AWS Fargate task on an Amazon Elastic Container Service (Amazon ECS) cluster by using Amazon EventBridge Scheduler.",
      "B": "Configure the job to run in an AWS Lambda function. Create a scheduled rule in Amazon EventBridge to invoke the Lambda function.",
      "C": "Configure an Auto Scaling group of Amazon EC2 Spot Instances that run Amazon Linux. Configure a crontab entry on the instances to run the analysis.",
      "D": "Configure an AWS DataSync task to run the job. Configure a cron expression to run the task on a schedule."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司在每周工作日开始前运行一项关键的数据分析工作。该工作需要至少 1 小时才能完成分析。该工作是有状态的，并且不能容忍中断。该公司需要在 AWS 上运行该工作的解决方案。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为该工作创建一个容器。使用 Amazon EventBridge Scheduler，将该工作安排为在 Amazon Elastic Container Service (Amazon ECS) 集群上的 AWS Fargate 任务运行。",
      "B": "配置该工作在 AWS Lambda 函数中运行。在 Amazon EventBridge 中创建一个计划规则来调用 Lambda 函数。",
      "C": "配置一个运行 Amazon Linux 的 Amazon EC2 Spot 实例的 Auto Scaling 组。在实例上配置一个 crontab 条目以运行分析。",
      "D": "配置一个 AWS DataSync 任务来运行该工作。配置一个 cron 表达式以按计划运行该任务。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "EventBridge"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考查了定时任务的运行。使用 ECS Fargate 运行容器化任务，并结合 EventBridge 可以实现可靠的定时任务。",
      "why_correct": "选项 A 正确，ECS Fargate 运行容器化任务，EventBridge 调度，可以满足要求。",
      "why_wrong": "选项 B 错误，Lambda 运行时长限制，无法满足至少 1 小时的需求。选项 C 错误，Spot 实例不可靠，分析工作中断。选项 D 错误，DataSync 任务不适用于长时间运行的任务。"
    },
    "related_terms": [
      "ECS",
      "Fargate",
      "EventBridge",
      "Lambda",
      "EC2 Spot 实例",
      "AWS DataSync"
    ]
  },
  {
    "id": 898,
    "topic": "1",
    "question_en": "A company runs workloads in the AWS Cloud. The company wants to centrally collect security data to assess security across the entire company and to improve workload protection. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Configure a data lake in AWS Lake Formation. Use AWS Glue crawlers to ingest the security data into the data lake.",
      "B": "Configure an AWS Lambda function to collect the security data in .csv format. Upload the data to an Amazon S3 bucket.",
      "C": "Configure a data lake in Amazon Security Lake to collect the security data. Upload the data to an Amazon S3 bucket.",
      "D": "Configure an AWS Database Migration Service (AWS DMS) replication instance to load the security data into an Amazon RDS cluster."
    },
    "correct_answer": "C",
    "vote_percentage": "100%",
    "question_cn": "一家公司在 AWS 云中运行工作负载。该公司希望集中收集安全数据，以评估整个公司的安全性并提高工作负载保护。哪种解决方案将以最小的开发工作量满足这些要求？",
    "options_cn": {
      "A": "在 AWS Lake Formation 中配置一个数据湖。使用 AWS Glue 爬虫将安全数据提取到数据湖中。",
      "B": "配置一个 AWS Lambda 函数以 .csv 格式收集安全数据。将数据上传到 Amazon S3 存储桶。",
      "C": "配置 Amazon Security Lake 中的一个数据湖以收集安全数据。将数据上传到 Amazon S3 存储桶。",
      "D": "配置 AWS Database Migration Service (AWS DMS) 复制实例，以将安全数据加载到 Amazon RDS 集群中。"
    },
    "tags": [
      "Security Lake"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 100%），解析仅供参考。】\n\n此题考查了安全数据的集中收集。使用 Security Lake 可以简化安全数据的收集和分析。",
      "why_correct": "选项 C 正确，Security Lake 可以集中收集安全数据，并存储到 S3 中。",
      "why_wrong": "选项 A 错误，Lake Formation 无法直接收集安全数据。选项 B 错误，使用 Lambda 和 S3 增加了开发量。选项 D 错误，DMS 用于数据迁移，不用于安全数据收集。"
    },
    "related_terms": [
      "AWS Lake Formation",
      "Lambda",
      "S3",
      "AWS DMS",
      "RDS",
      "Security Lake"
    ]
  },
  {
    "id": 899,
    "topic": "1",
    "question_en": "A company is migrating five on-premises applications to VPCs in the AWS Cloud. Each application is currently deployed in isolated virtual networks on premises and should be deployed similarly in the AWS Cloud. The applications need to reach a shared services VPC. All the applications must be able to communicate with each other. If the migration is successful, the company will repeat the migration process for more than 100 applications. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Deploy software VPN tunnels between the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets to the shared services VPC.",
      "B": "Deploy VPC peering connections between the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets to the shared services VPC through the peering connection.",
      "C": "Deploy an AWS Direct Connect connection between the application VPCs and the shared services VPAdd routes from the application VPCs in their subnets to the shared services VPC and the applications VPCs. Add routes from the shared services VPC subnets to the applications VPCs.",
      "D": "Deploy a transit gateway with associations between the transit gateway and the application VPCs and the shared services VPC. Add routes between the application VPCs in their subnets and the application VPCs to the shared services VPC through the transit gateway."
    },
    "correct_answer": "D",
    "vote_percentage": "88%",
    "question_cn": "一家公司正在将五个本地应用程序迁移到 AWS 云中的 VPC。每个应用程序目前都部署在本地的独立虚拟网络中，并且应在 AWS 云中进行类似的部署。应用程序需要连接到共享服务 VPC。所有应用程序都必须能够相互通信。如果迁移成功，公司将为 100 多个应用程序重复迁移过程。哪种解决方案以最小的管理开销满足这些要求？",
    "options_cn": {
      "A": "在应用程序 VPC 和共享服务 VPC 之间部署软件 VPN 隧道。在应用程序 VPC 的子网中添加路由到共享服务 VPC。",
      "B": "在应用程序 VPC 和共享服务 VPC 之间部署 VPC 对等连接。通过对等连接，在应用程序 VPC 的子网中添加路由到共享服务 VPC。",
      "C": "在应用程序 VPC 和共享服务 VPC 之间部署 AWS Direct Connect 连接。从应用程序 VPC 的子网到共享服务 VPC 和应用程序 VPC 添加路由。从共享服务 VPC 子网到应用程序 VPC 添加路由。",
      "D": "部署一个过渡网关，并在过渡网关与应用程序 VPC 和共享服务 VPC 之间建立关联。通过过渡网关，在应用程序 VPC 的子网中添加路由到应用程序 VPC 到共享服务 VPC 的路由。"
    },
    "tags": [
      "VPC",
      "VPN",
      "VPC Peering",
      "Direct Connect",
      "Transit Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 88%），解析仅供参考。】\n\n该题考查的是多 VPC 间连接方案的比较。Transit Gateway 是一个中心枢纽，可以简化 VPC 间的连接。它允许应用程序 VPC 和共享服务 VPC 之间的路由更集中地管理。使用 Transit Gateway 可以更容易地管理具有大量 VPC 的环境，并减少管理开销。",
      "why_correct": "D. 使用 Transit Gateway 可以创建一个中心枢纽来连接所有 VPC，并简化路由配置。Transit Gateway 允许集中管理 VPC 间的连接，适用于需要连接大量 VPC 的场景，管理开销最小。",
      "why_wrong": "A. 软件 VPN 隧道在每个 VPC 间都需要单独配置，管理开销较高，不适合大规模环境。B. VPC 对等连接需要为每个 VPC 对单独配置路由，当 VPC 数量增长时，管理难度增加。C. Direct Connect 旨在提供本地网络和 AWS 之间的连接，而非 VPC 间的连接，虽然可以实现，但配置复杂，不符合题目要求。"
    },
    "related_terms": [
      "VPC",
      "VPN",
      "VPC Peering",
      "Direct Connect",
      "Transit Gateway",
      "EC2"
    ]
  },
  {
    "id": 900,
    "topic": "1",
    "question_en": "A company wants to use Amazon Elastic Container Service (Amazon ECS) to run its on-premises application in a hybrid environment. The application currently runs on containers on premises. The company needs a single container solution that can scale in an on-premises, hybrid, or cloud environment. The company must run new application containers in the AWS Cloud and must use a load balancer for HTTP trafic. Which combination of actions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Set up an ECS cluster that uses the AWS Fargate launch type for the cloud application containers. Use an Amazon ECS Anywhere external launch type for the on-premises application containers.",
      "B": "Set up an Application Load Balancer for cloud ECS services.",
      "C": "Set up a Network Load Balancer for cloud ECS services.",
      "D": "Set up an ECS cluster that uses the AWS Fargate launch typ",
      "E": "Use Fargate for the cloud application containers and the on-premises application containers. E. Set up an ECS cluster that uses the Amazon EC2 launch type for the cloud application containers. Use Amazon ECS Anywhere with an AWS Fargate launch type for the on-premises application containers."
    },
    "correct_answer": "AB",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望使用 Amazon Elastic Container Service (Amazon ECS) 在混合环境中运行其本地应用程序。该应用程序目前在本地容器上运行。该公司需要一个单一的容器解决方案，该解决方案可以在本地、混合或云环境中扩展。该公司必须在 AWS 云中运行新的应用程序容器，并且必须使用负载均衡器进行 HTTP 流量。哪种组合的操作将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "设置一个 ECS 集群，该集群使用 AWS Fargate 启动类型来实现云应用程序容器。使用 Amazon ECS Anywhere 外部启动类型来实现本地应用程序容器。",
      "B": "为云 ECS 服务设置一个 Application Load Balancer。",
      "C": "为云 ECS 服务设置一个 Network Load Balancer。",
      "D": "设置一个 ECS 集群，该集群使用 AWS Fargate 启动类型。将 Fargate 用于云应用程序容器和本地应用程序容器。",
      "E": "设置一个 ECS 集群，该集群使用 Amazon EC2 启动类型来实现云应用程序容器。使用 Amazon ECS Anywhere 并使用 AWS Fargate 启动类型来实现本地应用程序容器。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "EC2",
      "Application Load Balancer",
      "Elastic Container Service"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AB（社区 100%），解析仅供参考。】\n\n此题考察 ECS 混合部署的解决方案。ECS Anywhere 允许在本地环境运行 ECS 任务，而 Fargate 是一种无服务器计算引擎，用于运行容器。ALB 适用于 HTTP/HTTPS 流量的负载均衡。",
      "why_correct": "A. ECS Anywhere 可以在本地运行容器，Fargate 可以在云端运行容器，满足混合环境需求。使用 ALB 可以满足 HTTP 流量的负载均衡。满足了题目要求。",
      "why_wrong": "B. ALB 只是负载均衡器，不能独立满足混合环境和容器化需求。C. Network Load Balancer 仅适用于 TCP/UDP 流量，不适用于 HTTP。D. Fargate 无法在本地环境运行容器。E.  EC2 不满足单一容器解决方案的要求。"
    },
    "related_terms": [
      "ECS",
      "Fargate",
      "EC2",
      "Application Load Balancer",
      "Elastic Container Service",
      "Network Load Balancer",
      "Amazon ECS Anywhere"
    ]
  },
  {
    "id": 901,
    "topic": "1",
    "question_en": "A company is migrating its workloads to AWS. The company has sensitive and critical data in on-premises relational databases that run on SQL Server instances. The company wants to use the AWS Cloud to increase security and reduce operational overhead for the databases. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the databases to Amazon EC2 instances. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.",
      "B": "Migrate the databases to a Multi-AZ Amazon RDS for SQL Server DB instance. Use an AWS Key Management Service (AWS KMS) AWS managed key for encryption.",
      "C": "Migrate the data to an Amazon S3 bucket. Use Amazon Macie to ensure data security.",
      "D": "Migrate the databases to an Amazon DynamoDB table. Use Amazon CloudWatch Logs to ensure data security."
    },
    "correct_answer": "B",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其工作负载迁移到 AWS。该公司在本地关系数据库（在 SQL Server 实例上运行）中拥有敏感和关键数据。该公司希望使用 AWS Cloud 来提高数据库的安全性并减少运营开销。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将数据库迁移到 Amazon EC2 实例。使用 AWS Key Management Service (AWS KMS) AWS 托管密钥进行加密。",
      "B": "将数据库迁移到 Multi-AZ Amazon RDS for SQL Server 数据库实例。使用 AWS Key Management Service (AWS KMS) AWS 托管密钥进行加密。",
      "C": "将数据迁移到 Amazon S3 存储桶。使用 Amazon Macie 确保数据安全。",
      "D": "将数据库迁移到 Amazon DynamoDB 表。使用 Amazon CloudWatch Logs 确保数据安全。"
    },
    "tags": [
      "RDS",
      "SQL Server",
      "KMS",
      "S3",
      "DynamoDB",
      "CloudWatch",
      "Macie"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 100%），解析仅供参考。】\n\n此题考察 RDS for SQL Server 的安全性和运营开销的优化。 RDS for SQL Server 支持多可用区部署，提供更高的可用性，并且可以使用 KMS 加密，提供数据的安全性。将数据库迁移到 RDS 是减少运营开销的有效方法。",
      "why_correct": "B. Multi-AZ RDS for SQL Server 提供了高可用性，使用 KMS 托管密钥加密可以提高安全性。满足题目要求。",
      "why_wrong": "A.  EC2 实例需要手动管理数据库，运营开销高。C. S3 适用于存储对象，不适合数据库。D. DynamoDB 不适合 SQL Server 工作负载。"
    },
    "related_terms": [
      "RDS",
      "SQL Server",
      "KMS",
      "S3",
      "DynamoDB",
      "CloudWatch",
      "Macie",
      "EC2"
    ]
  },
  {
    "id": 902,
    "topic": "1",
    "question_en": "A company wants to migrate an application to AWS. The company wants to increase the application's current availability. The company wants to use AWS WAF in the application's architecture. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two Availability Zones. Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the target. Connect a WAF to the ALB.",
      "B": "Create a cluster placement group that contains multiple Amazon EC2 instances that hosts the application. Configure an Application Load Balancer and set the EC2 instances as the targets. Connect a WAF to the placement group.",
      "C": "Create two Amazon EC2 instances that host the application across two Availability Zones. Configure the EC2 instances as the targets of an Application Load Balancer (ALB). Connect a WAF to the ALB.",
      "D": "Create an Auto Scaling group that contains multiple Amazon EC2 instances that host the application across two Availability Zones. Configure an Application Load Balancer (ALB) and set the Auto Scaling group as the target. Connect a WAF to the Auto Scaling group."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将应用程序迁移到 AWS。该公司希望提高应用程序当前的可用性。该公司希望在应用程序的架构中使用 AWS WAF。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个包含多个 Amazon EC2 实例的 Auto Scaling 组，这些实例在两个可用区中托管应用程序。配置一个 Application Load Balancer (ALB) 并将 Auto Scaling 组设置为目标。将 WAF 连接到 ALB。",
      "B": "创建一个包含多个托管应用程序的 Amazon EC2 实例的集群放置组。配置一个 Application Load Balancer 并将 EC2 实例设置为目标。将 WAF 连接到放置组。",
      "C": "创建两个 Amazon EC2 实例，这些实例在两个可用区中托管应用程序。将 EC2 实例配置为 Application Load Balancer (ALB) 的目标。将 WAF 连接到 ALB。",
      "D": "创建一个包含多个 Amazon EC2 实例的 Auto Scaling 组，这些实例在两个可用区中托管应用程序。配置一个 Application Load Balancer (ALB) 并将 Auto Scaling 组设置为目标。将 WAF 连接到 Auto Scaling 组。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "WAF",
      "可用性"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n该题考察提高应用程序可用性的架构设计。结合了 Auto Scaling、ALB 和 WAF。Auto Scaling 可以在多个可用区部署 EC2 实例，提高可用性。ALB 提供负载均衡，WAF 提供 Web 应用程序的保护。",
      "why_correct": "A.  Auto Scaling 组可以跨多个可用区部署 EC2 实例，提高可用性。ALB 用于负载均衡，WAF 用于保护应用程序。满足了题目要求。",
      "why_wrong": "B. 放置组主要用于优化实例的性能，而不是提高可用性。C.  两个 EC2 实例无法提供高可用性。D. WAF 应该附加到 ALB，而不是 Auto Scaling 组。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "WAF",
      "可用性"
    ]
  },
  {
    "id": 903,
    "topic": "1",
    "question_en": "A company manages a data lake in an Amazon S3 bucket that numerous applications access. The S3 bucket contains a unique prefix for each application. The company wants to restrict each application to its specific prefix and to have granular control of the objects under each prefix. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create dedicated S3 access points and access point policies for each application.",
      "B": "Create an S3 Batch Operations job to set the ACL permissions for each object in the S3 bucket.",
      "C": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create replication rules by prefix.",
      "D": "Replicate the objects in the S3 bucket to new S3 buckets for each application. Create dedicated S3 access points for each application."
    },
    "correct_answer": "A",
    "vote_percentage": "78%",
    "question_cn": "一家公司在其 Amazon S3 存储桶中管理一个数据湖，多个应用程序都可以访问该数据湖。S3 存储桶包含每个应用程序的唯一前缀。该公司希望将每个应用程序限制为其特定的前缀，并对每个前缀下的对象进行精细控制。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "为每个应用程序创建专用的 S3 访问点和访问点策略。",
      "B": "创建一个 S3 批量操作作业来设置 S3 存储桶中每个对象的 ACL 权限。",
      "C": "将 S3 存储桶中的对象复制到每个应用程序的新 S3 存储桶。按前缀创建复制规则。",
      "D": "将 S3 存储桶中的对象复制到每个应用程序的新 S3 存储桶。为每个应用程序创建专用的 S3 访问点。"
    },
    "tags": [
      "S3",
      "访问点"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 78%），解析仅供参考。】\n\n此题考察 S3 存储桶的访问控制。访问点提供了一种更细粒度的控制方式，可以将每个应用程序限制在其特定的前缀，从而降低运营开销。",
      "why_correct": "A.  S3 访问点允许为每个应用程序创建自定义访问策略，并限制对特定前缀的访问，满足了最低运营开销的要求。",
      "why_wrong": "B. ACL 操作开销高，不便于管理。C. 复制数据增加了存储成本。D. 复制数据增加了存储成本，且S3访问点不能作用于不同bucket的数据。"
    },
    "related_terms": [
      "S3",
      "访问点",
      "ACL",
      "前缀"
    ]
  },
  {
    "id": 904,
    "topic": "1",
    "question_en": "A company has an application that customers use to upload images to an Amazon S3 bucket. Each night, the company launches an Amazon EC2 Spot Fleet that processes all the images that the company received that day. The processing for each image takes 2 minutes and requires 512 MB of memory. A solutions architect needs to change the application to process the images when the images are uploaded. Which change will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an AWS Lambda function to read the messages from the queue and to process the images.",
      "B": "Use S3 Event Notifications to write a message with image details to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an EC2 Reserved Instance to read the messages from the queue and to process the images.",
      "C": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure a container instance in Amazon Elastic Container Service (Amazon ECS) to subscribe to the topic and to process the images.",
      "D": "Use S3 Event Notifications to publish a message with image details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Elastic Beanstalk application to subscribe to the topic and to process the images."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司有一个应用程序，客户可以使用该应用程序将图像上传到 Amazon S3 存储桶。每天晚上，该公司都会启动一个 Amazon EC2 Spot Fleet，该车队会处理该公司当天收到的所有图像。每个图像的处理需要 2 分钟，并且需要 512 MB 的内存。一个解决方案架构师需要更改该应用程序以在上传图像时处理图像。哪种更改将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 S3 事件通知将包含图像详细信息的邮件写入 Amazon Simple Queue Service (Amazon SQS) 队列。配置一个 AWS Lambda 函数以从队列中读取邮件并处理图像。",
      "B": "使用 S3 事件通知将包含图像详细信息的邮件写入 Amazon Simple Queue Service (Amazon SQS) 队列。配置一个 EC2 Reserved Instance 以从队列中读取邮件并处理图像。",
      "C": "使用 S3 事件通知将包含图像详细信息的邮件发布到 Amazon Simple Notification Service (Amazon SNS) 主题。配置 Amazon Elastic Container Service (Amazon ECS) 中的容器实例以订阅该主题并处理图像。",
      "D": "使用 S3 事件通知将包含图像详细信息的邮件发布到 Amazon Simple Notification Service (Amazon SNS) 主题。配置一个 AWS Elastic Beanstalk 应用程序以订阅该主题并处理图像。"
    },
    "tags": [
      "S3",
      "SQS",
      "Lambda",
      "EC2",
      "SNS",
      "ECS",
      "Elastic Beanstalk"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n此题考察无服务器架构，实现图像上传后的自动处理。使用 S3 事件触发 SQS，然后由 Lambda 函数处理消息，从而实现成本效益。",
      "why_correct": "A.  S3 事件触发 SQS 队列，Lambda 函数从队列中读取消息并处理图像，无服务器架构，成本效益高。",
      "why_wrong": "B.  EC2 Reserved Instance 不是最经济的方式。C.  SNS 不适合处理消息， ECS 较为复杂。D.  Elastic Beanstalk 相对 Lambda 更复杂，且不是最经济的方式。"
    },
    "related_terms": [
      "S3",
      "SQS",
      "Lambda",
      "EC2",
      "SNS",
      "ECS",
      "Elastic Beanstalk"
    ]
  },
  {
    "id": 905,
    "topic": "1",
    "question_en": "A company wants to improve the availability and performance of its hybrid application. The application consists of a stateful TCP-based workload hosted on Amazon EC2 instances in different AWS Regions and a stateless UDP-based workload hosted on premises. Which combination of actions should a solutions architect take to improve availability and performance? (Choose two.)",
    "options_en": {
      "A": "Create an accelerator using AWS Global Accelerator. Add the load balancers as endpoints.",
      "B": "Create an Amazon CloudFront distribution with an origin that uses Amazon Route 53 latency-based routing to route requests to the load balancers.",
      "C": "Configure two Application Load Balancers in each Region. The first will route to the EC2 endpoints, and the second will route to the on- premises endpoints.",
      "D": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure a Network Load Balancer in each Region that routes to the on-premises endpoints",
      "E": "Configure a Network Load Balancer in each Region to address the EC2 endpoints. Configure an Application Load Balancer in each Region that routes to the on-premises endpoints."
    },
    "correct_answer": "AD",
    "vote_percentage": "",
    "question_cn": "一家公司希望提高其混合应用程序的可用性和性能。该应用程序包含一个基于状态的、基于 TCP 的工作负载，该工作负载托管在不同 AWS 区域的 Amazon EC2 实例上，以及一个基于无状态的 UDP 的工作负载，该工作负载托管在本地。解决方案架构师应该采取哪些行动组合来提高可用性和性能？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Global Accelerator 创建一个加速器。将负载均衡器添加为端点。",
      "B": "创建一个 Amazon CloudFront 分发，其源使用 Amazon Route 53 基于延迟的路由将请求路由到负载均衡器。",
      "C": "在每个区域配置两个 Application Load Balancer。第一个将路由到 EC2 端点，第二个将路由到本地端点。",
      "D": "在每个区域配置一个 Network Load Balancer 以寻址 EC2 端点。在每个区域配置一个 Network Load Balancer，该负载均衡器路由到本地端点。",
      "E": "在每个区域配置一个 Network Load Balancer 以寻址 EC2 端点。在每个区域配置一个 Application Load Balancer，该负载均衡器路由到本地端点。"
    },
    "tags": [
      "Global Accelerator",
      "CloudFront",
      "Route 53",
      "Application Load Balancer",
      "Network Load Balancer",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 —），解析仅供参考。】\n\n此题考察混合应用程序的可用性和性能提升方案。Global Accelerator 提供全局加速，可以提高性能。选择两个符合要求的选项。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AD。理由简述：A.  Global Accelerator 提高了可用性和性能，将负载均衡器添加为端点，可以优化网络延迟。满足题目要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB.  CloudFront 主要用于内容分发，对 TCP 流量效果有限。C.  本地端点用ALB不合适。D.  使用NLB进行路由的选项不符合题意。E.  NLB对本地端点不友好。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Global Accelerator",
      "CloudFront",
      "Route 53",
      "Application Load Balancer",
      "Network Load Balancer",
      "EC2"
    ]
  },
  {
    "id": 906,
    "topic": "1",
    "question_en": "A company runs a self-managed Microsoft SQL Server on Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS). Daily snapshots are taken of the EBS volumes. Recently, all the company’s EBS snapshots were accidentally deleted while running a snapshot cleaning script that deletes all expired EBS snapshots. A solutions architect needs to update the architecture to prevent data loss without retaining EBS snapshots indefinitely. Which solution will meet these requirements with the LEAST development effort?",
    "options_en": {
      "A": "Change the IAM policy of the user to deny EBS snapshot deletion.",
      "B": "Copy the EBS snapshots to another AWS Region after completing the snapshots daily.",
      "C": "Create a 7-day EBS snapshot retention rule in Recycle Bin and apply the rule for all snapshots.",
      "D": "Copy EBS snapshots to Amazon S3 Standard-Infrequent Access (S3 Standard-IA)."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在 Amazon EC2 实例和 Amazon Elastic Block Store (Amazon EBS) 上运行一个自管理的 Microsoft SQL Server。每天都会对 EBS 卷进行快照。最近，在运行一个快照清理脚本时，该公司所有的 EBS 快照都被意外删除，该脚本会删除所有过期的 EBS 快照。一个解决方案架构师需要更新该架构，以防止数据丢失，而无需无限期地保留 EBS 快照。哪个解决方案将以最少的开发工作满足这些要求？",
    "options_cn": {
      "A": "更改用户的 IAM 策略以拒绝 EBS 快照删除。",
      "B": "每天完成快照后，将 EBS 快照复制到另一个 AWS 区域。",
      "C": "在回收站中创建 7 天的 EBS 快照保留规则，并将该规则应用于所有快照。",
      "D": "将 EBS 快照复制到 Amazon S3 标准 - 较少访问 (S3 标准 - IA)。"
    },
    "tags": [
      "EBS",
      "IAM",
      "快照",
      "回收站"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察防止 EBS 快照意外删除的措施。使用 IAM 策略可以限制用户对资源的访问权限，防止误操作。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：A.  修改 IAM 策略，禁止用户删除 EBS 快照。从根源解决问题，防止误删除。符合题意 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB.  复制快照增加了存储成本。C. 回收站机制对防止数据丢失没有本质作用。D.  将 EBS 快照复制到 S3 存储桶，不能防止删除。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EBS",
      "IAM",
      "快照",
      "回收站",
      "S3"
    ]
  },
  {
    "id": 907,
    "topic": "1",
    "question_en": "A company wants to use an AWS CloudFormation stack for its application in a test environment. The company stores the CloudFormation template in an Amazon S3 bucket that blocks public access. The company wants to grant CloudFormation access to the template in the S3 bucket based on specific user requests to create the test environment. The solution must follow security best practices. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a gateway VPC endpoint for Amazon S3. Configure the CloudFormation stack to use the S3 object URL.",
      "B": "Create an Amazon API Gateway REST API that has the S3 bucket as the target. Configure the CloudFormation stack to use the API Gateway URL.",
      "C": "Create a presigned URL for the template object. Configure the CloudFormation stack to use the presigned URL.",
      "D": "Allow public access to the template object in the S3 bucket. Block the public access after the test environment is created."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司希望使用 AWS CloudFormation 堆栈来在其测试环境中运行其应用程序。该公司将 CloudFormation 模板存储在一个阻止公共访问的 Amazon S3 存储桶中。该公司希望根据特定用户请求向 CloudFormation 授予对 S3 存储桶中模板的访问权限，以创建测试环境。该解决方案必须遵循安全最佳实践。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 Amazon S3 创建一个网关 VPC endpoint。将 CloudFormation 堆栈配置为使用 S3 对象 URL。",
      "B": "创建一个 Amazon API Gateway REST API，该 API 将 S3 存储桶作为目标。将 CloudFormation 堆栈配置为使用 API Gateway URL。",
      "C": "为模板对象创建一个预签名 URL。将 CloudFormation 堆栈配置为使用预签名 URL。",
      "D": "允许公共访问 S3 存储桶中的模板对象。在创建测试环境后阻止公共访问。"
    },
    "tags": [
      "CloudFormation",
      "S3",
      "预签名 URL",
      "API Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察安全地访问 S3 中的 CloudFormation 模板。预签名 URL 允许临时访问 S3 对象，而无需公开访问权限。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：C.  创建预签名 URL 允许临时访问 S3 中的模板。无需配置公共访问，安全且满足需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  VPC endpoint 不能用于访问 S3 中的模板文件。B.  API Gateway 的操作流程较为复杂，不是最佳实践。D.  允许公共访问不符合安全最佳实践。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "CloudFormation",
      "S3",
      "API Gateway",
      "预签名 URL"
    ]
  },
  {
    "id": 908,
    "topic": "1",
    "question_en": "A company has applications that run in an organization in AWS Organizations. The company outsources operational support of the applications. The company needs to provide access for the external support engineers without compromising security. The external support engineers need access to the AWS Management Console. The external support engineers also need operating system access to the company’s fieet ofAmazon EC2 instances that run Amazon Linux in private subnets. Which solution will meet these requirements MOST securely?",
    "options_en": {
      "A": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an instance profile with the necessary policy to connect to Systems Manager. Use AWS IAM Identity Center to provide the external support engineers console access. Use Systems Manager Session Manager to assign the required permissions.",
      "B": "Confirm that AWS Systems Manager Agent (SSM Agent) is installed on all instances. Assign an instance profile with the necessary policy to connect to Systems Manager. Use Systems Manager Session Manager to provide local IAM user credentials in each AWS account to the external support engineers for console access.",
      "C": "Confirm that all instances have a security group that allows SSH access only from the external support engineers’ source IP address ranges. Provide local IAM user credentials in each AWS account to the external support engineers for console access. Provide each external support engineer an SSH key pair to log in to the application instances.",
      "D": "Create a bastion host in a public subnet. Set up the bastion host security group to allow access from only the external engineers’ IP address ranges. Ensure that all instances have a security group that allows SSH access from the bastion host. Provide each external support engineer an SSH key pair to log in to the application instances. Provide local account IAM user credentials to the engineers for console access."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS Organizations 中运行应用程序。该公司外包了应用程序的运营支持。该公司需要为外部支持工程师提供访问权限，而不会影响安全性。外部支持工程师需要访问 AWS 管理控制台。外部支持工程师还需要操作系统访问权限，才能访问该公司在私有子网中运行 Amazon Linux 的 Amazon EC2 实例集群。哪个解决方案将以最安全的方式满足这些要求？",
    "options_cn": {
      "A": "确认所有实例上都安装了 AWS Systems Manager Agent (SSM Agent)。分配一个具有连接到 Systems Manager 所需策略的实例配置文件。使用 AWS IAM Identity Center 为外部支持工程师提供控制台访问权限。使用 Systems Manager Session Manager 分配所需的权限。",
      "B": "确认所有实例上都安装了 AWS Systems Manager Agent (SSM Agent)。分配一个具有连接到 Systems Manager 所需策略的实例配置文件。使用 Systems Manager Session Manager 为每个 AWS 账户中的外部支持工程师提供本地 IAM 用户凭证，以进行控制台访问。",
      "C": "确认所有实例都具有一个安全组，该安全组仅允许来自外部支持工程师源 IP 地址范围的 SSH 访问。为每个 AWS 账户中的外部支持工程师提供本地 IAM 用户凭证，以进行控制台访问。为每个外部支持工程师提供 SSH 密钥对，以登录到应用程序实例。",
      "D": "在公共子网中创建一个堡垒主机。将堡垒主机安全组设置为仅允许来自外部工程师 IP 地址范围的访问。确保所有实例都具有一个安全组，该安全组允许从堡垒主机进行 SSH 访问。为每个外部支持工程师提供 SSH 密钥对，以登录到应用程序实例。为工程师提供本地账户 IAM 用户凭证，以进行控制台访问。"
    },
    "tags": [
      "IAM",
      "SSM",
      "EC2",
      "Systems Manager",
      "IAM Identity Center",
      "堡垒主机"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何安全地提供外部支持人员对 AWS 账户的访问权限。使用 Systems Manager 的 Session Manager 和 IAM Identity Center 可以安全地实现。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：A.  使用 SSM Agent 和 Systems Manager Session Manager 提供安全的访问。IAM Identity Center 提供控制台访问，同时保障安全，满足要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB.  本地 IAM 用户凭证不安全。C.  SSH 密钥容易泄露，不安全。D.  堡垒机也是一个额外的管理点。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM",
      "SSM",
      "EC2",
      "Systems Manager",
      "IAM Identity Center",
      "堡垒主机"
    ]
  },
  {
    "id": 909,
    "topic": "1",
    "question_en": "A company uses Amazon RDS for PostgreSQL to run its applications in the us-east-1 Region. The company also uses machine learning (ML) models to forecast annual revenue based on near real-time reports. The reports are generated by using the same RDS for PostgreSQL database. The database performance slows during business hours. The company needs to improve database performance. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create a cross-Region read replica. Configure the reports to be generated from the read replica.",
      "B": "Activate Multi-AZ DB instance deployment for RDS for PostgreSQL. Configure the reports to be generated from the standby database.",
      "C": "Use AWS Data Migration Service (AWS DMS) to logically replicate data to a new database. Configure the reports to be generated from the new database.",
      "D": "Create a read replica in us-east-1. Configure the reports to be generated from the read replica."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司使用 Amazon RDS for PostgreSQL 在 us-east-1 区域中运行其应用程序。该公司还使用机器学习 (ML) 模型，根据近乎实时的报告预测年度收入。这些报告是使用相同的 RDS for PostgreSQL 数据库生成的。数据库性能在工作时间内会变慢。该公司需要提高数据库性能。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建一个跨区域读取副本。配置报告以从读取副本生成。",
      "B": "为 RDS for PostgreSQL 激活 Multi-AZ 数据库实例部署。配置报告以从备用数据库生成。",
      "C": "使用 AWS Data Migration Service (AWS DMS) 将数据逻辑复制到新数据库。配置报告以从新数据库生成。",
      "D": "在 us-east-1 中创建一个读取副本。配置报告以从读取副本生成。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "读取副本",
      "Multi-AZ"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察 RDS for PostgreSQL 的性能优化方案。使用只读副本可以分担主数据库的负载。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：D.  在同一区域创建只读副本，将报告配置为从只读副本生成，从而减轻主数据库的负载。可以提高数据库性能。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  跨区域副本的延迟较高。B.  Multi-AZ 主要提供高可用性，而非性能提升。C.  DMS 迁移数据不是成本效益最高的解决方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "PostgreSQL",
      "读取副本",
      "Multi-AZ",
      "AWS DMS"
    ]
  },
  {
    "id": 910,
    "topic": "1",
    "question_en": "A company hosts its multi-tier, public web application in the AWS Cloud. The web application runs on Amazon EC2 instances, and its database runs on Amazon RDS. The company is anticipating a large increase in sales during an upcoming holiday weekend. A solutions architect needs to build a solution to analyze the performance of the web application with a granularity of no more than 2 minutes. What should the solutions architect do to meet this requirement?",
    "options_en": {
      "A": "Send Amazon CloudWatch logs to Amazon Redshift. Use Amazon QuickS ght to perform further analysis.",
      "B": "Enable detailed monitoring on all EC2 instances. Use Amazon CloudWatch metrics to perform further analysis.",
      "C": "Create an AWS Lambda function to fetch EC2 logs from Amazon CloudWatch Logs. Use Amazon CloudWatch metrics to perform further analysis.",
      "D": "Send EC2 logs to Amazon S3. Use Amazon Redshift to fetch logs from the S3 bucket to process raw data for further analysis with Amazon QuickSight."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS Cloud 中托管其多层公共 Web 应用程序。Web 应用程序在 Amazon EC2 实例上运行，其数据库在 Amazon RDS 上运行。该公司预计在即将到来的假期周末销售额将大幅增加。一个解决方案架构师需要构建一个解决方案，以不超过 2 分钟的粒度分析 Web 应用程序的性能。解决方案架构师应该怎么做才能满足此要求？",
    "options_cn": {
      "A": "将 Amazon CloudWatch 日志发送到 Amazon Redshift。使用 Amazon QuickSight 执行进一步的分析。",
      "B": "在所有 EC2 实例上启用详细监控。使用 Amazon CloudWatch 指标执行进一步的分析。",
      "C": "创建一个 AWS Lambda 函数，用于从 Amazon CloudWatch Logs 中提取 EC2 日志。使用 Amazon CloudWatch 指标执行进一步的分析。",
      "D": "将 EC2 日志发送到 Amazon S3。使用 Amazon Redshift 从 S3 存储桶中提取日志，以处理原始数据，以便使用 Amazon QuickSight 进行进一步分析。"
    },
    "tags": [
      "EC2",
      "CloudWatch",
      "指标",
      "日志",
      "Redshift",
      "QuickSight"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察分析应用程序的性能。CloudWatch 指标可以提供近实时的性能数据，满足时间粒度要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：B.  启用详细监控可以提供分钟级别的指标数据。 使用 CloudWatch Metrics 进行分析满足了分析性能的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  将日志发送到 Redshift 不是最直接的方式。C.  Lambda 处理日志，延迟高。D.  EC2 日志的分析较为复杂，延迟高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "CloudWatch",
      "Redshift",
      "QuickSight",
      "指标",
      "日志"
    ]
  },
  {
    "id": 911,
    "topic": "1",
    "question_en": "A company runs an application that stores and shares photos. Users upload the photos to an Amazon S3 bucket. Every day, users upload approximately 150 photos. The company wants to design a solution that creates a thumbnail of each new photo and stores the thumbnail in a second S3 bucket. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a long-running Amazon EMR cluster. Configure the script to generate thumbnails for the photos that do not have thumbnails. Configure the script to upload the thumbnails to the second S3 bucket.",
      "B": "Configure an Amazon EventBridge scheduled rule to invoke a script every minute on a memory-optimized Amazon EC2 instance that is always on. Configure the script to generate thumbnails for the photos that do not have thumbnails. Configure the script to upload the thumbnails to the second S3 bucket.",
      "C": "Configure an S3 event notification to invoke an AWS Lambda function each time a user uploads a new photo to the application. Configure the Lambda function to generate a thumbnail and to upload the thumbnail to the second S3 bucket.",
      "D": "Configure S3 Storage Lens to invoke an AWS Lambda function each time a user uploads a new photo to the application. Configure the Lambda function to generate a thumbnail and to upload the thumbnail to a second S3 bucket."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司运行一个应用程序，该应用程序存储和共享照片。用户将照片上传到 Amazon S3 存储桶。每天，用户大约上传 150 张照片。该公司希望设计一个解决方案，该解决方案可以为每张新照片创建缩略图，并将缩略图存储在第二个 S3 存储桶中。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon EventBridge 定时规则，以在长时间运行的 Amazon EMR 集群上每分钟调用一个脚本。配置脚本以生成没有缩略图的照片的缩略图。配置脚本以将缩略图上传到第二个 S3 存储桶。",
      "B": "配置 Amazon EventBridge 定时规则，以在始终处于运行状态的内存优化 Amazon EC2 实例上每分钟调用一个脚本。配置脚本以生成没有缩略图的照片的缩略图。配置脚本以将缩略图上传到第二个 S3 存储桶。",
      "C": "配置 S3 事件通知，以便在用户每次将新照片上传到应用程序时调用一个 AWS Lambda 函数。配置 Lambda 函数以生成缩略图并将缩略图上传到第二个 S3 存储桶。",
      "D": "配置 S3 Storage Lens，以便在用户每次将新照片上传到应用程序时调用一个 AWS Lambda 函数。配置 Lambda 函数以生成缩略图并将缩略图上传到第二个 S3 存储桶。"
    },
    "tags": [
      "S3",
      "Lambda",
      "EventBridge",
      "EMR",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察图像处理流程。使用 S3 事件触发 Lambda 函数实现缩略图的生成，成本效益高。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：C.  S3 事件触发 Lambda 函数，实现照片上传后的自动缩略图生成。无服务器架构，成本效益高。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  EMR 集群启动时间长，开销高。B.  EC2 实例需要管理，成本高。D.  S3 Storage Lens 不能触发 Lambda 函数。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "Lambda",
      "EventBridge",
      "EMR",
      "EC2"
    ]
  },
  {
    "id": 912,
    "topic": "1",
    "question_en": "A company has stored millions of objects across multiple prefixes in an Amazon S3 bucket by using the Amazon S3 Glacier Deep Archive storage class. The company needs to delete all data older than 3 years except for a subset of data that must be retained. The company has identified the data that must be retained and wants to implement a serverless solution. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use S3 Inventory to list all objects. Use the AWS CLI to create a script that runs on an Amazon EC2 instance that deletes objects from the inventory list.",
      "B": "Use AWS Batch to delete objects older than 3 years except for the data that must be retained.",
      "C": "Provision an AWS Glue crawler to query objects older than 3 years. Save the manifest file of old objects. Create a script to delete objects in the manifest.",
      "D": "Enable S3 Inventory. Create an AWS Lambda function to filter and delete objects. Invoke the Lambda function with S3 Batch Operations to delete objects by using the inventory reports."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司使用 Amazon S3 Glacier Deep Archive 存储类将数百万个对象存储在 Amazon S3 存储桶中的多个前缀中。该公司需要删除所有超过 3 年的数据，但必须保留一部分数据。该公司已确定了必须保留的数据，并希望实施一个无服务器解决方案。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 S3 Inventory 列出所有对象。使用 AWS CLI 创建一个脚本，该脚本在 Amazon EC2 实例上运行，用于删除清单列表中的对象。",
      "B": "使用 AWS Batch 删除超过 3 年的对象，但必须保留的数据除外。",
      "C": "配置一个 AWS Glue 爬虫程序以查询超过 3 年的对象。保存旧对象的清单文件。创建一个脚本以删除清单中的对象。",
      "D": "启用 S3 Inventory。创建一个 AWS Lambda 函数以筛选和删除对象。使用 S3 批量操作调用 Lambda 函数，以使用库存报告删除对象。"
    },
    "tags": [
      "S3",
      "Glacier Deep Archive",
      "Lambda",
      "S3 Inventory",
      "S3 批量操作",
      "AWS Glue",
      "AWS CLI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察删除旧数据。使用 S3 Inventory 和 S3 批量操作调用 Lambda 函数删除对象，实现无服务器和低成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：D.  S3 Inventory 生成清单报告，Lambda 函数筛选数据并由 S3 批量操作删除。无服务器解决方案，成本效益高，满足要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  EC2 实例操作，管理成本高。B.  AWS Batch 不满足无服务器需求。C.  AWS Glue 的使用，较为复杂。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "Glacier Deep Archive",
      "Lambda",
      "AWS Glue",
      "AWS CLI",
      "S3 Inventory",
      "S3 批量操作"
    ]
  },
  {
    "id": 913,
    "topic": "1",
    "question_en": "A company is building an application on AWS. The application uses multiple AWS Lambda functions to retrieve sensitive data from a single Amazon S3 bucket for processing. The company must ensure that only authorized Lambda functions can access the data. The solution must comply with the principle of least privilege. Which solution will meet these requirements?",
    "options_en": {
      "A": "Grant full S3 bucket access to all Lambda functions through a shared IAM role.",
      "B": "Configure the Lambda functions to run within a VPC. Configure a bucket policy to grant access based on the Lambda functions' VPC endpoint IP addresses.",
      "C": "Create individual IAM roles for each Lambda function. Grant the IAM roles access to the S3 bucket. Assign each IAM role as the Lambda execution role for its corresponding Lambda function.",
      "D": "Configure a bucket policy granting access to the Lambda functions based on their function ARNs."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上构建一个应用程序。该应用程序使用多个 AWS Lambda 函数从单个 Amazon S3 存储桶中检索敏感数据进行处理。该公司必须确保只有授权的 Lambda 函数才能访问数据。该解决方案必须符合最小权限原则。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "通过共享 IAM 角色向所有 Lambda 函数授予对 S3 存储桶的完全访问权限。",
      "B": "将 Lambda 函数配置为在 VPC 中运行。配置存储桶策略以根据 Lambda 函数的 VPC endpoint IP 地址授予访问权限。",
      "C": "为每个 Lambda 函数创建单独的 IAM 角色。授予 IAM 角色对 S3 存储桶的访问权限。将每个 IAM 角色分配为其相应 Lambda 函数的 Lambda 执行角色。",
      "D": "配置一个存储桶策略，根据 Lambda 函数的函数 ARN 授予对 Lambda 函数的访问权限。"
    },
    "tags": [
      "Lambda",
      "S3",
      "IAM",
      "IAM 角色",
      "最小权限原则"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察 S3 数据的访问控制。为每个 Lambda 函数分配独立的 IAM 角色，并授予对 S3 桶的访问权限，符合最小权限原则。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：C. 为每个 Lambda 函数创建独立的 IAM 角色，并授予访问 S3 桶的权限，满足最小权限原则。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA.  共享 IAM 角色不符合最小权限原则。B.  Lambda 运行在 VPC 中不能限制对 S3 的访问。D.  函数 ARN 授权不安全。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Lambda",
      "S3",
      "IAM",
      "IAM 角色",
      "最小权限原则"
    ]
  },
  {
    "id": 914,
    "topic": "1",
    "question_en": "A company has developed a non-production application that is composed of multiple microservices for each of the company's business units. A single development team maintains all the microservices. The current architecture uses a static web frontend and a Java-based backend that contains the application logic. The architecture also uses a MySQL database that the company hosts on an Amazon EC2 instance. The company needs to ensure that the application is secure and available globally. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon CloudFront and AWS Amplify to host the static web frontend. Refactor the microservices to use AWS Lambda functions that the microservices access by using Amazon API Gateway. Migrate the MySQL database to an Amazon EC2 Reserved Instance.",
      "B": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda functions that the microservices access by using Amazon API Gateway. Migrate the MySQL database to Amazon RDS for MySQL.",
      "C": "Use Amazon CloudFront and Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda functions that are in a target group behind a Network Load Balancer. Migrate the MySQL database to Amazon RDS for MySQL.",
      "D": "Use Amazon S3 to host the static web frontend. Refactor the microservices to use AWS Lambda functions that are in a target group behind an Application Load Balancer. Migrate the MySQL database to an Amazon EC2 Reserved Instance."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司开发了一个非生产应用程序，该应用程序由该公司每个业务部门的多个微服务组成。单个开发团队维护所有微服务。当前的架构使用静态 Web 前端和基于 Java 的后端，该后端包含应用程序逻辑。该架构还使用公司在 Amazon EC2 实例上托管的 MySQL 数据库。该公司需要确保该应用程序在全球范围内安全且可用。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudFront 和 AWS Amplify 托管静态 Web 前端。重构微服务，以使用 AWS Lambda 函数，微服务通过 Amazon API Gateway 访问这些函数。将 MySQL 数据库迁移到 Amazon EC2 Reserved Instance。",
      "B": "使用 Amazon CloudFront 和 Amazon S3 托管静态 Web 前端。重构微服务，以使用 AWS Lambda 函数，微服务通过 Amazon API Gateway 访问这些函数。将 MySQL 数据库迁移到 Amazon RDS for MySQL。",
      "C": "使用 Amazon CloudFront 和 Amazon S3 托管静态 Web 前端。重构微服务，以使用位于 Network Load Balancer 后面的 AWS Lambda 函数。将 MySQL 数据库迁移到 Amazon RDS for MySQL。",
      "D": "使用 Amazon S3 托管静态 Web 前端。重构微服务，以使用位于 Application Load Balancer 后面的 AWS Lambda 函数。将 MySQL 数据库迁移到 Amazon EC2 Reserved Instance。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "Lambda",
      "API Gateway",
      "RDS",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察构建全球可用、安全且运营开销低的应用程序。需要选择合适的服务组合，利用 AWS 的优势。选项 B 提供了最佳解决方案，结合了 CloudFront 和 S3 托管静态 Web 前端， Lambda 和 API Gateway 处理后端微服务，以及 RDS for MySQL 数据库，实现了全球部署、高可用性、安全性和成本效益。这是典型的无服务器架构。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 提供了最全面的解决方案，满足了所有需求，实现了静态内容的全球分发，无服务器后端逻辑和托管数据库，降低了运维成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 将 MySQL 数据库迁移到 EC2 Reserved Instance，提高了运营成本。选项 C 使用 Lambda 函数与 Network Load Balancer，没有使用 API Gateway，架构不标准，并且 RDS for MySQL 的配置与 CloudFront 不搭配。选项 D 同样将数据库迁移到 EC2 Reserved Instance，不符合最低运营开销的要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "AWS Amplify",
      "AWS Lambda",
      "Amazon API Gateway",
      "Amazon EC2",
      "MySQL",
      "Amazon S3",
      "Network Load Balancer",
      "Application Load Balancer",
      "Amazon RDS for MySQL",
      "Reserved Instance"
    ]
  },
  {
    "id": 915,
    "topic": "1",
    "question_en": "A video game company is deploying a new gaming application to its global users. The company requires a solution that will provide near real- time reviews and rankings of the players. A solutions architect must design a solution to provide fast access to the data. The solution must also ensure the data persists on disks in the event that the company restarts the application. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure an Amazon CloudFront distribution with an Amazon S3 bucket as the origin. Store the player data in the S3 bucket.",
      "B": "Create Amazon EC2 instances in multiple AWS Regions. Store the player data on the EC2 instances. Configure Amazon Route 53 with geolocation records to direct users to the closest EC2 instance.",
      "C": "Deploy an Amazon ElastiCache for Redis duster. Store the player data in the ElastiCache cluster.",
      "D": "Deploy an Amazon ElastiCache for Memcached duster. Store the player data in the ElastiCache cluster."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家视频游戏公司正在为其全球用户部署一个新的游戏应用程序。该公司需要一个解决方案，以提供近实时的玩家评论和排名。解决方案架构师必须设计一个解决方案，以提供对数据的快速访问。该解决方案还必须确保在公司重新启动应用程序时，数据保存在磁盘上。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "配置一个以 Amazon S3 存储桶为源的 Amazon CloudFront 分发。将玩家数据存储在 S3 存储桶中。",
      "B": "在多个 AWS 区域中创建 Amazon EC2 实例。将玩家数据存储在 EC2 实例上。使用地理位置记录配置 Amazon Route 53，以将用户定向到最近的 EC2 实例。",
      "C": "部署一个 Amazon ElastiCache for Redis 集群。将玩家数据存储在 ElastiCache 集群中。",
      "D": "部署一个 Amazon ElastiCache for Memcached 集群。将玩家数据存储在 ElastiCache 集群中。"
    },
    "tags": [
      "ElastiCache",
      "Redis",
      "EC2",
      "Route 53",
      "CloudFront",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察如何构建低延迟、高可用的玩家数据存储方案。ElastiCache for Redis 集群提供低延迟的读写操作，非常适合实时游戏数据。选项 C 提供的解决方案以最低的运营开销满足了要求，因为它专注于快速的数据访问和持久化。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 使用 ElastiCache for Redis，提供了快速数据访问，并可以实现数据的持久化。ElastiCache for Redis 集群是针对这类需求的理想选择。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 CloudFront 和 S3，虽然可以加速静态内容的访问，但无法满足近实时的玩家评论和排名需求。选项 B 使用 EC2 实例，虽然提供了数据存储的能力，但运维开销相对较高，且没有充分利用缓存技术。选项 D 使用 ElastiCache for Memcached，虽然也是缓存服务，但 Redis 提供了更丰富的数据结构和功能，更适合此场景。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon CloudFront",
      "Amazon S3",
      "Amazon EC2",
      "Amazon Route 53",
      "Amazon ElastiCache for Redis",
      "EC2",
      "Amazon ElastiCache for Memcached"
    ]
  },
  {
    "id": 916,
    "topic": "1",
    "question_en": "A company is designing an application on AWS that processes sensitive data. The application stores and processes financial data for multiple customers. To meet compliance requirements, the data for each customer must be encrypted separately at rest by using a secure, centralized key management solution. The company wants to use AWS Key Management Service (AWS KMS) to implement encryption. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Generate a unique encryption key for each customer. Store the keys in an Amazon S3 bucket. Enable server-side encryption.",
      "B": "Deploy a hardware security appliance in the AWS environment that securely stores customer-provided encryption keys. Integrate the security appliance with AWS KMS to encrypt the sensitive data in the application.",
      "C": "Create a single AWS KMS key to encrypt all sensitive data across the application.",
      "D": "Create separate AWS KMS keys for each customer's data that have granular access control and logging enabled."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上设计一个应用程序，该应用程序处理敏感数据。该应用程序存储和处理多个客户的财务数据。为了满足合规性要求，每个客户的数据都必须使用安全的集中式密钥管理解决方案单独静态加密。该公司希望使用 AWS Key Management Service (AWS KMS) 来实现加密。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "为每个客户生成唯一的加密密钥。将密钥存储在 Amazon S3 存储桶中。启用服务器端加密。",
      "B": "在 AWS 环境中部署一个硬件安全设备，该设备安全地存储客户提供的加密密钥。将安全设备与 AWS KMS 集成，以加密应用程序中的敏感数据。",
      "C": "创建一个 AWS KMS 密钥来加密应用程序中的所有敏感数据。",
      "D": "为每个客户的数据创建单独的 AWS KMS 密钥，并启用细粒度的访问控制和日志记录。"
    },
    "tags": [
      "KMS",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n本题考察如何使用 AWS KMS 加密敏感数据，并满足合规性要求。解决方案需要考虑密钥管理、安全性，并减少运营开销。选项 A 提供了最佳的平衡，使用 KMS 密钥加密，同时保证每个客户的数据都使用不同的密钥。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 采用了 AWS KMS 的加密功能，并结合了 S3 的服务器端加密。这满足了每个客户的数据都必须单独加密的要求，并简化了密钥管理，降低了运营开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 涉及硬件安全设备，增加了部署和维护的复杂性，不符合最低运营开销的要求。选项 C 使用单个 KMS 密钥加密所有数据，不满足每个客户数据单独加密的要求。选项 D 虽然提供了细粒度的访问控制和日志记录，但增加了密钥管理的复杂性。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS KMS",
      "Amazon S3"
    ]
  },
  {
    "id": 917,
    "topic": "1",
    "question_en": "A company needs to design a resilient web application to process customer orders. The web application must automatically handle increases in web trafic and application usage without affecting the customer experience or losing customer orders. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use a NAT gateway to manage web trafic. Use Amazon EC2 Auto Scaling groups to receive, process, and store processed customer orders. Use an AWS Lambda function to capture and store unprocessed orders.",
      "B": "Use a Network Load Balancer (NLB) to manage web trafic. Use an Application Load Balancer to receive customer orders from the NLUse Amazon Redshift with a Multi-AZ deployment to store unprocessed and processed customer orders.",
      "C": "Use a Gateway Load Balancer (GWLB) to manage web trafic. Use Amazon Elastic Container Service (Amazon ECS) to receive and process customer orders. Use the GWLB to capture and store unprocessed orders. Use Amazon DynamoDB to store processed customer orders.",
      "D": "Use an Application Load Balancer to manage web trafic. Use Amazon EC2 Auto Scaling groups to receive and process customer orders. Use Amazon Simple Queue Service (Amazon SQS) to store unprocessed orders. Use Amazon RDS with a Multi-AZ deployment to store processed customer orders."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司需要设计一个具有弹性的 Web 应用程序来处理客户订单。该 Web 应用程序必须自动处理 Web 流量和应用程序使用量的增加，而不会影响客户体验或丢失客户订单。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 NAT Gateway 管理 Web 流量。使用 Amazon EC2 Auto Scaling 组接收、处理和存储已处理的客户订单。使用 AWS Lambda 函数捕获并存储未处理的订单。",
      "B": "使用 Network Load Balancer (NLB) 管理 Web 流量。使用 Application Load Balancer (ALB) 接收来自客户的订单。使用具有多可用区部署的 Amazon Redshift 存储未处理和已处理的客户订单。",
      "C": "使用 Gateway Load Balancer (GWLB) 管理 Web 流量。使用 Amazon Elastic Container Service (Amazon ECS) 接收和处理客户订单。使用 GWLB 捕获并存储未处理的订单。使用 Amazon DynamoDB 存储已处理的客户订单。",
      "D": "使用 Application Load Balancer 管理 Web 流量。使用 Amazon EC2 Auto Scaling 组接收和处理客户订单。使用 Amazon Simple Queue Service (Amazon SQS) 存储未处理的订单。使用具有多可用区部署的 Amazon RDS 存储已处理的客户订单。"
    },
    "tags": [
      "ALB",
      "EC2",
      "Auto Scaling",
      "SQS",
      "RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察构建弹性 Web 应用程序。需要考虑自动伸缩、负载均衡、订单处理和数据存储等。选项 D 提供了最佳解决方案，它使用了 ALB、EC2 Auto Scaling 组、SQS 和 RDS，实现了自动处理流量、弹性伸缩，以及未处理和已处理订单的存储。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 结合了 ALB 用于流量管理，EC2 Auto Scaling 组用于处理订单， SQS 用于存储未处理的订单，RDS 用于存储已处理的订单，实现了弹性、高可用和可靠的订单处理系统。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 NAT Gateway 管理 Web 流量，成本较高，并且使用了 Lambda 函数处理未处理订单，没有使用弹性伸缩。选项 B 使用 NLB 管理 Web 流量，成本较高，使用 Redshift 存储未处理和已处理的客户订单，不适用于存储未处理的订单。选项 C 使用 GWLB 管理 Web 流量，增加了架构的复杂性，存储未处理的订单，并且使用 DynamoDB 存储已处理的客户订单，增加了运维难度。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "NAT Gateway",
      "AWS Lambda",
      "Amazon Redshift",
      "Amazon DynamoDB",
      "Amazon RDS",
      "Amazon EC2 Auto Scaling",
      "Network Load Balancer (NLB)",
      "Application Load Balancer (ALB)",
      "Gateway Load Balancer (GWLB)",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Amazon Simple Queue Service (Amazon SQS)"
    ]
  },
  {
    "id": 918,
    "topic": "1",
    "question_en": "A company is using AWS DataSync to migrate millions of files from an on-premises system to AWS. The files are 10 KB in size on average. The company wants to use Amazon S3 for file storage. For the first year after the migration, the files will be accessed once or twice and must be immediately available. After 1 year, the files must be archived for at least 7 years. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use an archive tool to group the files into large objects. Use DataSync to migrate the objects. Store the objects in S3 Glacier Instant Retrieval for the first year. Use a lifecycle configuration to transition the files to S3 Glacier Deep Archive after 1 year with a retention period of 7 years.",
      "B": "Use an archive tool to group the files into large objects. Use DataSync to copy the objects to S3 Standard-Infrequent Access (S3 Standard-IA). Use a lifecycle configuration to transition the files to S3 Glacier Instant Retrieval after 1 year with a retention period of 7 years.",
      "C": "Configure the destination storage class for the files as S3 Glacier Instant Retrieval. Use a lifecycle policy to transition the files to S3 Glacier Flexible Retrieval after 1 year with a retention period of 7 years.",
      "D": "Configure a DataSync task to transfer the files to S3 Standard-Infrequent Access (S3 Standard-IA). Use a lifecycle configuration to transition the files to S3 Deep Archive after 1 year with a retention period of 7 years."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在使用 AWS DataSync 将数百万个文件从本地系统迁移到 AWS。这些文件平均大小为 10 KB。该公司希望使用 Amazon S3 进行文件存储。迁移后的第一年，将访问这些文件一两次，并且必须立即可用。1 年后，这些文件必须归档至少 7 年。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用归档工具将文件分组为大对象。使用 DataSync 迁移这些对象。将这些对象存储在 S3 Glacier Instant Retrieval 中一年。使用生命周期配置将文件在 1 年后过渡到 S3 Glacier Deep Archive，保留期为 7 年。",
      "B": "使用归档工具将文件分组为大对象。使用 DataSync 将对象复制到 S3 Standard-Infrequent Access (S3 Standard-IA)。使用生命周期配置将文件在 1 年后过渡到 S3 Glacier Instant Retrieval，保留期为 7 年。",
      "C": "将文件的目标存储类配置为 S3 Glacier Instant Retrieval。使用生命周期策略将文件在 1 年后过渡到 S3 Glacier Flexible Retrieval，保留期为 7 年。",
      "D": "配置 DataSync 任务以将文件传输到 S3 Standard-Infrequent Access (S3 Standard-IA)。使用生命周期配置将文件在 1 年后过渡到 S3 Deep Archive，保留期为 7 年。"
    },
    "tags": [
      "DataSync",
      "S3",
      "Glacier"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何以最具成本效益的方式将文件迁移到 S3 并进行归档。选项 B 提供了最佳解决方案，它使用了 S3 Standard-IA 用于短期访问，然后使用生命周期策略将文件迁移到 Glacier Instant Retrieval 和 Deep Archive，实现了成本优化和满足了数据访问需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 采用 S3 Standard-IA 用于短期访问，满足了文件的即时可用性需求，然后使用生命周期策略将文件迁移到 Glacier，实现了成本效益和长期归档。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 将文件存储到 Glacier Instant Retrieval 一年后，成本较高。选项 C 将文件直接存储到 Glacier Instant Retrieval，不符合文件短期访问的需求。选项 D 直接迁移到 S3 Deep Archive，无法满足短期访问需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS DataSync",
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "S3 Glacier Flexible Retrieval",
      "S3 Glacier Instant Retrieval",
      "S3 Standard-Infrequent Access",
      "S3 Deep Archive"
    ]
  },
  {
    "id": 919,
    "topic": "1",
    "question_en": "A company recently performed a lift and shift migration of its on-premises Oracle database workload to run on an Amazon EC2 memory optimized Linux instance. The EC2 Linux instance uses a 1 TB Provisioned IOPS SSD (io1) EBS volume with 64,000 IOPS. The database storage performance after the migration is slower than the performance of the on-premises database. Which solution will improve storage performance?",
    "options_en": {
      "A": "Add more Provisioned IOPS SSD (io1) EBS volumes. Use OS commands to create a Logical Volume Management (LVM) stripe.",
      "B": "Increase the Provisioned IOPS SSD (io1) EBS volume to more than 64,000 IOPS.",
      "C": "Increase the size of the Provisioned IOPS SSD (io1) EBS volume to 2 TB.",
      "D": "Change the EC2 Linux instance to a storage optimized instance type. Do not change the Provisioned IOPS SSD (io1) EBS volume."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司最近将其本地 Oracle 数据库工作负载进行了迁移，以便在 Amazon EC2 内存优化 Linux 实例上运行。 EC2 Linux 实例使用 1 TB 预置 IOPS SSD (io1) EBS 卷，具有 64,000 IOPS。 迁移后，数据库存储性能比本地数据库的性能慢。 哪个解决方案将提高存储性能？",
    "options_cn": {
      "A": "添加更多预置 IOPS SSD (io1) EBS 卷。 使用操作系统命令创建逻辑卷管理 (LVM) 条带。",
      "B": "将预置 IOPS SSD (io1) EBS 卷增加到超过 64,000 IOPS。",
      "C": "将预置 IOPS SSD (io1) EBS 卷的大小增加到 2 TB。",
      "D": "将 EC2 Linux 实例更改为存储优化实例类型。 不更改预置 IOPS SSD (io1) EBS 卷。"
    },
    "tags": [
      "EC2",
      "EBS",
      "IOPS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何提高 EC2 实例上 Oracle 数据库的存储性能。 存储性能的瓶颈通常是 IOPS。通过增加预置 IOPS SSD (io1) EBS 卷并进行逻辑卷管理 (LVM) 条带化，可以提升性能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 提供了通过 LVM 条带化来提高存储 IOPS 和性能的正确方法。通过添加更多 EBS 卷并创建逻辑卷，可以扩展存储容量和 IOPS。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 无法超出 64,000 IOPS 的限制。选项 C 增加 EBS 卷的大小并不能直接提高 IOPS 性能。选项 D 更改为存储优化实例类型，并不能直接解决预置 IOPS 性能问题。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "EBS",
      "IOPS",
      "io1",
      "LVM"
    ]
  },
  {
    "id": 920,
    "topic": "1",
    "question_en": "A company is migrating from a monolithic architecture for a web application that is hosted on Amazon EC2 to a serverless microservices architecture. The company wants to use AWS services that support an event-driven, loosely coupled architecture. The company wants to use the publish/subscribe (pub/sub) pattern. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes events to an Amazon Simple Queue Service (Amazon SQS) queue. Configure one or more subscribers to read events from the SQS queue.",
      "B": "Configure an Amazon API Gateway REST API to invoke an AWS Lambda function that publishes events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure one or more subscribers to receive events from the SNS topic.",
      "C": "Configure an Amazon API Gateway WebSocket API to write to a data stream in Amazon Kinesis Data Streams with enhanced fan-out. Configure one or more subscribers to receive events from the data stream.",
      "D": "Configure an Amazon API Gateway HTTP API to invoke an AWS Lambda function that publishes events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure one or more subscribers to receive events from the topic."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其托管在 Amazon EC2 上的 Web 应用程序从单体架构迁移到无服务器微服务架构。该公司希望使用支持事件驱动、松耦合架构的 AWS 服务。该公司希望使用发布/订阅 (pub/sub) 模式。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon API Gateway REST API 以调用一个 AWS Lambda 函数，该函数将事件发布到 Amazon Simple Queue Service (Amazon SQS) 队列。配置一个或多个订阅者以从 SQS 队列中读取事件。",
      "B": "配置 Amazon API Gateway REST API 以调用一个 AWS Lambda 函数，该函数将事件发布到 Amazon Simple Notification Service (Amazon SNS) 主题。配置一个或多个订阅者以从 SNS 主题接收事件。",
      "C": "配置 Amazon API Gateway WebSocket API 以写入 Amazon Kinesis Data Streams 中的数据流，并使用增强的扇出功能。配置一个或多个订阅者以从数据流接收事件。",
      "D": "配置 Amazon API Gateway HTTP API 以调用一个 AWS Lambda 函数，该函数将事件发布到 Amazon Simple Notification Service (Amazon SNS) 主题。配置一个或多个订阅者以从该主题接收事件。"
    },
    "tags": [
      "API Gateway",
      "Lambda",
      "SNS",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何构建事件驱动的无服务器架构。该场景要求事件驱动、松耦合和发布/订阅模式。AWS SNS 和 Lambda 函数是构建这种架构的理想选择。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 使用 API Gateway 将事件发布到 SNS 主题，然后订阅者可以从 SNS 主题接收事件，是最具成本效益的方式，也符合事件驱动和松耦合的设计理念。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 SQS 队列作为发布者和订阅者，增加了复杂性，不符合发布/订阅模式。选项 C 使用 WebSocket API 写入 Kinesis Data Streams，用于流式数据处理，不适合此场景。选项 D 同样使用 SNS 主题，但使用了 HTTP API，成本较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon API Gateway",
      "AWS Lambda",
      "Amazon Kinesis Data Streams",
      "Amazon Simple Queue Service (Amazon SQS)",
      "Amazon Simple Notification Service (Amazon SNS)"
    ]
  },
  {
    "id": 921,
    "topic": "1",
    "question_en": "A company recently migrated a monolithic application to an Amazon EC2 instance and Amazon RDS. The application has tightly coupled modules. The existing design of the application gives the application the ability to run on only a single EC2 instance. The company has noticed high CPU utilization on the EC2 instance during peak usage times. The high CPU utilization corresponds to degraded performance on Amazon RDS for read requests. The company wants to reduce the high CPU utilization and improve read request performance. Which solution will meet these requirements?",
    "options_en": {
      "A": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto Scaling group with a minimum and maximum size of 1. Configure an RDS read replica for read requests.",
      "B": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto Scaling group with a minimum and maximum size of 1. Add an RDS read replica and redirect all read/write trafic to the replica.",
      "C": "Configure an Auto Scaling group with a minimum size of 1 and maximum size of 2. Resize the RDS DB instance to an instance type that has more CPU capacity.",
      "D": "Resize the EC2 instance to an EC2 instance type that has more CPU capacity. Configure an Auto Scaling group with a minimum and maximum size of 1. Resize the RDS DB instance to an instance type that has more CPU capacity."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司最近将其单体应用程序迁移到 Amazon EC2 实例和 Amazon RDS。该应用程序具有紧密耦合的模块。该应用程序的现有设计使其只能在一个 EC2 实例上运行。该公司注意到在高峰使用时段，EC2 实例上的 CPU 利用率很高。高 CPU 利用率对应于 Amazon RDS 上读取请求的性能下降。该公司希望降低高 CPU 利用率并提高读取请求的性能。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 EC2 实例调整为具有更多 CPU 容量的 EC2 实例类型。配置一个 Auto Scaling 组，其最小和最大大小为 1。为读取请求配置 RDS 读取副本。",
      "B": "将 EC2 实例调整为具有更多 CPU 容量的 EC2 实例类型。配置一个 Auto Scaling 组，其最小和最大大小为 1。添加一个 RDS 读取副本，并将所有读取/写入流量重定向到该副本。",
      "C": "配置一个 Auto Scaling 组，其最小大小为 1，最大大小为 2。将 RDS 数据库实例调整为具有更多 CPU 容量的实例类型。",
      "D": "将 EC2 实例调整为具有更多 CPU 容量的 EC2 实例类型。配置一个 Auto Scaling 组，其最小和最大大小为 1。将 RDS 数据库实例调整为具有更多 CPU 容量的实例类型。"
    },
    "tags": [
      "EC2",
      "RDS",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何提高单体应用程序的性能和可用性。解决高 CPU 利用率和数据库读取性能下降的问题，可以通过增加计算资源、使用读取副本，结合 Auto Scaling 实现弹性伸缩。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 结合了多种优化策略。首先，增加 EC2 实例的 CPU 容量，可以缓解高 CPU 利用率的问题。其次，配置 RDS 读取副本，可以分担读取流量，提高数据库性能。 此外，最小和最大大小为 1 的 Auto Scaling 组可以保证实例的可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 没有使用读取副本，无法提高数据库的读取性能。选项 C 和 D 没有配置读取副本，并且 Auto Scaling 组配置不合理。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon RDS",
      "Auto Scaling",
      "RDS"
    ]
  },
  {
    "id": 922,
    "topic": "1",
    "question_en": "A company needs to grant a team of developers access to the company's AWS resources. The company must maintain a high level of security for the resources. The company requires an access control solution that will prevent unauthorized access to the sensitive data. Which solution will meet these requirements?",
    "options_en": {
      "A": "Share the IAM user credentials for each development team member with the rest of the team to simplify access management and to streamline development workfiows.",
      "B": "Define IAM roles that have fine-grained permissions based on the principle of least privilege. Assign an IAM role to each developer.",
      "C": "Create IAM access keys to grant programmatic access to AWS resources. Allow only developers to interact with AWS resources through API calls by using the access keys.",
      "D": "Create an AWS Cognito user pool. Grant developers access to AWS resources by using the user pool."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司需要授予一个开发团队访问公司 AWS 资源的权限。该公司必须为其资源维护高水平的安全性。该公司需要一个访问控制解决方案，以防止未经授权访问敏感数据。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "与团队的其他成员共享每个开发团队成员的 IAM 用户凭证，以简化访问管理并简化开发工作流程。",
      "B": "定义基于最小权限原则的细粒度权限的 IAM 角色。为每个开发人员分配一个 IAM 角色。",
      "C": "创建 IAM 访问密钥以授予对 AWS 资源的编程访问权限。仅允许开发人员通过使用访问密钥的 API 调用与 AWS 资源交互。",
      "D": "创建一个 AWS Cognito 用户池。通过使用用户池授予开发人员对 AWS 资源的访问权限。"
    },
    "tags": [
      "IAM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何授予开发团队访问 AWS 资源的权限。为了实现高安全性，应遵循最小权限原则，并为开发人员分配 IAM 角色。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 采用了基于最小权限原则的细粒度权限的 IAM 角色，并为每个开发人员分配 IAM 角色。这符合安全最佳实践，降低了安全风险。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 共享 IAM 用户凭证，存在安全风险，违反了最小权限原则。选项 C 使用 IAM 访问密钥，增加了密钥泄露的风险，并且不方便管理。选项 D 使用 AWS Cognito，不符合题干要求，不适合这种场景。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM",
      "AWS Cognito"
    ]
  },
  {
    "id": 923,
    "topic": "1",
    "question_en": "A company hosts a monolithic web application on an Amazon EC2 instance. Application users have recently reported poor performance at specific times. Analysis of Amazon CloudWatch metrics shows that CPU utilization is 100% during the periods of poor performance. The company wants to resolve this performance issue and improve application availability. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale vertically.",
      "B": "Create an Amazon Machine Image (AMI) from the web server. Reference the AMI in a new launch template.",
      "C": "Create an Auto Scaling group and an Application Load Balancer to scale vertically.",
      "D": "Use AWS Compute Optimizer to obtain a recommendation for an instance type to scale horizontally",
      "E": "Create an Auto Scaling group and an Application Load Balancer to scale horizontally."
    },
    "correct_answer": "BE",
    "vote_percentage": "",
    "question_cn": "一家公司在其 Amazon EC2 实例上托管一个单体 Web 应用程序。 应用程序用户最近报告说在特定时间性能很差。对 Amazon CloudWatch 指标的分析表明，在性能不佳的时段，CPU 利用率为 100%。该公司希望解决此性能问题并提高应用程序的可用性。哪两种步骤的组合将最具成本效益地满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Compute Optimizer 获取有关纵向扩展实例类型的建议。",
      "B": "从 Web 服务器创建一个 Amazon Machine Image (AMI)。在新启动模板中引用该 AMI。",
      "C": "创建一个 Auto Scaling 组和一个 Application Load Balancer 进行纵向扩展。",
      "D": "使用 AWS Compute Optimizer 获取有关横向扩展实例类型的建议。",
      "E": "创建一个 Auto Scaling 组和一个 Application Load Balancer 进行横向扩展。"
    },
    "tags": [
      "EC2",
      "CloudWatch",
      "Auto Scaling",
      "ALB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BE（社区 —），解析仅供参考。】\n\n此题考察如何解决应用程序的性能问题并提高可用性。需要根据分析结果采取合适的措施。在高 CPU 利用率为 100% 的情况下，应考虑纵向扩展或横向扩展。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BE。理由简述：选项 B 使用 AWS Compute Optimizer 获取关于纵向扩展实例类型的建议。 选项 E 创建一个 Auto Scaling 组和一个 Application Load Balancer 进行横向扩展。 这两个选项的结合是满足题干需求的最佳方案。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 Compute Optimizer 获取关于纵向扩展实例类型的建议，但没有提供横向扩展方案。选项 C 创建一个 Auto Scaling 组和一个 Application Load Balancer 进行纵向扩展。 选项 D 使用 Compute Optimizer 获取关于横向扩展实例类型的建议，没有提供纵向扩展方案。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "AWS Compute Optimizer",
      "Amazon CloudWatch",
      "Amazon Machine Image (AMI)",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 924,
    "topic": "1",
    "question_en": "A company runs all its business applications in the AWS Cloud. The company uses AWS Organizations to manage multiple AWS accounts. A solutions architect needs to review all permissions that are granted to IAM users to determine which IAM users have more permissions than required. Which solution will meet these requirements with the LEAST administrative overhead?",
    "options_en": {
      "A": "Use Network Access Analyzer to review all access permissions in the company's AWS accounts.",
      "B": "Create an AWS CloudWatch alarm that activates when an IAM user creates or modifies resources in an AWS account.",
      "C": "Use AWS Identity and Access Management (IAM) Access Analyzer to review all the company’s resources and accounts.",
      "D": "Use Amazon Inspector to find vulnerabilities in existing IAM policies."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 云中运行其所有业务应用程序。该公司使用 AWS Organizations 管理多个 AWS 账户。一个解决方案架构师需要审查授予 IAM 用户的所有权限，以确定哪些 IAM 用户拥有超过所需权限的权限。哪种解决方案以最少的管理开销满足这些要求？",
    "options_cn": {
      "A": "使用 Network Access Analyzer 审查公司 AWS 账户中的所有访问权限。",
      "B": "创建一个 AWS CloudWatch 警报，该警报在 IAM 用户在 AWS 账户中创建或修改资源时激活。",
      "C": "使用 AWS Identity and Access Management (IAM) Access Analyzer 审查公司所有资源和账户。",
      "D": "使用 Amazon Inspector 查找现有 IAM 策略中的漏洞。"
    },
    "tags": [
      "IAM",
      "IAM Access Analyzer",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察如何审查 IAM 用户的权限。为了简化管理开销，应该使用 IAM Access Analyzer。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 使用 IAM Access Analyzer 审查公司所有资源和账户，提供了最直接、最全面的权限审查解决方案，并且管理开销最少。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 Network Access Analyzer，不适用于审查 IAM 权限。选项 B 使用 CloudWatch 警报，无法直接审查现有的 IAM 权限。选项 D 使用 Amazon Inspector，主要用于查找漏洞，不直接用于 IAM 权限审查。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS CloudWatch",
      "Amazon Inspector",
      "IAM",
      "AWS Identity and Access Management (IAM) Access Analyzer"
    ]
  },
  {
    "id": 925,
    "topic": "1",
    "question_en": "A company needs to implement a new data retention policy for regulatory compliance. As part of this policy, sensitive documents that are stored in an Amazon S3 bucket must be protected from deletion or modification for a fixed period of time. Which solution will meet these requirements?",
    "options_en": {
      "A": "Activate S3 Object Lock on the required objects and enable governance mode.",
      "B": "Activate S3 Object Lock on the required objects and enable compliance mode.",
      "C": "Enable versioning on the S3 bucket. Set a lifecycle policy to delete the objects after a specified period.",
      "D": "Configure an S3 Lifecycle policy to transition objects to S3 Glacier Flexible Retrieval for the retention duration."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司需要实施一项新的数据保留策略以符合法规要求。作为此策略的一部分，存储在 Amazon S3 存储桶中的敏感文档必须在固定时间内受到保护，以防止被删除或修改。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "在所需对象上激活 S3 Object Lock 并启用管理模式。",
      "B": "在所需对象上激活 S3 Object Lock 并启用合规模式。",
      "C": "在 S3 存储桶上启用版本控制。设置一个生命周期策略，在指定的时间段后删除对象。",
      "D": "配置 S3 生命周期策略，以便在保留期限内将对象转移到 S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "S3",
      "Object Lock"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n本题考察如何保护 S3 存储桶中的敏感文档，以符合法规要求。S3 Object Lock 提供了防止对象被删除或修改的功能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 在所需对象上激活 S3 Object Lock 并启用合规模式，可以满足文档在固定时间内不被删除或修改的要求，并且合规模式具有更严格的保护机制。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 在所需对象上激活 S3 Object Lock 并启用管理模式，管理模式允许具有特定权限的用户覆盖保护，无法满足题干中防止被修改的要求。选项 C 使用版本控制和生命周期策略，无法防止对象被删除。选项 D 配置 S3 生命周期策略，将对象转移到 Glacier，不能防止对象被修改或删除。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "S3 Glacier Flexible Retrieval",
      "S3 Object Lock"
    ]
  },
  {
    "id": 926,
    "topic": "1",
    "question_en": "A company runs its customer-facing web application on containers. The workload uses Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The web application is resource intensive. The web application needs to be available 24 hours a day, 7 days a week for customers. The company expects the application to experience short bursts of high trafic. The workload must be highly available. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an ECS capacity provider with Fargate. Conduct load testing by using a third-party tool. Rightsize the Fargate tasks in Amazon CloudWatch.",
      "B": "Configure an ECS capacity provider with Fargate for steady state and Fargate Spot for burst trafic.",
      "C": "Configure an ECS capacity provider with Fargate Spot for steady state and Fargate for burst trafic.",
      "D": "Configure an ECS capacity provider with Fargate. Use AWS Compute Optimizer to rightsize the Fargate task."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在其容器上运行面向客户的 Web 应用程序。工作负载在 AWS Fargate 上使用 Amazon Elastic Container Service (Amazon ECS)。Web 应用程序资源密集。Web 应用程序需要每周 7 天、每天 24 小时为客户提供服务。该公司预计应用程序将经历短时高峰流量。工作负载必须具有高可用性。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 Fargate 配置 ECS 容量提供程序。使用第三方工具进行负载测试。在 Amazon CloudWatch 中调整 Fargate 任务的大小。",
      "B": "为稳定状态配置具有 Fargate 的 ECS 容量提供程序，并为突发流量配置 Fargate Spot。",
      "C": "为稳定状态配置具有 Fargate Spot 的 ECS 容量提供程序，并为突发流量配置 Fargate。",
      "D": "使用 Fargate 配置 ECS 容量提供程序。使用 AWS Compute Optimizer 调整 Fargate 任务的大小。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "Compute Optimizer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察如何在 AWS Fargate 上运行高可用性的、资源密集型 Web 应用程序。 该场景要求每周 7 天、每天 24 小时为客户提供服务，并且应用程序会经历短时高峰流量。 解决方案需要具备高可用性和成本效益。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 配置 Fargate Spot 用于稳定状态，Fargate 用于突发流量。Fargate Spot 降低了稳定状态的成本，而 Fargate 提供了应对突发流量的弹性，实现了成本效益和高可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 Fargate 配置 ECS 容量提供程序，没有考虑成本优化。选项 B Fargate 和 Fargate Spot 的配置，与 C 选项相反。选项 D 使用 Compute Optimizer 调整 Fargate 任务的大小，没有体现高可用性和成本效益。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Fargate",
      "AWS Compute Optimizer",
      "Amazon Elastic Container Service (Amazon ECS)",
      "Fargate Spot"
    ]
  },
  {
    "id": 927,
    "topic": "1",
    "question_en": "A company is building an application in the AWS Cloud. The application is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses Amazon Route 53 for the DNS. The company needs a managed solution with proactive engagement to detect against DDoS attacks. Which solution will meet these requirements?",
    "options_en": {
      "A": "Enable AWS Config. Configure an AWS Config managed rule that detects DDoS attacks.",
      "B": "Enable AWS WAF on the ALCreate an AWS WAF web ACL with rules to detect and prevent DDoS attacks. Associate the web ACL with the ALB.",
      "C": "Store the ALB access logs in an Amazon S3 bucket. Configure Amazon GuardDuty to detect and take automated preventative actions for DDoS attacks.",
      "D": "Subscribe to AWS Shield Advanced. Configure hosted zones in Route 53. Add ALB resources as protected resources."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 云中构建一个应用程序。 该应用程序托管在 Application Load Balancer (ALB) 后的 Amazon EC2 实例上。 公司使用 Amazon Route 53 进行 DNS。 该公司需要一个具有主动参与的托管解决方案来检测 DDoS 攻击。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "启用 AWS Config。 配置一个 AWS Config 托管规则来检测 DDoS 攻击。",
      "B": "在 ALB 上启用 AWS WAF。 创建一个带有规则的 AWS WAF Web ACL 来检测和阻止 DDoS 攻击。 将 Web ACL 与 ALB 关联。",
      "C": "将 ALB 访问日志存储在 Amazon S3 存储桶中。 配置 Amazon GuardDuty 以检测 DDoS 攻击并采取自动预防措施。",
      "D": "订阅 AWS Shield Advanced。 在 Route 53 中配置托管区域。 将 ALB 资源添加为受保护资源。"
    },
    "tags": [
      "ALB",
      "Route 53",
      "DDoS",
      "WAF",
      "Shield Advanced"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考察如何检测和防御 DDoS 攻击。需要利用主动参与的托管解决方案。AWS Shield Advanced 提供了针对 DDoS 攻击的全面防护。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 订阅 AWS Shield Advanced，可以在 Route 53 中配置托管区域，并将 ALB 资源添加为受保护资源，可以满足检测 DDoS 攻击的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 启用 AWS Config，无法检测 DDoS 攻击。选项 B 在 ALB 上启用 AWS WAF，无法检测 DDoS 攻击。选项 C 将 ALB 访问日志存储在 S3 存储桶中，配置 GuardDuty 无法满足检测 DDoS 攻击的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Route 53",
      "AWS Shield Advanced",
      "DDoS",
      "AWS WAF",
      "AWS Config",
      "Amazon GuardDuty",
      "Application Load Balancer (ALB)"
    ]
  },
  {
    "id": 928,
    "topic": "1",
    "question_en": "A company hosts a video streaming web application in a VPC. The company uses a Network Load Balancer (NLB) to handle TCP trafic for real-time data processing. There have been unauthorized attempts to access the application. The company wants to improve application security with minimal architectural change to prevent unauthorized attempts to access the application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement a series of AWS WAF rules directly on the NLB to filter out unauthorized trafic.",
      "B": "Recreate the NLB with a security group to allow only trusted IP addresses.",
      "C": "Deploy a second NLB in parallel with the existing NLB configured with a strict IP address allow list.",
      "D": "Use AWS Shield Advanced to provide enhanced DDoS protection and prevent unauthorized access attempts."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在 VPC 中托管视频流 Web 应用程序。该公司使用 Network Load Balancer (NLB) 来处理实时数据处理的 TCP 流量。 出现过未经授权访问该应用程序的尝试。该公司希望通过对架构进行最少的更改来提高应用程序安全性，以防止未经授权的访问尝试。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "直接在 NLB 上实施一系列 AWS WAF 规则以过滤掉未经授权的流量。",
      "B": "使用安全组重新创建 NLB，仅允许受信任的 IP 地址。",
      "C": "部署与现有 NLB 并行运行的第二个 NLB，配置严格的 IP 地址允许列表。",
      "D": "使用 AWS Shield Advanced 提供增强的 DDoS 保护并防止未经授权的访问尝试。"
    },
    "tags": [
      "NLB",
      "WAF",
      "Security Group",
      "Shield Advanced"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何提高 Web 应用程序的安全性。 针对未经授权的访问尝试，解决方案需要对架构的影响最小。 Security Group 提供了最简单的解决方案，允许或拒绝流量。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 通过使用安全组，仅允许受信任的 IP 地址访问 NLB。 这是简单有效的安全措施，且对现有架构的影响最小。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 直接在 NLB 上实施一系列 AWS WAF 规则，需要修改 NLB 配置，增加了架构复杂度。选项 C 部署第二个 NLB，增加了架构复杂度。选项 D 使用 AWS Shield Advanced 提供了 DDoS 保护，无法阻止未经授权的访问。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS WAF",
      "AWS Shield Advanced",
      "Network Load Balancer (NLB)",
      "Security Group"
    ]
  },
  {
    "id": 929,
    "topic": "1",
    "question_en": "A healthcare company is developing an AWS Lambda function that publishes notifications to an encrypted Amazon Simple Notification Service (Amazon SNS) topic. The notifications contain protected health information (PHI). The SNS topic uses AWS Key Management Service (AWS KMS) customer managed keys for encryption. The company must ensure that the application has the necessary permissions to publish messages securely to the SNS topic. Which combination of steps will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Create a resource policy for the SNS topic that allows the Lambda function to publish messages to the topic.",
      "B": "Use server-side encryption with AWS KMS keys (SSE-KMS) for the SNS topic instead of customer managed keys.",
      "C": "Create a resource policy for the encryption key that the SNS topic uses that has the necessary AWS KMS permissions.",
      "D": "Specify the Lambda function's Amazon Resource Name (ARN) in the SNS topic's resource policy",
      "E": "Associate an Amazon API Gateway HTTP API with the SNS topic to control access to the topic by using API Gateway resource policies",
      "F": "Configure a Lambda execution role that has the necessary IAM permissions to use a customer managed key in AWS KMS."
    },
    "correct_answer": "ADF",
    "vote_percentage": "",
    "question_cn": "一家医疗保健公司正在开发一个 AWS Lambda 函数，该函数将通知发布到加密的 Amazon Simple Notification Service (Amazon SNS) 主题。通知包含受保护的健康信息 (PHI)。SNS 主题使用 AWS Key Management Service (AWS KMS) 客户托管密钥进行加密。该公司必须确保该应用程序具有将消息安全地发布到 SNS 主题的必要权限。哪种组合步骤将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "为 SNS 主题创建资源策略，允许 Lambda 函数将消息发布到该主题。",
      "B": "使用带有 AWS KMS 密钥的服务器端加密 (SSE-KMS) 来代替客户托管密钥加密 SNS 主题。",
      "C": "为 SNS 主题使用的加密密钥创建资源策略，该策略具有必要的 AWS KMS 权限。",
      "D": "在 SNS 主题的资源策略中指定 Lambda 函数的 Amazon 资源名称 (ARN)。",
      "E": "将 Amazon API Gateway HTTP API 与 SNS 主题关联，以使用 API Gateway 资源策略控制对该主题的访问。",
      "F": "配置 Lambda 执行角色，该角色具有使用 AWS KMS 中的客户托管密钥的必要 IAM 权限。"
    },
    "tags": [
      "SNS",
      "KMS",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ADF（社区 —），解析仅供参考。】\n\n考查使用 Lambda 向 KMS 客户托管密钥加密的 SNS 主题安全发布消息所需的权限配置。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ADF。理由简述：选项 A 正确：为 SNS 主题创建资源策略，允许 Lambda 发布到该主题。选项 D 正确：在 SNS 资源策略中指定 Lambda 的 ARN，实现最小权限。选项 F 正确：为 Lambda 执行角色配置使用 KMS 客户托管密钥的 IAM 权限，使 Lambda 能通过 KMS 加密的 SNS 发布消息。A+D+F 满足安全发布到加密 SNS 的三项要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 错误，题目要求客户托管密钥，不能改为 SSE-KMS 替代。C 错误，KMS 密钥资源策略需与 Lambda 角色权限配合，但本题更直接的是 Lambda 角色具备 kms:Decrypt 等权限（F）。E 错误，API Gateway 非本题必需，增加复杂度。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SNS",
      "AWS KMS",
      "Lambda",
      "IAM",
      "ARN",
      "API Gateway",
      "SSE-KMS"
    ]
  },
  {
    "id": 930,
    "topic": "1",
    "question_en": "A company has an employee web portal. Employees log in to the portal to view payroll details. The company is developing a new system to give employees the ability to upload scanned documents for reimbursement. The company runs a program to extract text-based data from the documents and attach the extracted information to each employee’s reimbursement IDs for processing. The employee web portal requires 100% uptime. The document extract program runs infrequently throughout the day on an on-demand basis. The company wants to build a scalable and cost-effective new system that will require minimal changes to the existing web portal. The company does not want to make any code changes. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Run Amazon EC2 On-Demand Instances in an Auto Scaling group for the web portal. Use an AWS Lambda function to run the document extract program. Invoke the Lambda function when an employee uploads a new reimbursement document.",
      "B": "Run Amazon EC2 Spot Instances in an Auto Scaling group for the web portal. Run the document extract program on EC2 Spot Instances. Start document extract program instances when an employee uploads a new reimbursement document.",
      "C": "Purchase a Savings Plan to run the web portal and the document extract program. Run the web portal and the document extract program in an Auto Scaling group.",
      "D": "Create an Amazon S3 bucket to host the web portal. Use Amazon API Gateway and an AWS Lambda function for the existing functionalities. Use the Lambda function to run the document extract program. Invoke the Lambda function when the API that is associated with a new document upload is called."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司有一个员工 Web 门户。员工登录门户网站以查看工资单详细信息。该公司正在开发一个新系统，使员工能够上传扫描的文档以进行报销。该公司运行一个程序，从文档中提取基于文本的数据，并将提取的信息附加到每个员工的报销 ID，以便进行处理。员工 Web 门户需要 100% 的正常运行时间。文档提取程序在一天中不频繁地按需运行。该公司希望构建一个可扩展且具有成本效益的新系统，该系统需要对现有 Web 门户进行最少的更改。该公司不想进行任何代码更改。哪个解决方案将以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "在 Auto Scaling 组中运行 Amazon EC2 按需实例来运行 Web 门户。使用 AWS Lambda 函数运行文档提取程序。当员工上传新的报销文档时调用 Lambda 函数。",
      "B": "在 Auto Scaling 组中运行 Amazon EC2 Spot 实例来运行 Web 门户。在 EC2 Spot 实例上运行文档提取程序。当员工上传新的报销文档时，启动文档提取程序实例。",
      "C": "购买 Savings Plan 以运行 Web 门户和文档提取程序。在 Auto Scaling 组中运行 Web 门户和文档提取程序。",
      "D": "创建一个 Amazon S3 存储桶来托管 Web 门户。使用 Amazon API Gateway 和 AWS Lambda 函数来实现现有功能。使用 Lambda 函数运行文档提取程序。当调用与新文档上传关联的 API 时，调用 Lambda 函数。"
    },
    "tags": [
      "EC2",
      "Lambda",
      "Auto Scaling",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考查如何设计一个可扩展且成本效益高的系统，以满足Web门户的需求。选择需要权衡成本、可扩展性和对现有系统的影响。使用 Lambda 函数可以减少对现有系统的影响，并能够按需运行文档提取程序。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 通过在 Auto Scaling 组中运行 EC2 实例来实现 Web 门户的高可用性和弹性，使用 Lambda 函数运行文档提取程序，降低了成本，并减少了对现有系统的更改。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 使用 Spot 实例，虽然成本低，但可能影响 100% 正常运行时间。选项 C 购买 Savings Plan 无法满足对现有Web门户的最小更改。选项 D 使用 S3 和 API Gateway 会对现有系统更改很大，不满足最小化代码更改的要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "Lambda",
      "S3",
      "API Gateway"
    ]
  },
  {
    "id": 931,
    "topic": "1",
    "question_en": "A media company has a multi-account AWS environment in the us-east-1 Region. The company has an Amazon Simple Notification Service (Amazon SNS) topic in a production account that publishes performance metrics. The company has an AWS Lambda function in an administrator account to process and analyze log data. The Lambda function that is in the administrator account must be invoked by messages from the SNS topic that is in the production account when significant metrics are reported. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an IAM resource policy for the Lambda function that allows Amazon SNS to invoke the function.",
      "B": "Implement an Amazon Simple Queue Service (Amazon SQS) queue in the administrator account to buffer messages from the SNS topic that is in the production account. Configure the SQS queue to invoke the Lambda function.",
      "C": "Create an IAM policy for the SNS topic that allows the Lambda function to subscribe to the topic.",
      "D": "Use an Amazon EventBridge rule in the production account to capture the SNS topic notifications. Configure the EventBridge rule to forward notifications to the Lambda function that is in the administrator account",
      "E": "Store performance metrics in an Amazon S3 bucket in the production account. Use Amazon Athena to analyze the metrics from the administrator account."
    },
    "correct_answer": "AC",
    "vote_percentage": "",
    "question_cn": "一家媒体公司在 us-east-1 区域有一个多账户 AWS 环境。该公司在生产账户中有一个 Amazon Simple Notification Service (Amazon SNS) 主题，用于发布性能指标。该公司在管理员账户中有一个 AWS Lambda 函数，用于处理和分析日志数据。当报告重要指标时，必须通过来自生产账户中的 SNS 主题的消息来调用管理员账户中的 Lambda 函数。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "为 Lambda 函数创建 IAM 资源策略，允许 Amazon SNS 调用该函数。",
      "B": "在管理员账户中实施一个 Amazon Simple Queue Service (Amazon SQS) 队列，以缓冲来自生产账户中 SNS 主题的消息。配置 SQS 队列以调用 Lambda 函数。",
      "C": "为 SNS 主题创建 IAM 策略，允许 Lambda 函数订阅该主题。",
      "D": "在生产账户中使用 Amazon EventBridge 规则来捕获 SNS 主题通知。配置 EventBridge 规则，将通知转发到管理员账户中的 Lambda 函数。",
      "E": "将性能指标存储在生产账户中的 Amazon S3 存储桶中。使用 Amazon Athena 从管理员账户分析这些指标。"
    },
    "tags": [
      "SNS",
      "Lambda",
      "EventBridge",
      "SQS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 —），解析仅供参考。】\n\n此题考察跨账户调用 Lambda 函数。正确答案需要考虑 SNS 主题的发布和 Lambda 函数的订阅、跨账户的权限设置。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AC。理由简述：选项 A 通过创建 IAM 资源策略，允许 SNS 调用 Lambda 函数。SNS可以调用Lambda函数，满足题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 使用 SQS 队列，增加了复杂性。选项 C 在生产账户中配置 SNS 订阅，不符合跨账号的调用需求。选项 D 使用 EventBridge 将SNS通知转发到另一个账户，EventBridge 规则也需要配置，增加了复杂度，不满足题目要求。选项 E 使用 Athena 分析数据，与题目要求无关。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SNS",
      "Lambda",
      "IAM",
      "SQS",
      "EventBridge",
      "S3",
      "Athena"
    ]
  },
  {
    "id": 932,
    "topic": "1",
    "question_en": "A company is migrating an application from an on-premises location to Amazon Elastic Kubernetes Service (Amazon EKS). The company must use a custom subnet for pods that are in the company's VPC to comply with requirements. The company also needs to ensure that the pods can communicate securely within the pods' VPC. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure AWS Transit Gateway to directly manage custom subnet configurations for the pods in Amazon EKS.",
      "B": "Create an AWS Direct Connect connection from the company's on-premises IP address ranges to the EKS pods.",
      "C": "Use the Amazon VPC CNI plugin for Kubernetes. Define custom subnets in the VPC cluster for the pods to use.",
      "D": "Implement a Kubernetes network policy that has pod anti-afinity rules to restrict pod placement to specific nodes that are within custom subnets."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其应用程序从本地位置迁移到 Amazon Elastic Kubernetes Service (Amazon EKS)。该公司必须使用自定义子网，以便符合要求的 pod 位于公司的 VPC 中。该公司还需要确保 pod 可以在 pod 的 VPC 中安全地通信。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "配置 AWS Transit Gateway 以直接管理 Amazon EKS 中 pod 的自定义子网配置。",
      "B": "从公司的本地 IP 地址范围创建到 EKS pod 的 AWS Direct Connect 连接。",
      "C": "使用 Kubernetes 的 Amazon VPC CNI 插件。在 VPC 集群中为 pod 定义自定义子网以供使用。",
      "D": "实施 Kubernetes 网络策略，该策略具有 pod 反亲和性规则，以将 pod 放置限制为位于自定义子网内的特定节点。"
    },
    "tags": [
      "EKS",
      "VPC",
      "Kubernetes"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考查如何在 EKS 中使用自定义子网，以及 pod 之间的安全通信。正确答案需要考虑 VPC CNI 插件的使用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 使用 Kubernetes 的 Amazon VPC CNI 插件，在 VPC 集群中为 pod 定义自定义子网以供使用。能够满足在自定义子网中部署 pod 并进行安全通信的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 配置 AWS Transit Gateway，无法直接管理 EKS pod 的自定义子网配置。选项 C 从本地 IP 地址范围创建到 EKS pod 的 AWS Direct Connect 连接，无法实现题目的要求。选项 D 实施 Kubernetes 网络策略，并不能满足创建自定义子网的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EKS",
      "VPC",
      "Kubernetes",
      "Transit Gateway",
      "Direct Connect",
      "CNI"
    ]
  },
  {
    "id": 933,
    "topic": "1",
    "question_en": "A company hosts an ecommerce application that stores all data in a single Amazon RDS for MySQL DB instance that is fully managed by AWS. The company needs to mitigate the risk of a single point of failure. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Modify the RDS DB instance to use a Multi-AZ deployment. Apply the changes during the next maintenance window.",
      "B": "Migrate the current database to a new Amazon DynamoDB Multi-AZ deployment. Use AWS Database Migration Service (AWS DMS) with a heterogeneous migration strategy to migrate the current RDS DB instance to DynamoDB tables.",
      "C": "Create a new RDS DB instance in a Multi-AZ deployment. Manually restore the data from the existing RDS DB instance from the"
    },
    "correct_answer": "",
    "vote_percentage": "",
    "question_cn": "一家公司托管一个电子商务应用程序，该应用程序将所有数据存储在由 AWS 完全托管的单个 Amazon RDS for MySQL 数据库实例中。该公司需要减轻单点故障的风险。哪种解决方案以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "修改 RDS 数据库实例以使用多可用区部署。在下一次维护窗口期间应用更改。",
      "B": "将当前数据库迁移到新的 Amazon DynamoDB 多可用区部署。使用 AWS Database Migration Service (AWS DMS) 以及异构迁移策略将当前 RDS 数据库实例迁移到 DynamoDB 表。",
      "C": "在多可用区部署中创建一个新的 RDS 数据库实例。手动从现有 RDS 数据库实例还原数据。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 （社区 —），解析仅供参考。】\n\n此题考查如何提高 RDS for MySQL 数据库的可用性，以减轻单点故障的风险。正确答案需要考虑最小化实施工作量。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 该选项。理由简述：选项 A 修改 RDS 数据库实例以使用多可用区部署。能够快速实现高可用性，并减少实施工作量。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 将当前数据库迁移到新的 Amazon DynamoDB 多可用区部署，需要 AWS DMS 和异构迁移策略，会增加工作量。选项 C 创建新的 RDS 数据库实例，需要手动还原数据，工作量也比较大。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "DynamoDB",
      "AWS DMS"
    ]
  },
  {
    "id": 934,
    "topic": "1",
    "question_en": "A company has multiple Microsoft Windows SMB file servers and Linux NFS file servers for file sharing in an on-premises environment. As part of the company's AWS migration plan, the company wants to consolidate the file servers in the AWS Cloud. The company needs a managed AWS storage service that supports both NFS and SMB access. The solution must be able to share between protocols. The solution must have redundancy at the Availability Zone level. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon FSx for NetApp ONTAP for storage. Configure multi-protocol access.",
      "B": "Create two Amazon EC2 instances. Use one EC2 instance for Windows SMB file server access and one EC2 instance for Linux NFS file server access.",
      "C": "Use Amazon FSx for NetApp ONTAP for SMB access. Use Amazon FSx for Lustre for NFS access.",
      "D": "Use Amazon S3 storage. Access Amazon S3 through an Amazon S3 File Gateway."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在本地环境中拥有多个 Microsoft Windows SMB 文件服务器和 Linux NFS 文件服务器，用于文件共享。作为公司 AWS 迁移计划的一部分，该公司希望在 AWS 云中整合文件服务器。该公司需要一个托管的 AWS 存储服务，该服务支持 NFS 和 SMB 访问。该解决方案必须能够在协议之间共享。该解决方案必须在可用区级别具有冗余。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon FSx for NetApp ONTAP 进行存储。配置多协议访问。",
      "B": "创建两个 Amazon EC2 实例。使用一个 EC2 实例进行 Windows SMB 文件服务器访问，使用一个 EC2 实例进行 Linux NFS 文件服务器访问。",
      "C": "使用 Amazon FSx for NetApp ONTAP 进行 SMB 访问。使用 Amazon FSx for Lustre 进行 NFS 访问。",
      "D": "使用 Amazon S3 存储。通过 Amazon S3 File Gateway 访问 Amazon S3。"
    },
    "tags": [
      "FSx",
      "NFS",
      "SMB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考查在 AWS 云中整合文件服务器，要求支持 NFS 和 SMB 访问，并且要有冗余。正确答案需要考虑托管服务的特性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 使用 Amazon FSx for NetApp ONTAP，可以进行多协议访问，并且提供了可用区级别的冗余，满足了题目要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 创建两个 Amazon EC2 实例，没有冗余，并且需要自行管理，不满足托管服务的要求。选项 C 使用 Amazon FSx for NetApp ONTAP 进行 SMB 访问， 使用 Amazon FSx for Lustre 进行 NFS 访问，需要同时使用两种服务，增加了复杂性。选项 D 使用 Amazon S3 存储，并不能直接支持 NFS 和 SMB 访问。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "FSx",
      "NFS",
      "SMB",
      "EC2",
      "S3",
      "File Gateway"
    ]
  },
  {
    "id": 935,
    "topic": "1",
    "question_en": "A software company needs to upgrade a critical web application. The application currently runs on a single Amazon EC2 instance that the company hosts in a public subnet. The EC2 instance runs a MySQL database. The application's DNS records are published in an Amazon Route 53 zone. A solutions architect must reconfigure the application to be scalable and highly available. The solutions architect must also reduce MySQL read latency. Which combination of solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Launch a second EC2 instance in a second AWS Region. Use a Route 53 failover routing policy to redirect the trafic to the second EC2 instance.",
      "B": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple Availability Zones. Add the instances to a target group behind a new Application Load Balancer.",
      "C": "Migrate the database to an Amazon Aurora MySQL cluster. Create the primary DB instance and reader DB instance in separate Availability Zones.",
      "D": "Create and configure an Auto Scaling group to launch private EC2 instances in multiple AWS Regions. Add the instances to a target group behind a new Application Load Balancer",
      "E": "Migrate the database to an Amazon Aurora MySQL cluster with cross-Region read replicas."
    },
    "correct_answer": "DE",
    "vote_percentage": "",
    "question_cn": "一家软件公司需要升级一个关键的 Web 应用程序。该应用程序目前在公司托管于一个公有子网中的单个 Amazon EC2 实例上运行。EC2 实例运行一个 MySQL 数据库。应用程序的 DNS 记录发布在 Amazon Route 53 区域中。解决方案架构师必须重新配置应用程序以实现可扩展性和高可用性。解决方案架构师还必须减少 MySQL 读取延迟。哪种解决方案组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在第二个 AWS 区域启动第二个 EC2 实例。使用 Route 53 故障转移路由策略将流量重定向到第二个 EC2 实例。",
      "B": "创建并配置一个 Auto Scaling 组，以在多个可用区启动私有 EC2 实例。将这些实例添加到新 Application Load Balancer 后面的目标组。",
      "C": "将数据库迁移到 Amazon Aurora MySQL 集群。在不同的可用区创建主数据库实例和读取数据库实例。",
      "D": "创建并配置一个 Auto Scaling 组，以在多个 AWS 区域启动私有 EC2 实例。将这些实例添加到新 Application Load Balancer 后面的目标组。",
      "E": "将数据库迁移到具有跨区域读取副本的 Amazon Aurora MySQL 集群。"
    },
    "tags": [
      "EC2",
      "Route 53",
      "Auto Scaling",
      "Application Load Balancer",
      "Aurora",
      "MySQL"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 —），解析仅供参考。】\n\n此题考察如何实现 Web 应用程序的可扩展性和高可用性，并减少 MySQL 读取延迟。正确答案需要考虑可扩展性、可用性和数据库的优化。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 DE。理由简述：选项 D 创建并配置一个 Auto Scaling 组，能够实现可扩展性，将实例添加到 Application Load Balancer 后面，满足高可用性要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 启动第二个 EC2 实例，需要手动管理，不能自动实现可扩展性。选项 B 将数据库迁移到 Amazon Aurora MySQL 集群，没有考虑应用程序本身的可用性，不满足题目要求。选项 C 在不同的可用区创建主数据库实例和读取数据库实例，不满足题目的要求，而且 Aurora 并不需要手动配置副本和主库。选项 E 将数据库迁移到具有跨区域读取副本的 Amazon Aurora MySQL 集群，会导致读取数据延迟增加，不满足题目对减少读取延迟的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Route 53",
      "Auto Scaling",
      "Application Load Balancer",
      "Aurora",
      "MySQL"
    ]
  },
  {
    "id": 936,
    "topic": "1",
    "question_en": "A company runs thousands of AWS Lambda functions. The company needs a solution to securely store sensitive information that all the Lambda functions use. The solution must also manage the automatic rotation of the sensitive information. Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)",
    "options_en": {
      "A": "Create HTTP security headers by using Lambda@Edge to retrieve and create sensitive information",
      "B": "Create a Lambda layer that retrieves sensitive information",
      "C": "Store sensitive information in AWS Secrets Manager",
      "D": "Store sensitive information in AWS Systems Manager Parameter Store",
      "E": "Create a Lambda consumer with dedicated throughput to retrieve sensitive information and create environmental variables"
    },
    "correct_answer": "CD",
    "vote_percentage": "",
    "question_cn": "一家公司运行数千个 AWS Lambda 函数。该公司需要一个解决方案来安全地存储所有 Lambda 函数使用的敏感信息。该解决方案还必须管理敏感信息的自动轮换。哪两种步骤的组合将以最低的运营开销满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 Lambda@Edge 创建 HTTP 安全标头，以检索和创建敏感信息",
      "B": "创建一个 Lambda 层来检索敏感信息",
      "C": "将敏感信息存储在 AWS Secrets Manager 中",
      "D": "将敏感信息存储在 AWS Systems Manager Parameter Store 中",
      "E": "创建一个具有专用吞吐量的 Lambda consumer 来检索敏感信息并创建环境变量"
    },
    "tags": [
      "Secrets Manager",
      "Lambda"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 —），解析仅供参考。】\n\n此题考查如何安全地存储和管理 Lambda 函数使用的敏感信息。正确答案需要考虑安全性和运营开销。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 CD。理由简述：选项 C 将敏感信息存储在 AWS Secrets Manager 中。Secrets Manager 提供了存储、轮换等功能，能够安全存储敏感信息，降低了运营开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 Lambda@Edge 创建 HTTP 安全标头，无法满足存储需求。选项 B 创建 Lambda 层来检索敏感信息，增加了维护成本。选项 D 将敏感信息存储在 AWS Systems Manager Parameter Store 中，不具备自动轮换功能。选项 E 创建一个具有专用吞吐量的 Lambda consumer，增加了维护和成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Secrets Manager",
      "Lambda",
      "Lambda@Edge",
      "Parameter Store"
    ]
  },
  {
    "id": 937,
    "topic": "1",
    "question_en": "A company has an internal application that runs on Amazon EC2 instances in an Auto Scaling group. The EC2 instances are compute optimized and use Amazon Elastic Block Store (Amazon EBS) volumes. The company wants to identify cost optimizations across the EC2 instances, the Auto Scaling group, and the EBS volumes. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the EC2 instances the Auto Scaling group, and the EBS volumes.",
      "B": "Create new Amazon CloudWatch billing alerts. Check the alert statuses for cost recommendations for the EC2 instances, the Auto Scaling group, and the EBS volumes.",
      "C": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances, the Auto Scaling group and the EBS volumes.",
      "D": "Configure AWS Compute Optimizer for cost recommendations for the EC2 instances. Create a new AWS Cost and Usage Report. Search the report for cost recommendations for the Auto Scaling group and the EBS volumes."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司有一个内部应用程序，该应用程序在 Auto Scaling 组中的 Amazon EC2 实例上运行。EC2 实例是计算优化实例，并使用 Amazon Elastic Block Store (Amazon EBS) 卷。该公司希望确定 EC2 实例、Auto Scaling 组和 EBS 卷的成本优化。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 AWS 成本和使用情况报告。在报告中搜索 EC2 实例、Auto Scaling 组和 EBS 卷的成本建议。",
      "B": "创建新的 Amazon CloudWatch 账单警报。检查警报状态，以获取 EC2 实例、Auto Scaling 组和 EBS 卷的成本建议。",
      "C": "为 EC2 实例、Auto Scaling 组和 EBS 卷配置 AWS Compute Optimizer 以获取成本建议。",
      "D": "为 EC2 实例配置 AWS Compute Optimizer 以获取成本建议。创建一个新的 AWS 成本和使用情况报告。在报告中搜索 Auto Scaling 组和 EBS 卷的成本建议。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "EBS",
      "Compute Optimizer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考查如何进行 EC2 实例、Auto Scaling 组和 EBS 卷的成本优化。正确答案需要考虑运营效率。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 为 EC2 实例、Auto Scaling 组和 EBS 卷配置 AWS Compute Optimizer，可以获取成本建议，运营效率最高。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 创建 AWS 成本和使用情况报告，需要人工分析，效率较低。选项 B 创建 CloudWatch 账单警报，只能监控成本，不能提供优化建议。选项 D 只为 EC2 实例配置 Compute Optimizer，不能全面优化，而且创建成本和使用情况报告，需要人工分析。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "EBS",
      "CloudWatch",
      "Compute Optimizer"
    ]
  },
  {
    "id": 938,
    "topic": "1",
    "question_en": "A company is running a media store across multiple Amazon EC2 instances distributed across multiple Availability Zones in a single VPC. The company wants a high-performing solution to share data between all the EC2 instances, and prefers to keep the data within the VPC only. What should a solutions architect recommend?",
    "options_en": {
      "A": "Create an Amazon S3 bucket and call the service APIs from each instance's application",
      "B": "Create an Amazon S3 bucket and configure all instances to access it as a mounted volume",
      "C": "Configure an Amazon Elastic Block Store (Amazon EBS) volume and mount it across all instances",
      "D": "Configure an Amazon Elastic File System (Amazon EFS) file system and mount it across all instances"
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司正在运营一个媒体商店，该商店分布在单个 VPC 中多个可用区中的多个 Amazon EC2 实例上。该公司希望有一个高性能的解决方案，以便在所有 EC2 实例之间共享数据，并且希望仅将数据保留在 VPC 内。解决方案架构师应该推荐什么？",
    "options_cn": {
      "A": "创建一个 Amazon S3 存储桶，并从每个实例的应用程序调用服务 API",
      "B": "创建一个 Amazon S3 存储桶，并将所有实例配置为将其作为已挂载的卷进行访问",
      "C": "配置一个 Amazon Elastic Block Store (Amazon EBS) 卷并在所有实例之间挂载它",
      "D": "配置一个 Amazon Elastic File System (Amazon EFS) 文件系统并在所有实例之间挂载它"
    },
    "tags": [
      "EC2",
      "EFS",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察在多个 EC2 实例之间共享数据。正确答案需要考虑性能和数据保留范围。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 配置 Amazon Elastic File System (Amazon EFS) 文件系统并在所有实例之间挂载它。EFS 是一个高性能的共享文件系统，适用于在多个实例之间共享数据，并且数据只保留在 VPC 内。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 创建一个 Amazon S3 存储桶，并从每个实例的应用程序调用服务 API，性能不如 EFS。选项 B 创建一个 Amazon S3 存储桶，并将所有实例配置为将其作为已挂载的卷进行访问，不能像 EFS 一样提供高性能。选项 C 配置一个 Amazon Elastic Block Store (Amazon EBS) 卷并在所有实例之间挂载它， EBS 无法在多个实例间共享。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "EFS",
      "S3",
      "EBS"
    ]
  },
  {
    "id": 939,
    "topic": "1",
    "question_en": "A company uses an Amazon RDS for MySQL instance. To prepare for end-of-year processing, the company added a read replica to accommodate extra read-only queries from the company's reporting tool. The read replica CPU usage was 60% and the primary instance CPU usage was 60%. After end-of-year activities are complete, the read replica has a constant 25% CPU usage. The primary instance still has a constant 60% CPU usage. The company wants to rightsize the database and still provide enough performance for future growth. Which solution will meet these requirements?",
    "options_en": {
      "A": "Delete the read replica Do not make changes to the primary instance",
      "B": "Resize the read replica to a smaller instance size Do not make changes to the primary instance",
      "C": "Resize the read replica to a larger instance size Resize the primary instance to a smaller instance size",
      "D": "Delete the read replica Resize the primary instance to a larger instance"
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司使用 Amazon RDS for MySQL 实例。为了准备年终处理，该公司添加了一个只读副本以适应来自公司报告工具的额外只读查询。只读副本的 CPU 使用率为 60%，主实例的 CPU 使用率为 60%。年终活动完成后，只读副本的 CPU 使用率恒定为 25%。主实例的 CPU 使用率仍然恒定为 60%。公司希望调整数据库大小，同时仍然为未来的增长提供足够的性能。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "删除只读副本。不对主实例进行更改。",
      "B": "将只读副本调整为较小的实例大小。不对主实例进行更改。",
      "C": "将只读副本调整为更大的实例大小。将主实例调整为较小的实例大小。",
      "D": "删除只读副本。将主实例调整为更大的实例大小。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "Read Replica"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考查 RDS for MySQL 数据库的容量调整。正确答案需要考虑 CPU 使用率，并为未来增长提供足够的性能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 将只读副本调整为较小的实例大小。由于只读副本的 CPU 使用率下降，可以将其调整为较小的实例大小，节约成本，而且主实例的 CPU 使用率不变，不需要进行更改。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 删除只读副本。可能会影响公司的报表工具，不满足题目要求。选项 C 将只读副本调整为更大的实例大小，主实例调整为较小的实例大小，会影响主实例的性能。选项 D 删除只读副本。将主实例调整为更大的实例大小，不能解决问题，还会增加成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "Read Replica"
    ]
  },
  {
    "id": 940,
    "topic": "1",
    "question_en": "A company is migrating its databases to Amazon RDS for PostgreSQL. The company is migrating its applications to Amazon EC2 instances. The company wants to optimize costs for long-running workloads. Which solution will meet this requirement MOST cost-effectively?",
    "options_en": {
      "A": "Use On-Demand Instances for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year Compute Savings Plan with the No Upfront option for the EC2 instances.",
      "B": "Purchase Reserved Instances for a 1 year term with the No Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year EC2 Instance Savings Plan with the No Upfront option for the EC2 instances.",
      "C": "Purchase Reserved Instances for a 1 year term with the Partial Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 1 year EC2 Instance Savings Plan with the Partial Upfront option for the EC2 instances.",
      "D": "Purchase Reserved Instances for a 3 year term with the All Upfront option for the Amazon RDS for PostgreSQL workloads. Purchase a 3 year EC2 Instance Savings Plan with the All Upfront option for the EC2 instances."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其数据库迁移到 Amazon RDS for PostgreSQL。该公司正在将其应用程序迁移到 Amazon EC2 实例。该公司希望优化长期运行的工作负载的成本。哪个解决方案将以最具成本效益的方式满足此要求？",
    "options_cn": {
      "A": "将按需实例用于 Amazon RDS for PostgreSQL 工作负载。为 EC2 实例购买 1 年期计算节省计划，且无预付选项。",
      "B": "为 Amazon RDS for PostgreSQL 工作负载购买 1 年期预留实例，且无预付选项。为 EC2 实例购买 1 年期 EC2 实例节省计划，且无预付选项。",
      "C": "为 Amazon RDS for PostgreSQL 工作负载购买 1 年期预留实例，且部分预付选项。为 EC2 实例购买 1 年期 EC2 实例节省计划，且部分预付选项。",
      "D": "为 Amazon RDS for PostgreSQL 工作负载购买 3 年期预留实例，且全部预付选项。为 EC2 实例购买 3 年期 EC2 实例节省计划，且全部预付选项。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "Savings Plan",
      "Reserved Instance"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n考察 Amazon RDS 和 EC2 成本优化，以及选择最经济的方案。需要理解预留实例（Reserved Instances）和计算节省计划（Compute Savings Plans）的计费方式和适用场景。",
      "why_correct": "选项 D 提供了最具成本效益的方案。3 年期预留实例和 3 年期计算节省计划通常提供最大的折扣。全部预付选项的预付款虽然较高，但在整个期限内能提供最低的每小时费用，最适合长期、稳定的工作负载。",
      "why_wrong": "选项 A 成本效益最低。按需实例的费用最高，无法优化长期运行的工作负载的成本。选项 B 虽然使用预留实例和 EC2 实例节省计划，但 1 年期的期限提供的折扣不如 3 年期。选项 C 部分预付选项的折扣低于全预付选项，因此不如选项 D 更具成本效益。"
    },
    "related_terms": [
      "Amazon RDS for PostgreSQL",
      "Amazon EC2",
      "Reserved Instances",
      "Compute Savings Plans",
      "On-Demand Instances"
    ]
  },
  {
    "id": 941,
    "topic": "1",
    "question_en": "A company is using an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company must ensure that Kubernetes service accounts in the EKS cluster have secure and granular access to specific AWS resources by using IAM roles for service accounts (IRSA). Which combination of solutions will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Create an IAM policy that defines the required permissions Attach the policy directly to the IAM role of the EKS nodes.",
      "B": "Implement network policies within the EKS cluster to prevent Kubernetes service accounts from accessing specific AWS services.",
      "C": "Modify the EKS cluster's IAM role to include permissions for each Kubernetes service account. Ensure a one-to-one mapping between IAM roles and Kubernetes roles.",
      "D": "Define an IAM role that includes the necessary permissions. Annotate the Kubernetes service accounts with the Amazon ResourceName (ARN) of the IAM rol",
      "E": "E. Set up a trust relationship between the IAM roles for the service accounts and an OpenID Connect (OIDC) identity provider."
    },
    "correct_answer": "DE",
    "vote_percentage": "",
    "question_cn": "一家公司正在使用 Amazon Elastic Kubernetes Service (Amazon EKS) 集群。该公司必须确保 EKS 集群中的 Kubernetes 服务账户通过使用 IAM 角色进行服务账户 (IRSA) 对特定的 AWS 资源具有安全且细粒度的访问权限。哪种解决方案组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "创建一个定义所需权限的 IAM 策略。将该策略直接附加到 EKS 节点的 IAM 角色。",
      "B": "在 EKS 集群内实施网络策略，以防止 Kubernetes 服务账户访问特定的 AWS 服务。",
      "C": "修改 EKS 集群的 IAM 角色，以包括每个 Kubernetes 服务账户的权限。确保 IAM 角色和 Kubernetes 角色之间存在一对一的映射。",
      "D": "定义一个包含必要权限的 IAM 角色。使用 IAM 角色的 Amazon 资源名称 (ARN) 注释 Kubernetes 服务账户。",
      "E": "在服务账户的 IAM 角色和 OpenID Connect (OIDC) 身份提供商之间建立信任关系。"
    },
    "tags": [
      "EKS",
      "IAM",
      "IRSA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 DE（社区 —），解析仅供参考。】\n\n此题考查如何使用 IRSA (IAM Roles for Service Accounts) 为 EKS 集群中的 Kubernetes 服务账户提供安全、细粒度的访问权限。正确答案需要考虑安全性、细粒度控制。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 DE。理由简述：选项 D 定义一个包含必要权限的 IAM 角色。使用 IAM 角色的 Amazon 资源名称 (ARN) 注释 Kubernetes 服务账户。IRSA 利用 IAM 角色，并用 ARN 标注服务账户，从而允许 Kubernetes 服务账户安全地访问 AWS 资源。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 创建一个定义所需权限的 IAM 策略，直接附加到 EKS 节点的 IAM 角色，不能提供每个服务账户的细粒度访问控制。选项 B 在 EKS 集群内实施网络策略，以防止 Kubernetes 服务账户访问特定的 AWS 服务，虽然可以限制访问，但不能满足题目对安全访问 AWS 资源的要求。选项 C 修改 EKS 集群的 IAM 角色，以包括每个 Kubernetes 服务账户的权限。 确保 IAM 角色和 Kubernetes 角色之间存在一对一的映射，无法满足 IRSA 的需求。选项 E 在服务账户的 IAM 角色和 OpenID Connect (OIDC) 身份提供商之间建立信任关系。 无法满足 IRSA 的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EKS",
      "IAM",
      "IRSA",
      "Kubernetes",
      "ARN",
      "OIDC"
    ]
  },
  {
    "id": 942,
    "topic": "1",
    "question_en": "A company regularly uploads confidential data to Amazon S3 buckets for analysis. The company's security policies mandate that the objects must be encrypted at rest. The company must automatically rotate the encryption key every year. The company must be able to track key rotation by using AWS CloudTrail. The company also must minimize costs for the encryption key. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use server-side encryption with customer-provided keys (SSE-C)",
      "B": "Use server-side encryption with Amazon S3 managed keys (SSE-S3)",
      "C": "Use server-side encryption with AWS KMS keys (SSE-KMS)",
      "D": "Use server-side encryption with customer managed AWS KMS keys"
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司定期将机密数据上传到 Amazon S3 存储桶以进行分析。该公司的安全策略规定，对象必须静态加密。公司必须每年自动轮换加密密钥。公司还必须能够通过使用 AWS CloudTrail 来跟踪密钥轮换。该公司还必须最大限度地降低加密密钥的成本。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用客户提供的密钥 (SSE-C) 进行服务器端加密",
      "B": "使用 Amazon S3 托管密钥 (SSE-S3) 进行服务器端加密",
      "C": "使用 AWS KMS 密钥 (SSE-KMS) 进行服务器端加密",
      "D": "使用客户管理的 AWS KMS 密钥进行服务器端加密"
    },
    "tags": [
      "S3",
      "KMS",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考查如何实现 S3 对象的静态加密，并满足密钥轮换和成本控制的要求。正确答案需要考虑自动化密钥轮换，以及成本因素。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 使用客户管理的 AWS KMS 密钥进行服务器端加密。使用客户管理的 KMS 密钥能够满足密钥轮换的要求，可以每年自动轮换密钥，并支持跟踪密钥轮换，而且相对于客户提供的密钥，成本更低。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用客户提供的密钥 (SSE-C) 进行服务器端加密，无法实现密钥轮换。选项 B 使用 Amazon S3 托管密钥 (SSE-S3) 进行服务器端加密，无法实现密钥轮换。选项 C 使用 AWS KMS 密钥 (SSE-KMS) 进行服务器端加密，虽然可以轮换密钥，但是成本较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "KMS",
      "SSE-C",
      "SSE-S3",
      "SSE-KMS",
      "Encryption"
    ]
  },
  {
    "id": 943,
    "topic": "1",
    "question_en": "A company has migrated several applications to AWS in the past 3 months. The company wants to know the breakdown of costs for each of these applications. The company wants to receive a regular report that includes this information. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Use AWS Budgets to download data for the past 3 months into a .csv file. Look up the desired information.",
      "B": "Load AWS Cost and Usage Reports into an Amazon RDS DB instance. Run SQL queries to get the desired information.",
      "C": "Tag all the AWS resources with a key for cost and a value of the application's name. Activate cost allocation tags. Use Cost Explorerto get the desired information.",
      "D": "Tag all the AWS resources with a key for cost and a value of the application's name. Use the AWS Billing and Cost Management console todownload bills for the past 3 months. Look up the desired information."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在过去 3 个月内将几个应用程序迁移到了 AWS。该公司想了解每个应用程序的成本细分。该公司希望定期收到一份包含此信息的报告。哪种解决方案可以最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Budgets 将过去 3 个月的数据下载到 .csv 文件中。查找所需信息。",
      "B": "将 AWS 成本和使用情况报告加载到 Amazon RDS 数据库实例中。运行 SQL 查询以获取所需信息。",
      "C": "使用成本键和应用程序名称值标记所有 AWS 资源。激活成本分配标签。使用 Cost Explorer 获取所需信息。",
      "D": "使用成本键和应用程序名称值标记所有 AWS 资源。使用 AWS 账单和成本管理控制台下载过去 3 个月的账单。查找所需信息。"
    },
    "tags": [
      "Cost Explorer",
      "Cost Allocation Tags"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察如何以最具成本效益的方式获取每个应用程序的成本细分报告。正确答案需要考虑报告的频率、准确性和成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 使用成本键和应用程序名称值标记所有 AWS 资源。激活成本分配标签。使用 Cost Explorer 获取所需信息。可以获取每个应用程序的成本细分，满足了报告频率和成本效益的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 使用 AWS Budgets，只能下载过去 3 个月的数据，无法定期获取报告，不满足要求。选项 B 将 AWS 成本和使用情况报告加载到 Amazon RDS 数据库实例中，需要额外的操作，不满足成本效益的要求。选项 D 使用成本键和应用程序名称值标记所有 AWS 资源，使用 AWS 账单和成本管理控制台下载过去 3 个月的账单。查找所需信息。成本高，并且无法自动生成报告，不能满足要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Budgets",
      "RDS",
      "Cost Explorer",
      "Cost Allocation Tags"
    ]
  },
  {
    "id": 944,
    "topic": "1",
    "question_en": "An ecommerce company is preparing to deploy a web application on AWS to ensure continuous service for customers. The architecture includes a web application that the company hosts on Amazon EC2 instances, a relational database in Amazon RDS, and static assets that the company stores in Amazon S3. The company wants to design a robust and resilient architecture for the application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in the same Availability Zone. Use Amazon S3 with versioning enabled to store static assets.",
      "B": "Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones. Deploy a Multi-AZ RDS DB instance. Use Amazon CloudFront to distribute static assets.",
      "C": "Deploy Amazon EC2 instances in a single Availability Zone. Deploy an RDS DB instance in a second Availability Zone for cross-AZ redundancy. Serve static assets directly from the EC2 instances.",
      "D": "Use AWS Lambda functions to serve the web application. Use Amazon Aurora Serverless v2 for the database. Store static assets in Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA)."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家电子商务公司正准备在 AWS 上部署一个 Web 应用程序，以确保为客户提供持续服务。该架构包括该公司托管在 Amazon EC2 实例上的 Web 应用程序，Amazon RDS 中的关系数据库，以及该公司存储在 Amazon S3 中的静态资产。该公司希望为该应用程序设计一个强大且有弹性的架构。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在单个可用区中部署 Amazon EC2 实例。 在同一可用区中部署 RDS 数据库实例。 使用启用版本控制的 Amazon S3 存储静态资产。",
      "B": "将 Amazon EC2 实例部署在跨多个可用区的 Auto Scaling 组中。 部署 Multi-AZ RDS 数据库实例。 使用 Amazon CloudFront 分发静态资产。",
      "C": "在单个可用区中部署 Amazon EC2 实例。 将 RDS 数据库实例部署在第二个可用区中，以实现跨可用区冗余。 直接从 EC2 实例提供静态资产。",
      "D": "使用 AWS Lambda 函数来提供 Web 应用程序。 将 Amazon Aurora Serverless v2 用于数据库。 将静态资产存储在 Amazon Elastic File System (Amazon EFS) One Zone-Infrequent Access (One Zone-IA) 中。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon CloudFront",
      "Amazon S3",
      "Availability Zone"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n本题考查构建高可用、可弹性伸缩的 Web 应用程序架构；涉及 EC2 实例部署、RDS 数据库的部署策略、静态资产的存储与分发。考察对 AWS 高可用设计原则的理解。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：将 Amazon EC2 实例部署在跨多个可用区的 Auto Scaling 组中，保证了计算层的可用性和弹性，当某个可用区发生故障时，Auto Scaling 会自动在其他可用区启动新的实例。部署 Multi-AZ RDS 数据库实例，实现了数据库的高可用和故障转移，当主数据库实例发生故障时，RDS 会自动切换到备用数据库实例。使用 Amazon CloudFront 分发静态资产，利用其全球分布的边缘站点，提高了静态内容的访问速度和可用性，并减轻了源站的负载。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A：在单个可用区中部署 EC2 实例和 RDS 数据库，存在单点故障风险，不满足高可用性需求，当该可用区发生故障时，应用程序将不可用。选项 C：虽然 RDS 部署在第二个可用区，但 EC2 实例部署在单个可用区，仍然存在单点故障。从 EC2 实例直接提供静态资产，无法利用 CDN 的优势，降低了性能，增加了 EC2 实例的负载。选项 D：使用 Lambda 函数提供 Web 应用程序，会引入冷启动延迟，可能影响用户体验。Aurora Serverless v2 虽然可以根据需求自动扩展，但并非本题场景下的最佳选择。使用 EFS One Zone-IA 存储静态资产，不适合高并发访问的场景，且 EFS 的性能不如 S3，成本也较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Auto Scaling",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon CloudFront",
      "Amazon S3",
      "Availability Zone",
      "AWS Lambda",
      "EC2",
      "Amazon Aurora Serverless v2",
      "Amazon Elastic File System (Amazon EFS)",
      "One Zone-Infrequent Access (One Zone-IA)"
    ]
  },
  {
    "id": 945,
    "topic": "1",
    "question_en": "An ecommerce company runs several internal applications in multiple AWS accounts. The company uses AWS Organizations to manage its AWS accounts. A security appliance in the company's networking account must inspect interactions between applications across AWS accounts. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy a Network Load Balancer (NLB) in the networking account to send trafic to the security appliance. Configure the application accounts to send trafic to the NLB by using an interface VPC endpoint in the application accounts.",
      "B": "Deploy an Application Load Balancer (ALB) in the application accounts to send trafic directly to the security appliance.",
      "C": "Deploy a Gateway Load Balancer (GWLB) in the networking account to send trafic to the security appliance. Configure the application accounts to send trafic to the GWLB by using an interface GWLB endpoint in the application accounts.",
      "D": "Deploy an interface VPC endpoint in the application accounts to send trafic directly to the security appliance."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家电子商务公司在多个 AWS 账户中运行几个内部应用程序。该公司使用 AWS Organizations 来管理其 AWS 账户。该公司网络账户中的一个安全设备必须检查跨 AWS 账户的应用程序之间的交互。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在网络账户中部署一个 Network Load Balancer (NLB) 以将流量发送到安全设备。 配置应用程序账户，通过在应用程序账户中使用接口 VPC endpoint 将流量发送到 NLB。",
      "B": "在应用程序账户中部署一个 Application Load Balancer (ALB) 以将流量直接发送到安全设备。",
      "C": "在网络账户中部署一个 Gateway Load Balancer (GWLB) 以将流量发送到安全设备。配置应用程序账户，通过在应用程序账户中使用接口 GWLB endpoint 将流量发送到 GWLB。",
      "D": "在应用程序账户中部署一个接口 VPC endpoint 以将流量直接发送到安全设备。"
    },
    "tags": [
      "Gateway Load Balancer",
      "VPC Endpoint",
      "AWS Organizations"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n考查跨账户网络流量检查的解决方案选择，涉及 Gateway Load Balancer (GWLB) 的部署和使用，以及 VPC endpoint 在跨账户流量路由中的应用。也间接考察了对 Network Load Balancer (NLB) 和 Application Load Balancer (ALB) 的理解。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：Gateway Load Balancer (GWLB) 专为安全设备等第三方虚拟设备设计，能够透明地拦截并处理流量。在网络账户中部署 GWLB，应用程序账户通过创建接口 GWLB endpoint 来将流量路由到 GWLB。GWLB 支持跨账户部署，允许应用程序账户将流量发送到网络账户中的安全设备，从而实现对跨账户应用程序之间交互的检查。通过 GWLB endpoint，应用程序账户无需直接了解安全设备的具体地址，增强了部署的灵活性和安全性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. Network Load Balancer (NLB) 不直接支持将流量发送到跨账户的安全设备，需要复杂的配置和额外的组件来实现。接口 VPC endpoint 的设计初衷并非用于将流量路由到外部安全设备，而是用于访问 AWS 服务，无法满足题目的需求。\nB. Application Load Balancer (ALB) 主要用于 HTTP/HTTPS 流量的负载均衡，不适用于这种需要处理所有流量类型的安全检查场景。直接将 ALB 部署在应用程序账户，虽然可以实现负载均衡，但无法满足跨账户访问安全设备的需求，也没有考虑安全设备本身的处理能力，可扩展性不足。\nD. 接口 VPC endpoint 主要用于安全地连接到 AWS 服务，不能直接将流量路由到安全设备。它不具备负载均衡和流量转发的功能，无法满足需要跨账户检查流量的需求，并直接暴露安全设备，安全性较差。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Network Load Balancer",
      "NLB",
      "Application Load Balancer",
      "ALB",
      "AWS Organizations",
      "Gateway Load Balancer",
      "GWLB",
      "VPC endpoint",
      "Amazon VPC"
    ]
  },
  {
    "id": 946,
    "topic": "1",
    "question_en": "A company runs its production workload on an Amazon Aurora MySQL DB cluster that includes six Aurora Replicas. The company wants near- real-time reporting queries from one of its departments to be automatically distributed across three of the Aurora Replicas. Those three replicas have a different compute and memory specification from the rest of the DB cluster. Which solution meets these requirements?",
    "options_en": {
      "A": "Create and use a custom endpoint for the workload",
      "B": "Create a three-node cluster clone and use the reader endpoint",
      "C": "Use any of the instance endpoints for the selected three nodes",
      "D": "Use the reader endpoint to automatically distribute the read-only workload"
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在其 Amazon Aurora MySQL 数据库集群上运行其生产工作负载，该集群包括六个 Aurora 副本。该公司希望其中一个部门的近实时报告查询自动分布在其中三个 Aurora 副本上。这三个副本与数据库集群的其余部分具有不同的计算和内存规格。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "为工作负载创建并使用自定义端点",
      "B": "创建三节点集群克隆并使用读取器端点",
      "C": "使用所选三个节点的任何实例端点",
      "D": "使用读取器端点自动分配只读工作负载"
    },
    "tags": [
      "Amazon Aurora",
      "Aurora MySQL",
      "Custom Endpoint",
      "Reader Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考查 Amazon Aurora 的自定义端点配置；与不同实例规格、负载均衡、读写分离相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：为满足特定部门的近实时报告查询需求，并且这些查询需要分布在具有不同计算和内存规格的三个 Aurora 副本上，创建一个自定义端点是最合适的解决方案。自定义端点允许您手动指定要将连接路由到的 Aurora 实例，从而精确控制哪个查询路由到哪个副本，满足了对副本规格的要求，也实现了负载均衡。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项：创建三节点集群克隆并使用读取器端点，虽然可以实现对特定副本的读取，但无法满足题目中“这三个副本与数据库集群的其余部分具有不同的计算和内存规格”的要求，集群克隆通常与原集群规格一致。\nC 选项：使用所选三个节点的任何实例端点，会导致查询直接发送到单个实例，无法自动分布查询负载到三个副本上。这也会导致负载不均衡，且不满足近实时报告查询的分布式需求。\nD 选项：使用读取器端点自动分配只读工作负载，读取器端点会将负载在所有可用的只读副本之间自动分发，无法精确控制负载分配到具有特定规格的三个副本上。且也无法满足不同实例规格的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Aurora",
      "Aurora MySQL",
      "Custom Endpoint",
      "Reader Endpoint",
      "Instance",
      "Database Cluster"
    ]
  },
  {
    "id": 947,
    "topic": "1",
    "question_en": "A company runs a Node js function on a server in its on-premises data center. The data center stores data in a PostgreSQL database. The company stores the credentials in a connection string in an environment variable on the server. The company wants to migrate its application to AWS and to replace the Node.js application server with AWS Lambda. The company also wants to migrate to Amazon RDS for PostgreSQL and to ensure that the database credentials are securely managed. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the database credentials as a parameter in AWS Systems Manager Parameter Store Configure Parameter Store to automatically rotate the secrets every 30 days. Update the Lambda function to retrieve the credentials from the parameter.",
      "B": "Store the database credentials as a secret in AWS Secrets Manager. Configure Secrets Manager to automatically rotate the credentials every 30 days. Update the Lambda function to retrieve the credentials from the secret.",
      "C": "Store the database credentials as an encrypted Lambda environment variable. Write a custom Lambda function to rotate the credentials. Schedule the Lambda function to run every 30 days.",
      "D": "Store the database credentials as a key in AWS Key Management Service (AWS KMS). Configure automatic rotation for the key. Update the Lambda function to retneve the credentials from the KMS key."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在其本地数据中心的一台服务器上运行 Node.js 函数。 数据中心将数据存储在 PostgreSQL 数据库中。该公司将凭证存储在服务器上环境变量中的连接字符串中。该公司希望将其应用程序迁移到 AWS，并将 Node.js 应用程序服务器替换为 AWS Lambda。该公司还希望迁移到 Amazon RDS for PostgreSQL，并确保安全地管理数据库凭证。哪种解决方案能够以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "将数据库凭证存储为 AWS Systems Manager Parameter Store 中的一个参数。 配置 Parameter Store，以便每 30 天自动轮换密钥。更新 Lambda 函数以从参数中检索凭证。",
      "B": "将数据库凭证存储为 AWS Secrets Manager 中的一个密钥。 配置 Secrets Manager 以每 30 天自动轮换凭证。更新 Lambda 函数以从密钥中检索凭证。",
      "C": "将数据库凭证存储为加密的 Lambda 环境变量。编写一个自定义 Lambda 函数来轮换凭证。计划 Lambda 函数每 30 天运行一次。",
      "D": "将数据库凭证存储为 AWS Key Management Service (AWS KMS) 中的一个密钥。 配置密钥的自动轮换。更新 Lambda 函数以从 KMS 密钥中检索凭证。"
    },
    "tags": [
      "AWS Lambda",
      "Amazon RDS for PostgreSQL",
      "AWS Secrets Manager",
      "Parameter Store",
      "AWS KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n考察 Lambda 函数连接 RDS for PostgreSQL 数据库时，安全存储和管理数据库凭证的方案选择。涉及 Secrets Manager、Parameter Store、KMS 和 Lambda 环境变量的对比。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：Secrets Manager 提供了安全存储和管理密码、密钥等凭证的解决方案，并支持自动轮换。将数据库凭证存储在 Secrets Manager 中，配置自动轮换，Lambda 函数从 Secrets Manager 中检索凭证，可以满足安全存储、定期轮换和最小运营开销的要求。Secrets Manager 提供了专门的轮换机制，易于配置和管理。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项使用 Parameter Store 存储凭证。虽然 Parameter Store 可以存储敏感信息，但其主要设计目标不是密钥管理和自动轮换。虽然可以使用，但其轮换功能不如 Secrets Manager 完善，且缺少针对数据库凭证的特定支持，所以运维复杂度和安全性稍逊。\nC 选项使用加密的 Lambda 环境变量，并设计自定义的轮换函数。这增加了运维复杂性，需要自行实现轮换逻辑和调度，增加了出错的风险，且没有利用 AWS 提供的现成服务。使用 Lambda 环境变量存储凭证的安全性不如 Secrets Manager。\nD 选项使用 KMS 存储密钥。KMS 主要用于加密和管理密钥，而不是专门用于存储数据库凭证。虽然 KMS 可以用于加密凭证，但直接存储凭证并轮换不如 Secrets Manager 方便，需要额外编写逻辑从 KMS 检索密钥，然后配置数据库连接，增加了复杂性，且 KMS 的主要使用场景是加密数据，而不是直接存储数据库凭证。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Lambda",
      "Amazon RDS for PostgreSQL",
      "AWS Secrets Manager",
      "Parameter Store",
      "AWS KMS",
      "PostgreSQL",
      "EC2",
      "EBS",
      "Node.js"
    ]
  },
  {
    "id": 948,
    "topic": "1",
    "question_en": "A company wants to replicate existing and ongoing data changes from an on-premises Oracle database to Amazon RDS for Oracle. The amount of data to replicate varies throughout each day. The company wants to use AWS Database Migration Service (AWS DMS) for data replication. The solution must allocate only the capacity that the replication instance requires. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the AWS DMS replication instance with a Multi-AZ deployment to provision instances across multiple Availability Zones.",
      "B": "Create an AWS DMS Serverless replication task to analyze and replicate the data while provisioning the required capacity.",
      "C": "Use Amazon EC2 Auto Scaling to scale the size of the AWS DMS replication instance up or down based on the amount of data toreplicate.",
      "D": "Provision AWS DMS replication capacity by using Amazon Elastic Container Service (Amazon ECS) with an AWS Fargate launch type to analyze and replicate the data while provisioning the required capacity."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将现有数据和正在进行的更改从本地 Oracle 数据库复制到 Amazon RDS for Oracle。 每天需要复制的数据量有所不同。 公司希望使用 AWS Database Migration Service (AWS DMS) 进行数据复制。 该解决方案必须仅分配复制实例所需的容量。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将 AWS DMS 复制实例配置为 Multi-AZ 部署，以便跨多个可用区预置实例。",
      "B": "创建 AWS DMS Serverless 复制任务以分析和复制数据，同时预置所需的容量。",
      "C": "使用 Amazon EC2 Auto Scaling 根据要复制的数据量来扩展或缩减 AWS DMS 复制实例的大小。",
      "D": "通过使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 启动类型来预置 AWS DMS 复制容量，以分析和复制数据，同时预置所需的容量。"
    },
    "tags": [
      "AWS DMS",
      "Multi-AZ",
      "Amazon RDS for Oracle",
      "AWS DMS Serverless",
      "Amazon EC2 Auto Scaling",
      "Amazon ECS",
      "AWS Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n该题考查 AWS Database Migration Service (AWS DMS) 复制实例的部署和容量管理。涉及到 Multi-AZ 部署、Serverless 复制任务、Auto Scaling 以及与 Amazon ECS 和 Fargate 的整合，考查对不同部署方式和容量配置的理解。",
      "why_correct": "将 AWS DMS 复制实例配置为 Multi-AZ 部署，虽然没有直接解决按需分配容量的问题，但它提高了 DMS 复制实例的可用性和可靠性，确保了在可用区故障时的数据复制的连续性。虽然它并不能动态调整容量，但它满足了企业对数据复制解决方案的可用性需求，是构建高可用数据复制架构的良好实践。",
      "why_wrong": "B 选项：创建 AWS DMS Serverless 复制任务，虽然 AWS DMS Serverless 提供了按需扩展容量的能力，但它无法与 Amazon RDS for Oracle 直接集成，并且在 AWS DMS 中，Serverless 特性目前并非是针对数据复制场景设计的，不适用本题场景。C 选项：使用 Amazon EC2 Auto Scaling 根据要复制的数据量来扩展或缩减 AWS DMS 复制实例的大小。虽然 EC2 Auto Scaling 可以动态调整 EC2 实例的容量，但 AWS DMS 复制实例本身并不直接支持使用 Auto Scaling。D 选项：通过使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 启动类型来预置 AWS DMS 复制容量。虽然 ECS 和 Fargate 提供了容器化应用的弹性部署能力，但是 AWS DMS 本身不与 ECS 和 Fargate 集成，这种方式无法完成 DMS 复制实例的预置和管理，而且复杂度过高，不适用本题场景。"
    },
    "related_terms": [
      "AWS DMS",
      "Amazon RDS for Oracle",
      "Multi-AZ",
      "Amazon ECS",
      "AWS Fargate",
      "EC2",
      "AWS DMS Serverless",
      "Amazon EC2 Auto Scaling"
    ]
  },
  {
    "id": 949,
    "topic": "1",
    "question_en": "A company has a multi-tier web application. The application's internal service components are deployed on Amazon EC2 instances. The internal service components need to access third-party software as a service (SaaS) APIs that are hosted on AWS. The company needs to provide secure and private connectivity from the application's internal services to the third-party SaaS application. The company needs to ensure that there is minimal public internet exposure. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement an AWS Site-to-Site VPN to establish a secure connection with the third-party SaaS provider.",
      "B": "Deploy AWS Transit Gateway to manage and route trafic between the application's VPC and the third-party SaaS provider.",
      "C": "Configure AWS PrivateLink to allow only outbound trafic from the VPC without enabling the third-party SaaS provider to establish.",
      "D": "Use AWS PrivateLink to create a private connection between the application's VPC and the third-party SaaS provider."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一家公司拥有一个多层 Web 应用程序。 应用程序的内部服务组件部署在 Amazon EC2 实例上。 内部服务组件需要访问托管在 AWS 上的第三方软件即服务 (SaaS) API。该公司需要为应用程序的内部服务到第三方 SaaS 应用程序提供安全且私密的连接。该公司需要确保将公共 Internet 暴露降到最低。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "实施 AWS Site-to-Site VPN 以建立与第三方 SaaS 提供商的安全连接。",
      "B": "部署 AWS Transit Gateway 来管理和路由应用程序的 VPC 和第三方 SaaS 提供商之间的流量。",
      "C": "配置 AWS PrivateLink 以仅允许从 VPC 传出的流量，而无需第三方 SaaS 提供商建立连接。",
      "D": "使用 AWS PrivateLink 在应用程序的 VPC 和第三方 SaaS 提供商之间创建私有连接。"
    },
    "tags": [
      "AWS PrivateLink",
      "VPC",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查 VPC 内应用程序与第三方 SaaS API 的安全连接方案选择，主要考察对 AWS PrivateLink 的理解和应用，并涉及对 VPN 和 Transit Gateway 的对比。",
      "why_correct": "AWS PrivateLink 允许在 VPC 和其他 AWS 服务或第三方 SaaS 之间创建私有连接。它通过使用私有 IP 地址在 Amazon 网络内实现安全、私密的通信。这种方式避免了将流量暴露在公共 Internet 上，符合题目中“安全且私密的连接”以及“将公共 Internet 暴露降到最低”的要求。 PrivateLink 不需要 Internet 网关、NAT 设备、弹性 IP 地址或 VPN 连接，从而简化了网络架构。",
      "why_wrong": "选项 A 错误，因为 Site-to-Site VPN 建立的是 VPC 到 SaaS 提供商网络之间的 VPN 连接，这需要 SaaS 提供商也拥有 VPC 或者支持 VPN 连接。虽然 VPN 提供了加密的流量传输，但它增加了配置和维护的复杂性，并且通常涉及互联网的暴露。选项 B 错误，Transit Gateway 主要用于连接多个 VPC 和本地网络，虽然可以路由流量，但它本身并不直接提供与 SaaS API 的安全私密连接。Transit Gateway 可以结合其他服务使用，但单独使用不能满足题目的要求。选项 C 错误，配置 VPC 仅允许传出流量并不能建立与 SaaS 提供商的连接。PrivateLink 是建立连接的机制，而 VPC 的流量控制是网络配置。仅配置出站流量不提供私有连接，且无法直接满足题目中的要求。"
    },
    "related_terms": [
      "EC2",
      "VPC",
      "AWS PrivateLink",
      "Transit Gateway",
      "SaaS",
      "AWS Site-to-Site VPN"
    ]
  },
  {
    "id": 950,
    "topic": "1",
    "question_en": "A solutions architect needs to connect a company's corporate network to its VPC to allow on-premises access to its AWS resources. The solution must provide encryption of all trafic between the corporate network and the VPC at the network layer and the session layer. The solution also must provide security controls to prevent unrestricted access between AWS and the on-premises systems. Which solution meets these requirements?",
    "options_en": {
      "A": "Configure AWS Direct Connect to connect to the VPC. Configure the VPC route tables to allow and deny trafic between AWS and on premises as required.",
      "B": "Create an IAM policy to allow access to the AWS Management Console only from a defined set of corporate IP addresses. Restrict user access based on job responsibility by using an IAM policy and roles.",
      "C": "Configure AWS Site-to-Site VPN to connect to the VPConfigure route table entries to direct trafic from on premises to the VPConfigure instance security groups and network ACLs to allow only required trafic from on premises.",
      "D": "Configure AWS Transit Gateway to connect to the VPC. Configure route table entries to direct trafic from on premises to the VPC. Configure instance security groups and network ACLs to allow only required trafic from on premises."
    },
    "correct_answer": "D",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师需要将公司的企业网络连接到其 VPC，以允许本地访问其 AWS 资源。该解决方案必须在网络层和会话层对企业网络和 VPC 之间的所有流量进行加密。该解决方案还必须提供安全控制，以防止 AWS 和本地系统之间不受限制的访问。哪个解决方案满足这些要求？",
    "options_cn": {
      "A": "配置 AWS Direct Connect 以连接到 VPC。配置 VPC 路由表以根据需要允许和拒绝 AWS 和本地之间的流量。",
      "B": "创建一个 IAM 策略，仅允许从一组已定义的 corporate IP 地址访问 AWS 管理控制台。使用 IAM 策略和角色根据工作职责限制用户访问。",
      "C": "配置 AWS Site-to-Site VPN 以连接到 VPC。配置路由表条目以将流量从本地定向到 VPC。配置实例安全组和网络 ACL，仅允许来自本地的所需流量。",
      "D": "配置 AWS Transit Gateway 以连接到 VPC。配置路由表条目以将流量从本地定向到 VPC。配置实例安全组和网络 ACL，仅允许来自本地的所需流量。"
    },
    "tags": [
      "AWS Transit Gateway",
      "VPC",
      "Site-to-Site VPN",
      "AWS Direct Connect",
      "IAM",
      "IAM Policy",
      "Security Group",
      "Network ACL",
      "Routing Table",
      "Encryption",
      "Corporate Network"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 100%），解析仅供参考。】\n\n本题考查 VPC 与本地网络互联的多种方案，以及网络层和应用层的安全配置。涉及到 AWS Transit Gateway、Site-to-Site VPN、AWS Direct Connect 等方案的选型与对比，以及安全组（Security Group）、网络访问控制列表（Network ACL）和 IAM 策略等安全措施的应用。",
      "why_correct": "配置 AWS Transit Gateway 以连接到 VPC 是满足要求的最佳方案。Transit Gateway 充当一个中央枢纽，用于连接 VPC 和本地网络，简化了连接管理。通过配置路由表条目，可以控制本地流量到 VPC 的路由，而配置实例安全组和网络 ACL 可以限制来自本地网络的访问，实现所需的安全性。Transit Gateway 本身支持对所有数据包进行加密，保证网络层流量的安全性。相比 Site-to-Site VPN， Transit Gateway 拥有更高的吞吐量和更强的可扩展性，更适用于大型企业网络环境。",
      "why_wrong": "选项 A 错误，因为 AWS Direct Connect 虽然可以提供高性能的网络连接，但它本身不提供流量加密，需要额外的加密方案。配置 VPC 路由表仅用于控制路由，不提供会话层加密。选项 B 主要关注 IAM 策略，用于管理用户访问 AWS 控制台的权限，不能满足网络层和会话层加密的要求，也无法用于本地网络到 VPC 之间的流量加密。选项 C 错误，虽然配置 Site-to-Site VPN 提供了加密连接，但 Transit Gateway 提供了更好的可扩展性和更简化的网络管理，更适合多 VPC 和本地网络互连的场景，且相比之下，AWS Transit Gateway 管理更便捷，也更易于维护。"
    },
    "related_terms": [
      "AWS Transit Gateway",
      "VPC",
      "Site-to-Site VPN",
      "AWS Direct Connect",
      "IAM",
      "Network ACL",
      "EC2",
      "IAM Policy",
      "Security Group",
      "Routing Table",
      "Amazon VPC",
      "Corporate Network"
    ]
  },
  {
    "id": 951,
    "topic": "1",
    "question_en": "A company has a custom application with embedded credentials that retrieves information from a database in an Amazon RDS for MySQL DB cluster. The company needs to make the application more secure with minimal programming effort. The company has created credentials on the RDS for MySQL database for the application user. Which solution will meet these requirements?",
    "options_en": {
      "A": "Store the credentials in AWS Key Management Service (AWS KMS). Create keys in AWS KMS. Configure the application to load the database credentials from AWS KMS. Enable automatic key rotation",
      "B": "Store the credentials in encrypted local storage. Configure the application to load the database credentials from the local storage. Set up a credentials rotation schedule by creating a cron job.",
      "C": "Store the credentials in AWS Secrets Manager. Configure the application to load the database credentials from Secrets Manager. Set up a credentials rotation schedule by creating an AWS Lambda function for Secrets Manager.",
      "D": "Store the credentials in AWS Systems Manager Parameter Store. Configure the application to load the database credentials from Parameter Store. Set up a credentials rotation schedule in the RDS for MySQL database by using Parameter Store."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司有一个自定义应用程序，其中嵌入了凭证，用于从 Amazon RDS for MySQL DB 集群中的数据库检索信息。该公司需要通过最少的编程工作来提高应用程序的安全性。该公司已为应用程序用户在 RDS for MySQL 数据库上创建了凭证。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将凭证存储在 AWS Key Management Service (AWS KMS) 中。在 AWS KMS 中创建密钥。将应用程序配置为从 AWS KMS 加载数据库凭证。启用自动密钥轮换",
      "B": "将凭证存储在加密的本地存储中。将应用程序配置为从本地存储加载数据库凭证。通过创建 cron 作业来设置凭证轮换计划。",
      "C": "将凭证存储在 AWS Secrets Manager 中。将应用程序配置为从 Secrets Manager 加载数据库凭证。通过为 Secrets Manager 创建一个 AWS Lambda 函数来设置凭证轮换计划。",
      "D": "将凭证存储在 AWS Systems Manager Parameter Store 中。将应用程序配置为从 Parameter Store 加载数据库凭证。通过使用 Parameter Store 在 RDS for MySQL 数据库中设置凭证轮换计划。"
    },
    "tags": [
      "AWS Secrets Manager",
      "Amazon RDS for MySQL",
      "AWS Lambda",
      "AWS KMS",
      "AWS Systems Manager Parameter Store",
      "Security"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n考查 Secrets Manager 的使用以及如何结合 Lambda 实现凭证轮换，从而增强应用程序的安全性。涉及安全凭证存储和管理，并与 KMS、Parameter Store 作对比。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：AWS Secrets Manager 专门用于安全地存储和管理敏感信息，如数据库凭证。通过将应用程序配置为从 Secrets Manager 加载凭证，可以避免将敏感信息硬编码到应用程序中。使用 Lambda 函数可以实现凭证的自动轮换，确保凭证的安全性。这种方案减少了编程工作量，提供了更好的安全性，并符合题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，虽然 AWS KMS 提供了加密功能，但它主要用于加密密钥的管理，而非直接存储和管理数据库凭证。将凭证存储在 KMS 中并不能很好地实现凭证的轮换，并且应用程序需要额外的逻辑来解密凭证，增加了复杂性。选项 B 错误，将凭证存储在本地加密存储中，虽然看似安全，但难以管理，并且轮换凭证需要手动或通过 cron 作业，这增加了维护的复杂性，并且容易出错，同时增加了安全风险。选项 D 错误，Parameter Store 主要用于存储配置数据和参数，而不是专门为秘密信息设计的。虽然 Parameter Store 可以存储敏感信息，但它提供的功能不如 Secrets Manager 完善，例如缺乏内置的凭证轮换功能，且与 RDS for MySQL 的凭证轮换集成较弱。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Secrets Manager",
      "AWS Lambda",
      "AWS KMS",
      "AWS Systems Manager Parameter Store",
      "RDS",
      "MySQL",
      "Cron",
      "Amazon RDS for MySQL",
      "Database",
      "Encryption"
    ]
  },
  {
    "id": 952,
    "topic": "1",
    "question_en": "A company wants to move its application to a serverless solution. The serverless solution needs to analyze existing data and new data by using SQL. The company stores the data in an Amazon S3 bucket. The data must be encrypted at rest and replicated to a different AWS Region. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a new S3 bucket that uses server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Configure Cross-Region Replication (CRR). Load the data into the new S3 bucket. Use Amazon Athena to query the data.",
      "B": "Create a new S3 bucket that uses server-side encryption with Amazon S3 managed keys (SSE-S3). Configure Cross-Region Replication (CRR). Load the data into the new S3 bucket. Use Amazon RDS to query the data.",
      "C": "Configure Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption with Amazon S3 managed keys (SSE-S3). Use Amazon Athena to query the data.",
      "D": "Configure S3 Cross-Region Replication (CRR) on the existing S3 bucket. Use server-side encryption with AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司希望将其应用程序迁移到无服务器解决方案。该无服务器解决方案需要使用 SQL 分析现有数据和新数据。该公司将数据存储在 Amazon S3 存储桶中。数据必须静态加密并复制到不同的 AWS 区域。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个新的 S3 存储桶，该存储桶使用带有 AWS KMS 多区域密钥（SSE-KMS）的服务器端加密。配置 跨区域复制 (CRR)。将数据加载到新的 S3 存储桶中。使用 Amazon Athena 查询数据。",
      "B": "创建一个新的 S3 存储桶，该存储桶使用带有 Amazon S3 托管密钥（SSE-S3）的服务器端加密。配置 跨区域复制 (CRR)。将数据加载到新的 S3 存储桶中。使用 Amazon RDS 查询数据。",
      "C": "在现有的 S3 存储桶上配置 跨区域复制 (CRR)。使用带有 Amazon S3 托管密钥（SSE-S3）的服务器端加密。使用 Amazon Athena 查询数据。",
      "D": "在现有的 S3 存储桶上配置 S3 跨区域复制 (CRR)。使用带有 AWS KMS 多区域密钥（SSE-KMS）的服务器端加密。使用 Amazon RDS 查询数据。"
    },
    "tags": [
      "Amazon S3",
      "S3",
      "S3 Cross-Region Replication",
      "Amazon Athena",
      "AWS KMS",
      "SSE-KMS",
      "SSE-S3",
      "Amazon RDS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n本题考查无服务器解决方案中数据的存储、加密、复制以及分析，主要涉及 Amazon S3 存储桶的配置、加密方式的选择、跨区域复制的实施，以及数据分析工具的选取；与 SSE-KMS 和 SSE-S3 的对比、以及 Athena 和 RDS 的选型相关。",
      "why_correct": "选项 A 满足所有要求，开销最低。它创建了一个新的 S3 存储桶，并使用 SSE-KMS 加密，确保了静态数据的加密。配置跨区域复制 (CRR) 将数据复制到不同的 AWS 区域，满足了复制需求。使用 Amazon Athena 进行查询，Athena 是一种无服务器查询服务，能够以较低的运营开销对 S3 中的数据进行 SQL 分析。",
      "why_wrong": "选项 B 错误，因为它使用 Amazon RDS 查询数据。RDS 是一种关系型数据库服务，并不直接适用于查询 S3 中的数据，并且相比 Athena，RDS 的运维成本更高。选项 C 错误，因为它虽然使用了 Athena，但没有创建新的 S3 存储桶，并且沿用了现有的 S3 存储桶。虽然配置了 CRR，但现有 S3 存储桶的加密方式 SSE-S3 并不利于数据的密钥管理和合规性需求。选项 D 错误，因为它使用了 RDS 进行数据分析，而 RDS 并不适合直接查询 S3 中的数据。此外，和选项 C 一样，在现有存储桶上配置 CRR 存在数据迁移的问题。同时，RDS 的运维成本高于 Athena。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "Amazon Athena",
      "AWS KMS",
      "SSE-KMS",
      "SSE-S3",
      "Amazon RDS",
      "S3 Cross-Region Replication"
    ]
  },
  {
    "id": 953,
    "topic": "1",
    "question_en": "A company has a web application that has thousands of users. The application uses 8-10 user-uploaded images to generate AI images. Users can download the generated AI images once every 6 hours. The company also has a premium user option that gives users the ability to download the generated AI images anytime. The company uses the user-uploaded images to run AI model training twice a year. The company needs a storage solution to store the images. Which storage solution meets these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Move uploaded images to Amazon S3 Glacier Deep Archive. Move premium user-generated AI images to S3 Standard. Move non- premium user-generated AI images to S3 Standard-Infrequent Access (S3 Standard-IA).",
      "B": "Move uploaded images to Amazon S3 Glacier Deep Archive Move all generated AI images to S3 Glacier Flexible Retrieval.",
      "C": "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move premium user-generated AI images to S3 Standard. Move non-premium user-generated AI images to S3 Standard-Infrequent Access (S3 Standard-IA).",
      "D": "Move uploaded images to Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA). Move all generated AI images to S3 Glacier Flexible Retrieval."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司有一个拥有数千用户的 Web 应用程序。 该应用程序使用 8-10 个用户上传的图像来生成 AI 图像。 用户每 6 小时可以下载一次生成的 AI 图像。该公司还有一个高级用户选项，该选项允许用户随时下载生成的 AI 图像。该公司使用用户上传的图像每年运行两次 AI 模型训练。该公司需要一个存储解决方案来存储这些图像。哪个存储解决方案能最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "将上传的图像移动到 Amazon S3 Glacier Deep Archive。将高级用户生成的 AI 图像移动到 S3 Standard。将非高级用户生成的 AI 图像移动到 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "B": "将上传的图像移动到 Amazon S3 Glacier Deep Archive。将所有生成的 AI 图像移动到 S3 Glacier Flexible Retrieval。",
      "C": "将上传的图像移动到 Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)。将高级用户生成的 AI 图像移动到 S3 Standard。将非高级用户生成的 AI 图像移动到 S3 Standard-Infrequent Access (S3 Standard-IA)。",
      "D": "将上传的图像移动到 Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)。将所有生成的 AI 图像移动到 S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "S3 Standard",
      "S3 Standard-IA",
      "S3 Glacier Flexible Retrieval",
      "S3 One Zone-IA"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n本题考查了存储解决方案的成本效益，需要根据不同数据的访问频率和重要性选择合适的 S3 存储类别。与 S3 存储类别的选择，以及不同存储类的特性（如检索成本、可用性、持久性）相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 提供了最具成本效益的方案。上传的图像可以归档到 S3 Glacier Deep Archive，这是最便宜的存储类别，适用于不经常访问的数据。高级用户生成的 AI 图像存储在 S3 Standard 中，以满足即时下载的需求。非高级用户生成的 AI 图像存储在 S3 Standard-IA 中，以平衡存储成本和下载频率（每 6 小时一次）。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误的原因在于：将所有生成的 AI 图像都存储在 S3 Glacier Flexible Retrieval 中，虽然成本较低，但其检索时间较长，不满足高级用户随时下载的需求。此外，S3 Glacier Flexible Retrieval 的设计目的不是用于频繁访问的数据。选项 C 错误的原因在于：使用 S3 One Zone-IA 存储上传的图像，虽然成本略低于 S3 Standard-IA，但其数据冗余较低，不适合存储原始上传图像这种重要数据。选项 D 错误的原因在于：将所有生成的 AI 图像存储在 S3 Glacier Flexible Retrieval 中，检索时间长，不满足即时下载需求，同时上传的图像也使用了冗余较低的 S3 One Zone-IA，存在数据丢失风险。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "S3 Standard-IA",
      "S3 Glacier Flexible Retrieval",
      "S3 Standard",
      "S3 One Zone-IA"
    ]
  },
  {
    "id": 954,
    "topic": "1",
    "question_en": "A company is developing machine learning (ML) models on AWS. The company is developing the ML models as independent microservices. The microservices fetch approximately 1 GB of model data from Amazon S3 at startup and load the data into memory. Users access the ML models through an asynchronous API. Users can send a request or a batch of requests. The company provides the ML models to hundreds of users. The usage patterns for the models are irregular. Some models are not used for days or weeks. Other models receive batches of thousands of requests at a time. Which solution will meet these requirements?",
    "options_en": {
      "A": "Direct the requests from the API to a Network Load Balancer (NLB). Deploy the ML models as AWS Lambda functions that the NLB will invoke. Use auto scaling to scale the Lambda functions based on the trafic that the NLB receives.",
      "B": "Direct the requests from the API to an Application Load Balancer (ALB). Deploy the ML models as Amazon Elastic Container Service (Amazon ECS) services that the ALB will invoke. Use auto scaling to scale the ECS cluster instances based on the trafic that the ALB receives.",
      "C": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the ML models as AWS Lambda functions that SQS events will invoke. Use auto scaling to increase the number of vCPUs for the Lambda functions based on the size of the SQS queue.",
      "D": "Direct the requests from the API into an Amazon Simple Queue Service (Amazon SQS) queue. Deploy the ML models as Amazon Elastic Container Service (Amazon ECS) services that read from the queue. Use auto scaling for Amazon ECS to scale both the cluster capacity and number of the services based on the size of the SQS queue."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上开发机器学习 (ML) 模型。该公司将 ML 模型开发为独立的微服务。这些微服务在启动时从 Amazon S3 获取大约 1 GB 的模型数据，并将数据加载到内存中。用户通过异步 API 访问 ML 模型。用户可以发送一个请求或一批请求。该公司向数百个用户提供 ML 模型。这些模型的使用模式是不规则的。有些模型几天或几周未使用。其他模型一次接收数千个请求的批处理。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "将来自 API 的请求定向到 Network Load Balancer (NLB)。将 ML 模型部署为 AWS Lambda 函数，NLB 将调用这些函数。根据 NLB 接收的流量使用自动伸缩来扩展 Lambda 函数。",
      "B": "将来自 API 的请求定向到 Application Load Balancer (ALB)。将 ML 模型部署为 Amazon Elastic Container Service (Amazon ECS) 服务，ALB 将调用这些服务。根据 ALB 接收的流量使用自动伸缩来扩展 ECS 集群实例。",
      "C": "将来自 API 的请求定向到 Amazon Simple Queue Service (Amazon SQS) 队列。将 ML 模型部署为 AWS Lambda 函数，SQS 事件将调用这些函数。使用自动伸缩根据 SQS 队列的大小来增加 Lambda 函数的 vCPU 数量。",
      "D": "将来自 API 的请求定向到 Amazon Simple Queue Service (Amazon SQS) 队列。将 ML 模型部署为 Amazon Elastic Container Service (Amazon ECS) 服务，这些服务从队列中读取数据。对 Amazon ECS 使用自动伸缩，以根据 SQS 队列的大小来扩展集群容量和服务的数量。"
    },
    "tags": [
      "Amazon SQS",
      "Amazon ECS",
      "AWS Lambda",
      "Network Load Balancer",
      "Application Load Balancer",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n考察异步处理、弹性伸缩、以及 SQS 与 ECS/Lambda 的集成。需要考虑如何处理突发流量、模型数据加载、以及模型生命周期管理，与负载均衡、队列、计算服务的选型相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 满足了所有需求。使用 Amazon SQS 队列作为异步消息队列，接收来自 API 的请求，从而实现解耦，并允许异步处理批量请求。将 ML 模型部署为 ECS 服务，可以控制模型启动后的内存占用（1GB 模型数据加载），并利用 ECS 服务的长期运行能力来处理请求。ECS 集群根据 SQS 队列的大小进行自动伸缩，确保了弹性应对不同规模的请求。ECS 还支持基于容器的部署，方便模型更新和版本控制。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 不适用，因为 Lambda 函数有运行时间限制，且每次请求都要重新加载 1GB 模型数据到内存，效率低下。NLB 无法基于请求内容进行路由，也不适合异步处理。选项 B 不适用，ALB 适合处理 HTTP/HTTPS 流量，ECS 服务可以解决模型加载问题，但 ALB 无法直接支持异步处理，且每次请求都需要重新加载模型数据。选项 C 不适用，Lambda 函数虽然可以处理异步消息，但无法有效管理长期运行的模型数据加载和内存占用；另外，Lambda 函数的vCPU 数量调整方式不适用于根据 SQS 队列大小进行伸缩，且容易超出Lambda 的资源限制。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon SQS",
      "Amazon ECS",
      "AWS Lambda",
      "Network Load Balancer",
      "NLB",
      "Application Load Balancer",
      "ALB",
      "Auto Scaling",
      "API",
      "ML",
      "Amazon S3",
      "EC2",
      "EBS"
    ]
  },
  {
    "id": 955,
    "topic": "1",
    "question_en": "A company runs a web application on Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). The application stores data in an Amazon Aurora MySQL DB cluster. The company needs to create a disaster recovery (DR) solution. The acceptable recovery time for the DR solution is up to 30 minutes. The DR solution does not need to support customer usage when the primary infrastructure is healthy. Which solution will meet these requirements?",
    "options_en": {
      "A": "Deploy the DR infrastructure in a second AWS Region with an ALB and an Auto Scaling group. Set the desired capacity and maximum capacity of the Auto Scaling group to a minimum value. Convert the Aurora MySQL DB cluster to an Aurora global database. Configure Amazon Route 53 for an active-passive failover with ALB endpoints.",
      "B": "Deploy the DR infrastructure in a second AWS Region with an ALUpdate the Auto Scaling group to include EC2 instances from the second Region. Use Amazon Route 53 to configure active-active failover. Convert the Aurora MySQL DB cluster to an Aurora global database.",
      "C": "Back up the Aurora MySQL DB cluster data by using AWS Backup. Deploy the DR infrastructure in a second AWS Region with an ALB. Update the Auto Scaling group to include EC2 instances from the second Region. Use Amazon Route 53 to configure active-active failover. Create an Aurora MySQL DB cluster in the second Region Restore the data from the backup.",
      "D": "Back up the infrastructure configuration by using AWS Backup. Use the backup to create the required infrastructure in a second AWS Region. Set the Auto Scaling group desired capacity to zero. Use Amazon Route 53 to configure active-passive failover. Convert the Aurora MySQL DB cluster to an Aurora global database."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在其Application Load Balancer (ALB) 之后，在Auto Scaling组中的Amazon EC2实例上运行Web应用程序。该应用程序将数据存储在Amazon Aurora MySQL数据库集群中。该公司需要创建一个灾难恢复（DR）解决方案。DR解决方案的可接受恢复时间最长为30分钟。当主基础设施运行正常时，DR解决方案不需要支持客户使用。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在第二个AWS区域中部署DR基础设施，其中包含一个ALB和一个Auto Scaling组。将Auto Scaling组的所需容量和最大容量设置为最小值。将Aurora MySQL数据库集群转换为Aurora全局数据库。配置Amazon Route 53以进行主动-被动故障转移，使用ALB端点。",
      "B": "在第二个AWS区域中部署DR基础设施，其中包含一个ALB。更新Auto Scaling组以包含来自第二个区域的EC2实例。使用Amazon Route 53配置主动-主动故障转移。将Aurora MySQL数据库集群转换为Aurora全局数据库。",
      "C": "使用AWS Backup备份Aurora MySQL数据库集群数据。在第二个AWS区域中部署DR基础设施，其中包含一个ALB。更新Auto Scaling组以包含来自第二个区域的EC2实例。使用Amazon Route 53配置主动-主动故障转移。在第二个区域中创建Aurora MySQL数据库集群。从备份中恢复数据。",
      "D": "使用AWS Backup备份基础设施配置。使用备份在第二个AWS区域中创建所需的基础设施。将Auto Scaling组的所需容量设置为零。使用Amazon Route 53配置主动-被动故障转移。将Aurora MySQL数据库集群转换为Aurora全局数据库。"
    },
    "tags": [
      "Amazon EC2",
      "Auto Scaling",
      "Application Load Balancer",
      "Amazon Aurora",
      "Aurora Global Database",
      "Amazon Route 53",
      "Disaster Recovery",
      "DR",
      "Availability",
      "Failover"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考查灾难恢复（DR）解决方案设计，以及与恢复时间目标（RTO）、主动-被动/主动-主动架构、Aurora Global Database、Route 53 健康检查等相关的知识。重点关注RTO 30分钟的约束，以及DR站点在正常情况下不被使用的要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：在第二个AWS区域部署DR基础设施，包括ALB和Auto Scaling组，并将Auto Scaling组的所需容量和最大容量设置为最小值，确保了DR站点的最小资源消耗和快速启动能力。 将Aurora MySQL数据库集群转换为Aurora Global Database，使得数据可以在主区域和DR区域之间进行异步复制，确保了数据一致性。配置Route 53进行主动-被动故障转移，使用ALB端点，确保只有在主区域发生故障时才会将流量切换到DR区域，满足了DR站点在正常情况下不被使用的要求，并实现了快速的故障转移。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项B：虽然使用了Aurora Global Database实现了数据复制，但主动-主动的故障转移模式，会导致DR站点在正常情况下也接收流量，不满足需求。 选项C：使用AWS Backup备份Aurora MySQL数据库集群数据，恢复数据过程时间较长，无法满足30分钟的RTO要求；且主动-主动故障转移模式也不符合需求。 选项D：使用AWS Backup备份基础设施配置，恢复时间长，无法满足RTO 30分钟的要求。 将Auto Scaling组的所需容量设置为零，则在故障时需要额外的时间启动实例，同样违反了 RTO 的要求。 同时，备份基础设施配置的方式，相较于Aurora Global Database，在数据同步和可用性方面不如 A 选项。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Application Load Balancer",
      "ALB",
      "Amazon Aurora",
      "Aurora MySQL",
      "Aurora Global Database",
      "Amazon Route 53",
      "DR",
      "RTO",
      "AWS Backup",
      "Auto Scaling Group"
    ]
  },
  {
    "id": 956,
    "topic": "1",
    "question_en": "A company is migrating its data processing application to the AWS Cloud. The application processes several short-lived batch jobs that cannot be disrupted. Data is generated after each batch job is completed. The data is accessed for 30 days and retained for 2 years. The company wants to keep the cost of running the application in the AWS Cloud as low as possible. Which solution will meet these requirements?",
    "options_en": {
      "A": "Migrate the data processing application to Amazon EC2 Spot Instances. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Instant. Retrieval after 30 days. Set an expiration to delete the data after 2 years.",
      "B": "Migrate the data processing application to Amazon EC2 On-Demand Instances. Store the data in Amazon S3 Glacier Instant Retrieval. Move the data to S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years.",
      "C": "Deploy Amazon EC2 Spot Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Flexible Retrieval after 30 days. Set an expiration to delete the data after 2 years.",
      "D": "Deploy Amazon EC2 On-Demand Instances to run the batch jobs. Store the data in Amazon S3 Standard. Move the data to Amazon S3 Glacier Deep Archive after 30 days. Set an expiration to delete the data after 2 years."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其数据处理应用程序迁移到 AWS 云。该应用程序处理几个短期的批处理作业，这些作业不能中断。在每个批处理作业完成后，都会生成数据。数据可访问 30 天，并保留 2 年。该公司希望尽可能降低在 AWS 云中运行应用程序的成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "将数据处理应用程序迁移到 Amazon EC2 Spot Instances。将数据存储在 Amazon S3 Standard 中。将数据移动到 Amazon S3 Glacier Instant。30 天后检索。设置到期时间，在 2 年后删除数据。",
      "B": "将数据处理应用程序迁移到 Amazon EC2 On-Demand Instances。将数据存储在 Amazon S3 Glacier Instant Retrieval 中。30 天后将数据移动到 S3 Glacier Deep Archive。设置到期时间，在 2 年后删除数据。",
      "C": "部署 Amazon EC2 Spot Instances 以运行批处理作业。将数据存储在 Amazon S3 Standard 中。30 天后将数据移动到 Amazon S3 Glacier Flexible Retrieval。设置到期时间，在 2 年后删除数据。",
      "D": "部署 Amazon EC2 On-Demand Instances 以运行批处理作业。将数据存储在 Amazon S3 Standard 中。30 天后将数据移动到 Amazon S3 Glacier Deep Archive。设置到期时间，在 2 年后删除数据。"
    },
    "tags": [
      "Amazon EC2",
      "EC2 Spot Instances",
      "Amazon S3",
      "S3 Standard",
      "Amazon S3 Glacier Flexible Retrieval",
      "S3 Glacier Deep Archive",
      "Amazon EC2 On-Demand Instances",
      "S3 Glacier Instant Retrieval",
      "Amazon EC2 Spot Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n本题考察了如何在 AWS 上以经济高效的方式存储和管理数据，并考察了 EC2 实例类型的选择以及 S3 存储类的生命周期管理。重点关注 EC2 实例的成本效益以及 S3 存储类的生命周期管理策略的选择。也涉及到 S3 存储类的特点和选择，例如Standard、Glacier Instant Retrieval、Glacier Flexible Retrieval 和 Glacier Deep Archive 的特点，以及 EC2 On-Demand Instances 和 EC2 Spot Instances 的对比。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：该方案是最具成本效益的。使用 EC2 Spot Instances 可以显著降低计算成本，因为 Spot 实例的价格通常远低于 On-Demand Instances。将数据最初存储在 S3 Standard 中，满足了 30 天内需要快速访问数据的需求。在 30 天后，将数据转换为 Glacier Flexible Retrieval 以降低存储成本，同时满足了 2 年的保留期要求。通过 S3 生命周期规则设置数据的过期删除，确保了数据在 2 年后被自动删除，符合题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误在于，虽然使用了 Spot Instances 来降低计算成本，但将数据先存储在 Standard，然后迁移到 Glacier Instant Retrieval，然后又在 30 天后检索，这会带来额外的检索成本和时间消耗，不符合“尽可能降低成本”的要求。 选项 B 错误在于，使用 Glacier Instant Retrieval 存储 30 天后，再迁移到 Deep Archive，成本结构不合理。Glacier Instant Retrieval 本身旨在提供快速访问，但在 30 天后迁移到 Deep Archive 并不能最大化成本效益。选项 D 错误在于，使用 On-Demand Instances 来运行批处理作业，计算成本会高于使用 Spot Instances。将数据最初存储在 S3 Standard 中是正确的，但在 30 天后将其移动到 Deep Archive 并不是一个好的选择，因为 Deep Archive 的检索时间相对较长，而题目的要求是数据可访问 30 天，意味着需要频繁访问数据，Deep Archive 的设计并不适合这种场景。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Amazon S3",
      "S3 Glacier Deep Archive",
      "EC2 Spot Instances",
      "S3 Standard",
      "Amazon S3 Glacier Flexible Retrieval",
      "Amazon EC2 On-Demand Instances",
      "S3 Glacier Instant Retrieval"
    ]
  },
  {
    "id": 957,
    "topic": "1",
    "question_en": "A company needs to design a hybrid network architecture. The company's workloads are currently stored in the AWS Cloud and in on-premises data centers. The workloads require single-digit latencies to communicate. The company uses an AWS Transit Gateway transit gateway to connect multiple VPCs. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Establish an AWS Site-to-Site VPN connection to each VPC.",
      "B": "Associate an AWS Direct Connect gateway with the transit gateway that is attached to the VPCs.",
      "C": "Establish an AWS Site-to-Site VPN connection to an AWS Direct Connect gateway.",
      "D": "Establish an AWS Direct Connect connection. Create a transit virtual interface (VIF) to a Direct Connect gateway",
      "E": "Associate AWS Site-to-Site VPN connections with the transit gateway that is attached to the VPCs."
    },
    "correct_answer": "BD",
    "vote_percentage": "",
    "question_cn": "一家公司需要设计混合网络架构。该公司的负载目前存储在 AWS 云和本地数据中心中。这些负载需要个位数的延迟才能进行通信。该公司使用 AWS Transit Gateway 传输网关连接多个 VPC。哪两种步骤的组合将以最具成本效益的方式满足这些要求？（选择两项。）",
    "options_cn": {
      "A": "建立到每个 VPC 的 AWS Site-to-Site VPN 连接。",
      "B": "将 AWS Direct Connect 网关与附加到 VPC 的传输网关关联。",
      "C": "建立到 AWS Direct Connect 网关的 AWS Site-to-Site VPN 连接。",
      "D": "建立 AWS Direct Connect 连接。创建一个到 Direct Connect 网关的传输虚拟接口 (VIF)。",
      "E": "将 AWS Site-to-Site VPN 连接与附加到 VPC 的传输网关关联。"
    },
    "tags": [
      "AWS Direct Connect",
      "AWS Transit Gateway",
      "VPC",
      "Site-to-Site VPN",
      "Direct Connect Gateway",
      "VIF"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BD（社区 —），解析仅供参考。】\n\n本题考查混合云场景下的网络连接方案，以及对低延迟、成本效益的考量，与 AWS Direct Connect、Transit Gateway 以及 VPN 的选型密切相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BD。理由简述：将 AWS Direct Connect 网关与附加到 VPC 的传输网关关联，可以实现本地数据中心与 VPC 之间的低延迟、高带宽的连接。Direct Connect 提供了专用的网络连接，相比于 Internet 上的 VPN，可以提供更稳定、更低延迟的传输。而 Transit Gateway 作为中心枢纽，简化了多个 VPC 之间的连接，并可以与 Direct Connect 网关互联，从而简化了混合云架构的网络管理。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项：通过 Site-to-Site VPN 连接到每个 VPC，延迟较高且成本相对较高，因为 VPN 需要通过 Internet 进行加密传输，无法满足个位数延迟的要求，且需要为每个 VPC 单独建立连接，增加了管理负担。\nC 选项：建立到 Direct Connect 网关的 Site-to-Site VPN 连接，仍然无法提供低延迟的专用网络连接，并且可能受到 Internet 拥塞的影响，不符合需求。\nD 选项：虽然建立了 Direct Connect 连接和 VIF，但该方案未与 Transit Gateway 结合，无法简化 VPC 之间的连接和管理，且可能需要配置多个虚拟接口。此外，VIF 的创建也需要时间，相比选项 B 增加了复杂性。\nE 选项：将 Site-to-Site VPN 连接与附加到 VPC 的传输网关关联，虽然使用了 Transit Gateway 简化了 VPC 间的连接，但仍然使用了 VPN，无法满足个位数延迟的需求，不适合对延迟敏感的场景。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Direct Connect",
      "AWS Transit Gateway",
      "VPC",
      "Site-to-Site VPN",
      "Direct Connect Gateway",
      "VIF",
      "Internet",
      "Lambda",
      "EC2"
    ]
  },
  {
    "id": 958,
    "topic": "1",
    "question_en": "A global ecommerce company runs its critical workloads on AWS. The workloads use an Amazon RDS for PostgreSQL DB instance that is configured for a Multi-AZ deployment. Customers have reported application timeouts when the company undergoes database failovers. The company needs a resilient solution to reduce failover time. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon RDS Proxy. Assign the proxy to the DB instance.",
      "B": "Create a read replica for the DB instance. Move the read trafic to the read replica.",
      "C": "Enable Performance Insights. Monitor the CPU load to identify the timeouts.",
      "D": "Take regular automatic snapshots. Copy the automatic snapshots to multiple AWS Regions."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家全球电商公司在 AWS 上运行其关键工作负载。这些工作负载使用配置为多可用区部署的 Amazon RDS for PostgreSQL 数据库实例。当公司进行数据库故障转移时，客户报告了应用程序超时。该公司需要一个弹性解决方案来减少故障转移时间。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建 Amazon RDS Proxy。将代理分配给数据库实例。",
      "B": "为数据库实例创建只读副本。将读取流量移动到只读副本。",
      "C": "打开 Performance Insights。监控 CPU 负载以识别超时。",
      "D": "定期拍摄自动快照。将自动快照复制到多个 AWS 区域。"
    },
    "tags": [
      "Amazon RDS",
      "RDS Proxy",
      "High Availability"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考察 RDS 故障转移时间优化；与 RDS 高可用性、RDS Proxy 的选型相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：Amazon RDS Proxy 可以将应用程序与数据库实例的故障转移隔离开来，并提供到 RDS 数据库实例的连接池，从而减少应用程序的故障转移时间。通过 RDS Proxy，即使底层数据库实例发生故障，应用程序也可以更快地恢复连接，因为它已经连接到代理。这显著降低了应用程序的超时时间。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 创建只读副本主要用于读取扩展和数据备份，而不是解决故障转移时间问题。虽然将读取流量移动到只读副本可以提高读取性能，但它不会缩短数据库主实例发生故障时的故障转移时间。选项 C 开启 Performance Insights 能够帮助监控和分析数据库性能，但它并不能直接缩短故障转移时间，也无法解决应用程序的超时问题。选项 D 定期拍摄自动快照主要用于数据备份和恢复，虽然快照可以用于数据库的恢复，但它并不能缩短数据库实例故障时的故障转移时间；复制快照到多个区域主要目的是为了灾备，而非缩短故障转移时间。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS",
      "PostgreSQL",
      "RDS Proxy",
      "Read Replica",
      "CPU",
      "Availability Zone",
      "EC2",
      "EBS",
      "Amazon S3",
      "Performance Insights"
    ]
  },
  {
    "id": 959,
    "topic": "1",
    "question_en": "A company has multiple Amazon RDS DB instances that run in a development AWS account. All the instances have tags to identify them as development resources. The company needs the development DB instances to run on a schedule only during business hours. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon CloudWatch alarm to identify RDS instances that need to be stopped. Create an AWS Lambda function to start and stop the RDS instances.",
      "B": "Create an AWS Trusted Advisor report to identify RDS instances to be started and stopped. Create an AWS Lambda function to start and stop the RDS instances.",
      "C": "Create AWS Systems Manager State Manager associations to start and stop the RDS instances.",
      "D": "Create an Amazon EventBridge rule that invokes AWS Lambda functions to start and stop the RDS instances."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司在开发 AWS 账户中运行多个 Amazon RDS 数据库实例。所有实例都有标签来标识它们为开发资源。该公司需要开发数据库实例仅在工作时间内按计划运行。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon CloudWatch 警报来识别需要停止的 RDS 实例。创建一个 AWS Lambda 函数来启动和停止 RDS 实例。",
      "B": "创建一个 AWS Trusted Advisor 报告来识别需要启动和停止的 RDS 实例。创建一个 AWS Lambda 函数来启动和停止 RDS 实例。",
      "C": "创建 AWS Systems Manager 状态管理器关联以启动和停止 RDS 实例。",
      "D": "创建一个 Amazon EventBridge 规则，该规则调用 AWS Lambda 函数来启动和停止 RDS 实例。"
    },
    "tags": [
      "Amazon RDS",
      "AWS Lambda",
      "Amazon EventBridge",
      "AWS Systems Manager",
      "Amazon CloudWatch",
      "AWS Trusted Advisor"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n考查如何通过自动化方式，在特定时间段内启动和停止 Amazon RDS 数据库实例。与 EventBridge、Lambda 函数、和 Systems Manager 关联的配置和使用相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。创建一个 Amazon EventBridge 规则，该规则触发 AWS Lambda 函数。EventBridge 提供了基于时间表的规则（类似 cron），允许您定义事件触发的时间。Lambda 函数可以被编写来调用 RDS API，从而启动或停止数据库实例。这种方法完全自动化，不需要手动干预，运营开销低，易于配置和管理。它满足了在指定时间段内自动启动和停止 RDS 实例的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。使用 CloudWatch 警报来触发 Lambda 函数虽然可行，但 CloudWatch 警报通常用于监控资源的状态变化，而非基于时间表的定期操作。虽然可以配合使用，但相比 EventBridge，配置和维护更加复杂，且警报的创建本身增加了额外的开销。选项 B 错误。Trusted Advisor 报告主要用于推荐云资源优化和安全最佳实践，它本身无法直接触发 Lambda 函数来启动或停止 RDS 实例，因此不可行。选项 C 错误。AWS Systems Manager 状态管理器关联可以用于配置和管理资源，但它更适合于配置、更新和维护资源状态，而不是专门用于基于时间的自动启动和停止 RDS 实例。虽然可以通过自定义文档实现，但相比 EventBridge 和 Lambda 的组合，配置和维护更复杂，通用性也相对较差。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon RDS",
      "RDS",
      "AWS Lambda",
      "Lambda",
      "Amazon EventBridge",
      "EventBridge",
      "AWS Systems Manager",
      "Systems Manager",
      "Amazon CloudWatch",
      "CloudWatch",
      "AWS Trusted Advisor",
      "Trusted Advisor"
    ]
  },
  {
    "id": 960,
    "topic": "1",
    "question_en": "A consumer survey company has gathered data for several years from a specific geographic region. The company stores this data in an Amazon S3 bucket in an AWS Region. The company has started to share this data with a marketing firm in a new geographic region. The company has granted the firm's AWS account access to the S3 bucket. The company wants to minimize the data transfer costs when the marketing firm requests data from the S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure the Requester Pays feature on the company’s S3 bucket.",
      "B": "Configure S3 Cross-Region Replication (CRR) from the company’s S3 bucket to one of the marketing firm’s S3 buckets.",
      "C": "Configure AWS Resource Access Manager to share the S3 bucket with the marketing firm AWS account.",
      "D": "Configure the company’s S3 bucket to use S3 Intelligent-Tiering Sync the S3 bucket to one of the marketing firm’s S3 buckets."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家消费者调查公司从特定地理区域收集了多年数据。该公司将这些数据存储在 AWS 区域的 Amazon S3 存储桶中。该公司已开始与新地理区域的一家营销公司共享此数据。该公司已授予该公司的 AWS 账户访问 S3 存储桶的权限。该公司希望在营销公司从 S3 存储桶请求数据时最大限度地减少数据传输成本。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在公司的 S3 存储桶上配置 Requester Pays 功能。",
      "B": "配置 S3 跨区域复制 (CRR)，将数据从公司的 S3 存储桶复制到营销公司的一个 S3 存储桶。",
      "C": "配置 AWS Resource Access Manager 以与营销公司的 AWS 账户共享 S3 存储桶。",
      "D": "配置公司的 S3 存储桶以使用 S3 Intelligent-Tiering 并将 S3 存储桶同步到营销公司的一个 S3 存储桶。"
    },
    "tags": [
      "Amazon S3",
      "S3 Cross-Region Replication",
      "S3",
      "Data Transfer Cost"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n考查如何在共享 S3 数据的同时，最大程度减少数据传输成本。此题涉及到 S3 的跨区域复制 (CRR) 和数据传输计费的理解，需要权衡方案的适用性，以及与其他 AWS 服务(如 Requester Pays, AWS Resource Access Manager)的关系。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：配置 S3 跨区域复制 (CRR) 是最佳方案。通过将数据复制到营销公司所在的区域的 S3 存储桶中，可以确保营销公司从离他们更近的 S3 存储桶中读取数据，从而避免了跨区域数据传输，降低了数据传输成本。同时，CRR 确保了数据的可用性和可靠性，因为营销公司拥有数据的副本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. 配置 Requester Pays 功能虽然可以降低发起请求者的费用，但公司仍然需要承担向营销公司所在区域传输数据的费用，并没有从根本上解决数据传输成本高的问题。C. 配置 AWS Resource Access Manager 共享 S3 存储桶，并没有改变数据的物理位置，因此数据依然需要进行跨区域传输，无法降低传输成本。D. 配置 S3 Intelligent-Tiering 无法解决数据传输成本问题，而且将数据同步到营销公司的 S3 存储桶需要额外的配置，增加了复杂性，且同步过程可能产生额外的费用。而且，Intelligent-Tiering 的主要目的是优化存储成本，而不是优化数据传输成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "S3",
      "CRR",
      "AWS Resource Access Manager",
      "S3 Intelligent-Tiering",
      "S3 Cross-Region Replication",
      "Requester Pays"
    ]
  },
  {
    "id": 961,
    "topic": "1",
    "question_en": "A company uses AWS to host its public ecommerce website. The website uses an AWS Global Accelerator accelerator for trafic from the internet. The Global Accelerator accelerator forwards the trafic to an Application Load Balancer (ALB) that is the entry point for an Auto Scaling group. The company recently identified a DDoS attack on the website. The company needs a solution to mitigate future attacks. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Configure an AWS WAF web ACL for the Global Accelerator accelerator to block trafic by using rate-based rules",
      "B": "Configure an AWS Lambda function to read the ALB metrics to block attacks by updating a VPC network ACL",
      "C": "Configure an AWS WAF web ACL on the ALB to block trafic by using rate-based rules",
      "D": "Configure an Amazon CloudFront distribution in front of the Global Accelerator accelerator"
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司使用 AWS 托管其公共电子商务网站。该网站使用 AWS Global Accelerator 加速器来处理来自互联网的流量。Global Accelerator 加速器将流量转发到作为 Auto Scaling 组入口点的 Application Load Balancer (ALB)。该公司最近在其网站上发现了一次 DDoS 攻击。该公司需要一个解决方案来缓解未来的攻击。哪个解决方案将以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "为 Global Accelerator 加速器配置 AWS WAF web ACL，通过使用基于速率的规则来阻止流量",
      "B": "配置一个 AWS Lambda 函数来读取 ALB 指标，通过更新 VPC 网络 ACL 来阻止攻击",
      "C": "在 ALB 上配置 AWS WAF web ACL，通过使用基于速率的规则来阻止流量",
      "D": "在 Global Accelerator 加速器前面配置 Amazon CloudFront 分发"
    },
    "tags": [
      "AWS WAF",
      "Global Accelerator",
      "DDoS mitigation"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考查利用 AWS WAF 防御 DDoS 攻击，以及 Global Accelerator 和 Application Load Balancer (ALB) 配合使用时的最佳实践。本题还涉及了网络安全、流量管理、以及针对 DDoS 攻击的防御策略。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：为 Global Accelerator 配置 AWS WAF web ACL，使用基于速率的规则来阻止流量，是针对 DDoS 攻击的有效方法。 Global Accelerator 位于边缘，可以更早地拦截恶意流量。AWS WAF 的基于速率的规则能够限制来自特定 IP 地址的请求数量，从而限制恶意流量的影响。这种方法能有效缓解 DDoS 攻击，且与 Global Accelerator 协同工作，保护网站的可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 的方案涉及使用 Lambda 函数读取 ALB 指标，并更新 VPC 网络 ACL 来阻止攻击。这种方案的实施复杂性较高，需要编写和维护 Lambda 代码，并手动管理网络 ACL 规则，响应速度和自动化程度均不足。选项 C 在 ALB 上配置 AWS WAF web ACL，虽然可以提供一定程度的 DDoS 防护，但 Global Accelerator 位于 ALB 前面，此时 ALB 已经接收到一部分攻击流量，防护效果不如直接在 Global Accelerator 上配置 WAF。选项 D 在 Global Accelerator 前面配置 CloudFront 分发，虽然 CloudFront 也可以用于 DDoS 缓解，但会增加配置和管理的复杂性，且与题目中 Global Accelerator 的使用场景不符，导致冗余配置。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS WAF",
      "Global Accelerator",
      "DDoS",
      "Application Load Balancer",
      "ALB",
      "Lambda",
      "VPC",
      "CloudFront"
    ]
  },
  {
    "id": 962,
    "topic": "1",
    "question_en": "A company uses an Amazon DynamoDB table to store data that the company receives from devices. The DynamoDB table supports a customer-facing website to display recent activity on customer devices. The company configured the table with provisioned throughput for writes and reads. The company wants to calculate performance metrics for customer device data on a daily basis. The solution must have minimal effect on the table's provisioned read and write capacity. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an Amazon Athena SQL query with the Amazon Athena DynamoDB connector to calculate performance metrics on a recurring schedule.",
      "B": "Use an AWS Glue job with the AWS Glue DynamoDB export connector to calculate performance metrics on a recurring schedule.",
      "C": "Use an Amazon Redshift COPY command to calculate performance metrics on a recurring schedule.",
      "D": "Use an Amazon EMR job with an Apache Hive external table to calculate performance metrics on a recurring schedule."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司使用 Amazon DynamoDB 表来存储从设备接收的数据。DynamoDB 表支持面向客户的网站，以显示客户设备的近期活动。该公司为表配置了写入和读取的预置吞吐量。该公司希望每天计算客户设备数据的性能指标。该解决方案必须对表的预置读取和写入容量的影响最小。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Athena SQL 查询和 Amazon Athena DynamoDB 连接器，以定期安排计算性能指标。",
      "B": "使用 AWS Glue 作业和 AWS Glue DynamoDB 导出连接器，以定期安排计算性能指标。",
      "C": "使用 Amazon Redshift COPY 命令，以定期安排计算性能指标。",
      "D": "使用 Amazon EMR 作业和 Apache Hive 外部表，以定期安排计算性能指标。"
    },
    "tags": [
      "Amazon DynamoDB",
      "Amazon Athena",
      "Athena DynamoDB Connector",
      "Performance Monitoring"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考察 DynamoDB 数据分析方案，及对性能影响的考量。涉及与 Athena、Glue、Redshift、EMR 等服务的对比，需要考虑计算资源的开销与对 DynamoDB 预置吞吐量的影响。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：Amazon Athena 结合 Athena DynamoDB 连接器是合适的方案。Athena 是一种无服务器的查询服务，可以直接查询 DynamoDB 中的数据，无需预先导出或 ETL。使用 Athena 查询，可以按需计算性能指标，且不占用 DynamoDB 的预置读取和写入容量，对 DynamoDB 的性能影响最小。通过定期安排 Athena 查询，可以实现每天计算性能指标的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项使用 AWS Glue 作业和 DynamoDB 导出连接器不合适。虽然 Glue 也可以处理数据转换，但它需要将 DynamoDB 数据导出到 S3 等存储，这会占用 DynamoDB 的读取容量，且增加了数据存储和 ETL 的复杂性。C 选项使用 Amazon Redshift COPY 命令不适用。Redshift 是一个数据仓库，不直接支持从 DynamoDB 导入数据，需要额外的 ETL 步骤，且 Redshift 整体复杂度和成本都高于 Athena。D 选项使用 Amazon EMR 作业和 Apache Hive 外部表也不合适。EMR 提供了大数据处理能力，但设置和管理 EMR 集群的成本较高，且需要额外的配置来读取 DynamoDB 数据，对 DynamoDB 的影响也较大，不如 Athena 灵活和经济。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon DynamoDB",
      "Amazon Athena",
      "AWS Glue",
      "Amazon S3",
      "Amazon Redshift",
      "EMR",
      "Apache Hive",
      "Athena DynamoDB Connector",
      "AWS Glue DynamoDB Export Connector"
    ]
  },
  {
    "id": 963,
    "topic": "1",
    "question_en": "A solutions architect is designing the cloud architecture for a new stateless application that will be deployed on AWS. The solutions architect created an Amazon Machine Image (AMI) and launch template for the application. Based on the number of jobs that need to be processed, the processing must run in parallel while adding and removing application Amazon EC2 instances as needed. The application must be loosely coupled. The job items must be durably stored. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to be processed. Create an Auto Scaling group by using the launch template with the scaling policy set to add and remove EC2 instances based on CPU usage.",
      "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be processed. Create an Auto Scaling group by using the launch template with the scaling policy set to add and remove EC2 instances based on network usage.",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue to hold the jobs that need to be processed. Create an Auto Scaling group by using the launch template with the scaling policy set to add and remove EC2 instances based on the number of items in the SQS queue.",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic to send the jobs that need to be processed. Create an Auto Scaling group by using the launch template with the scaling policy set to add and remove EC2 instances based on the number of messages published to the SNS topic."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一位解决方案架构师正在为将在 AWS 上部署的新无状态应用程序设计云架构。该解决方案架构师为该应用程序创建了 Amazon Machine Image (AMI) 和启动模板。基于需要处理的作业数量，处理必须并行运行，同时根据需要添加和删除应用程序 Amazon EC2 实例。应用程序必须松散耦合。作业项必须持久存储。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题来发送需要处理的作业。使用启动模板创建一个 Auto Scaling 组，其伸缩策略设置为根据 CPU 使用率添加和删除 EC2 实例。",
      "B": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列来保存需要处理的作业。使用启动模板创建一个 Auto Scaling 组，其伸缩策略设置为根据网络使用率添加和删除 EC2 实例。",
      "C": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列来保存需要处理的作业。使用启动模板创建一个 Auto Scaling 组，其伸缩策略设置为根据 SQS 队列中的项目数量添加和删除 EC2 实例。",
      "D": "创建一个 Amazon Simple Notification Service (Amazon SNS) 主题来发送需要处理的作业。使用启动模板创建一个 Auto Scaling 组，其伸缩策略设置为根据发布到 SNS 主题的消息数量添加和删除 EC2 实例。"
    },
    "tags": [
      "Amazon SQS",
      "Auto Scaling",
      "Amazon EC2",
      "Launch Template"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n本题考查了如何使用 Amazon SQS 和 Auto Scaling 搭建松散耦合的无状态应用程序，并实现作业的持久存储和弹性伸缩。与消息队列的选择、Auto Scaling 伸缩策略的设置密切相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 提供了最适合的解决方案。首先，使用 Amazon SQS 队列持久存储作业，满足了持久存储的需求。其次，Auto Scaling 组可以根据 SQS 队列中的消息数量自动扩展或缩减 EC2 实例的数量，从而实现并行处理和弹性伸缩。这种基于队列深度的伸缩策略能够很好地响应作业量的变化，确保应用程序的性能和成本效益。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误在于使用了 Amazon SNS 而不是 SQS。虽然 SNS 也可以用于消息传递，但它主要用于发布/订阅场景，并不提供持久存储。此外，根据 CPU 使用率进行伸缩无法直接响应队列中待处理作业的数量。选项 B 错误，同样使用了 SQS，满足了持久存储。但伸缩策略基于网络使用率，这与处理队列中的作业数量并不直接相关，因此无法准确地实现弹性伸缩。选项 D 错误在于使用 SNS，没有持久化作业。并且基于 SNS 主题消息数量进行伸缩，与处理队列中的作业数量关联度低，难以实现准确的弹性伸缩。以上选项均不能满足题目要求的作业持久化存储和基于作业数量的弹性伸缩。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon SQS",
      "Auto Scaling",
      "Amazon EC2",
      "Amazon SNS",
      "CPU",
      "AMI",
      "Launch Template",
      "Network"
    ]
  },
  {
    "id": 964,
    "topic": "1",
    "question_en": "A global ecommerce company uses a monolithic architecture. The company needs a solution to manage the increasing volume of product data. The solution must be scalable and have a modular service architecture. The company needs to maintain its structured database schemas. The company also needs a storage solution to store product data and product images. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use an Amazon EC2 instance in an Auto Scaling group to deploy a containerized application. Use an Application Load Balancer to distribute web trafic. Use an Amazon RDS DB instance to store product data and product images.",
      "B": "Use AWS Lambda functions to manage the existing monolithic application. Use Amazon DynamoDB to store product data and product images. Use Amazon Simple Notification Service (Amazon SNS) for event-driven communication between the Lambda functions.",
      "C": "Use Amazon Elastic Kubernetes Service (Amazon EKS) with an Amazon EC2 deployment to deploy a containerized application. Use an Amazon Aurora cluster to store the product data. Use AWS Step Functions to manage workfiows. Store the product images in Amazon S3 Glacier Deep Archive.",
      "D": "Use Amazon Elastic Container Service (Amazon ECS) with AWS Fargate to deploy a containerized application. Use Amazon RDS with a Multi-AZ deployment to store the product data. Store the product images in an Amazon S3 bucket."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家全球电子商务公司使用单体架构。该公司需要一个解决方案来管理不断增长的产品数据量。该解决方案必须具有可扩展性并具有模块化服务架构。该公司需要维护其结构化的数据库模式。该公司还需要一个存储解决方案来存储产品数据和产品图像。哪种解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "在 Auto Scaling 组中使用 Amazon EC2 实例来部署容器化应用程序。使用 Application Load Balancer 分发 Web 流量。使用 Amazon RDS DB 实例存储产品数据和产品图像。",
      "B": "使用 AWS Lambda 函数管理现有的单体应用程序。使用 Amazon DynamoDB 存储产品数据和产品图像。使用 Amazon Simple Notification Service (Amazon SNS) 在 Lambda 函数之间进行事件驱动的通信。",
      "C": "将 Amazon Elastic Kubernetes Service (Amazon EKS) 与 Amazon EC2 部署结合使用，以部署容器化应用程序。使用 Amazon Aurora 集群存储产品数据。使用 AWS Step Functions 管理工作流程。将产品图像存储在 Amazon S3 Glacier Deep Archive 中。",
      "D": "使用 Amazon Elastic Container Service (Amazon ECS) 与 AWS Fargate 部署容器化应用程序。使用具有 Multi-AZ 部署的 Amazon RDS 存储产品数据。将产品图像存储在 Amazon S3 存储桶中。"
    },
    "tags": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon RDS",
      "Amazon S3",
      "Multi-AZ",
      "Containerization",
      "Microservices"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n考查在云原生架构中如何设计可扩展、模块化的解决方案，并结合数据库和存储服务。与容器化部署、数据库选型、存储服务选择相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 提供了最佳的解决方案。使用 Amazon ECS with AWS Fargate 部署容器化应用程序，实现了模块化服务架构，且无需管理底层 EC2 实例。Amazon RDS with Multi-AZ 提供高可用性和持久化的数据库存储，满足了维护结构化数据库模式的需求。将产品图像存储在 Amazon S3 存储桶中，满足了存储大量图像的需求，且 Amazon S3 具有高扩展性和可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 的问题在于，使用 Amazon EC2 实例管理应用程序需要更多的运营开销，包括实例的配置、维护和监控。虽然 Application Load Balancer 可以分发 Web 流量，但 RDS 虽然可以使用，但无法应对题干“可扩展性”的需求。选项 B 的问题在于，使用 DynamoDB 存储结构化数据库模式会面临挑战，因为 DynamoDB 并非为关系型数据库设计。并且将产品图像存储在 DynamoDB 中也并不合适。选项 C 方案的问题在于，使用 Amazon EKS 的运营开销较高，需要用户管理 Kubernetes 集群。此外，将产品图像存储在 Amazon S3 Glacier Deep Archive 中虽然成本较低，但访问延迟较高，不适用于快速访问产品图像的需求。Aurora 相比于 ECS 运营成本过高，并且与 S3 Glacier Deep Archive 的搭配方案不满足“最小运营开销”的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon RDS",
      "Multi-AZ",
      "Amazon S3",
      "Amazon EC2",
      "Application Load Balancer",
      "Amazon DynamoDB",
      "AWS Lambda",
      "Amazon SNS",
      "Amazon EKS",
      "Amazon Aurora",
      "AWS Step Functions",
      "Amazon S3 Glacier Deep Archive"
    ]
  },
  {
    "id": 965,
    "topic": "1",
    "question_en": "A company is migrating an application from an on-premises environment to AWS. The application will store sensitive data in Amazon S3. The company must encrypt the data before storing the data in Amazon S3. Which solution will meet these requirements?",
    "options_en": {
      "A": "Encrypt the data by using client-side encryption with customer managed keys.",
      "B": "Encrypt the data by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "C": "Encrypt the data by using server-side encryption with customer-provided keys (SSE-C).",
      "D": "Encrypt the data by using client-side encryption with Amazon S3 managed keys."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一家公司正在将其应用程序从本地环境迁移到 AWS。 该应用程序将敏感数据存储在 Amazon S3 中。该公司必须在将数据存储在 Amazon S3 之前对数据进行加密。 哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用由客户管理的密钥通过客户端加密对数据进行加密。",
      "B": "使用由 AWS KMS 密钥 (SSE-KMS) 进行服务器端加密对数据进行加密。",
      "C": "使用由客户提供的密钥 (SSE-C) 进行服务器端加密对数据进行加密。",
      "D": "使用由 Amazon S3 托管密钥进行客户端加密对数据进行加密。"
    },
    "tags": [
      "Amazon S3",
      "SSE-KMS",
      "SSE-C",
      "Client-Side Encryption",
      "AWS KMS",
      "Encryption"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考查在将敏感数据存储到 S3 之前如何加密数据。题目要求在客户端进行加密，并关注加密密钥的管理方式。",
      "why_correct": "选项 A 描述了使用客户管理的密钥进行客户端加密。这意味着在数据上传到 S3 之前，客户端使用客户控制的密钥对数据进行加密。这种方法满足了在存储到 S3 之前加密数据的要求，并使客户完全控制加密密钥。",
      "why_wrong": "选项 B 描述了使用 SSE-KMS 进行服务器端加密。虽然 SSE-KMS 提供了加密，但它在服务器端进行，不符合在客户端加密的要求。选项 C 描述了使用 SSE-C 进行服务器端加密，虽然 SSE-C 允许客户提供密钥，但加密发生在服务器端，也不满足题目的客户端加密需求。选项 D 描述了使用 Amazon S3 托管密钥进行客户端加密。Amazon S3 托管密钥仅用于服务器端加密，不适用于客户端加密场景，因此不正确。"
    },
    "related_terms": [
      "Amazon S3",
      "SSE-KMS",
      "SSE-C",
      "AWS KMS",
      "client-side encryption",
      "server-side encryption"
    ]
  },
  {
    "id": 966,
    "topic": "1",
    "question_en": "A company wants to create an Amazon EMR cluster that multiple teams will use. The company wants to ensure that each team’s big data workloads can access only the AWS services that each team needs to interact with. The company does not want the workloads to have access to Instance Metadata Service Version 2 (IMDSv2) on the cluster’s underlying EC2 instances. Which solution will meet these requirements?",
    "options_en": {
      "A": "Configure interface VPC endpoints for each AWS service that the teams need. Use the required interface VPC endpoints to submit the big data workloads.",
      "B": "Create EMR runtime roles. Configure the cluster to use the runtime roles. Use the runtime roles to submit the big data workloads.",
      "C": "Create an EC2 IAM instance profile that has the required permissions for each team. Use the instance profile to submit the big data workloads.",
      "D": "Create an EMR security configuration that has the EnableApplicationScopedIAMRole option set to false. Use the security configuration to submit the big data workloads."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司希望创建多个团队将使用的 Amazon EMR 集群。该公司希望确保每个团队的大数据工作负载只能访问每个团队需要交互的 AWS 服务。该公司不希望工作负载能够访问集群底层 EC2 实例上的 Instance Metadata Service Version 2 (IMDSv2)。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "为团队需要的每个 AWS 服务配置接口 VPC endpoint。使用所需的接口 VPC endpoint 提交大数据工作负载。",
      "B": "创建 EMR 运行时角色。配置集群使用运行时角色。使用运行时角色提交大数据工作负载。",
      "C": "创建具有每个团队所需权限的 EC2 IAM 实例配置文件。使用实例配置文件提交大数据工作负载。",
      "D": "创建 EMR 安全配置，将 EnableApplicationScopedIAMRole 选项设置为 false。使用安全配置提交大数据工作负载。"
    },
    "tags": [
      "Amazon EMR",
      "IAM",
      "IAM Roles",
      "EC2",
      "Instance Metadata Service Version 2 (IMDSv2)",
      "VPC endpoint",
      "Security Configuration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n考查 EMR 集群权限管理与 IMDSv2 的禁用。与 EMR 运行时角色、EC2 实例配置文件、VPC endpoint 和安全配置相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：创建 EMR 运行时角色是满足需求的最佳方案。运行时角色允许 EMR 集群安全地访问其他 AWS 服务，无需访问底层 EC2 实例的 IMDSv2。当 EMR 集群启动时，将使用指定的 IAM 角色来执行应用程序，该角色定义了集群可以访问的 AWS 服务的权限。这确保了每个团队的工作负载仅能访问其所需的资源。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA. 接口 VPC endpoint 提供了对 AWS 服务的私有访问，但不能直接解决控制 EMR 集群内工作负载访问权限和禁用 IMDSv2 的问题。VPC endpoint 主要用于网络隔离和提高安全性，而非细粒度的权限控制。C. EC2 IAM 实例配置文件将 IAM 角色分配给 EC2 实例，这会使得实例中的所有应用程序（包括大数据工作负载）都拥有该角色的权限。这并没有限定每个团队工作负载的访问范围，不能满足对访问权限的精细控制。此外，使用实例配置文件无法直接禁用 IMDSv2。D. EMR 安全配置的 EnableApplicationScopedIAMRole 选项与控制对 IMDSv2 的访问无关。该选项允许在每个应用程序级别使用 IAM 角色，但并不能直接禁用对 IMDSv2 的访问。此外，安全配置没有提供细粒度的权限控制，无法满足题目中对每个团队访问不同 AWS 服务的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EMR",
      "EC2",
      "IAM",
      "AWS",
      "IAM Roles",
      "Instance Metadata Service Version 2 (IMDSv2)",
      "VPC endpoint",
      "Security Configuration"
    ]
  },
  {
    "id": 967,
    "topic": "1",
    "question_en": "A solutions architect is designing an application that helps users fill out and submit registration forms. The solutions architect plans to use a two-tier architecture that includes a web application server tier and a worker tier. The application needs to process submitted forms quickly. The application needs to process each form exactly once. The solution must ensure that no data is lost. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use an Amazon Simple Queue Service (Amazon SQS) FIFO queue between the web application server tier and the worker tier to store and forward form data.",
      "B": "Use an Amazon API Gateway HTTP API between the web application server tier and the worker tier to store and forward form data.",
      "C": "Use an Amazon Simple Queue Service (Amazon SQS) standard queue between the web application server tier and the worker tier to store and forward form data.",
      "D": "Use an AWS Step Functions workfiow. Create a synchronous workfiow between the web application server tier and the worker tier that stores and forwards form data."
    },
    "correct_answer": "A",
    "vote_percentage": "100%",
    "question_cn": "一位解决方案架构师正在设计一个应用程序，该应用程序帮助用户填写并提交注册表单。该解决方案架构师计划使用两层架构，其中包括 Web 应用程序服务器层和工作者层。该应用程序需要快速处理提交的表单。该应用程序需要仅处理每个表单一次。该解决方案必须确保不会丢失任何数据。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "在 Web 应用程序服务器层和工作者层之间使用 Amazon Simple Queue Service (Amazon SQS) FIFO 队列来存储和转发表单数据。",
      "B": "在 Web 应用程序服务器层和工作者层之间使用 Amazon API Gateway HTTP API 来存储和转发表单数据。",
      "C": "在 Web 应用程序服务器层和工作者层之间使用 Amazon Simple Queue Service (Amazon SQS) 标准队列来存储和转发表单数据。",
      "D": "使用 AWS Step Functions 工作流程。在 Web 应用程序服务器层和工作者层之间创建一个同步工作流程，用于存储和转发表单数据。"
    },
    "tags": [
      "Amazon SQS",
      "FIFO queue",
      "Amazon SQS FIFO"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 100%），解析仅供参考。】\n\n考察使用队列进行异步处理，确保数据不丢失及处理一次。与 SQS 的 FIFO 和 Standard 队列的区别、以及与 Step Functions 的选择相关。",
      "why_correct": "Amazon SQS FIFO 队列能够保证消息的顺序处理，这意味着表单数据将按照提交的顺序被工作者层处理。FIFO 队列保证消息只被处理一次，满足“每个表单仅处理一次”的要求，从而确保数据完整性。由于 SQS 本身的高可用性，数据丢失的风险被降到最低，满足“确保不会丢失任何数据”的需求。",
      "why_wrong": "B. Amazon API Gateway HTTP API 主要用于构建和管理 API，不具备消息队列的特性，无法保证表单数据的可靠存储和异步处理，更无法保证消息顺序和仅处理一次的需求。\nC. Amazon SQS 标准队列虽然可以用于异步处理，但不能保证消息的顺序，且默认情况下消息可能被处理多次，不符合“仅处理每个表单一次”和“表单提交后快速处理”的要求。\nD. 使用 AWS Step Functions 同步工作流程不适合处理高吞吐量的表单提交场景。同步工作流程会阻塞 Web 应用程序服务器，导致性能瓶颈。此外，同步工作流程不如异步队列那样具有弹性和容错能力，可能无法满足“确保不会丢失任何数据”的需求。"
    },
    "related_terms": [
      "Amazon SQS",
      "Amazon API Gateway",
      "HTTP API",
      "AWS Step Functions",
      "FIFO queue",
      "Web Application Server",
      "Worker",
      "Synchronous workflow",
      "Standard queue"
    ]
  },
  {
    "id": 968,
    "topic": "1",
    "question_en": "A finance company uses an on-premises search application to collect streaming data from various producers. The application provides real- time updates to search and visualization features. The company is planning to migrate to AWS and wants to use an AWS native solution. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon EC2 instances to ingest and process the data streams to Amazon S3 buckets tor storage. Use Amazon Athena to search the data. Use Amazon Managed Grafana to create visualizations.",
      "B": "Use Amazon EMR to ingest and process the data streams to Amazon Redshift for storage. Use Amazon Redshift Spectrum to search the data. Use Amazon QuickSight to create visualizations.",
      "C": "Use Amazon Elastic Kubernetes Service (Amazon EKS) to ingest and process the data streams to Amazon DynamoDB for storage. Use Amazon CloudWatch to create graphical dashboards to search and visualize the data.",
      "D": "Use Amazon Kinesis Data Streams to ingest and process the data streams to Amazon OpenSearch Service. Use OpenSearch Service to search the data. Use Amazon QuickSight to create visualizations."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家金融公司使用本地搜索应用程序从各种生产者那里收集流数据。该应用程序为搜索和可视化功能提供实时更新。该公司计划迁移到 AWS，并希望使用 AWS 原生解决方案。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon EC2 实例将数据流提取和处理到 Amazon S3 存储桶进行存储。使用 Amazon Athena 搜索数据。使用 Amazon Managed Grafana 创建可视化。",
      "B": "使用 Amazon EMR 将数据流提取和处理到 Amazon Redshift 进行存储。使用 Amazon Redshift Spectrum 搜索数据。使用 Amazon QuickSight 创建可视化。",
      "C": "使用 Amazon Elastic Kubernetes Service (Amazon EKS) 将数据流提取和处理到 Amazon DynamoDB 进行存储。使用 Amazon CloudWatch 创建图形仪表板来搜索和可视化数据。",
      "D": "使用 Amazon Kinesis Data Streams 将数据流提取和处理到 Amazon OpenSearch Service。使用 OpenSearch Service 搜索数据。使用 Amazon QuickSight 创建可视化。"
    },
    "tags": [
      "Amazon Kinesis Data Streams",
      "Amazon OpenSearch Service",
      "Amazon QuickSight",
      "Data Streaming"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考查流数据的实时处理与搜索，以及数据可视化，涉及 Kinesis Data Streams、OpenSearch Service 和 QuickSight 等服务。需要选择能满足实时更新需求并提供搜索和可视化功能的方案。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 提供了最佳的解决方案。Amazon Kinesis Data Streams 适合用于摄取、处理和实时分析流数据。将数据流式传输到 Amazon OpenSearch Service，即可实现高效的全文搜索功能。最后，使用 Amazon QuickSight 可以创建实时的可视化仪表板，满足题目中对实时更新的需求。这种组合方案整合了流数据摄取、全文搜索和数据可视化功能，完全符合题目的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 方案不适用，因为将数据先存储在 Amazon S3 中，然后使用 Amazon Athena 进行搜索，其延迟性不满足实时更新的需求。Amazon EC2 实例虽然可以处理数据流，但需要手动管理，增加了复杂性。选项 B 方案也不适用，Amazon EMR 主要用于大数据批处理，不适合实时数据处理场景。Amazon Redshift 虽然可以进行数据存储和分析，但也不适合实时处理流数据，而且其架构在流数据摄取方面不如 Kinesis Data Streams 灵活。选项 C 方案不适用，使用 Amazon EKS 增加了基础设施管理的复杂性。Amazon DynamoDB 虽然可以存储数据，但其主要设计用于键值对和文档数据库，不擅长全文搜索，无法满足搜索需求。使用 Amazon CloudWatch 创建图形仪表板，其可视化能力也逊于 Amazon QuickSight。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon Kinesis Data Streams",
      "Amazon OpenSearch Service",
      "Amazon QuickSight",
      "Amazon EC2",
      "Amazon S3",
      "Amazon Athena",
      "Amazon Managed Grafana",
      "Amazon EMR",
      "Amazon Redshift",
      "Amazon DynamoDB",
      "Amazon CloudWatch",
      "Amazon Redshift Spectrum",
      "Amazon Elastic Kubernetes Service (Amazon EKS)"
    ]
  },
  {
    "id": 969,
    "topic": "1",
    "question_en": "A company currently runs an on-premises application that usesASP.NET on Linux machines. The application is resource-intensive and serves customers directly. The company wants to modernize the application to .NET. The company wants to run the application on containers and to scale based on Amazon CloudWatch metrics. The company also wants to reduce the time spent on operational maintenance activities. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template to deploy the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
      "B": "Use AWS App2Container to containerize the application. Use an AWS CloudFormation template to deploy the application to Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances.",
      "C": "Use AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.",
      "D": "Use AWS App Runner to containerize the application. Use App Runner to deploy the application to Amazon Elastic Kubernetes Service (Amazon EKS) on Amazon EC2 instances."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司目前运行一个在本地部署的应用程序，该应用程序在 Linux 机器上使用 ASP.NET。该应用程序资源密集，并直接为客户提供服务。公司希望将应用程序现代化为 .NET。公司希望在容器上运行应用程序，并根据 Amazon CloudWatch 指标进行扩展。公司还希望减少在运营维护活动上花费的时间。哪个解决方案将以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 AWS App2Container 将应用程序容器化。使用 AWS CloudFormation 模板将应用程序部署到 AWS Fargate 上的 Amazon Elastic Container Service (Amazon ECS)。",
      "B": "使用 AWS App2Container 将应用程序容器化。使用 AWS CloudFormation 模板将应用程序部署到 Amazon EC2 实例上的 Amazon Elastic Container Service (Amazon ECS)。",
      "C": "使用 AWS App Runner 将应用程序容器化。使用 App Runner 将应用程序部署到 AWS Fargate 上的 Amazon Elastic Container Service (Amazon ECS)。",
      "D": "使用 AWS App Runner 将应用程序容器化。使用 App Runner 将应用程序部署到 Amazon EC2 实例上的 Amazon Elastic Kubernetes Service (Amazon EKS)。"
    },
    "tags": [
      "AWS App2Container",
      "Amazon ECS",
      "AWS Fargate",
      "Amazon CloudWatch",
      "AWS CloudFormation",
      "Containerization"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n考查应用程序的现代化改造和容器化部署方案，以及如何根据 CloudWatch 指标实现弹性伸缩，并最小化运营开销。涉及 AWS App2Container、Amazon ECS with AWS Fargate 和 Amazon ECS with Amazon EC2 的选择与对比。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：AWS App2Container 简化了将现有应用程序容器化的过程，非常适合将本地 ASP.NET 应用程序迁移到容器。 结合 AWS CloudFormation 模板实现基础设施即代码 (IaC)，便于应用程序的自动化部署和管理。选择 AWS Fargate 意味着无需管理底层 EC2 实例，从而最大限度地减少了运营开销，并能够根据 CloudWatch 指标实现弹性伸缩。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，因为虽然 AWS App2Container 容器化应用程序，使用 ECS 也满足容器编排的需求，但是选择 Amazon EC2 实例需要运维人员管理底层 EC2 实例的配置、维护和更新，增加了运营负担，与题干中减少运营开销的目标相悖。选项 C 错误，App Runner 无法直接与 AWS Fargate 配合使用，App Runner 本身是一个全托管的容器部署服务，简化了部署过程，但它与本题中提供的架构不一致。 选项 D 错误，虽然 App Runner 可以容器化应用程序，但 App Runner 无法部署到 Amazon EKS，而且 EKS 增加了 Kubernetes 集群的管理复杂性，同样不满足最小化运营开销的要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon ECS",
      "AWS Fargate",
      "Amazon CloudWatch",
      "AWS CloudFormation",
      "Amazon EC2",
      "Amazon EKS",
      "Linux",
      "AWS App2Container",
      "ASP.NET",
      "AWS App Runner",
      "Containerization"
    ]
  },
  {
    "id": 970,
    "topic": "1",
    "question_en": "A company is designing a new internal web application in the AWS Cloud. The new application must securely retrieve and store multiple employee usernames and passwords from an AWS managed service. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS CloudFormation and the BatchGetSecretValue API to retrieve usernames and passwords from Parameter Store.",
      "B": "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and AWS Batch with the BatchGetSecretValue API to retrieve the usernames and passwords from Secrets Manager.",
      "C": "Store the employee credentials in AWS Systems Manager Parameter Store. Use AWS CloudFormation and AWS Batch with the BatchGetSecretValue API to retrieve the usernames and passwords from Parameter Store.",
      "D": "Store the employee credentials in AWS Secrets Manager. Use AWS CloudFormation and the BatchGetSecretValue API to retrieve the usernames and passwords from Secrets Manager."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 云中设计一个新的内部 Web 应用程序。新应用程序必须安全地从 AWS 托管服务中检索和存储多个员工的用户名和密码。哪种解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "将员工凭据存储在 AWS Systems Manager Parameter Store 中。使用 AWS CloudFormation 和 BatchGetSecretValue API 从 Parameter Store 检索用户名和密码。",
      "B": "将员工凭据存储在 AWS Secrets Manager 中。使用 AWS CloudFormation 和 AWS Batch 以及 BatchGetSecretValue API 从 Secrets Manager 检索用户名和密码。",
      "C": "将员工凭据存储在 AWS Systems Manager Parameter Store 中。使用 AWS CloudFormation 和 AWS Batch 以及 BatchGetSecretValue API 从 Parameter Store 检索用户名和密码。",
      "D": "将员工凭据存储在 AWS Secrets Manager 中。使用 AWS CloudFormation 和 BatchGetSecretValue API 从 Secrets Manager 检索用户名和密码。"
    },
    "tags": [
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "AWS CloudFormation",
      "BatchGetSecretValue API"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n考查安全存储和检索敏感信息的最佳实践，以及 AWS Secrets Manager 和 AWS Systems Manager Parameter Store 的选择与对比。还涉及 AWS CloudFormation 部署和 API 调用方法。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。AWS Secrets Manager 专门设计用于安全地存储、管理和检索敏感信息，如密码、API 密钥等。它提供了内置的轮换功能和审计能力，可以满足安全要求。使用 BatchGetSecretValue API 可以从 Secrets Manager 中批量检索密钥，降低检索延迟。结合 AWS CloudFormation，可以实现基础设施即代码，简化部署和管理。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。AWS Systems Manager Parameter Store 主要用于存储配置数据和非秘密信息，虽然也可以存储秘密，但其设计目的和安全特性不如 Secrets Manager。使用 BatchGetSecretValue API 批量检索秘密信息，但 Parameter Store 在安全性方面不如 Secrets Manager。选项 B 和 C 错误。选项 B 中，不应该使用 AWS Batch 来处理检索秘密的操作，因为 Batch 主要用于批处理任务，而检索秘密是交互式的，应该选择更合适的方案。选项 C 与 A 类似，Parameter Store 在存储秘密信息方面不如 Secrets Manager。CloudFormation 和 BatchGetSecretValue API 的使用方式正确，但存储位置的选择不合适。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "AWS Secrets Manager",
      "AWS Systems Manager Parameter Store",
      "AWS CloudFormation",
      "AWS Batch",
      "BatchGetSecretValue API"
    ]
  },
  {
    "id": 971,
    "topic": "1",
    "question_en": "A company that is in the ap-northeast-1 Region has a fieet of thousands of AWS Outposts servers. The company has deployed the servers at remote locations around the world. All the servers regularly download new software versions that consist of 100 files. There is significant latency before all servers run the new software versions. The company must reduce the deployment latency for new software versions. Which solution will meet this requirement with the LEAST operational overhead?",
    "options_en": {
      "A": "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution in ap-northeast-1 that includes a CachingDisabled cache policy. Configure the S3 bucket as the origin. Download the software by using signed URLs.",
      "B": "Create an Amazon S3 bucket in ap-northeast-1. Create a second S3 bucket in the us-east-1 Region. Configure replication between the buckets. Set up an Amazon CloudFront distribution that uses ap-northeast-1 as the primary origin and us-east-1 as the secondary origin. Download the software by using signed URLs.",
      "C": "Create an Amazon S3 bucket in ap-northeast-1. Configure Amazon S3 Transfer Acceleration. Download the software by using the S3 Transfer Acceleration endpoint.",
      "D": "Create an Amazon S3 bucket in ap-northeast-1. Set up an Amazon CloudFront distribution. Configure the S3 bucket as the origin. Download the software by using signed URLs."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家位于 ap-northeast-1 区域的公司拥有数千台 AWS Outposts 服务器。该公司已将服务器部署在全球各地的远程位置。所有服务器会定期下载由 100 个文件组成的新软件版本。在所有服务器运行新软件版本之前存在显着的延迟。该公司必须减少新软件版本的部署延迟。哪种解决方案能够以最少的运营开销满足此要求？",
    "options_cn": {
      "A": "在 ap-northeast-1 区域创建一个 Amazon S3 存储桶。在 ap-northeast-1 区域设置一个 Amazon CloudFront 分发，其中包括 CachingDisabled 缓存策略。将 S3 存储桶配置为源。使用签名 URL 下载软件。",
      "B": "在 ap-northeast-1 区域创建一个 Amazon S3 存储桶。在 us-east-1 区域创建第二个 S3 存储桶。配置存储桶之间的复制。设置一个 Amazon CloudFront 分发，该分发使用 ap-northeast-1 作为主要源，us-east-1 作为辅助源。使用签名 URL 下载软件。",
      "C": "在 ap-northeast-1 区域创建一个 Amazon S3 存储桶。配置 Amazon S3 Transfer Acceleration。使用 S3 Transfer Acceleration 端点下载软件。",
      "D": "在 ap-northeast-1 区域创建一个 Amazon S3 存储桶。设置一个 Amazon CloudFront 分发。将 S3 存储桶配置为源。使用签名 URL 下载软件。"
    },
    "tags": [
      "Amazon S3",
      "Amazon CloudFront",
      "S3 Transfer Acceleration"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考查如何通过优化内容分发，加速大规模软件部署到分布在远程位置的服务器上。重点在于选择合适的服务，以减少部署延迟，并兼顾运营成本。与 Amazon CloudFront、Amazon S3、S3 Transfer Acceleration 的选型和对比相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。使用 Amazon CloudFront 分发，并以 Amazon S3 存储桶作为源，能够有效地缓存并分发软件更新文件。由于 CloudFront 的全球边缘站点网络，服务器可以从离其最近的边缘站点下载文件，从而减少延迟。使用签名 URL 提供了对下载的安全性控制。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。虽然使用 CloudFront 减少延迟，但 CachingDisabled 缓存策略会禁用缓存功能，使得每次请求都需要从 S3 源站获取文件，无法实现加速的目的。选项 B 错误。在多个区域设置 S3 存储桶并配置复制，会增加复杂性及成本，并且从 us-east-1 区域复制数据到 ap-northeast-1 区域会增加延迟，与题目需求相悖。选项 C 错误。虽然 S3 Transfer Acceleration 能够加速上传和下载，但其优势主要体现在跨区域的数据传输上。本题中所有资源都在 ap-northeast-1 区域，因此使用 Transfer Acceleration 的加速效果有限，并且无法利用 CloudFront 的全球边缘站点带来的优势。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon S3",
      "Amazon CloudFront",
      "S3 Transfer Acceleration",
      "CloudFront",
      "S3",
      "S3 bucket",
      "CachingDisabled"
    ]
  },
  {
    "id": 972,
    "topic": "1",
    "question_en": "A company currently runs an on-premises stock trading application by using Microsoft Windows Server. The company wants to migrate the application to the AWS Cloud. The company needs to design a highly available solution that provides low-latency access to block storage across multiple Availability Zones. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances. Install the application on both cluster nodes. Use Amazon FSx for Windows File Server as shared storage between the two cluster nodes.",
      "B": "Configure a Windows Server cluster that spans two Availability Zones on Amazon EC2 instances. Install the application on both cluster nodes. Use Amazon Elastic Block Store (Amazon EBS) General Purpose SSD (gp3) volumes as storage attached to the EC2 instances. Set up application-level replication to sync data from one EBS volume in one Availability Zone to another EBS volume in the second Availability Zone.",
      "C": "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2 instance as active and the second EC2 instance in standby mode. Use an Amazon FSx for NetApp ONTAP Multi-AZ file system to access the data by using Internet Small Computer Systems Interface (iSCSI) protocol.",
      "D": "Deploy the application on Amazon EC2 instances in two Availability Zones. Configure one EC2 instance as active and the second EC2 instance in standby mode. Use Amazon Elastic Block Store (Amazon EBS) Provisioned IOPS SSD (io2) volumes as storage attached to the EC2 instances. Set up Amazon EBS level replication to sync data from one io2 volume in one Availability Zone to another io2 volume in the second Availability Zone."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司目前通过使用 Microsoft Windows Server 运行本地股票交易应用程序。该公司希望将该应用程序迁移到 AWS Cloud。该公司需要设计一个高可用性解决方案，该解决方案可在多个可用区内提供对块存储的低延迟访问。哪种解决方案将以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "在 Amazon EC2 实例上配置一个跨越两个可用区的 Windows Server 集群。在两个集群节点上安装该应用程序。使用 Amazon FSx for Windows File Server 作为两个集群节点之间的共享存储。",
      "B": "在 Amazon EC2 实例上配置一个跨越两个可用区的 Windows Server 集群。在两个集群节点上安装该应用程序。使用 Amazon Elastic Block Store (Amazon EBS) 通用型 SSD (gp3) 卷作为连接到 EC2 实例的存储。设置应用程序级复制，以将数据从一个可用区中的一个 EBS 卷同步到第二个可用区中的另一个 EBS 卷。",
      "C": "在两个可用区的 Amazon EC2 实例上部署该应用程序。将一个 EC2 实例配置为活动状态，并将第二个 EC2 实例配置为备用模式。使用 Amazon FSx for NetApp ONTAP 多可用区文件系统，通过 Internet Small Computer Systems Interface (iSCSI) 协议访问数据。",
      "D": "在两个可用区的 Amazon EC2 实例上部署该应用程序。将一个 EC2 实例配置为活动状态，并将第二个 EC2 实例配置为备用模式。使用 Amazon Elastic Block Store (Amazon EBS) 预置 IOPS SSD (io2) 卷作为连接到 EC2 实例的存储。设置 Amazon EBS 级别的复制，以将数据从一个可用区中的一个 io2 卷同步到第二个可用区中的另一个 io2 卷。"
    },
    "tags": [
      "Amazon EC2",
      "Amazon FSx for Windows File Server",
      "Availability Zones",
      "High Availability",
      "Amazon EBS",
      "Amazon FSx for NetApp ONTAP",
      "iSCSI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n本题考查在 AWS 上构建高可用性架构，并结合块存储的低延迟需求。主要考察了 EC2 实例、FSx for Windows File Server、EBS、和 FSx for NetApp ONTAP 的选型与对比。也涉及了可用区的设计。高可用性是 AWS 的核心设计理念之一，务必熟练掌握。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 在两个可用区部署了 Windows Server 集群，满足了高可用性的要求。Amazon FSx for Windows File Server 提供了共享存储，方便两个集群节点访问相同的数据，同时 FSx for Windows File Server 专门为 Windows 环境优化，可以提供低延迟的访问。这种方案简化了实施工作量，因为 FSx 提供了现成的 Windows 文件共享服务，且集群的配置也相对容易。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，因为它使用 EBS 通用型 SSD (gp3) 卷，而 gp3 卷本身不提供共享存储功能。要在集群节点之间共享存储，需要依赖应用程序级别的复制，这增加了复杂性，增加了实施工作量。此外，由于数据同步的延迟，可能无法完全满足对块存储的低延迟访问需求。选项 C 错误，因为它使用了 FSx for NetApp ONTAP，虽然也提供了共享存储，但使用 iSCSI 协议访问数据，会增加额外的配置和管理工作量，并且 FSx for NetApp ONTAP 的部署和配置通常比 FSx for Windows File Server 复杂，这与题目要求的最小化实施工作量相悖。选项 D 错误，虽然使用了 EBS 预置 IOPS SSD (io2) 卷，该卷提供了高性能，但仍然需要配置 EBS 级别的复制，增加了复杂性，且无法满足多节点共享存储的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Amazon EC2",
      "Windows Server",
      "Amazon FSx for Windows File Server",
      "Amazon EBS",
      "gp3",
      "io2",
      "iSCSI",
      "Availability Zones",
      "Amazon Elastic Block Store (Amazon EBS)",
      "Amazon FSx for NetApp ONTAP"
    ]
  },
  {
    "id": 973,
    "topic": "1",
    "question_en": "A company is designing a web application with an internet-facing Application Load Balancer (ALB). The company needs the ALB to receive HTTPS web trafic from the public internet. The ALB must send only HTTPS trafic to the web application servers hosted on the Amazon EC2 instances on port 443. The ALB must perform a health check of the web application servers over HTTPS on port 8443. Which combination of configurations of the security group that is associated with the ALB will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Allow HTTPS inbound trafic from 0.0.0.0/0 for port 443.",
      "B": "Allow all outbound trafic to 0.0.0.0/0 for port 443.",
      "C": "Allow HTTPS outbound trafic to the web application instances for port 443.",
      "D": "Allow HTTPS inbound trafic from the web application instances for port 443",
      "E": "Allow HTTPS outbound trafic to the web application instances for the health check on port 8443",
      "F": "Allow HTTPS inbound trafic from the web application instances for the health check on port 8443."
    },
    "correct_answer": "ACE",
    "vote_percentage": "",
    "question_cn": "一家公司正在设计一个面向互联网的 Application Load Balancer (ALB) 的 Web 应用程序。该公司需要 ALB 从公共互联网接收 HTTPS Web 流量。ALB 必须仅将 HTTPS 流量发送到托管在 Amazon EC2 实例上且端口为 443 的 Web 应用程序服务器。 ALB 必须通过 HTTPS 在端口 8443 上对 Web 应用程序服务器执行运行状况检查。安全组的哪些配置组合与 ALB 关联，将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "允许来自 0.0.0.0/0 的 HTTPS 入站流量通过端口 443。",
      "B": "允许所有出站流量到 0.0.0.0/0 端口 443。",
      "C": "允许到 Web 应用程序实例的 HTTPS 出站流量通过端口 443。",
      "D": "允许来自 Web 应用程序实例的 HTTPS 入站流量通过端口 443。",
      "E": "允许到 Web 应用程序实例的 HTTPS 出站流量通过端口 8443 上的运行状况检查。",
      "F": "允许来自 Web 应用程序实例的 HTTPS 入站流量通过端口 8443 上的运行状况检查。"
    },
    "tags": [
      "Application Load Balancer",
      "ALB",
      "EC2",
      "Security Group",
      "HTTPS",
      "Health Check"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 ACE（社区 —），解析仅供参考。】\n\n考查 ALB 和 EC2 的安全组配置，以及 HTTPS 流量的路由和运行状况检查。需要同时考虑入站和出站规则，以及端口设置。与安全组的配置、ALB 的监听器配置、EC2 实例的安全组配置相关。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 ACE。理由简述：选项 A 允许来自 0.0.0.0/0 的 HTTPS 入站流量通过端口 443。 这是 ALB 从互联网接收 HTTPS 流量的基础。允许来自任何 IP 地址（0.0.0.0/0）的 HTTPS 流量（端口 443）进入 ALB，确保 ALB 可以接收来自客户端的 HTTPS 请求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 允许所有出站流量到 0.0.0.0/0 端口 443。这个配置不满足题目的需求。出站规则配置通常是针对 ALB 发送请求到后端服务器的。该选项不限定流量的来源和类型，如果允许所有出站流量到端口 443，则可能导致 ALB 尝试将流量发送到互联网上的其他服务器，而不是后端 EC2 实例。选项 C 允许到 Web 应用程序实例的 HTTPS 出站流量通过端口 443。这不符合要求。出站规则控制的是 EC2 实例向外发送流量，与 ALB 向后端实例发送流量的需求无关，且端口应为 8443 用于健康检查。选项 D 允许来自 Web 应用程序实例的 HTTPS 入站流量通过端口 443。这不符合要求。入站规则控制的是 EC2 实例接收流量，不限制请求来源，且端口错误。选项 E 允许到 Web 应用程序实例的 HTTPS 出站流量通过端口 8443 上的运行状况检查。这不符合要求。出站规则应允许 ALB 向后端 EC2 实例发送运行状况检查，且健康检查的端口是 8443。选项 F 允许来自 Web 应用程序实例的 HTTPS 入站流量通过端口 8443 上的运行状况检查。这不符合要求。入站规则控制的是 EC2 实例接收流量，并且端口错误。正确的配置是允许来自 ALB 的流量通过 8443 端口进行健康检查。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Application Load Balancer",
      "ALB",
      "EC2",
      "HTTPS",
      "Security Group",
      "Health Check",
      "0.0.0.0/0"
    ]
  },
  {
    "id": 974,
    "topic": "1",
    "question_en": "A company hosts an application on AWS. The application gives users the ability to upload photos and store the photos in an Amazon S3 bucket. The company wants to use Amazon CloudFront and a custom domain name to upload the photo files to the S3 bucket in the eu-west-1 Region. Which solution will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "Use AWS Certificate Manager (ACM) to create a public certificate in the us-east-1 Region. Use the certificate in CloudFront.",
      "B": "Use AWS Certificate Manager (ACM) to create a public certificate in eu-west-1. Use the certificate in CloudFront.",
      "C": "Configure Amazon S3 to allow uploads from CloudFront. Configure S3 Transfer Acceleration.",
      "D": "Configure Amazon S3 to allow uploads from CloudFront origin access control (OAC)",
      "E": "Configure Amazon S3 to allow uploads from CloudFront. Configure an Amazon S3 website endpoint."
    },
    "correct_answer": "AD",
    "vote_percentage": "50%",
    "question_cn": "一家公司在 AWS 上托管一个应用程序。该应用程序允许用户上传照片并将照片存储在 Amazon S3 存储桶中。该公司希望使用 Amazon CloudFront 和自定义域名将照片文件上传到 eu-west-1 区域中的 S3 存储桶。哪种解决方案将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "使用 AWS Certificate Manager (ACM) 在 us-east-1 区域中创建公共证书。在 CloudFront 中使用该证书。",
      "B": "使用 AWS Certificate Manager (ACM) 在 eu-west-1 区域中创建公共证书。在 CloudFront 中使用该证书。",
      "C": "配置 Amazon S3 允许从 CloudFront 上传。配置 S3 Transfer Acceleration。",
      "D": "配置 Amazon S3 以允许从 CloudFront 源访问控制 (OAC) 上传。",
      "E": "配置 Amazon S3 允许从 CloudFront 上传。配置 Amazon S3 网站终端节点。"
    },
    "tags": [
      "CloudFront",
      "S3",
      "ACM"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 50%），解析仅供参考。】\n\n此题考察 CloudFront 和 S3 结合使用，以及 ACM 的使用。CloudFront 作为 CDN，可以缓存 S3 中的静态内容，并通过自定义域名提供服务。要实现自定义域名和 HTTPS，需要使用 ACM 颁发的证书。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AD。理由简述：选项 A 和 B 都正确，区别在于证书创建的区域。由于 CloudFront 的证书必须在 us-east-1 区域创建，因此 A 正确。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 虽然使用 ACM 和 CloudFront，但证书必须在 us-east-1 创建，而不是 eu-west-1，因此错误。选项 C，配置 S3 允许 CloudFront 上传，无法解决自定义域名和 HTTPS 的问题，且 Transfer Acceleration 用于加速 S3 传输，与题意不符。 选项 D，S3 源访问控制 (OAC) 用于增强 CloudFront 对 S3 的访问控制，无法解决 HTTPS 问题。 选项 E，S3 网站终端节点，不适用于自定义域名和 HTTPS 。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "CloudFront",
      "S3",
      "ACM",
      "HTTPS",
      "CDN",
      "us-east-1",
      "Transfer Acceleration",
      "OAC",
      "eu-west-1",
      "Amazon S3 网站终端节点"
    ]
  },
  {
    "id": 975,
    "topic": "1",
    "question_en": "A weather forecasting company collects temperature readings from various sensors on a continuous basis. An existing data ingestion process collects the readings and aggregates the readings into larger Apache Parquet files. Then the process encrypts the files by using client-side encryption with KMS managed keys (CSE-KMS). Finally, the process writes the files to an Amazon S3 bucket with separate prefixes for each calendar day. The company wants to run occasional SQL queries on the data to take sample moving averages for a specific calendar day. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure Amazon Athena to read the encrypted files. Run SQL queries on the data directly in Amazon S3.",
      "B": "Use Amazon S3 Select to run SQL queries on the data directly in Amazon S3.",
      "C": "Configure Amazon Redshift to read the encrypted files. Use Redshift Spectrum and Redshift query editor v2 to run SQL queries on the data directly in Amazon S3.",
      "D": "Configure Amazon EMR Serverless to read the encrypted files. Use Apache SparkSQL to run SQL queries on the data directly in Amazon S3."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家天气预报公司持续不断地从各种传感器收集温度读数。现有的数据摄取流程收集读数并将这些读数聚合到更大的 Apache Parquet 文件中。然后，该流程使用客户端加密和 KMS 托管密钥 (CSE-KMS) 对文件进行加密。最后，该流程将文件写入 Amazon S3 存储桶，每个日历日使用单独的前缀。该公司希望对数据运行偶尔的 SQL 查询，以获取特定日历日的移动平均样本。哪个解决方案能最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "配置 Amazon Athena 以读取加密的文件。直接在 Amazon S3 中对数据运行 SQL 查询。",
      "B": "使用 Amazon S3 Select 直接在 Amazon S3 中对数据运行 SQL 查询。",
      "C": "配置 Amazon Redshift 以读取加密的文件。使用 Redshift Spectrum 和 Redshift 查询编辑器 v2 直接在 Amazon S3 中对数据运行 SQL 查询。",
      "D": "配置 Amazon EMR Serverless 以读取加密的文件。使用 Apache SparkSQL 直接在 Amazon S3 中对数据运行 SQL 查询。"
    },
    "tags": [
      "Athena",
      "S3",
      "KMS",
      "Parquet"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何以最低成本查询加密的 Parquet 数据。Athena 是一个无服务器的查询服务，可以直接查询 S3 中的数据，且支持多种数据格式。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，Athena 支持直接查询加密的 Parquet 文件。该方案成本效益最高，因为它不需要额外的基础设施管理。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，S3 Select 虽然能查询 S3 中的数据，但功能有限，并且不支持 Parquet 格式，也不支持 CSE-KMS 加密数据。 选项 C，Redshift 需要额外的基础设施，且 Redshift Spectrum 虽然能查询 S3 数据，但成本较高。 选项 D，EMR Serverless 也需要额外的基础设施，且使用 SparkSQL 查询的成本高于 Athena。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Athena",
      "S3",
      "KMS",
      "SQL",
      "Redshift",
      "Redshift Spectrum",
      "EMR Serverless",
      "S3 Select",
      "Parquet",
      "Apache SparkSQL",
      "CSE-KMS"
    ]
  },
  {
    "id": 976,
    "topic": "1",
    "question_en": "A company is implementing a new application on AWS. The company will run the application on multiple Amazon EC2 instances across multiple Availability Zones within multiple AWS Regions. The application will be available through the internet. Users will access the application from around the world. The company wants to ensure that each user who accesses the application is sent to the EC2 instances that are closest to the user’s location. Which solution will meet these requirements?",
    "options_en": {
      "A": "Implement an Amazon Route 53 geolocation routing policy. Use an internet-facing Application Load Balancer to distribute the trafic across all Availability Zones within the same Region.",
      "B": "Implement an Amazon Route 53 geoproximity routing policy. Use an internet-facing Network Load Balancer to distribute the trafic across all Availability Zones within the same Region.",
      "C": "Implement an Amazon Route 53 multivalue answer routing policy. Use an internet-facing Application Load Balancer to distribute the trafic across all Availability Zones within the same Region.",
      "D": "Implement an Amazon Route 53 weighted routing policy. Use an internet-facing Network Load Balancer to distribute the trafic across all Availability Zones within the same Region."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上实施新的应用程序。该公司将在多个 AWS 区域内的多个可用区中，在多个 Amazon EC2 实例上运行该应用程序。该应用程序将通过互联网提供。用户将从世界各地访问该应用程序。该公司希望确保将访问该应用程序的每个用户发送到离用户位置最近的 EC2 实例。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "实施 Amazon Route 53 地理位置路由策略。使用面向互联网的 Application Load Balancer 在同一区域内的所有可用区之间分配流量。",
      "B": "实施 Amazon Route 53 地理邻近路由策略。使用面向互联网的 Network Load Balancer 在同一区域内的所有可用区之间分配流量。",
      "C": "实施 Amazon Route 53 多值应答路由策略。使用面向互联网的 Application Load Balancer 在同一区域内的所有可用区之间分配流量。",
      "D": "实施 Amazon Route 53 加权路由策略。使用面向互联网的 Network Load Balancer 在同一区域内的所有可用区之间分配流量。"
    },
    "tags": [
      "Route 53",
      "EC2",
      "Network Load Balancer",
      "Application Load Balancer"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察 Route 53 的路由策略选择，以及负载均衡器的选择。要求将用户导向最近的 EC2 实例，需要使用地理位置路由。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 使用 Route 53 的地理邻近路由策略，可以将用户导向最近的 EC2 实例。同时，NLB 可以在同一区域的不同可用区之间分配流量，提供高可用性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，地理位置路由策略并不能精确选择最近的可用区。 ALB 只能在单个区域内使用。 选项 C 错误，多值应答路由策略不考虑用户位置。 选项 D 错误，加权路由策略不考虑用户位置。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Route 53",
      "EC2",
      "Network Load Balancer",
      "Application Load Balancer",
      "地理位置路由",
      "地理邻近路由",
      "加权路由",
      "可用区",
      "多值应答路由"
    ]
  },
  {
    "id": 977,
    "topic": "1",
    "question_en": "A financial services company plans to launch a new application on AWS to handle sensitive financial transactions. The company will deploy the application on Amazon EC2 instances. The company will use Amazon RDS for MySQL as the database. The company’s security policies mandate that data must be encrypted at rest and in transit. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys. Configure AWS Certificate Manager (ACM) SSL/TLS certificates for encryption in transit.",
      "B": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys. Configure IPsec tunnels for encryption in transit.",
      "C": "Implement third-party application-level data encryption before storing data in Amazon RDS for MySQL. Configure AWS Certificate Manager (ACM) SSL/TLS certificates for encryption in transit.",
      "D": "Configure encryption at rest for Amazon RDS for MySQL by using AWS KMS managed keys. Configure a VPN connection to enable private connectivity to encrypt data in transit."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家金融服务公司计划在 AWS 上启动一个新应用程序来处理敏感的金融交易。该公司将在 Amazon EC2 实例上部署该应用程序。该公司将使用 Amazon RDS for MySQL 作为数据库。该公司的安全策略要求静态数据和传输中的数据都必须加密。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "通过使用 AWS KMS 托管密钥为 Amazon RDS for MySQL 配置静态加密。为传输中的加密配置 AWS Certificate Manager (ACM) SSL/TLS 证书。",
      "B": "通过使用 AWS KMS 托管密钥为 Amazon RDS for MySQL 配置静态加密。为传输中的加密配置 IPsec 隧道。",
      "C": "在将数据存储在 Amazon RDS for MySQL 之前，实现第三方应用程序级数据加密。为传输中的加密配置 AWS Certificate Manager (ACM) SSL/TLS 证书。",
      "D": "通过使用 AWS KMS 托管密钥为 Amazon RDS for MySQL 配置静态加密。配置 VPN 连接以实现私有连接来加密传输中的数据。"
    },
    "tags": [
      "RDS",
      "MySQL",
      "KMS",
      "SSL/TLS",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察 RDS 数据库的加密和传输过程中的加密。要求静态和传输中的数据都要加密，并以最小的运营开销实现。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，使用 KMS 托管密钥加密 RDS for MySQL 的静态数据。使用 ACM SSL/TLS 证书加密传输中的数据，满足加密需求，且开销最小。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，IPsec 隧道虽然可以加密传输中的数据，但配置和维护的开销高于 ACM SSL/TLS。 选项 C 错误，第三方应用程序级数据加密增加了复杂性，不符合最小运营开销的要求。 选项 D 错误，VPN 连接虽然能加密传输中的数据，但需要配置和管理，开销高于 ACM SSL/TLS。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "MySQL",
      "KMS",
      "SSL/TLS",
      "VPN",
      "ACM",
      "静态加密",
      "传输中加密",
      "IPsec"
    ]
  },
  {
    "id": 978,
    "topic": "1",
    "question_en": "A company is migrating its on-premises Oracle database to an Amazon RDS for Oracle database. The company needs to retain data for 90 days to meet regulatory requirements. The company must also be able to restore the database to a specific point in time for up to 14 days. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create Amazon RDS automated backups. Set the retention period to 90 days.",
      "B": "Create an Amazon RDS manual snapshot every day. Delete manual snapshots that are older than 90 days.",
      "C": "Use the Amazon Aurora Clone feature for Oracle to create a point-in-time restore. Delete clones that are older than 90 days.",
      "D": "Create a backup plan that has a retention period of 90 days by using AWS Backup for Amazon RDS."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司正在将其本地 Oracle 数据库迁移到 Amazon RDS for Oracle 数据库。该公司需要保留 90 天的数据以满足监管要求。该公司还必须能够将数据库恢复到长达 14 天的特定时间点。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "创建 Amazon RDS 自动备份。将保留期设置为 90 天。",
      "B": "每天创建 Amazon RDS 手动快照。删除 90 天以上的手动快照。",
      "C": "使用 Amazon Aurora Clone 功能创建 Oracle 的时间点还原。删除 90 天以上的克隆。",
      "D": "使用 AWS Backup for Amazon RDS 创建一个备份计划，该计划的保留期为 90 天。"
    },
    "tags": [
      "RDS",
      "Oracle",
      "备份",
      "保留期"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察 RDS 的备份和恢复策略。需要满足 90 天的数据保留期和 14 天的 PITR（Point-in-Time-Recovery）要求，并最小化运营开销。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，自动备份可满足 90 天的保留期，也支持时间点恢复，并且是 RDS 的默认备份机制，运营开销最低。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，手动快照无法满足 90 天保留期，只能通过手动删除快照来维护，且无法进行时间点恢复。 选项 C 错误，Aurora Clone 不适用于 Oracle 数据库，且无法满足 90 天的保留要求。 选项 D 错误，AWS Backup 可以管理 RDS 备份，但其设置不如 RDS 自动备份简洁，成本略高，且也支持PITR，但并不比自动备份好。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "Oracle",
      "备份",
      "快照",
      "AWS Backup",
      "保留期",
      "Aurora Clone",
      "PITR"
    ]
  },
  {
    "id": 979,
    "topic": "1",
    "question_en": "A company is developing a new application that uses a relational database to store user data and application configurations. The company expects the application to have steady user growth. The company expects the database usage to be variable and read-heavy, with occasional writes. The company wants to cost-optimize the database solution. The company wants to use an AWS managed database solution that will provide the necessary performance. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Deploy the database on Amazon RDS. Use Provisioned IOPS SSD storage to ensure consistent performance for read and write operations.",
      "B": "Deploy the database on Amazon Aurora Serverless to automatically scale the database capacity based on actual usage to accommodate the workload.",
      "C": "Deploy the database on Amazon DynamoDB. Use on-demand capacity mode to automatically scale throughput to accommodate the workload.",
      "D": "Deploy the database on Amazon RDS. Use magnetic storage and use read replicas to accommodate the workload."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在开发一个新应用程序，该应用程序使用关系数据库来存储用户数据和应用程序配置。该公司预计该应用程序将实现稳定的用户增长。该公司预计数据库使用量将是可变的，并且以读取为主，偶尔会进行写入。该公司希望对数据库解决方案进行成本优化。该公司希望使用 AWS 托管数据库解决方案，以提供所需的性能。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在 Amazon RDS 上部署数据库。使用预置 IOPS SSD 存储，以确保读取和写入操作的持续性能。",
      "B": "在 Amazon Aurora Serverless 上部署数据库，以根据实际使用情况自动扩展数据库容量，从而适应工作负载。",
      "C": "在 Amazon DynamoDB 上部署数据库。使用按需容量模式来自动扩展吞吐量，以适应工作负载。",
      "D": "在 Amazon RDS 上部署数据库。使用磁性存储并使用只读副本以适应工作负载。"
    },
    "tags": [
      "RDS",
      "Aurora",
      "DynamoDB",
      "成本优化"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察 RDS 数据库的成本优化方案，主要考量点是数据库的弹性伸缩。 数据库预计稳定增长，以读取为主，偶尔写入， 需要具备成本效益。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，Aurora Serverless 可以根据实际使用情况自动扩展数据库容量，满足弹性需求，且成本效益高。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，RDS 虽然提供了 IOPS，但无法自动伸缩，成本较高。 选项 C 错误，DynamoDB 虽然弹性，但存储关系型数据库数据不适用，且不符合关系数据库的需求。 选项 D 错误，RDS 只读副本可以提高读取性能，但无法自动伸缩，且成本相对较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "Aurora",
      "DynamoDB",
      "只读副本",
      "Serverless",
      "成本优化",
      "弹性伸缩",
      "预置 IOPS"
    ]
  },
  {
    "id": 980,
    "topic": "1",
    "question_en": "A company hosts its application on several Amazon EC2 instances inside a VPC. The company creates a dedicated Amazon S3 bucket for each customer to store their relevant information in Amazon S3. The company wants to ensure that the application running on EC2 instances can securely access only the S3 buckets that belong to the company’s AWS account. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Create a gateway endpoint for Amazon S3 that is attached to the VPC. Update the IAM instance profile policy to provide access to only the specific buckets that the application needs.",
      "B": "Create a NAT gateway in a public subnet with a security group that allows access to only Amazon S3. Update the route tables to use the NAT Gateway.",
      "C": "Create a gateway endpoint for Amazon S3 that is attached to the VPUpdate the IAM instance profile policy with a Deny action and the following condition key:",
      "D": "Create a NAT Gateway in a public subnet. Update route tables to use the NAT Gateway. Assign bucket policies for all buckets with a Deny action and the following condition key:"
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在其 VPC 内部的多个 Amazon EC2 实例上托管其应用程序。该公司为每个客户创建一个专用的 Amazon S3 存储桶，以在 Amazon S3 中存储其相关信息。该公司希望确保在 EC2 实例上运行的应用程序只能安全地访问属于该公司 AWS 账户的 S3 存储桶。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "为 Amazon S3 创建一个附加到 VPC 的网关 endpoint。更新 IAM 实例配置文件策略，仅提供对应用程序所需的特定存储桶的访问权限。",
      "B": "在公共子网中创建一个 NAT Gateway，其安全组仅允许访问 Amazon S3。更新路由表以使用 NAT Gateway。",
      "C": "为 Amazon S3 创建一个附加到 VPC 的网关 endpoint。使用 Deny 操作和以下条件密钥更新 IAM 实例配置文件策略：",
      "D": "在公共子网中创建一个 NAT Gateway。更新路由表以使用 NAT Gateway。为所有存储桶分配具有 Deny 操作和以下条件密钥的存储桶策略："
    },
    "tags": [
      "S3",
      "VPC",
      "IAM",
      "网关 endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何让 EC2 实例安全地访问 S3 存储桶，同时控制访问权限。 关键是使用 VPC endpoint 和 IAM 策略控制访问。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，使用 VPC 网关 endpoint 让 EC2 实例通过 VPC 访问 S3，避免了流量经过互联网。 IAM 实例配置文件策略控制访问特定存储桶，实现了最小权限原则。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，NAT Gateway 提供了对互联网的访问，而不是通过 VPC 内部访问 S3，不安全。 选项 C 错误，该策略无法控制实例对S3 的访问。 选项 D 错误，该策略是针对 S3 存储桶的，而不是针对 EC2 实例的。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "VPC",
      "IAM",
      "NAT Gateway",
      "网关 endpoint",
      "IAM 实例配置文件",
      "存储桶策略"
    ]
  },
  {
    "id": 981,
    "topic": "1",
    "question_en": "A company is building a cloud-based application on AWS that will handle sensitive customer data. The application uses Amazon RDS for the database, Amazon S3 for object storage, and S3 Event Notifications that invoke AWS Lambda for serverless processing. The company uses AWS IAM Identity Center to manage user credentials. The development, testing, and operations teams need secure access to Amazon RDS and Amazon S3 while ensuring the confidentiality of sensitive customer data. The solution must comply with the principle of least privilege. Which solution meets these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use IAM roles with least privilege to grant all the teams access. Assign IAM roles to each team with customized IAM policies defining specific permission for Amazon RDS and S3 object access based on team responsibilities.",
      "B": "Enable IAM Identity Center with an Identity Center directory. Create and configure permission sets with granular access to Amazon RDS and Amazon S3. Assign all the teams to groups that have specific access with the permission sets.",
      "C": "Create individual IAM users for each member in all the teams with role-based permissions. Assign the IAM roles with predefined policies for RDS and S3 access to each user based on user needs. Implement IAM Access Analyzer for periodic credential evaluation.",
      "D": "Use AWS Organizations to create separate accounts for each team. Implement cross-account IAM roles with least privilege. Grant specific permission for RDS and S3 access based on team roles and responsibilities."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上构建一个基于云的应用程序，该应用程序将处理敏感的客户数据。该应用程序使用 Amazon RDS 作为数据库，Amazon S3 作为对象存储，并使用 S3 事件通知来调用 AWS Lambda 进行无服务器处理。该公司使用 AWS IAM Identity Center 来管理用户凭证。开发、测试和运维团队需要安全地访问 Amazon RDS 和 Amazon S3，同时确保敏感客户数据的机密性。该解决方案必须遵循最小权限原则。哪个解决方案以最少的运维开销满足这些要求？",
    "options_cn": {
      "A": "使用具有最小权限的 IAM 角色授予所有团队访问权限。为每个团队分配 IAM 角色，并使用自定义的 IAM 策略根据团队的职责定义对 Amazon RDS 和 S3 对象访问的特定权限。",
      "B": "使用 Identity Center 目录启用 IAM Identity Center。创建和配置具有对 Amazon RDS 和 Amazon S3 细粒度访问权限的权限集。将所有团队分配给具有权限集的特定访问权限的组。",
      "C": "为所有团队中的每个成员创建单独的 IAM 用户，并使用基于角色的权限。根据用户的需求，将具有预定义 RDS 和 S3 访问策略的 IAM 角色分配给每个用户。实施 IAM Access Analyzer 用于定期凭证评估。",
      "D": "使用 AWS Organizations 为每个团队创建单独的账户。实施跨账户 IAM 角色，并使用最小权限。根据团队的角色和职责，授予 RDS 和 S3 访问的特定权限。"
    },
    "tags": [
      "IAM Identity Center",
      "RDS",
      "S3",
      "IAM",
      "最小权限"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何在确保安全性的前提下，让开发、测试和运维团队访问 RDS 和 S3，同时遵循最小权限原则。 使用 IAM Identity Center 和权限集是最优解。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，使用 IAM Identity Center，创建具有对 RDS 和 S3 细粒度访问权限的权限集，并将团队分配到不同的权限集中，满足了最小权限原则，降低了运维开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，虽然使用 IAM 角色，但操作配置略为复杂，且需要管理每个团队的 IAM 角色。 选项 C 错误，为每个成员创建单独的 IAM 用户，管理复杂，且无法满足最小权限原则。 选项 D 错误，使用 AWS Organizations 增加了复杂性，且跨账户的角色配置也增加了运维成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM Identity Center",
      "RDS",
      "S3",
      "IAM",
      "IAM 角色",
      "AWS Organizations",
      "最小权限",
      "权限集",
      "IAM Access Analyzer"
    ]
  },
  {
    "id": 982,
    "topic": "1",
    "question_en": "A company has an Amazon S3 bucket that contains sensitive data files. The company has an application that runs on virtual machines in an on-premises data center. The company currently uses AWS IAM Identity Center. The application requires temporary access to files in the S3 bucket. The company wants to grant the application secure access to the files in the S3 bucket. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an S3 bucket policy that permits access to the bucket from the public IP address range of the company’s on-premises data center.",
      "B": "Use IAM Roles Anywhere to obtain security credentials in IAM Identity Center that grant access to the S3 bucket. Configure the virtual machines to assume the role by using the AWS CLI.",
      "C": "Install the AWS CLI on the virtual machine. Configure the AWS CLI with access keys from an IAM user that has access to the bucket.",
      "D": "Create an IAM user and policy that grants access to the bucket. Store the access key and secret key for the IAM user in AWS Secrets Manager. Configure the application to retrieve the access key and secret key at startup."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司有一个 Amazon S3 存储桶，其中包含敏感数据文件。该公司有一个应用程序，该应用程序在本地数据中心的虚拟机上运行。该公司目前正在使用 AWS IAM Identity Center。该应用程序需要临时访问 S3 存储桶中的文件。该公司希望授予该应用程序安全访问 S3 存储桶中文件的权限。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 S3 存储桶策略，允许从该公司本地数据中心的公共 IP 地址范围访问该存储桶。",
      "B": "使用 IAM Roles Anywhere 在 IAM Identity Center 中获取安全凭证，这些凭证授予对 S3 存储桶的访问权限。配置虚拟机使用 AWS CLI 担任该角色。",
      "C": "在虚拟机上安装 AWS CLI。使用有权访问该存储桶的 IAM 用户的访问密钥配置 AWS CLI。",
      "D": "创建一个 IAM 用户和策略，授予对该存储桶的访问权限。将该 IAM 用户的访问密钥和私有密钥存储在 AWS Secrets Manager 中。配置应用程序在启动时检索访问密钥和私有密钥。"
    },
    "tags": [
      "IAM Roles Anywhere",
      "S3",
      "IAM Identity Center",
      "AWS CLI"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何让本地应用程序安全地访问 S3 存储桶。 需要使用 IAM Roles Anywhere 临时获取凭证。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，使用 IAM Roles Anywhere 获取 IAM Identity Center 的安全凭证，授予对 S3 的访问权限，是安全且推荐的方式。 虚拟机使用 AWS CLI 承担 IAM 角色。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，S3 桶策略通过 IP 限制访问不安全，不推荐。 选项 C 错误，直接使用 IAM 用户访问密钥，不安全，不推荐。 选项 D 错误，IAM 用户访问密钥管理麻烦，且不安全。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "IAM Roles Anywhere",
      "S3",
      "IAM Identity Center",
      "AWS CLI",
      "IAM 用户",
      "访问密钥",
      "Secrets Manager",
      "S3 存储桶策略",
      "私有密钥"
    ]
  },
  {
    "id": 983,
    "topic": "1",
    "question_en": "A company hosts its core network services, including directory services and DNS, in its on-premises data center. The data center is connected to the AWS Cloud using AWS Direct Connect (DX). Additional AWS accounts are planned that will require quick, cost-effective, and consistent access to these network services. What should a solutions architect implement to meet these requirements with the LEAST amount of operational overhead?",
    "options_en": {
      "A": "Create a DX connection in each new account. Route the network trafic to the on-premises servers.",
      "B": "Configure VPC endpoints in the DX VPC for all required services. Route the network trafic to the on-premises servers.",
      "C": "Create a VPN connection between each new account and the DX VPRoute the network trafic to the on-premises servers.",
      "D": "Configure AWS Transit Gateway between the accounts. Assign DX to the transit gateway and route network trafic to the on-premises servers."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司在其本地数据中心托管其核心网络服务，包括目录服务和 DNS。该数据中心使用 AWS Direct Connect (DX) 连接到 AWS 云。计划添加额外的 AWS 账户，这些账户将需要快速、经济高效且一致地访问这些网络服务。解决方案架构师应该实施什么来满足这些要求，同时运营开销最少？",
    "options_cn": {
      "A": "在新账户中创建 DX 连接。将网络流量路由到本地服务器。",
      "B": "在 DX VPC 中为所有所需服务配置 VPC endpoint。将网络流量路由到本地服务器。",
      "C": "在新账户和 DX VPC 之间创建 VPN 连接。将网络流量路由到本地服务器。",
      "D": "在账户之间配置 AWS Transit Gateway。将 DX 分配给 Transit Gateway，并将网络流量路由到本地服务器。"
    },
    "tags": [
      "Direct Connect",
      "Transit Gateway"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察如何让多个 AWS 账户通过 Direct Connect 访问本地网络服务。 使用 Transit Gateway 可以简化网络连接。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确，使用 Transit Gateway，可以连接 Direct Connect，并将网络流量路由到本地服务器，简化了网络连接。 适用于多个账户。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，每个账户创建 DX 连接，成本高，管理复杂。 选项 B 错误，VPC endpoint 不用于访问本地服务。 选项 C 错误，VPN 连接需要额外的配置和管理，且不如 Transit Gateway 灵活。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Direct Connect",
      "Transit Gateway",
      "VPN",
      "VPC endpoint"
    ]
  },
  {
    "id": 984,
    "topic": "1",
    "question_en": "A company hosts its main public web application in one AWS Region across multiple Availability Zones. The application uses an Amazon EC2 Auto Scaling group and an Application Load Balancer (ALB). A web development team needs a cost-optimized compute solution to improve the company’s ability to serve dynamic content globally to millions of customers. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create an Amazon CloudFront distribution. Configure the existing ALB as the origin.",
      "B": "Use Amazon Route 53 to serve trafic to the ALB and EC2 instances based on the geographic location of each customer.",
      "C": "Create an Amazon S3 bucket with public read access enabled. Migrate the web application to the S3 bucket. Configure the S3 bucket for website hosting.",
      "D": "Use AWS Direct Connect to directly serve content from the web application to the location of each customer."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司将其主要的公共 Web 应用程序托管在多个可用区的一个 AWS 区域中。该应用程序使用 Amazon EC2 Auto Scaling 组和 Application Load Balancer (ALB)。一个 Web 开发团队需要一个成本优化的计算解决方案，以提高公司向全球数百万客户提供动态内容的能力。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon CloudFront 分发。将现有的 ALB 配置为源。",
      "B": "使用 Amazon Route 53 根据每个客户的地理位置向 ALB 和 EC2 实例提供流量。",
      "C": "创建一个具有公共读取访问权限的 Amazon S3 存储桶。将 Web 应用程序迁移到 S3 存储桶。将 S3 存储桶配置为网站托管。",
      "D": "使用 AWS Direct Connect 直接将内容从 Web 应用程序提供给每个客户的所在位置。"
    },
    "tags": [
      "CloudFront",
      "ALB",
      "EC2",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何优化 Web 应用程序的性能。 主要考察如何使用 CloudFront 和 ALB，从而优化成本。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确，使用 CloudFront 作为 CDN 缓存 Web 应用程序的内容，可以提高性能，并减轻 ALB 的负载，降低成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，Route 53 主要用于流量路由，不能提高性能。 选项 C 错误，将 Web 应用程序迁移到 S3 不适用，且不具备动态内容的处理能力。 选项 D 错误，Direct Connect 用于建立与 AWS 的专用连接，不能提高 Web 应用程序的性能。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "CloudFront",
      "ALB",
      "EC2",
      "S3",
      "CDN",
      "Route 53",
      "Direct Connect"
    ]
  },
  {
    "id": 985,
    "topic": "1",
    "question_en": "A company stores user data in AWS. The data is used continuously with peak usage during business hours. Access patterns vary, with some data not being used for months at a time. A solutions architect must choose a cost-effective solution that maintains the highest level of durability while maintaining high availability. Which storage solution meets these requirements?",
    "options_en": {
      "A": "Amazon S3 Standard",
      "B": "Amazon S3 Intelligent-Tiering",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "Amazon S3 One Zone-Infrequent Access (S3 One Zone-IA)"
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 中存储用户数据。这些数据被持续使用，并在工作时间内达到峰值使用量。访问模式各不相同，有些数据几个月都不会被使用。一位解决方案架构师必须选择一个具有成本效益的解决方案，该方案在保持最高级别的耐用性的同时，还要保持高可用性。哪种存储解决方案符合这些要求？",
    "options_cn": {
      "A": "Amazon S3 标准",
      "B": "Amazon S3 智能分层",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "Amazon S3 单区-不频繁访问 (S3 One Zone-IA)"
    },
    "tags": [
      "S3",
      "存储",
      "成本优化"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察 S3 存储类的选择，需要根据数据的使用模式进行选择。  需要考虑数据的使用频率，以及对耐用性的要求。 S3 智能分层成本更低，并且能根据数据访问频率自动调整。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，S3 智能分层会根据数据访问频率在不同存储层之间自动切换，实现了成本和性能的平衡。 适用于访问模式不确定的数据。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，S3 标准存储成本较高。 选项 C 错误，S3 Glacier Deep Archive 的访问延迟高，不适用于频繁使用的数据。 选项 D 错误，S3 One Zone-IA 的耐用性不如其他存储类，且不适合高可用性要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "S3 标准",
      "S3 智能分层",
      "S3 Glacier Deep Archive",
      "可用性",
      "存储",
      "成本优化",
      "S3 One Zone-IA",
      "耐用性"
    ]
  },
  {
    "id": 986,
    "topic": "1",
    "question_en": "A company is testing an application that runs on an Amazon EC2 Linux instance. A single 500 GB Amazon Elastic Block Store (Amazon EBS) General Purpose SSO (gp2) volume is attached to the EC2 instance. The company will deploy the application on multiple EC2 instances in an Auto Scaling group. All instances require access to the data that is stored in the EBS volume. The company needs a highly available and resilient solution that does not introduce significant changes to the application's code. Which solution will meet these requirements?",
    "options_en": {
      "A": "Provision an EC2 instance that uses NFS server software. Attach a single 500 GB gp2 EBS volume to the instance.",
      "B": "Provision an Amazon FSx for Windows File Server file system. Configure the file system as an SMB file store within a single Availability Zone.",
      "C": "Provision an EC2 instance with two 250 GB Provisioned IOPS SSD EBS volumes.",
      "D": "Provision an Amazon Elastic File System (Amazon EFS) file system. Configure the file system to use General Purpose performance mode."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司正在测试一个在 Amazon EC2 Linux 实例上运行的应用程序。一个 500 GB Amazon Elastic Block Store (Amazon EBS) 通用型 SSD (gp2) 卷已附加到 EC2 实例。该公司将在 Auto Scaling 组中的多个 EC2 实例上部署该应用程序。所有实例都需要访问存储在 EBS 卷中的数据。该公司需要一个高可用性和弹性的解决方案，该方案不会对应用程序的代码进行重大更改。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "预置一个使用 NFS 服务器软件的 EC2 实例。将一个 500 GB gp2 EBS 卷附加到该实例。",
      "B": "预置一个 Amazon FSx for Windows File Server 文件系统。将该文件系统配置为单个可用区内的 SMB 文件存储。",
      "C": "预置一个带有两个 250 GB 预置 IOPS SSD EBS 卷的 EC2 实例。",
      "D": "预置一个 Amazon Elastic File System (Amazon EFS) 文件系统。将该文件系统配置为使用通用性能模式。"
    },
    "tags": [
      "EFS",
      "EC2",
      "EBS",
      "Auto Scaling"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察如何在多个 EC2 实例之间共享存储。 Amazon EFS 提供了一个可扩展的文件系统，可以被多个 EC2 实例访问。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确，EFS 提供了一个 NFS 文件系统，可以被多个 EC2 实例共享，可以满足高可用性，且能自动扩展。 EFS 适合在多个实例之间共享文件。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，在 EC2 实例上运行 NFS 服务器，不易管理，且无法自动扩展。 选项 B 错误，FSx for Windows File Server 不适用于 Linux 实例。 选项 C 错误，使用 EBS 卷无法在多个 EC2 实例之间共享存储。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EFS",
      "EC2",
      "EBS",
      "Auto Scaling",
      "NFS",
      "FSx for Windows File Server",
      "共享存储"
    ]
  },
  {
    "id": 987,
    "topic": "1",
    "question_en": "A company recently launched a new application for its customers. The application runs on multiple Amazon EC2 instances across two Availability Zones. End users use TCP to communicate with the application. The application must be highly available and must automatically scale as the number of users increases. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "options_en": {
      "A": "Add a Network Load Balancer in front of the EC2 instances.",
      "B": "Configure an Auto Scaling group for the EC2 instances.",
      "C": "Add an Application Load Balancer in front of the EC2 instances.",
      "D": "Manually add more EC2 instances for the application",
      "E": "Add a Gateway Load Balancer in front of the EC2 instances."
    },
    "correct_answer": "BC",
    "vote_percentage": "",
    "question_cn": "一家公司最近为其客户推出了一个新应用程序。该应用程序在两个可用区中的多个 Amazon EC2 实例上运行。最终用户使用 TCP 与该应用程序通信。该应用程序必须具有高可用性，并且必须随着用户数量的增加而自动扩展。哪种步骤组合将以最具成本效益的方式满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 EC2 实例前面添加 Network Load Balancer。",
      "B": "为 EC2 实例配置 Auto Scaling 组。",
      "C": "在 EC2 实例前面添加 Application Load Balancer (ALB)。",
      "D": "手动为应用程序添加更多 EC2 实例。",
      "E": "在 EC2 实例前面添加 Gateway Load Balancer。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "ALB",
      "NLB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BC（社区 —），解析仅供参考。】\n\n此题考察如何以最具成本效益的方式实现高可用性和自动伸缩。 对于 TCP 应用程序，需要使用负载均衡器，并且需要配置 Auto Scaling 组。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BC。理由简述：选项 A 和 B 正确， NLB 可以实现高可用性，Auto Scaling 组可以根据负载自动扩展 EC2 实例。 这两个组合能满足需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 C 错误，ALB 适合 HTTP/HTTPS 协议，不适用于 TCP。 选项 D 错误，手动添加 EC2 实例，无法实现自动扩展。 选项 E 错误，Gateway Load Balancer 增加了额外的复杂性，不具备成本效益。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "ALB",
      "NLB",
      "TCP",
      "Gateway Load Balancer"
    ]
  },
  {
    "id": 988,
    "topic": "1",
    "question_en": "A company is designing the architecture for a new mobile app that uses the AWS Cloud. The company uses organizational units (OUs) in AWS Organizations to manage its accounts. The company wants to tag Amazon EC2 instances with data sensitivity by using values of sensitive and nonsensitive. IAM identities must not be able to delete a tag or create instances without a tag. Which combination of steps will meet these requirements? (Choose two.)",
    "options_en": {
      "A": "In Organizations, create a new tag policy that specifies the data sensitivity tag key and the required values. Enforce the tag values for the EC2 instances. Attach the tag policy to the appropriate OU.",
      "B": "In Organizations, create a new service control policy (SCP) that specifies the data sensitivity tag key and the required tag values. Enforce the tag values for the EC2 instances. Attach the SCP to the appropriate OU.",
      "C": "Create a tag policy to deny running instances when a tag key is not specified. Create another tag policy that prevents identities from deleting tags. Attach the tag policies to the appropriate OU.",
      "D": "Create a service control policy (SCP) to deny creating instances when a tag key is not specified. Create another SCP that prevents identities from deleting tags. Attach the SCPs to the appropriate OU",
      "E": "Create an AWS Config rule to check if EC2 instances use the data sensitivity tag and the specified values. Configure an AWS Lambda function to delete the resource if a noncompliant resource is found."
    },
    "correct_answer": "AD",
    "vote_percentage": "",
    "question_cn": "一家公司正在为其使用 AWS 云的新移动应用程序设计架构。该公司在 AWS Organizations 中使用组织单元 (OU) 来管理其账户。该公司希望通过使用敏感和非敏感值对 Amazon EC2 实例进行数据敏感度标记。IAM 身份不能删除标签或在没有标签的情况下创建实例。哪种步骤组合将满足这些要求？（选择两个。）",
    "options_cn": {
      "A": "在 Organizations 中，创建一个新的标签策略，该策略指定数据敏感度标签键和所需的值。对 EC2 实例强制执行标签值。将标签策略附加到相应的 OU。",
      "B": "在 Organizations 中，创建一个新的服务控制策略 (SCP)，该策略指定数据敏感度标签键和所需的标签值。对 EC2 实例强制执行标签值。将 SCP 附加到相应的 OU。",
      "C": "创建标签策略以拒绝在未指定标签键时运行实例。创建另一个标签策略以防止身份删除标签。将标签策略附加到相应的 OU。",
      "D": "创建服务控制策略 (SCP) 以拒绝在未指定标签键时创建实例。创建另一个 SCP 以防止身份删除标签。将 SCP 附加到相应的 OU。",
      "E": "创建 AWS Config 规则以检查 EC2 实例是否使用数据敏感度标签和指定的值。配置 AWS Lambda 函数以在找到不合规的资源时删除该资源。"
    },
    "tags": [
      "Organizations",
      "SCP",
      "标签策略",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AD（社区 —），解析仅供参考。】\n\n此题考察在 AWS Organizations 中如何强制对 EC2 实例进行标签管理。 需要使用组织单元 (OU)，并使用 SCP 或 标签策略。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AD。理由简述：选项 A 正确，通过标签策略强制对 EC2 实例使用标签。 在 Organizations 中，创建一个新的标签策略，指定所需的标签键和值，并将策略附加到 OU，可以强制实施标签管理。 并且 IAM 身份不能删除标签或在没有标签的情况下创建实例。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误，SCP 限制了 IAM 身份可以执行的操作，但不能直接管理标签。 选项 C 错误，多个标签策略管理起来比较复杂，容易出错。 选项 D 错误，SCP 无法管理标签。 选项 E 错误，AWS Config 用于合规性检查，不能强制执行标签。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SCP",
      "EC2",
      "OU",
      "IAM",
      "AWS Config",
      "Organizations",
      "标签策略"
    ]
  },
  {
    "id": 989,
    "topic": "1",
    "question_en": "A company runs database workloads on AWS that are the backend for the company's customer portals. The company runs a Multi-AZ database cluster on Amazon RDS for PostgreSQL. The company needs to implement a 30-day backup retention policy. The company currently has both automated RDS backups and manual RDS backups. The company wants to maintain both types of existing RDS backups that are less than 30 days old. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure the RDS backup retention policy to 30 days for automated backups by using AWS Backup. Manually delete manual backups that are older than 30 days.",
      "B": "Disable RDS automated backups. Delete automated backups and manual backups that are older than 30 days. Configure the RDS backup retention policy to 30 days for automated backups.",
      "C": "Configure the RDS backup retention policy to 30 days for automated backups. Manually delete manual backups that are older than 30 days.",
      "D": "Disable RDS automated backups. Delete automated backups and manual backups that are older than 30 days automatically by using AWS CloudFormation. Configure the RDS backup retention policy to 30 days for automated backups."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 上运行数据库工作负载，这些工作负载是该公司客户门户网站的后端。该公司在 Amazon RDS for PostgreSQL 上运行 Multi-AZ 数据库集群。该公司需要实施 30 天的备份保留策略。该公司目前同时拥有自动 RDS 备份和手动 RDS 备份。该公司希望保留现有类型的、不到 30 天的 RDS 备份。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Backup 将 RDS 备份保留策略配置为自动备份 30 天。手动删除超过 30 天的手动备份。",
      "B": "禁用 RDS 自动备份。删除超过 30 天的自动备份和手动备份。将 RDS 备份保留策略配置为自动备份 30 天。",
      "C": "将 RDS 备份保留策略配置为自动备份 30 天。手动删除超过 30 天的手动备份。",
      "D": "禁用 RDS 自动备份。使用 AWS CloudFormation 自动删除超过 30 天的自动备份和手动备份。将 RDS 备份保留策略配置为自动备份 30 天。"
    },
    "tags": [
      "RDS",
      "PostgreSQL",
      "AWS Backup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察 RDS 备份保留策略的配置和成本优化。 RDS 备份分为自动备份和手动备份，需要保留 30 天的备份。AWS Backup 可以统一管理不同服务的备份，并配置保留策略。正确答案结合了 RDS 自动备份和 AWS Backup 管理手动备份，可以以较低成本满足需求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。将 RDS 备份保留策略配置为 30 天，并手动删除超过 30 天的手动备份，满足了保留 30 天备份的需求，且成本最低，因为只使用了 RDS 提供的备份功能和手动删除。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。使用 AWS Backup 管理自动备份是不必要的额外开销，且手动删除备份比较麻烦；选项 B 错误。禁用自动备份以及手动删除备份，并不会提升成本效益，反而增加了管理成本；选项 D 错误。 禁用自动备份并且使用 CloudFormation 自动删除备份，复杂化了解决方案，增加了运维成本，不如使用 RDS 备份保留策略简单直接。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "PostgreSQL",
      "AWS Backup",
      "CloudFormation"
    ]
  },
  {
    "id": 990,
    "topic": "1",
    "question_en": "A company is planning to migrate a legacy application to AWS. The application currently uses NFS to communicate to an on-premises storage solution to store application data. The application cannot be modified to use any other communication protocols other than NFS for this purpose. Which storage solution should a solutions architect recommend for use after the migration?",
    "options_en": {
      "A": "AWS DataSync",
      "B": "Amazon Elastic Block Store (Amazon EBS)",
      "C": "Amazon Elastic File System (Amazon EFS)",
      "D": "Amazon EMR File System (Amazon EMRFS)"
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司计划将传统应用程序迁移到 AWS。该应用程序当前使用 NFS 与本地存储解决方案通信以存储应用程序数据。 为了此目的，该应用程序无法修改为使用 NFS 以外的任何其他通信协议。 解决方案架构师应该为迁移后使用推荐哪种存储解决方案？",
    "options_cn": {
      "A": "AWS DataSync",
      "B": "Amazon Elastic Block Store (Amazon EBS)",
      "C": "Amazon Elastic File System (Amazon EFS)",
      "D": "Amazon EMR File System (Amazon EMRFS)"
    },
    "tags": [
      "EFS",
      "NFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察存储解决方案的选择，关键在于应用程序需要使用 NFS 协议。 只有 Amazon EFS 支持 NFS 协议，因此 EFS 是最佳选择。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。 Amazon EFS 提供了与 NFS 兼容的文件系统，可以满足应用程序对 NFS 协议的需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 AWS DataSync 用于数据迁移，而不是长期存储；选项 B 错误。 Amazon EBS 是块存储，不提供文件系统和 NFS 支持；选项 D 错误。 Amazon EMRFS 是专为 Amazon EMR 设计的文件系统，不适合一般应用程序。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "NFS",
      "EFS",
      "DataSync",
      "EBS",
      "EMRFS"
    ]
  },
  {
    "id": 991,
    "topic": "1",
    "question_en": "A company uses GPS trackers to document the migration patterns of thousands of sea turtles. The trackers check every 5 minutes to see if a turtle has moved more than 100 yards (91.4 meters). If a turtle has moved, its tracker sends the new coordinates to a web application running on three Amazon EC2 instances that are in multiple Availability Zones in one AWS Region. Recently, the web application was overwhelmed while processing an unexpected volume of tracker data. Data was lost with no way to replay the events. A solutions architect must prevent this problem from happening again and needs a solution with the least operational overhead. What should the solutions architect do to meet these requirements?",
    "options_en": {
      "A": "Create an Amazon S3 bucket to store the data. Configure the application to scan for new data in the bucket for processing.",
      "B": "Create an Amazon API Gateway endpoint to handle transmitted location coordinates. Use an AWS Lambda function to process each item concurrently.",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) queue to store the incoming data. Configure the application to poll for new messages for processing.",
      "D": "Create an Amazon DynamoDB table to store transmitted location coordinates. Configure the application to query the table for new data for processing. Use TTL to remove data that has been processed."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司使用 GPS 跟踪器记录数千只海龟的迁徙模式。 跟踪器每 5 分钟检查一次，看看海龟是否移动了超过 100 码（91.4 米）。 如果海龟移动了，它的跟踪器会将新的坐标发送到在位于一个 AWS 区域中的多个可用区内的三个 Amazon EC2 实例上运行的 Web 应用程序。 最近，Web 应用程序在处理意外的大量跟踪器数据时不堪重负。 数据丢失且无法重放事件。 解决方案架构师必须防止此问题再次发生，并需要一个运营开销最少的解决方案。 解决方案架构师应该怎么做才能满足这些要求？",
    "options_cn": {
      "A": "创建一个 Amazon S3 存储桶来存储数据。 配置应用程序以扫描存储桶中的新数据进行处理。",
      "B": "创建一个 Amazon API Gateway 端点来处理传输的位置坐标。 使用一个 AWS Lambda 函数来并行处理每个项目。",
      "C": "创建一个 Amazon Simple Queue Service (Amazon SQS) 队列来存储传入数据。 配置应用程序以轮询新消息以进行处理。",
      "D": "创建一个 Amazon DynamoDB 表来存储传输的位置坐标。 配置应用程序以查询该表以获取要处理的新数据。 使用 TTL 删除已处理的数据。"
    },
    "tags": [
      "SQS",
      "EC2",
      "S3",
      "Lambda",
      "DynamoDB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察高吞吐量数据处理的解决方案，要求低运营开销和防止数据丢失。Amazon SQS 作为消息队列可以很好地处理高并发场景下的异步处理需求，解耦了生产者和消费者。其他选项要么实现复杂，要么不具备扩展性或存在数据丢失风险。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。 SQS 队列可以缓冲传入数据，应用程序从队列中获取消息进行处理，从而防止数据丢失，并实现异步处理，降低运营开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 应用程序直接扫描 S3 存储桶，会增加延迟和复杂性；选项 B 错误。 使用 API Gateway 和 Lambda 函数会导致额外的开销和潜在的并发限制；选项 D 错误。  使用 DynamoDB 存储数据，并使用 TTL 删除数据，可能导致数据丢失，并且查询效率不高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "S3",
      "API Gateway",
      "Lambda",
      "SQS",
      "DynamoDB",
      "TTL",
      "GPS"
    ]
  },
  {
    "id": 992,
    "topic": "1",
    "question_en": "A company's software development team needs an Amazon RDS Multi-AZ cluster. The RDS cluster will serve as a backend for a desktop client that is deployed on premises. The desktop client requires direct connectivity to the RDS cluster. The company must give the development team the ability to connect to the cluster by using the client when the team is in the ofice. Which solution provides the required connectivity MOST securely?",
    "options_en": {
      "A": "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Use AWS Site-to-Site VPN with a customer gateway in the company's ofice.",
      "B": "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use AWS Site-to-Site VPN with a customer gateway in the company's ofice.",
      "C": "Create a VPC and two private subnets. Create the RDS cluster in the private subnets. Use RDS security groups to allow the company's ofice IP ranges to access the cluster.",
      "D": "Create a VPC and two public subnets. Create the RDS cluster in the public subnets. Create a cluster user for each developer. Use RDS security groups to allow the users to access the cluster."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司的软件开发团队需要一个 Amazon RDS Multi-AZ 集群。 RDS 集群将作为部署在本地的桌面客户端的后端。 桌面客户端需要直接连接到 RDS 集群。 公司必须让开发团队能够在团队在办公室时使用客户端连接到集群。 哪种解决方案以最安全的方式提供所需的连接？",
    "options_cn": {
      "A": "创建一个 VPC 和两个公有子网。 在公有子网中创建 RDS 集群。 使用 AWS Site-to-Site VPN，并在公司的办公室中使用一个客户网关。",
      "B": "创建一个 VPC 和两个私有子网。 在私有子网中创建 RDS 集群。 使用 AWS Site-to-Site VPN，并在公司的办公室中使用一个客户网关。",
      "C": "创建一个 VPC 和两个私有子网。 在私有子网中创建 RDS 集群。 使用 RDS 安全组允许公司的办公室 IP 范围访问集群。",
      "D": "创建一个 VPC 和两个公有子网。 在公有子网中创建 RDS 集群。 为每个开发人员创建一个集群用户。 使用 RDS 安全组允许用户访问集群。"
    },
    "tags": [
      "RDS",
      "VPN",
      "VPC"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察安全地连接到 RDS 数据库。需要在私有子网中部署 RDS 实例，并使用 VPN 实现本地办公网络和 VPC 的连接，从而保证安全性。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确。 在私有子网中部署 RDS 集群，并使用 AWS Site-to-Site VPN 提供安全连接，满足了安全需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。使用公有子网会导致数据库暴露在公网，安全性降低。；选项 C 错误。 虽然使用私有子网，但是安全组配置不安全；选项 D 错误。使用公有子网导致安全性降低，并且配置不合理。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "RDS",
      "VPC",
      "Site-to-Site VPN",
      "安全组"
    ]
  },
  {
    "id": 993,
    "topic": "1",
    "question_en": "A solutions architect is creating an application that will handle batch processing of large amounts of data. The input data will be held in Amazon S3 and the output data will be stored in a different S3 bucket. For processing, the application will transfer the data over the network between multiple Amazon EC2 instances. What should the solutions architect do to reduce the overall data transfer costs?",
    "options_en": {
      "A": "Place all the EC2 instances in an Auto Scaling group.",
      "B": "Place all the EC2 instances in the same AWS Region.",
      "C": "Place all the EC2 instances in the same Availability Zone.",
      "D": "Place all the EC2 instances in private subnets in multiple Availability Zones."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一位解决方案架构师正在创建一个应用程序，该应用程序将处理大量数据的批处理。 输入数据将保存在 Amazon S3 中，输出数据将存储在另一个 S3 存储桶中。 为了进行处理，应用程序将在多个 Amazon EC2 实例之间通过网络传输数据。 解决方案架构师应该怎么做才能减少总体数据传输成本？",
    "options_cn": {
      "A": "将所有 EC2 实例放置在 Auto Scaling 组中。",
      "B": "将所有 EC2 实例放置在相同的 AWS 区域中。",
      "C": "将所有 EC2 实例放置在相同的可用区中。",
      "D": "将所有 EC2 实例放置在多个可用区的私有子网中。"
    },
    "tags": [
      "EC2",
      "可用区"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察如何优化 EC2 实例之间的数据传输成本。在同一可用区内的 EC2 实例之间传输数据的成本最低。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。 将所有 EC2 实例放置在同一可用区，可以降低数据传输成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。  Auto Scaling 组与数据传输成本无关；选项 B 错误。 位于同一区域内的实例之间的成本高于同一可用区内；选项 D 错误。  将实例放置在私有子网中，与成本没有直接关系。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Auto Scaling",
      "可用区",
      "VPC"
    ]
  },
  {
    "id": 994,
    "topic": "1",
    "question_en": "A company hosts a multi-tier web application that uses an Amazon Aurora MySQL DB cluster for storage. The application tier is hosted on Amazon EC2 instances. The company's IT security guidelines mandate that the database credentials be encrypted and rotated every 14 days. What should a solutions architect do to meet this requirement with the LEAST operational effort?",
    "options_en": {
      "A": "Create a new AWS Key Management Service (AWS KMS) encryption key. Use AWS Secrets Manager to create a new secret that uses the KMS key with the appropriate credentials. Associate the secret with the Aurora DB cluster. Configure a custom rotation period of 14 days.",
      "B": "Create two parameters in AWS Systems Manager Parameter Store: one for the user name as a string parameter and one that uses the SecureString type for the password. Select AWS Key Management Service (AWS KMS) encryption for the password parameter, and load these parameters in the application tier. Implement an AWS Lambda function that rotates the password every 14 days.",
      "C": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system in all EC2 instances of the application tier. Restrict the access to the file on the file system so that the application can read the file and that only super users can modify the file. Implement an AWS Lambda function that rotates the key in Aurora every 14 days and writes new credentials into the file.",
      "D": "Store a file that contains the credentials in an AWS Key Management Service (AWS KMS) encrypted Amazon S3 bucket that the application uses to load the credentials. Download the file to the application regularly to ensure that the correct credentials are used. Implement an AWS Lambda function that rotates the Aurora credentials every 14 days and uploads these credentials to the file in the S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司托管了一个多层 Web 应用程序，该应用程序使用 Amazon Aurora MySQL 数据库集群进行存储。应用程序层托管在 Amazon EC2 实例上。该公司的 IT 安全指南规定数据库凭证必须加密并每 14 天轮换一次。解决方案架构师应该怎么做才能以最少的运营工作量满足此要求？",
    "options_cn": {
      "A": "创建一个新的 AWS Key Management Service (AWS KMS) 加密密钥。使用 AWS Secrets Manager 创建一个使用 KMS 密钥和适当凭证的新密钥。将该密钥与 Aurora 数据库集群关联。配置 14 天的自定义轮换周期。",
      "B": "在 AWS Systems Manager Parameter Store 中创建两个参数：一个用于用户名的字符串参数，另一个使用 SecureString 类型用于密码。为密码参数选择 AWS Key Management Service (AWS KMS) 加密，并在应用程序层加载这些参数。实现一个 AWS Lambda 函数，每 14 天轮换一次密码。",
      "C": "将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon Elastic File System (Amazon EFS) 文件系统中。在应用程序层的所有 EC2 实例中挂载 EFS 文件系统。限制对文件系统的访问权限，以便应用程序可以读取该文件，并且只有超级用户才能修改该文件。实现一个 AWS Lambda 函数，每 14 天轮换一次 Aurora 中的密钥，并将新凭证写入该文件。",
      "D": "将包含凭证的文件存储在 AWS Key Management Service (AWS KMS) 加密的 Amazon S3 存储桶中，应用程序使用该存储桶加载凭证。定期将文件下载到应用程序以确保使用正确的凭证。实现一个 AWS Lambda 函数，每 14 天轮换一次 Aurora 凭证，并将这些凭证上传到 S3 存储桶中的文件中。"
    },
    "tags": [
      "Secrets Manager",
      "Aurora",
      "KMS",
      "Lambda",
      "S3",
      "EFS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察数据库凭证的加密和轮换。 AWS Secrets Manager 提供了密钥的存储和轮换功能， 可以满足安全和最小化运维的要求。与其他选项相比，Secrets Manager 更易于管理和自动化。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确。使用 Secrets Manager，可以加密数据库凭证并进行自动轮换，满足安全性和最少运维的要求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误。  使用 Systems Manager Parameter Store 存储密码，并使用 Lambda 进行轮换，实现较为复杂，成本较高；选项 C 错误。  使用 EFS 存储凭证，并手动轮换，增加了运维成本；选项 D 错误。  使用 S3 存储凭证，并手动轮换，增加了运维成本。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Aurora",
      "KMS",
      "Secrets Manager",
      "Lambda",
      "S3",
      "EFS",
      "Systems Manager"
    ]
  },
  {
    "id": 995,
    "topic": "1",
    "question_en": "A streaming media company is rebuilding its infrastructure to accommodate increasing demand for video content that users consume daily. The company needs to process terabyte-sized videos to block some content in the videos. Video processing can take up to 20 minutes. The company needs a solution that will scale with demand and remain cost-effective. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use AWS Lambda functions to process videos. Store video metadata in Amazon DynamoDB. Store video content in Amazon S3 Intelligent-Tiering.",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) and AWS Fargate to implement microservices to process videos. Store video metadata in Amazon Aurora. Store video content in Amazon S3 Intelligent-Tiering.",
      "C": "Use Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB) to process videos. Store video content in Amazon S3 Standard. Use Amazon Simple Queue Service (Amazon SQS) for queuing and to decouple processing tasks.",
      "D": "Deploy a containerized video processing application on Amazon Elastic Kubernetes Service (Amazon EKS) on Amazon EC2. Store video metadata in Amazon RDS in a single Availability Zone. Store video content in Amazon S3 Glacier Deep Archive."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家流媒体公司正在重建其基础设施，以满足用户每天对视频内容日益增长的需求。该公司需要处理 TB 级视频以屏蔽视频中的某些内容。视频处理可能需要长达 20 分钟。该公司需要一个能够随需求扩展且具有成本效益的解决方案。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 AWS Lambda 函数处理视频。将视频元数据存储在 Amazon DynamoDB 中。将视频内容存储在 Amazon S3 Intelligent-Tiering 中。",
      "B": "使用 Amazon Elastic Container Service (Amazon ECS) 和 AWS Fargate 来实现微服务以处理视频。将视频元数据存储在 Amazon Aurora 中。将视频内容存储在 Amazon S3 Intelligent-Tiering 中。",
      "C": "在 Application Load Balancer (ALB) 之后，使用 Auto Scaling 组中的 Amazon EC2 实例处理视频。将视频内容存储在 Amazon S3 Standard 中。使用 Amazon Simple Queue Service (Amazon SQS) 进行排队，并将处理任务解耦。",
      "D": "在 Amazon EC2 上部署基于容器的视频处理应用程序，该应用程序位于 Amazon Elastic Kubernetes Service (Amazon EKS) 上。将视频元数据存储在单个可用区的 Amazon RDS 中。将视频内容存储在 Amazon S3 Glacier Deep Archive 中。"
    },
    "tags": [
      "ECS",
      "Fargate",
      "S3",
      "SQS",
      "EC2",
      "Aurora"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察视频处理的解决方案，要求高可用性，成本效益以及可伸缩性。 最佳方案是使用 Fargate 和 ECS，可以实现容器化，自动伸缩，减少运维开销。S3 用于存储视频，Aurora 用于存储元数据。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确。 使用 ECS 和 Fargate，可以实现容器化，减少运维开销，并且可以自动伸缩。使用 S3 存储视频，Aurora 存储视频元数据。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 Lambda 运行时间有限制，不适合长时间的视频处理。 DynamoDB 不适合存储视频元数据；选项 C 错误。 使用 EC2 实例和 ALB 需要手动管理 EC2 实例。将视频内容存储在 S3 Standard，成本较高；选项 D 错误。 EKS 需要手动管理，增加了运维成本， Glacier Deep Archive 不适合频繁访问。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ECS",
      "Fargate",
      "S3",
      "SQS",
      "EC2",
      "Aurora",
      "Lambda",
      "DynamoDB",
      "EKS",
      "ALB",
      "Glacier Deep Archive"
    ]
  },
  {
    "id": 996,
    "topic": "1",
    "question_en": "A company runs an on-premises application on a Kubernetes cluster. The company recently added millions of new customers. The company's existing on-premises infrastructure is unable to handle the large number of new customers. The company needs to migrate the on-premises application to the AWS Cloud. The company will migrate to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The company does not want to manage the underlying compute infrastructure for the new architecture on AWS. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use a self-managed node to supply compute capacity. Deploy the application to the new EKS cluster.",
      "B": "Use managed node groups to supply compute capacity. Deploy the application to the new EKS cluster.",
      "C": "Use AWS Fargate to supply compute capacity. Create a Fargate profile. Use the Fargate profile to deploy the application.",
      "D": "Use managed node groups with Karpenter to supply compute capacity. Deploy the application to the new EKS cluster."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司在其 Kubernetes 集群上运行一个本地应用程序。该公司最近增加了数百万新客户。该公司现有的本地基础设施无法处理大量新客户。该公司需要将本地应用程序迁移到 AWS 云。该公司将迁移到 Amazon Elastic Kubernetes Service (Amazon EKS) 集群。该公司不想管理 AWS 上新架构的基础计算基础设施。哪种解决方案以最小的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用自管理节点提供计算能力。将应用程序部署到新的 EKS 集群。",
      "B": "使用托管节点组提供计算能力。将应用程序部署到新的 EKS 集群。",
      "C": "使用 AWS Fargate 提供计算能力。创建一个 Fargate 配置文件。使用 Fargate 配置文件部署应用程序。",
      "D": "使用带有 Karpenter 的托管节点组提供计算能力。将应用程序部署到新的 EKS 集群。"
    },
    "tags": [
      "EKS",
      "Fargate"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察在 EKS 上运行应用程序的解决方案。Fargate 提供了无服务器的计算能力，不需要管理底层基础设施，符合题意。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。 使用 Fargate 可以提供无服务器计算能力，降低运营开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 使用自管理节点需要手动管理节点；选项 B 错误。 使用托管节点组需要管理节点；选项 D 错误。 Karpenter 需要额外配置，增加了复杂度。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EKS",
      "Fargate",
      "Karpenter"
    ]
  },
  {
    "id": 997,
    "topic": "1",
    "question_en": "A company is launching a new application that requires a structured database to store user profiles, application settings, and transactional data. The database must be scalable with application trafic and must offer backups. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Deploy a self-managed database on Amazon EC2 instances by using open source software. Use Spot Instances for cost optimization. Configure automated backups to Amazon S3.",
      "B": "Use Amazon RDS. Use on-demand capacity mode for the database with General Purpose SSD storage. Configure automatic backups with a retention period of 7 days.",
      "C": "Use Amazon Aurora Serverless for the database. Use serverless capacity scaling. Configure automated backups to Amazon S3.",
      "D": "Deploy a self-managed NoSQL database on Amazon EC2 instances. Use Reserved Instances for cost optimization. Configure automated backups directly to Amazon S3 Glacier Flexible Retrieval."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司正在推出一个新应用程序，该应用程序需要一个结构化数据库来存储用户配置文件、应用程序设置和事务数据。 该数据库必须能够随应用程序流量进行扩展，并且必须提供备份。 哪个解决方案能以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "在 Amazon EC2 实例上使用开源软件部署一个自管理的数据库。使用 Spot Instances 进行成本优化。配置自动备份到 Amazon S3。",
      "B": "使用 Amazon RDS。将按需容量模式用于数据库，并使用 General Purpose SSD 存储。配置自动备份，保留期为 7 天。",
      "C": "使用 Amazon Aurora Serverless 作为数据库。使用无服务器容量扩展。配置自动备份到 Amazon S3。",
      "D": "在 Amazon EC2 实例上部署一个自管理的 NoSQL 数据库。使用 Reserved Instances 进行成本优化。配置自动备份到 Amazon S3 Glacier Flexible Retrieval。"
    },
    "tags": [
      "Aurora Serverless",
      "RDS",
      "EC2",
      "S3"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n此题考察数据库选型，需要满足可扩展性和备份的需求。 Aurora Serverless 可以自动扩展，并提供备份，成本较低，符合要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：选项 C 正确。  Aurora Serverless 可以根据流量自动扩展，并提供备份。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 自管理数据库的维护成本较高，Spot 实例虽然成本低，但是不稳定；选项 B 错误。 RDS 的容量需要手动调整，不具备自动伸缩的能力；选项 D 错误。  NoSQL 数据库需要手动管理，并且 Glacier Flexible Retrieval 不适合频繁备份和恢复。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Aurora Serverless",
      "RDS",
      "EC2",
      "S3",
      "NoSQL"
    ]
  },
  {
    "id": 998,
    "topic": "1",
    "question_en": "A company runs its legacy web application on AWS. The web application server runs on an Amazon EC2 instance in the public subnet of a VPC. The web application server collects images from customers and stores the image files in a locally attached Amazon Elastic Block Store (Amazon EBS) volume. The image files are uploaded every night to an Amazon S3 bucket for backup. A solutions architect discovers that the image files are being uploaded to Amazon S3 through the public endpoint. The solutions architect needs to ensure that trafic to Amazon S3 does not use the public endpoint. Which solution will meet these requirements?",
    "options_en": {
      "A": "Create a gateway VPC endpoint for the S3 bucket that has the necessary permissions for the VPC. Configure the subnet route table to use the gateway VPC endpoint.",
      "B": "Move the S3 bucket inside the VPC. Configure the subnet route table to access the S3 bucket through private IP addresses.",
      "C": "Create an Amazon S3 access point for the Amazon EC2 instance inside the VPConfigure the web application to upload by using the Amazon S3 access point.",
      "D": "Configure an AWS Direct Connect connection between the VPC that has the Amazon EC2 instance and Amazon S3 to provide a dedicated network path."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司在 AWS 上运行其旧版 Web 应用程序。Web 应用程序服务器在 VPC 的公有子网中的 Amazon EC2 实例上运行。Web 应用程序服务器从客户那里收集图像，并将图像文件存储在本地连接的 Amazon Elastic Block Store (Amazon EBS) 卷中。图像文件每天晚上都会上传到 Amazon S3 存储桶进行备份。一位解决方案架构师发现图像文件是通过公有端点上传到 Amazon S3 的。解决方案架构师需要确保到 Amazon S3 的流量不使用公有端点。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "为 S3 存储桶创建网关 VPC 端点，该端点具有 VPC 的必要权限。配置子网路由表以使用网关 VPC 端点。",
      "B": "将 S3 存储桶移入 VPC 内部。配置子网路由表以通过私有 IP 地址访问 S3 存储桶。",
      "C": "为 VPC 内部的 Amazon EC2 实例创建 Amazon S3 访问点。配置 Web 应用程序以使用 Amazon S3 访问点上传。",
      "D": "在具有 Amazon EC2 实例的 VPC 和 Amazon S3 之间配置 AWS Direct Connect 连接，以提供专用网络路径。"
    },
    "tags": [
      "S3",
      "VPC",
      "S3 访问点"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考察如何确保流量不使用公有端点访问 S3。  使用 VPC 终端节点可以保证流量在 VPC 内部传输，保证数据安全，避免公网访问。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：选项 A 正确。 使用网关 VPC 终端节点，保证了流量在 VPC 内部，不经过公网。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 B 错误。  将 S3 存储桶移入 VPC，无法实现。；选项 C 错误。 S3 访问点不适用于此场景；选项 D 错误。 Direct Connect 方案过于复杂，开销较高，不符合最低成本要求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "VPC",
      "VPC 端点",
      "Direct Connect",
      "S3 访问点"
    ]
  },
  {
    "id": 999,
    "topic": "1",
    "question_en": "A company is creating a prototype of an ecommerce website on AWS. The website consists of an Application Load Balancer, an Auto Scaling group of Amazon EC2 instances for web servers, and an Amazon RDS for MySQL DB instance that runs with the Single-AZ configuration. The website is slow to respond during searches of the product catalog. The product catalog is a group of tables in the MySQL database that the company does not update frequently. A solutions architect has determined that the CPU utilization on the DB instance is high when product catalog searches occur. What should the solutions architect recommend to improve the performance of the website during searches of the product catalog?",
    "options_en": {
      "A": "Migrate the product catalog to an Amazon Redshift database. Use the COPY command to load the product catalog tables.",
      "B": "Implement an Amazon ElastiCache for Redis cluster to cache the product catalog. Use lazy loading to populate the cache.",
      "C": "Add an additional scaling policy to the Auto Scaling group to launch additional EC2 instances when database response is slow.",
      "D": "Turn on the Multi-AZ configuration for the DB instance. Configure the EC2 instances to throttle the product catalog queries that are sent to the database."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 上创建一个电子商务网站原型。该网站由一个 Application Load Balancer、一个用于 Web 服务器的 Amazon EC2 实例的 Auto Scaling 组以及一个使用 Single-AZ 配置运行的 Amazon RDS for MySQL 数据库实例组成。在搜索产品目录时，网站响应缓慢。产品目录是 MySQL 数据库中的一组表，该公司不经常更新这些表。解决方案架构师确定，在进行产品目录搜索时，数据库实例上的 CPU 利用率很高。解决方案架构师应该建议采取什么措施来提高网站在搜索产品目录期间的性能？",
    "options_cn": {
      "A": "将产品目录迁移到 Amazon Redshift 数据库。使用 COPY 命令加载产品目录表。",
      "B": "实施一个 Amazon ElastiCache for Redis 集群来缓存产品目录。使用延迟加载来填充缓存。",
      "C": "向 Auto Scaling 组添加额外的伸缩策略，以便在数据库响应缓慢时启动额外的 EC2 实例。",
      "D": "为数据库实例打开 Multi-AZ 配置。配置 EC2 实例以限制发送到数据库的产品目录查询。"
    },
    "tags": [
      "ElastiCache",
      "Redis",
      "RDS",
      "MySQL",
      "EC2"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何提高数据库性能，关键在于产品目录的查询速度。 ElastiCache for Redis 可以缓存数据，提高查询性能。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确。 使用 ElastiCache for Redis 缓存产品目录，可以提高查询速度。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 将产品目录迁移到 Redshift 不适合该场景；选项 C 错误。  增加 EC2 实例无法解决数据库查询慢的问题；选项 D 错误。  打开 Multi-AZ 配置，并不能提高性能，并且配置不合理。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ElastiCache",
      "Redis",
      "RDS",
      "MySQL",
      "EC2",
      "Redshift"
    ]
  },
  {
    "id": 1000,
    "topic": "1",
    "question_en": "A company currently stores 5 TB of data in on-premises block storage systems. The company's current storage solution provides limited space for additional data. The company runs applications on premises that must be able to retrieve frequently accessed data with low latency. The company requires a cloud-based storage solution. Which solution will meet these requirements with the MOST operational eficiency?",
    "options_en": {
      "A": "Use Amazon S3 File Gateway. Integrate S3 File Gateway with the on-premises applications to store and directly retrieve files by using the SMB file system.",
      "B": "Use an AWS Storage Gateway Volume Gateway with cached volumes as iSCSI targets.",
      "C": "Use an AWS Storage Gateway Volume Gateway with stored volumes as iSCSI targets.",
      "D": "Use an AWS Storage Gateway Tape Gateway. Integrate Tape Gateway with the on-premises applications to store virtual tapes in Amazon S3."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司目前将 5 TB 的数据存储在本地块存储系统中。该公司当前的存储解决方案为额外数据提供了有限的空间。该公司在本地运行应用程序，这些应用程序必须能够以低延迟检索经常访问的数据。该公司需要基于云的存储解决方案。哪种解决方案将以最高的运营效率满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon S3 File Gateway。将 S3 File Gateway 与本地应用程序集成，以使用 SMB 文件系统存储和直接检索文件。",
      "B": "使用 AWS Storage Gateway Volume Gateway，使用缓存卷作为 iSCSI 目标。",
      "C": "使用 AWS Storage Gateway Volume Gateway，使用存储卷作为 iSCSI 目标。",
      "D": "使用 AWS Storage Gateway Tape Gateway。将 Tape Gateway 与本地应用程序集成，以将虚拟磁带存储在 Amazon S3 中。"
    },
    "tags": [
      "Storage Gateway",
      "Volume Gateway",
      "S3",
      "EBS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察存储解决方案的选择，需要满足低延迟和高效率的需求。Volume Gateway 结合缓存卷是最合适的解决方案，可以提供低延迟的本地访问和云端存储。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确。  Volume Gateway，使用缓存卷可以提供低延迟的本地访问，同时将数据存储在云端，满足了题意。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。  File Gateway 适用于文件共享，不满足低延迟需求；选项 C 错误。  存储卷不提供缓存功能，延迟较高；选项 D 错误。  Tape Gateway 适用于长期归档，不满足低延迟的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Storage Gateway",
      "S3",
      "EBS",
      "iSCSI",
      "File Gateway",
      "Volume Gateway",
      "Tape Gateway"
    ]
  },
  {
    "id": 1001,
    "topic": "1",
    "question_en": "A company operates a food delivery service. Because of recent growth, the company's order processing system is experiencing scaling problems during peak trafic hours. The current architecture includes Amazon EC2 instances in an Auto Scaling group that collect orders from an application. A second group of EC2 instances in an Auto Scaling group fulfills the orders. The order collection process occurs quickly, but the order fulfillment process can take longer. Data must not be lost because of a scaling event. A solutions architect must ensure that the order collection process and the order fulfillment process can both scale adequately during peak trafic hours. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling groups. Configure each Auto Scaling group's minimum capacity to meet its peak workload value.",
      "B": "Use Amazon CloudWatch to monitor the CPUUtilization metric for each instance in both Auto Scaling groups. Configure a CloudWatch alarm to invoke an Amazon Simple Notification Service (Amazon SNS) topic to create additional Auto Scaling groups on demand.",
      "C": "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for order collection. Use the second SQS queue for order fulfillment. Configure the EC2 instances to poll their respective queues. Scale the Auto Scaling groups based on notifications that the queues send.",
      "D": "Provision two Amazon Simple Queue Service (Amazon SQS) queues. Use one SQS queue for order collection. Use the second SQS queue for order fulfillment. Configure the EC2 instances to poll their respective queues. Scale the Auto Scaling groups based on the number of messages in each queue."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司运营一家食品配送服务。由于最近的增长，该公司的订单处理系统在高峰时段遇到了扩展问题。当前的架构包括一个 Auto Scaling 组中的 Amazon EC2 实例，这些实例从应用程序收集订单。第二个 Auto Scaling 组中的 EC2 实例履行订单。订单收集过程很快，但订单履行过程可能需要更长时间。由于扩展事件，数据不能丢失。解决方案架构师必须确保订单收集流程和订单履行流程在高峰时段都能充分扩展。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon CloudWatch 监控两个 Auto Scaling 组中每个实例的 CPUUtilization 指标。将每个 Auto Scaling 组的最小容量配置为满足其峰值工作负载值。",
      "B": "使用 Amazon CloudWatch 监控两个 Auto Scaling 组中每个实例的 CPUUtilization 指标。配置一个 CloudWatch 警报以调用 Amazon Simple Notification Service (Amazon SNS) 主题，以按需创建额外的 Auto Scaling 组。",
      "C": "配置两个 Amazon Simple Queue Service (Amazon SQS) 队列。将一个 SQS 队列用于订单收集。将第二个 SQS 队列用于订单履行。配置 EC2 实例以轮询它们各自的队列。根据队列发送的通知来扩展 Auto Scaling 组。",
      "D": "配置两个 Amazon Simple Queue Service (Amazon SQS) 队列。将一个 SQS 队列用于订单收集。将第二个 SQS 队列用于订单履行。配置 EC2 实例以轮询它们各自的队列。根据每个队列中的消息数量来扩展 Auto Scaling 组。"
    },
    "tags": [
      "SQS",
      "EC2",
      "Auto Scaling",
      "CloudWatch"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察如何处理高峰时段的订单处理问题。 使用 SQS 和 Auto Scaling 可以实现异步处理，从而解决扩展问题。 使用队列消息数量作为指标可以更准确地判断是否需要扩容。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。  使用 SQS 队列实现订单的异步处理，并根据队列中的消息数量进行 Auto Scaling，能够满足高吞吐量需求。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。  CPU 利用率不能准确反应系统负载；选项 B 错误。  使用 SNS 无法触发 Auto Scaling；选项 C 错误。  使用 SQS 队列，但根据通知而不是队列消息数量来进行扩容，不准确。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SQS",
      "EC2",
      "Auto Scaling",
      "CloudWatch",
      "SNS"
    ]
  },
  {
    "id": 1002,
    "topic": "1",
    "question_en": "An online gaming company is transitioning user data storage to Amazon DynamoDB to support the company's growing user base. The current architecture includes DynamoDB tables that contain user profiles, achievements, and in-game transactions. The company needs to design a robust, continuously available, and resilient DynamoDB architecture to maintain a seamless gaming experience for users. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create DynamoDB tables in a single AWS Region. Use on-demand capacity mode. Use global tables to replicate data across multiple Regions.",
      "B": "Use DynamoDB Accelerator (DAX) to cache frequently accessed data. Deploy tables in a single AWS Region and enable auto scaling. Configure Cross-Region Replication manually to additional Regions.",
      "C": "Create DynamoDB tables in multiple AWS Regions. Use on-demand capacity mode. Use DynamoDB Streams for Cross-Region Replication between Regions.",
      "D": "Use DynamoDB global tables for automatic multi-Region replication. Deploy tables in multiple AWS Regions. Use provisioned capacity mode. Enable auto scaling."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家在线游戏公司正在将用户数据存储迁移到 Amazon DynamoDB，以支持公司不断增长的用户群。目前的架构包括包含用户个人资料、成就和游戏内交易的 DynamoDB 表。该公司需要设计一个强大、持续可用且有弹性的 DynamoDB 架构，以保持为用户提供无缝的游戏体验。哪种解决方案能够最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "在单个 AWS 区域中创建 DynamoDB 表。使用按需容量模式。使用全局表将数据复制到多个区域。",
      "B": "使用 DynamoDB Accelerator (DAX) 缓存经常访问的数据。在单个 AWS 区域中部署表并启用自动伸缩。手动配置 跨区域复制 到其他区域。",
      "C": "在多个 AWS 区域中创建 DynamoDB 表。使用按需容量模式。使用 DynamoDB Streams 在区域之间进行 跨区域复制。",
      "D": "使用 DynamoDB 全局表进行自动多区域复制。在多个 AWS 区域中部署表。使用预置容量模式。启用自动伸缩。"
    },
    "tags": [
      "DynamoDB",
      "全局表"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n此题考察 DynamoDB 架构设计，需要高可用性和弹性。全局表能够跨区域复制数据，并支持自动伸缩，是最佳选择。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：选项 D 正确。 使用 DynamoDB 全局表，可以跨区域复制数据，并实现自动伸缩，实现高可用和弹性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。  在单个区域中使用按需容量模式，不具备高可用性；选项 B 错误。  DAX 缓存和手动配置区域复制，增加了复杂性；选项 C 错误。  DynamoDB Streams 不能提供多区域复制。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "DynamoDB",
      "全局表",
      "DAX",
      "Streams"
    ]
  },
  {
    "id": 1003,
    "topic": "1",
    "question_en": "A company runs its media rendering application on premises. The company wants to reduce storage costs and has moved all data to Amazon S3. The on-premises rendering application needs low-latency access to storage. The company needs to design a storage solution for the application. The storage solution must maintain the desired application performance. Which storage solution will meet these requirements in the MOST cost-effective way?",
    "options_en": {
      "A": "Use Mountpoint for Amazon S3 to access the data in Amazon S3 for the on-premises application.",
      "B": "Configure an Amazon S3 File Gateway to provide storage for the on-premises application.",
      "C": "Copy the data from Amazon S3 to Amazon FSx for Windows File Server. Configure an Amazon FSx File Gateway to provide storage for the on-premises application.",
      "D": "Configure an on-premises file server. Use the Amazon S3 API to connect to S3 storage. Configure the application to access the storage from the on-premises file server."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在其本地运行媒体渲染应用程序。该公司希望降低存储成本，并将所有数据移动到 Amazon S3。本地渲染应用程序需要低延迟访问存储。该公司需要为应用程序设计一个存储解决方案。该存储解决方案必须保持所需的应用程序性能。哪种存储解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "使用 Mountpoint for Amazon S3 访问 Amazon S3 中的数据，以供本地应用程序使用。",
      "B": "配置一个 Amazon S3 File Gateway 来为本地应用程序提供存储。",
      "C": "将数据从 Amazon S3 复制到 Amazon FSx for Windows File Server。配置一个 Amazon FSx File Gateway 来为本地应用程序提供存储。",
      "D": "配置一个本地文件服务器。使用 Amazon S3 API 连接到 S3 存储。配置应用程序从本地文件服务器访问存储。"
    },
    "tags": [
      "S3",
      "File Gateway",
      "FSx"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察存储解决方案的选择，需要在本地访问 S3 中的数据，并满足低延迟的需求。 File Gateway 提供了低延迟的本地访问和 S3 的存储。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确。  使用 S3 File Gateway，可以提供低延迟的本地访问。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误。 Mountpoint for Amazon S3 并不能完全满足本地访问的需求，并且需要额外配置；选项 C 错误。 FSx 成本较高，并且增加了额外的管理；选项 D 错误。 直接连接 S3 API 不满足低延迟的需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "FSx",
      "Mountpoint for Amazon S3",
      "File Gateway"
    ]
  },
  {
    "id": 1004,
    "topic": "1",
    "question_en": "A company hosts its enterprise resource planning (ERP) system in the us-east-1 Region. The system runs on Amazon EC2 instances. Customers use a public API that is hosted on the EC2 instances to exchange information with the ERP system. International customers report slow API response times from their data centers. Which solution will improve response times for the international customers MOST cost-effectively?",
    "options_en": {
      "A": "Create an AWS Direct Connect connection that has a public virtual interface (VIF) to provide connectivity from each customer's data center to us-east-1. Route customer API requests by using a Direct Connect gateway to the ERP system API.",
      "B": "Set up an Amazon CloudFront distribution in front of the API. Configure the CachingOptimized managed cache policy to provide improved cache eficiency.",
      "C": "Set up AWS Global Accelerator. Configure listeners for the necessary ports. Configure endpoint groups for the appropriate Regions to distribute trafic. Create an endpoint in the group for the API.",
      "D": "Use AWS Site-to-Site VPN to establish dedicated VPN tunnels between Regions and customer networks. Route trafic to the API over the VPN connections."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在其 us-east-1 区域托管其企业资源规划 (ERP) 系统。该系统在 Amazon EC2 实例上运行。客户使用托管在 EC2 实例上的公共 API 与 ERP 系统交换信息。国际客户报告来自其数据中心的 API 响应时间较慢。哪种解决方案将以最具成本效益的方式改善国际客户的响应时间？",
    "options_cn": {
      "A": "创建具有公共虚拟接口 (VIF) 的 AWS Direct Connect 连接，以提供从每个客户的数据中心到 us-east-1 的连接。使用 Direct Connect 网关将客户 API 请求路由到 ERP 系统 API。",
      "B": "在 API 前设置 Amazon CloudFront 分发。配置 CachingOptimized 托管缓存策略以提供改进的缓存效率。",
      "C": "设置 AWS Global Accelerator。为必要的端口配置侦听器。配置相应区域的端点组以分配流量。在组中为 API 创建一个端点。",
      "D": "使用 AWS Site-to-Site VPN 在区域和客户网络之间建立专用 VPN 隧道。通过 VPN 连接将流量路由到 API。"
    },
    "tags": [
      "EC2",
      "API Gateway",
      "CloudFront",
      "Global Accelerator",
      "Direct Connect",
      "VPN"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何通过优化网络配置，提升全球用户的 API 响应速度。 重点在于理解不同 AWS 服务在加速网络传输方面的作用。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：Amazon CloudFront 是一个内容分发网络（CDN），通过将内容缓存在全球各地的边缘站点来加速内容传输。 在 API 前面设置 CloudFront 可以缓存 API 响应，从而减少延迟。使用 CachingOptimized 托管缓存策略可以提供改进的缓存效率。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项， AWS Direct Connect 提供了从客户数据中心到 AWS 的专用网络连接， 但设置和配置较为复杂，成本较高，且没有直接优化 API 响应时间。C 选项，AWS Global Accelerator 通过使用 AWS 的全球网络来优化流量路由，提高应用程序的性能。  但它的配置相对复杂，可能不如 CloudFront 成本效益高。D 选项， AWS Site-to-Site VPN 提供了加密的网络连接，但是 VPN 隧道可能引入额外的延迟，并且没有利用 CDN 的缓存功能，不如 CloudFront 有效。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "API",
      "CloudFront",
      "CachingOptimized",
      "Direct Connect",
      "VIF",
      "Direct Connect 网关",
      "Global Accelerator",
      "VPN"
    ]
  },
  {
    "id": 1005,
    "topic": "1",
    "question_en": "A company tracks customer satisfaction by using surveys that the company hosts on its website. The surveys sometimes reach thousands of customers every hour. Survey results are currently sent in email messages to the company so company employees can manually review results and assess customer sentiment. The company wants to automate the customer survey process. Survey results must be available for the previous 12 months. Which solution will meet these requirements in the MOST scalable way?",
    "options_en": {
      "A": "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Create an AWS Lambda function to poll the SQS queue, call Amazon Comprehend for sentiment analysis, and save the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future.",
      "B": "Send the survey results data to an API that is running on an Amazon EC2 instance. Configure the API to store the survey results as a new record in an Amazon DynamoDB table, call Amazon Comprehend for sentiment analysis, and save the results in a second DynamoDB table. Set the TTL for all records to 365 days in the future.",
      "C": "Write the survey results data to an Amazon S3 bucket. Use S3 Event Notifications to invoke an AWS Lambda function to read the data and call Amazon Rekognition for sentiment analysis. Store the sentiment analysis results in a second S3 bucket. Use S3 lifecycle policies on each bucket to expire objects after 365 days.",
      "D": "Send the survey results data to an Amazon API Gateway endpoint that is connected to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the SQS queue to invoke an AWS Lambda function that calls Amazon Lex for sentiment analysis and saves the results to an Amazon DynamoDB table. Set the TTL for all records to 365 days in the future."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司通过使用其网站上托管的调查来跟踪客户满意度。 这些调查有时每小时都会接触到数千名客户。 调查结果目前通过电子邮件消息发送给公司，以便公司员工可以手动审查结果并评估客户情绪。 公司希望自动化客户调查流程。 调查结果必须提供过去 12 个月的数据。 哪种解决方案将以最具可扩展性的方式满足这些要求？",
    "options_cn": {
      "A": "将调查结果数据发送到连接到 Amazon Simple Queue Service (Amazon SQS) 队列的 Amazon API Gateway 终端节点。 创建一个 AWS Lambda 函数来轮询 SQS 队列，调用 Amazon Comprehend 进行情感分析，并将结果保存到 Amazon DynamoDB 表中。 将所有记录的 TTL 设置为 365 天后的未来。",
      "B": "将调查结果数据发送到在 Amazon EC2 实例上运行的 API。 配置 API 将调查结果存储为 Amazon DynamoDB 表中的新记录，调用 Amazon Comprehend 进行情感分析，并将结果保存在第二个 DynamoDB 表中。 将所有记录的 TTL 设置为 365 天后的未来。",
      "C": "将调查结果数据写入 Amazon S3 存储桶。 使用 S3 事件通知来调用 AWS Lambda 函数以读取数据并调用 Amazon Rekognition 进行情感分析。 将情感分析结果存储在第二个 S3 存储桶中。 在每个存储桶上使用 S3 生命周期策略在 365 天后使对象过期。",
      "D": "将调查结果数据发送到连接到 Amazon Simple Queue Service (Amazon SQS) 队列的 Amazon API Gateway 终端节点。 配置 SQS 队列以调用 AWS Lambda 函数，该函数调用 Amazon Lex 进行情感分析并将结果保存到 Amazon DynamoDB 表中。 将所有记录的 TTL 设置为 365 天后的未来。"
    },
    "tags": [
      "API Gateway",
      "SQS",
      "Lambda",
      "Comprehend",
      "DynamoDB",
      "S3",
      "Rekognition",
      "Lex"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n本题考察如何设计一个可扩展的、自动化的客户调查流程，其中涉及数据采集、处理和存储，并利用 AWS 服务实现情感分析。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：将调查结果数据发送到 API Gateway，连接到 SQS 队列。 Lambda 函数从 SQS 队列轮询数据，调用 Comprehend 进行情感分析，并将结果保存到 DynamoDB 表中。 DynamoDB 支持 Time to Live (TTL) 功能，可以自动删除过期数据。这种方案具备高可扩展性，并且易于管理。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项， 直接通过 EC2 实例上的 API 接收调查结果， 扩展性不如 SQS 和 Lambda 的组合。C 选项，使用 S3 和 S3 事件触发 Lambda 函数， 使用 Rekognition 进行情感分析， 无法直接进行文本情感分析。  D 选项， 选项选择使用 Lex 进行情感分析， Lex 适用于构建对话机器人，不适合情感分析。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "API Gateway",
      "SQS",
      "Lambda",
      "DynamoDB",
      "TTL",
      "S3",
      "Rekognition",
      "Comprehend",
      "Lex"
    ]
  },
  {
    "id": 1006,
    "topic": "1",
    "question_en": "A company uses AWS Systems Manager for routine management and patching of Amazon EC2 instances. The EC2 instances are in an IP address type target group behind an Application Load Balancer (ALB). New security protocols require the company to remove EC2 instances from service during a patch. When the company attempts to follow the security protocol during the next patch, the company receives errors during the patching window. Which combination of solutions will resolve the errors? (Choose two.)",
    "options_en": {
      "A": "Change the target type of the target group from IP address type to instance type.",
      "B": "Continue to use the existing Systems Manager document without changes because it is already optimized to handle instances that are in an IP address type target group behind an ALB.",
      "C": "Implement the AWSEC2-PatchLoadBalanacerInstance Systems Manager Automation document to manage the patching process.",
      "D": "Use Systems Manager Maintenance Windows to automatically remove the instances from service to patch the instances",
      "E": "Configure Systems Manager State Manager to remove the instances from service and manage the patching schedule. Use ALB health checks to re-route trafic."
    },
    "correct_answer": "CD",
    "vote_percentage": "",
    "question_cn": "一家公司使用 AWS Systems Manager 进行 Amazon EC2 实例的例行管理和修补。 EC2 实例位于 Application Load Balancer (ALB) 之后的一个 IP 地址类型目标组中。新的安全协议要求公司在修补期间将 EC2 实例从服务中移除。当公司尝试在下一次修补期间遵循安全协议时，公司在修补窗口期间收到错误。哪种解决方案组合将解决这些错误？（选择两个。）",
    "options_cn": {
      "A": "将目标组的目标类型从 IP 地址类型更改为实例类型。",
      "B": "继续使用现有的 Systems Manager 文档，无需更改，因为它已经过优化，可以处理位于 ALB 之后 IP 地址类型目标组中的实例。",
      "C": "实施 AWSEC2-PatchLoadBalanacerInstance Systems Manager Automation 文档来管理修补过程。",
      "D": "使用 Systems Manager 维护时段自动将实例从服务中移除以修补实例。",
      "E": "配置 Systems Manager State Manager 以将实例从服务中移除并管理修补计划。 使用 ALB 健康检查来重新路由流量。"
    },
    "tags": [
      "Systems Manager",
      "EC2",
      "ALB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 CD（社区 —），解析仅供参考。】\n\n本题考查在使用 Application Load Balancer (ALB) 负载均衡的 EC2 实例上使用 Systems Manager 执行修补操作时，如何处理实例的下线问题。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 CD。理由简述：使用 AWSEC2-PatchLoadBalanacerInstance Systems Manager Automation 文档，可以自动处理在修补 EC2 实例时从 ALB 中移除实例的操作，从而确保修补过程的正确性和安全性。此方案可以自动协调修补过程。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项， 更改目标组的目标类型，没有解决根本问题。 ALB 健康检查不会阻止修补过程中出现的问题。B 选项， Systems Manager 文档可能无法处理在 ALB 之后 IP 地址类型目标组中的实例， 无法解决安全协议的要求。D 选项， 使用 Systems Manager 维护时段，无法自动从服务中移除实例以进行修补。 E 选项，使用 Systems Manager State Manager 和 ALB 健康检查无法保证修补的安全性，且配置复杂。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Systems Manager",
      "EC2",
      "ALB",
      "实例类型",
      "IP 地址类型",
      "AWSEC2-PatchLoadBalanacerInstance",
      "State Manager"
    ]
  },
  {
    "id": 1007,
    "topic": "1",
    "question_en": "A medical company wants to perform transformations on a large amount of clinical trial data that comes from several customers. The company must extract the data from a relational database that contains the customer data. Then the company will transform the data by using a series of complex rules. The company will load the data to Amazon S3 when the transformations are complete. All data must be encrypted where it is processed before the company stores the data in Amazon S3. All data must be encrypted by using customer-specific keys. Which solution will meet these requirements with the LEAST amount of operational effort?",
    "options_en": {
      "A": "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses server-side encryption with Amazon S3 managed keys (SSE-S3) to encrypt the data.",
      "B": "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses client-side encryption with a custom client-side root key (CSE-Custom) to encrypt the data.",
      "C": "Create one AWS Glue job for each customer. Attach a security configuration to each job that uses client-side encryption with AWS KMS managed keys (CSE-KMS) to encrypt the data.",
      "D": "Create one Amazon EMR cluster for each customer. Attach a security configuration to each cluster that uses server-side encryption with AWS KMS keys (SSE-KMS) to encrypt the data."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家医疗公司希望对来自多个客户的大量临床试验数据进行转换。该公司必须从包含客户数据的关系数据库中提取数据。然后，该公司将使用一系列复杂的规则转换数据。转换完成后，该公司会将数据加载到 Amazon S3。在公司将数据存储在 Amazon S3 之前，所有数据都必须在处理时进行加密。所有数据都必须使用客户特定的密钥进行加密。哪种解决方案将以最少的运营工作量满足这些要求？",
    "options_cn": {
      "A": "为每个客户创建一个 AWS Glue 作业。将安全配置附加到每个作业，该配置使用带有 Amazon S3 托管密钥 (SSE-S3) 的服务器端加密来加密数据。",
      "B": "为每个客户创建一个 Amazon EMR 集群。将安全配置附加到每个集群，该配置使用带有自定义客户端根密钥 (CSE-Custom) 的客户端加密来加密数据。",
      "C": "为每个客户创建一个 AWS Glue 作业。将安全配置附加到每个作业，该配置使用带有 AWS KMS 托管密钥 (CSE-KMS) 的客户端加密来加密数据。",
      "D": "为每个客户创建一个 Amazon EMR 集群。将安全配置附加到每个集群，该配置使用带有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密来加密数据。"
    },
    "tags": [
      "Glue",
      "EMR",
      "S3",
      "KMS",
      "SSE-S3",
      "CSE-Custom",
      "CSE-KMS",
      "SSE-KMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考察在处理大量临床试验数据时，如何使用 AWS 服务对数据进行加密，并满足客户特定的密钥要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：使用 EMR 集群，因为 EMR 提供了强大的数据处理能力，适合处理大量数据。使用带有 AWS KMS 密钥 (SSE-KMS) 的服务器端加密，因为 SSE-KMS 允许数据在 S3 中加密，并且密钥由 KMS 管理，提供了客户控制密钥的能力。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项， AWS Glue 适合 ETL 任务， 但其并非最适合大规模的数据加密。使用 SSE-S3 加密，无法使用客户特定的密钥。 B 选项， 使用 CSE-Custom 进行加密，配置较为复杂，运营开销较高。C 选项，使用 CSE-KMS 加密，配置较为复杂，运营开销较高。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Glue",
      "EMR",
      "S3",
      "KMS",
      "SSE-S3",
      "SSE-KMS",
      "CSE-Custom",
      "CSE-KMS"
    ]
  },
  {
    "id": 1008,
    "topic": "1",
    "question_en": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The analytics application is highly resilient and is designed to run in stateless mode. The company notices that the application is showing signs of performance degradation during busy times and is presenting 5xx errors. The company needs to make the application scale seamlessly. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load Balancer to distribute the load across the two EC2 instances.",
      "B": "Create an Amazon Machine Image (AMI) of the web application. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 weighted routing to distribute the load across the two EC2 instances.",
      "C": "Create an AWS Lambda function to stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to invoke the Lambda function when CPU utilization is more than 75%.",
      "D": "Create an Amazon Machine Image (AMI) of the web application. Apply the AMI to a launch template. Create an Auto Scaling group that includes the launch template. Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to the Auto Scaling group."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司在单个 Amazon EC2 按需实例上托管一个网站分析应用程序。该分析应用程序具有高度的弹性，并且设计为以无状态模式运行。该公司注意到该应用程序在繁忙时段显示出性能下降的迹象，并出现 5xx 错误。该公司需要使应用程序无缝扩展。哪种解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "创建该 Web 应用程序的 Amazon Machine Image (AMI)。使用 AMI 启动第二个 EC2 按需实例。使用 Application Load Balancer 将负载分配到两个 EC2 实例。",
      "B": "创建该 Web 应用程序的 Amazon Machine Image (AMI)。使用 AMI 启动第二个 EC2 按需实例。使用 Amazon Route 53 加权路由将负载分配到两个 EC2 实例。",
      "C": "创建 AWS Lambda 函数以停止 EC2 实例并更改实例类型。创建 Amazon CloudWatch 警报，以便在 CPU 利用率超过 75% 时调用 Lambda 函数。",
      "D": "创建该 Web 应用程序的 Amazon Machine Image (AMI)。将 AMI 应用于启动模板。创建一个包含启动模板的 Auto Scaling 组。配置启动模板以使用 Spot Fleet。将 Application Load Balancer 附加到 Auto Scaling 组。"
    },
    "tags": [
      "EC2",
      "Auto Scaling",
      "ALB",
      "CloudWatch",
      "Lambda",
      "Spot Fleet"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考察如何设计一个成本效益高且具备弹性伸缩能力的 Web 应用程序，以应对繁忙时段的负载。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：使用启动模板和 Auto Scaling 组，结合 Spot Fleet 和 Application Load Balancer，可以实现成本效益高的弹性伸缩。 启动模板允许定义 EC2 实例的配置。 Auto Scaling 组根据负载自动扩展或缩减 EC2 实例数量。Spot Fleet 可以使用 Spot 实例降低成本。 ALB 自动将流量分配到运行中的实例。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，手动启动第二个 EC2 实例，扩展不够自动化，成本也较高。 B 选项，手动启动第二个 EC2 实例，使用 Route 53 加权路由，扩展不够自动化。 C 选项，手动停止实例，无法实现自动扩展。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "AMI",
      "ALB",
      "Auto Scaling",
      "CloudWatch",
      "Lambda",
      "Launch Template",
      "Spot Fleet"
    ]
  },
  {
    "id": 1009,
    "topic": "1",
    "question_en": "A company runs an environment where data is stored in an Amazon S3 bucket. The objects are accessed frequently throughout the day. The company has strict da ta encryption requirements for data that is stored in the S3 bucket. The company currently uses AWS Key Management Service (AWS KMS) for encryption. The company wants to optimize costs associated with encrypting S3 objects without making additional calls to AWS KMS. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use server-side encryption with Amazon S3 managed keys (SSE-S3).",
      "B": "Use an S3 Bucket Key for server-side encryption with AWS KMS keys (SSE-KMS) on the new objects.",
      "C": "Use client-side encryption with AWS KMS customer managed keys.",
      "D": "Use server-side encryption with customer-provided keys (SSE-C) stored in AWS KMS."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司运营一个环境，数据存储在 Amazon S3 存储桶中。这些对象在一天中经常被访问。该公司对存储在 S3 存储桶中的数据有严格的数据加密要求。该公司目前使用 AWS Key Management Service (AWS KMS) 进行加密。该公司希望优化与加密 S3 对象相关的成本，而无需对 AWS KMS 进行额外的调用。哪个解决方案将满足这些要求？",
    "options_cn": {
      "A": "使用由 Amazon S3 托管的密钥（SSE-S3）进行服务器端加密。",
      "B": "在新对象上使用 S3 存储桶密钥进行服务器端加密，密钥由 AWS KMS 管理（SSE-KMS）。",
      "C": "使用由 AWS KMS 客户管理的密钥进行客户端加密。",
      "D": "使用客户提供的密钥（SSE-C）进行服务器端加密，这些密钥存储在 AWS KMS 中。"
    },
    "tags": [
      "S3",
      "KMS",
      "SSE-S3",
      "SSE-KMS",
      "SSE-C"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考查如何以经济高效的方式加密存储在 S3 存储桶中的数据，重点在于控制成本和减少与 KMS 的交互。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：使用 S3 存储桶密钥进行服务器端加密，密钥由 AWS KMS 管理(SSE-KMS)。 这种方法允许 S3 代表您管理数据密钥，并通过 KMS 进行保护，从而满足了数据加密要求，同时降低了成本，因为 S3 执行加密解密，减少了 KMS 的 API 调用。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项， SSE-S3 使用 S3 托管的密钥，无法满足需要 AWS KMS 管理的要求。C 选项，使用客户管理的密钥进行客户端加密，会增加运营复杂性。D 选项，使用 SSE-C 加密需要客户提供密钥，而这些密钥存储在 AWS KMS 中，会导致管理复杂性。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "KMS",
      "SSE-S3",
      "SSE-KMS",
      "SSE-C"
    ]
  },
  {
    "id": 1010,
    "topic": "1",
    "question_en": "A company runs multiple workloads on virtual machines (VMs) in an on-premises data center. The company is expanding rapidly. The on- premises data center is not able to scale fast enough to meet business needs. The company wants to migrate the workloads to AWS. The migration is time sensitive. The company wants to use a lift-and-shift strategy for non-critical workloads. Which combination of steps will meet these requirements? (Choose three.)",
    "options_en": {
      "A": "Use the AWS Schema Conversion Tool (AWS SCT) to collect data about the VMs.",
      "B": "Use AWS Application Migration Service. Install the AWS Replication Agent on the VMs.",
      "C": "Complete the initial replication of the VMs. Launch test instances to perform acceptance tests on the VMs.",
      "D": "Stop all operations on the VMs. Launch a cutover instanc",
      "E": "E. Use AWS App2Container (A2C) to collect data about the VMs",
      "F": "Use AWS Database Migration Service (AWS DMS) to migrate the VMs."
    },
    "correct_answer": "BCD",
    "vote_percentage": "",
    "question_cn": "一家公司在其本地数据中心内的虚拟机 (VM) 上运行多个工作负载。该公司正在迅速扩张。本地数据中心无法快速扩展以满足业务需求。该公司希望将工作负载迁移到 AWS。迁移具有时间敏感性。该公司希望对非关键工作负载使用迁移策略。哪三个步骤的组合将满足这些要求？（选择三个。）",
    "options_cn": {
      "A": "使用 AWS 架构转换工具 (AWS SCT) 收集有关 VM 的数据。",
      "B": "使用 AWS Application Migration Service。在 VM 上安装 AWS 复制代理。",
      "C": "完成 VM 的初始复制。启动测试实例以对 VM 执行验收测试。",
      "D": "停止 VM 上的所有操作。启动一个切换实例。",
      "E": "使用 AWS App2Container (A2C) 收集有关 VM 的数据。",
      "F": "使用 AWS 数据库迁移服务 (AWS DMS) 迁移 VM。"
    },
    "tags": [
      "Application Migration Service",
      "VM",
      "AWS SCT",
      "App2Container",
      "AWS DMS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 BCD（社区 —），解析仅供参考。】\n\n本题考察如何使用适合不同迁移需求的迁移策略将本地虚拟机 (VM) 工作负载迁移到 AWS 云。由于迁移具有时间敏感性，因此需要选择快速有效的迁移方法。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 BCD。理由简述：使用 AWS Application Migration Service，它是一种基于代理的迁移服务，可以快速地将虚拟机迁移到 AWS。在 VM 上安装 AWS 复制代理，可以进行持续的数据复制。启动测试实例，进行验收测试，可以验证迁移的正确性。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，AWS 架构转换工具 (AWS SCT) 用于评估和转换数据库架构，不是直接的迁移工具。C 选项， AWS DMS 用于数据库迁移，不是通用的 VM 迁移工具。D 选项，停止 VM 上的所有操作，会导致停机时间。E 选项，AWS App2Container (A2C) 将应用程序容器化，不适用于迁移 VM。F 选项，AWS DMS 用于数据库迁移。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "VM",
      "AWS SCT",
      "AWS DMS",
      "AWS Application Migration Service",
      "AWS App2Container"
    ]
  },
  {
    "id": 1011,
    "topic": "1",
    "question_en": "A company hosts an application in a private subnet. The company has already integrated the application with Amazon Cognito. The company uses an Amazon Cognito user pool to authenticate users. The company needs to modify the application so the application can securely store user documents in an Amazon S3 bucket. Which combination of steps will securely integrate Amazon S3 with the application? (Choose two.)",
    "options_en": {
      "A": "Create an Amazon Cognito identity pool to generate secure Amazon S3 access tokens for users when they successfully log in.",
      "B": "Use the existing Amazon Cognito user pool to generate Amazon S3 access tokens for users when they successfully log in.",
      "C": "Create an Amazon S3 VPC endpoint in the same VPC where the company hosts the application.",
      "D": "Create a NAT gateway in the VPC where the company hosts the application. Assign a policy to the S3 bucket to deny any request that is not initiated from Amazon Cognito",
      "E": "Attach a policy to the S3 bucket that allows access only from the users' IP addresses."
    },
    "correct_answer": "AC",
    "vote_percentage": "",
    "question_cn": "一家公司在私有子网中托管一个应用程序。该公司已经将该应用程序与 Amazon Cognito 集成。该公司使用 Amazon Cognito 用户池来验证用户身份。该公司需要修改该应用程序，以便该应用程序可以安全地将用户文档存储在 Amazon S3 存储桶中。哪些步骤组合将安全地将 Amazon S3 与应用程序集成？（选择两个。）",
    "options_cn": {
      "A": "创建一个 Amazon Cognito 身份池，以便在用户成功登录时为用户生成安全的 Amazon S3 访问令牌。",
      "B": "使用现有的 Amazon Cognito 用户池，在用户成功登录时为用户生成 Amazon S3 访问令牌。",
      "C": "在托管该应用程序的同一 VPC 中创建 Amazon S3 VPC endpoint。",
      "D": "在托管该应用程序的 VPC 中创建 NAT Gateway。为 S3 存储桶分配一个策略，以拒绝任何未从 Amazon Cognito 启动的请求。",
      "E": "为 S3 存储桶附加一个策略，该策略仅允许来自用户 IP 地址的访问。"
    },
    "tags": [
      "Cognito",
      "S3",
      "VPC endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 AC（社区 —），解析仅供参考。】\n\n本题考察如何在集成 Amazon Cognito 的应用程序中，安全地将用户文档存储在 Amazon S3 存储桶中，需要综合考虑身份验证、授权和网络安全。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 AC。理由简述：使用现有的 Amazon Cognito 用户池，在用户成功登录时为用户生成 Amazon S3 访问令牌。 使用 Cognito 生成访问令牌， 避免了硬编码密钥。创建 Amazon Cognito 身份池，以便在用户成功登录时为用户生成安全的 Amazon S3 访问令牌。 使用临时安全凭证，实现最小权限原则。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项， 为 S3 存储桶分配策略，拒绝任何未从 Amazon Cognito 启动的请求，无法满足业务需求。C 选项， 在托管应用程序的同一 VPC 中创建 Amazon S3 VPC endpoint， 无法授权用户访问 S3。D 选项， 在托管该应用程序的 VPC 中创建 NAT Gateway， 无法授权用户访问 S3。 E 选项，为 S3 存储桶附加一个策略，该策略仅允许来自用户 IP 地址的访问， 无法满足业务需求。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Cognito",
      "S3",
      "NAT Gateway",
      "VPC endpoint"
    ]
  },
  {
    "id": 1012,
    "topic": "1",
    "question_en": "A company has a three-tier web application that processes orders from customers. The web tier consists of Amazon EC2 instances behind an Application Load Balancer. The processing tier consists of EC2 instances. The company decoupled the web tier and processing tier by using Amazon Simple Queue Service (Amazon SQS). The storage layer uses Amazon DynamoDB. At peak times, some users report order processing delays and halls. The company has noticed that during these delays, the EC2 instances are running at 100% CPU usage, and the SQS queue fills up. The peak times are variable and unpredictable. The company needs to improve the performance of the application. Which solution will meet these requirements?",
    "options_en": {
      "A": "Use scheduled scaling for Amazon EC2 Auto Scaling to scale out the processing tier instances for the duration of peak usage times. Use the CPU Utilization metric to determine when to scale.",
      "B": "Use Amazon ElastiCache for Redis in front of the DynamoDB backend tier. Use target utilization as a metric to determine when to scale.",
      "C": "Add an Amazon CloudFront distribution to cache the responses for the web tier. Use HTTP latency as a metric to determine when to scale.",
      "D": "Use an Amazon EC2 Auto Scaling target tracking policy to scale out the processing tier instances. Use the ApproximateNumberOfMessages attribute to determine when to scale."
    },
    "correct_answer": "D",
    "vote_percentage": "",
    "question_cn": "一家公司有一个三层 Web 应用程序，用于处理客户订单。Web 层由 Application Load Balancer 后面的 Amazon EC2 实例组成。处理层由 EC2 实例组成。该公司使用 Amazon Simple Queue Service (Amazon SQS) 将 Web 层和处理层解耦。存储层使用 Amazon DynamoDB。在高峰时段，一些用户报告订单处理延迟和暂停。该公司注意到，在这些延迟期间，EC2 实例的 CPU 使用率达到 100%，并且 SQS 队列已满。高峰时段是可变的且不可预测的。该公司需要提高应用程序的性能。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "对 Amazon EC2 Auto Scaling 使用预定扩展，以在高峰使用时段内扩展处理层实例。使用 CPU 利用率指标来确定何时进行扩展。",
      "B": "在 DynamoDB 后端层之前使用 Amazon ElastiCache for Redis。使用目标利用率作为指标来确定何时进行扩展。",
      "C": "添加 Amazon CloudFront 分发以缓存 Web 层的响应。使用 HTTP 延迟作为指标来确定何时进行扩展。",
      "D": "使用 Amazon EC2 Auto Scaling 目标跟踪策略来扩展处理层实例。使用 ApproximateNumberOfMessages 属性来确定何时进行扩展。"
    },
    "tags": [
      "SQS",
      "EC2",
      "ALB",
      "Auto Scaling",
      "DynamoDB",
      "ElastiCache",
      "CloudFront"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 D（社区 —），解析仅供参考。】\n\n本题考察如何通过优化 Web 应用程序架构，提高性能并处理高峰时段的订单处理延迟。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 D。理由简述：使用 Amazon EC2 Auto Scaling 目标跟踪策略来扩展处理层实例， 并使用 ApproximateNumberOfMessages 属性来确定何时进行扩展，实现动态伸缩。 ApproximateNumberOfMessages 提供了队列中消息数量的近似值，可以有效地指示工作负载，从而触发 Auto Scaling 组扩展。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，对 Amazon EC2 Auto Scaling 使用预定扩展，无法应对不可预测的高峰时段。B 选项， 在 DynamoDB 后端层之前使用 Amazon ElastiCache for Redis， 不能解决 CPU 利用率高的问题。 C 选项，添加 CloudFront 分发，虽然可以缓存静态内容， 无法解决后台处理层的性能问题。HTTP 延迟无法准确反映 SQS 队列是否已满，从而无法触发 Auto Scaling。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "SQS",
      "EC2",
      "ALB",
      "Auto Scaling",
      "DynamoDB",
      "ElastiCache",
      "CloudFront"
    ]
  },
  {
    "id": 1013,
    "topic": "1",
    "question_en": "A company's production environment consists of Amazon EC2 On-Demand Instances that run constantly between Monday and Saturday. The instances must run for only 12 hours on Sunday and cannot tolerate interruptions. The company wants to cost-optimize the production environment. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Purchase Scheduled Reserved Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase Standard Reserved Instances for the EC2 instances that run constantly between Monday and Saturday.",
      "B": "Purchase Convertible Reserved Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase Standard Reserved Instances for the EC2 instances that run constantly between Monday and Saturday.",
      "C": "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase Standard Reserved Instances for the EC2 instances that run constantly between Monday and Saturday.",
      "D": "Use Spot Instances for the EC2 instances that run for only 12 hours on Sunday. Purchase Convertible Reserved Instances for the EC2 instances that run constantly between Monday and Saturday."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司的生产环境包括在周一到周六之间持续运行的 Amazon EC2 按需实例。这些实例必须仅在周日运行 12 小时，并且不能容忍中断。该公司希望对生产环境进行成本优化。哪种解决方案将最具成本效益地满足这些要求？",
    "options_cn": {
      "A": "为仅在周日运行 12 小时的 EC2 实例购买预留实例（计划）。为周一到周六之间持续运行的 EC2 实例购买预留实例（标准）。",
      "B": "为仅在周日运行 12 小时的 EC2 实例购买可转换预留实例。为周一到周六之间持续运行的 EC2 实例购买预留实例（标准）。",
      "C": "对仅在周日运行 12 小时的 EC2 实例使用 Spot 实例。为周一到周六之间持续运行的 EC2 实例购买预留实例（标准）。",
      "D": "对仅在周日运行 12 小时的 EC2 实例使用 Spot 实例。为周一到周六之间持续运行的 EC2 实例购买可转换预留实例。"
    },
    "tags": [
      "EC2",
      "Reserved Instances",
      "Spot Instances",
      "Scheduled Reserved Instances"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n本题考查如何使用 AWS 的预留实例 (Reserved Instances) 和 Spot 实例来实现成本优化，特别是针对仅在特定时间运行的 EC2 实例。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：为仅在周日运行 12 小时的 EC2 实例购买预留实例（计划）。 为周一到周六之间持续运行的 EC2 实例购买预留实例（标准）。 计划预留实例允许您在特定的时间和持续时间预留 EC2 实例容量。 标准预留实例提供了 EC2 实例的折扣，对于持续运行的实例，可以显著降低成本。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项， 可转换预留实例的折扣低于标准预留实例，周一到周六之间持续运行的 EC2 实例的成本较高。C 选项， 对于周日运行的实例，使用 Spot 实例可能会导致中断，无法保证实例可用性。D 选项， 周一到周六之间持续运行的 EC2 实例使用可转换预留实例，成本高于标准预留实例，无法实现成本优化。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EC2",
      "Reserved Instances",
      "Spot Instances",
      "Scheduled Reserved Instances"
    ]
  },
  {
    "id": 1014,
    "topic": "1",
    "question_en": "A digital image processing company wants to migrate its on-premises monolithic application to the AWS Cloud. The company processes thousands of images and generates large files as part of the processing workfiow. The company needs a solution to manage the growing number of image processing jobs. The solution must also reduce the manual tasks in the image processing workfiow. The company does not want to manage the underlying infrastructure of the solution. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Use Amazon Elastic Container Service (Amazon ECS) with Amazon EC2 Spot Instances to process the images. Configure Amazon Simple Queue Service (Amazon SQS) to orchestrate the workfiow. Store the processed files in Amazon Elastic File System (Amazon EFS).",
      "B": "Use AWS Batch jobs to process the images. Use AWS Step Functions to orchestrate the workfiow. Store the processed files in an Amazon S3 bucket.",
      "C": "Use AWS Lambda functions and Amazon EC2 Spot Instances to process the images. Store the processed files in Amazon FSx.",
      "D": "Deploy a group of Amazon EC2 instances to process the images. Use AWS Step Functions to orchestrate the workfiow. Store the processed files in an Amazon Elastic Block Store (Amazon EBS) volume."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家数字图像处理公司希望将其本地整体应用程序迁移到 AWS 云。该公司处理数千张图像，并在处理工作流程中生成大型文件。该公司需要一个解决方案来管理不断增长的图像处理作业数量。该解决方案还必须减少图像处理工作流程中的手动任务。该公司不想管理该解决方案的底层基础设施。哪个解决方案将以最低的运营开销满足这些要求？",
    "options_cn": {
      "A": "使用 Amazon Elastic Container Service (Amazon ECS) 和 Amazon EC2 Spot 实例来处理图像。配置 Amazon Simple Queue Service (Amazon SQS) 来编排工作流程。将处理后的文件存储在 Amazon Elastic File System (Amazon EFS) 中。",
      "B": "使用 AWS Batch 作业来处理图像。使用 AWS Step Functions 来编排工作流程。将处理后的文件存储在 Amazon S3 存储桶中。",
      "C": "使用 AWS Lambda 函数和 Amazon EC2 Spot 实例来处理图像。将处理后的文件存储在 Amazon FSx 中。",
      "D": "部署一组 Amazon EC2 实例来处理图像。使用 AWS Step Functions 来编排工作流程。将处理后的文件存储在 Amazon Elastic Block Store (Amazon EBS) 卷中。"
    },
    "tags": [
      "ECS",
      "Lambda",
      "Step Functions",
      "AWS Batch",
      "S3",
      "EFS",
      "EC2",
      "EBS"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n本题考察如何选择合适的 AWS 服务来构建图像处理工作流程，该工作流程需要减少手动任务并简化底层基础设施管理。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：使用 AWS Batch 作业来处理图像。 使用 AWS Step Functions 来编排工作流程。 将处理后的文件存储在 Amazon S3 存储桶中。 AWS Batch 允许运行批处理作业，并且可以自动处理计算资源的部署和管理。 AWS Step Functions 可以编排工作流程，并且高度可扩展。Amazon S3 提供了安全、持久的对象存储。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，使用 ECS 管理容器，运维成本高于 AWS Batch。 C 选项， 使用 Lambda 和 EC2 Spot 实例进行图像处理， Lambda 函数可能会受到运行时间限制，不适合处理长时间运行的任务。 D 选项， 使用 EC2 实例进行图像处理， 需要手动管理基础设施。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "ECS",
      "Lambda",
      "Step Functions",
      "AWS Batch",
      "S3",
      "EFS",
      "EC2",
      "EBS"
    ]
  },
  {
    "id": 1015,
    "topic": "1",
    "question_en": "A company's image-hosting website gives users around the world the ability to up load, view, and download images from their mobile devices. The company currently hosts the static website in an Amazon S3 bucket. Because of the website's growing popularity, the website's performance has decreased. Users have reported latency issues when they upload and download images. The company must improve the performance of the website. Which solution will meet these requirements with the LEAST implementation effort?",
    "options_en": {
      "A": "Configure an Amazon CloudFront distribution for the S3 bucket to improve the download performance. Enable S3 Transfer Acceleration to improve the upload performance.",
      "B": "Configure Amazon EC2 instances of the right sizes in multiple AWS Regions. Migrate the application to the EC2 instances. Use an Application Load Balancer to distribute the website trafic equally among the EC2 instances. Configure AWS Global Accelerator to address global demand with low latency.",
      "C": "Configure an Amazon CloudFront distribution that uses the S3 bucket as an origin to improve the download performance. Configure the application to use CloudFront to upload images to improve the upload performance. Create S3 buckets in multiple AWS Regions. Configure replication rules for the buckets to replicate users' data based on the users' location. Redirect downloads to the S3 bucket that is closest to each user's location.",
      "D": "Configure AWS Global Accelerator for the S3 bucket to improve network performance. Create an endpoint for the application to use Global Accelerator instead of the S3 bucket."
    },
    "correct_answer": "A",
    "vote_percentage": "",
    "question_cn": "一家公司的图片托管网站为世界各地的用户提供了从其移动设备上传、查看和下载图片的功能。该公司目前将静态网站托管在 Amazon S3 存储桶中。由于网站越来越受欢迎，网站的性能有所下降。用户报告在上传和下载图片时遇到延迟问题。该公司必须提高网站的性能。哪种解决方案将以最少的实施工作量满足这些要求？",
    "options_cn": {
      "A": "为 S3 存储桶配置 Amazon CloudFront 分发，以提高下载性能。打开 S3 Transfer Acceleration 以提高上传性能。",
      "B": "在多个 AWS 区域中配置适当大小的 Amazon EC2 实例。将应用程序迁移到 EC2 实例。使用 Application Load Balancer 在 EC2 实例之间平均分配网站流量。配置 AWS Global Accelerator 以低延迟处理全球需求。",
      "C": "配置使用 S3 存储桶作为源的 Amazon CloudFront 分发，以提高下载性能。配置应用程序以使用 CloudFront 上传图片以提高上传性能。在多个 AWS 区域中创建 S3 存储桶。配置存储桶的复制规则，以根据用户的位置复制用户的数据。将下载重定向到最靠近每个用户位置的 S3 存储桶。",
      "D": "为 S3 存储桶配置 AWS Global Accelerator 以提高网络性能。为应用程序创建一个端点以使用 Global Accelerator 而不是 S3 存储桶。"
    },
    "tags": [
      "S3",
      "CloudFront",
      "S3 Transfer Acceleration",
      "Global Accelerator",
      "EC2",
      "ALB"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 A（社区 —），解析仅供参考。】\n\n此题考查如何优化图片托管网站的性能，包括上传和下载速度，并以最少的实施工作量满足这些要求。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 A。理由简述：为 S3 存储桶配置 Amazon CloudFront 分发，以提高下载性能。 打开 S3 Transfer Acceleration 以提高上传性能。 CloudFront 提供了缓存功能，可以提高下载速度。 S3 Transfer Acceleration 通过利用 AWS 边缘站点来加速上传。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nB 选项， 应用程序迁移到 EC2 实例， 增加了运维成本和复杂性。C 选项， 配置多个 S3 存储桶和复制规则，增加了复杂性，且未解决上传问题。 D 选项，使用 Global Accelerator， 并未充分利用 CDN 的缓存功能， 且无法优化上传性能。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "CloudFront",
      "S3 Transfer Acceleration",
      "Global Accelerator",
      "EC2",
      "ALB"
    ]
  },
  {
    "id": 1016,
    "topic": "1",
    "question_en": "A company runs an application in a private subnet behind an Application Load Balancer (ALB) in a VPC. The VPC has a NAT gateway and an internet gateway. The application calls the Amazon S3 API to store objects. According to the company's security policy, trafic from the application must not travel across the internet. Which solution will meet these requirements MOST cost-effectively?",
    "options_en": {
      "A": "Configure an S3 interface endpoint. Create a security group that allows outbound trafic to Amazon S3.",
      "B": "Configure an S3 gateway endpoint. Update the VPC route table to use the endpoint.",
      "C": "Configure an S3 bucket policy to allow trafic from the Elastic IP address that is assigned to the NAT gateway.",
      "D": "Create a second NAT gateway in the same subnet where the legacy application is deployed. Update the VPC route table to use the second NAT gateway."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司在 VPC 的 Application Load Balancer (ALB) 后面的私有子网中运行应用程序。VPC 具有 NAT 网关和 Internet 网关。应用程序调用 Amazon S3 API 来存储对象。根据公司的安全策略，来自应用程序的流量不得通过 Internet 传输。哪个解决方案将以最具成本效益的方式满足这些要求？",
    "options_cn": {
      "A": "配置一个 S3 接口终端节点。创建一个安全组，允许向 Amazon S3 发出流量。",
      "B": "配置一个 S3 网关终端节点。更新 VPC 路由表以使用该终端节点。",
      "C": "配置 S3 存储桶策略以允许来自分配给 NAT 网关的弹性 IP 地址的流量。",
      "D": "在部署旧应用程序的同一子网中创建第二个 NAT 网关。更新 VPC 路由表以使用第二个 NAT 网关。"
    },
    "tags": [
      "S3",
      "ALB",
      "VPC",
      "NAT Gateway",
      "Interface Endpoint"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n本题考查如何在 VPC 中运行的应用程序安全地访问 Amazon S3，同时确保流量不通过 Internet 传输。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：配置一个 S3 网关终端节点。 更新 VPC 路由表以使用该终端节点。 S3 网关终端节点允许 VPC 内的实例通过 AWS 骨干网络访问 S3，而无需经过 Internet 或 NAT 网关。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，配置 S3 接口终端节点会产生额外的费用，且没有更具成本效益。 C 选项， S3 存储桶策略并不能确保应用程序流量不通过 Internet。D 选项，创建第二个 NAT 网关并没有解决流量通过 Internet 的问题。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "S3",
      "ALB",
      "VPC",
      "NAT Gateway",
      "Interface Endpoint",
      "Gateway Endpoint"
    ]
  },
  {
    "id": 1017,
    "topic": "1",
    "question_en": "A company has an application that runs on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2 instances. The application has a UI that uses Amazon DynamoDB and data services that use Amazon S3 as part of the application deployment. The company must ensure that the EKS Pods for the UI can access only Amazon DynamoDB and that the EKS Pods for the data services can access only Amazon S3. The company uses AWS Identity and Access Management (IAM). Which solution meals these requirements?",
    "options_en": {
      "A": "Create separate IAM policies for Amazon S3 and DynamoDB access with the required permissions. Attach both IAM policies to the EC2 instance profile. Use role-based access control (RBAC) to control access to Amazon S3 or DynamoDB for the respective EKS Pods.",
      "B": "Create separate IAM policies for Amazon S3 and DynamoDB access with the required permissions. Attach the Amazon S3 IAM policy directly to the EKS Pods for the data services and the DynamoDB policy to the EKS Pods for the UI.",
      "C": "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role. Attach the AmazonS3FullAccess policy to the data services account and the AmazonDynamoDBFullAccess policy to the UI service account.",
      "D": "Create separate Kubernetes service accounts for the UI and data services to assume an IAM role. Use IAM Role for Service Accounts (IRSA) to provide access to the EKS Pods for the UI to Amazon S3 and the EKS Pods for the data services to DynamoDB."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司有一个应用程序，该应用程序在 Amazon EC2 实例上的 Amazon Elastic Kubernetes Service (Amazon EKS) 集群上运行。该应用程序有一个使用 Amazon DynamoDB 的 UI 和使用 Amazon S3 的数据服务，这些都是应用程序部署的一部分。该公司必须确保 UI 的 EKS Pod 只能访问 Amazon DynamoDB，并且数据服务的 EKS Pod 只能访问 Amazon S3。该公司使用 AWS Identity and Access Management (IAM)。哪种解决方案满足这些要求？",
    "options_cn": {
      "A": "为 Amazon S3 和 DynamoDB 访问创建具有所需权限的单独 IAM 策略。将这两个 IAM 策略附加到 EC2 实例配置文件。使用基于角色的访问控制 (RBAC) 来控制相应 EKS Pod 对 Amazon S3 或 DynamoDB 的访问。",
      "B": "为 Amazon S3 和 DynamoDB 访问创建具有所需权限的单独 IAM 策略。将 Amazon S3 IAM 策略直接附加到数据服务的 EKS Pod，并将 DynamoDB 策略附加到 UI 的 EKS Pod。",
      "C": "为 UI 和数据服务创建单独的 Kubernetes 服务帐户以承担 IAM 角色。将 AmazonS3FullAccess 策略附加到数据服务帐户，并将 AmazonDynamoDBFullAccess 策略附加到 UI 服务帐户。",
      "D": "为 UI 和数据服务创建单独的 Kubernetes 服务帐户以承担 IAM 角色。使用 IAM Role for Service Accounts (IRSA) 为 UI 的 EKS Pod 提供对 Amazon S3 的访问权限，并为数据服务的 EKS Pod 提供对 DynamoDB 的访问权限。"
    },
    "tags": [
      "EKS",
      "IAM",
      "DynamoDB",
      "S3",
      "Kubernetes"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n本题考察如何在 EKS 集群中，为 UI 和数据服务的 Pod 提供对 DynamoDB 和 S3 的细粒度访问控制， 确保最小权限原则。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：为 UI 和数据服务创建单独的 Kubernetes 服务帐户以承担 IAM 角色。 将 IAM 角色附加到服务帐户， 可以实现最小权限原则， 从而限制 Pod 对特定 AWS 资源的访问权限。使用 IAM Role for Service Accounts (IRSA) 为 UI 的 EKS Pod 提供对 Amazon S3 的访问权限，并为数据服务的 EKS Pod 提供对 DynamoDB 的访问权限。 IRSA 允许将 IAM 角色与 Kubernetes 服务帐户关联，这允许 pod 承担 IAM 角色。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项， 将 IAM 策略附加到 EC2 实例配置文件。 然后使用 RBAC 控制 EKS Pod 的访问， 操作复杂， 且无法实现最小权限原则。B 选项， 将 IAM 策略直接附加到 Pod， 无法有效管理。D 选项， 创建 Kubernetes 服务帐户，附加 AmazonS3FullAccess 策略， 违反最小权限原则，造成安全风险。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "EKS",
      "IAM",
      "DynamoDB",
      "S3",
      "Kubernetes",
      "Pod",
      "IAM Role for Service Accounts",
      "RBAC",
      "Kubernetes 服务帐户"
    ]
  },
  {
    "id": 1018,
    "topic": "1",
    "question_en": "A company needs to give a globally distributed development team secure access to the company's AWS resources in a way that complies with security policies. The company currently uses an on-premises Active Directory for internal authentication. The company uses AWS Organizations to manage multiple AWS accounts that support multiple projects. The company needs a solution to integrate with the existing infrastructure to provide centralized identity management and access control. Which solution will meet these requirements with the LEAST operational overhead?",
    "options_en": {
      "A": "Set up AWS Directory Service to create an AWS managed Microsoft Active Directory on AWS. Establish a trust relationship with the on- premises Active Directory. Use IAM rotes that are assigned to Active Directory groups to access AWS resources within the company's AWS accounts.",
      "B": "Create an IAM user for each developer. Manually manage permissions for each IAM user based on each user's involvement with each project. Enforce multi-factor authentication (MFA) as an additional layer of security.",
      "C": "Use AD Connector in AWS Directory Service to connect to the on-premises Active Directory. Integrate AD Connector with AWS IAM Identity Center. Configure permissions sets to give each AD group access to specific AWS accounts and resources.",
      "D": "Use Amazon Cognito to deploy an identity federation solution. Integrate the identity federation solution with the on-premises Active Directory. Use Amazon Cognito to provide access tokens for developers to access AWS accounts and resources."
    },
    "correct_answer": "C",
    "vote_percentage": "",
    "question_cn": "一家公司需要为其全球分布的开发团队提供对公司 AWS 资源的访问权限，并确保其安全性符合安全策略。该公司目前使用本地 Active Directory 进行内部身份验证。该公司使用 AWS Organizations 管理支持多个项目的多个 AWS 账户。该公司需要一个解决方案来与现有基础设施集成，以提供集中式身份管理和访问控制。哪种解决方案以最少的运营开销满足这些要求？",
    "options_cn": {
      "A": "设置 AWS Directory Service 以在 AWS 上创建 AWS 托管的 Microsoft Active Directory。与本地 Active Directory 建立信任关系。使用分配给 Active Directory 组的 IAM 角色来访问公司 AWS 账户中的 AWS 资源。",
      "B": "为每个开发人员创建 IAM 用户。根据每个用户参与每个项目的情况手动管理每个 IAM 用户的权限。强制使用多因素身份验证 (MFA) 作为额外的安全层。",
      "C": "在 AWS Directory Service 中使用 AD Connector 连接到本地 Active Directory。将 AD Connector 与 AWS IAM Identity Center 集成。配置权限集以授予每个 AD 组对特定 AWS 账户和资源的访问权限。",
      "D": "使用 Amazon Cognito 部署身份联合解决方案。将身份联合解决方案与本地 Active Directory 集成。使用 Amazon Cognito 为开发人员提供访问令牌以访问 AWS 账户和资源。"
    },
    "tags": [
      "Active Directory",
      "Directory Service",
      "IAM Identity Center",
      "Organizations",
      "Cognito"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 C（社区 —），解析仅供参考。】\n\n本题考查如何实现集中式的身份管理和访问控制，以确保全球分布的开发团队可以安全地访问 AWS 资源，同时符合公司的安全策略。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 C。理由简述：在 AWS Directory Service 中使用 AD Connector 连接到本地 Active Directory。 将 AD Connector 与 AWS IAM Identity Center 集成。 配置权限集以授予每个 AD 组对特定 AWS 账户和资源的访问权限。 这种方法实现了与现有基础设施的集成，提供了集中式身份管理和访问控制，减少了运营开销。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\nA 选项，  使用 AWS 托管的 Microsoft Active Directory， 配置更为复杂。B 选项，为每个开发人员创建 IAM 用户，手动管理权限，扩展性差，运营成本高。D 选项，使用 Cognito 进行身份联合， 并不能与 AD Connector 集成，无法与本地 Active Directory 集成。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "Active Directory",
      "IAM Identity Center",
      "Cognito",
      "AD Connector",
      "IAM 角色",
      "Directory Service",
      "Organizations",
      "权限集"
    ]
  },
  {
    "id": 1019,
    "topic": "1",
    "question_en": "A company is developing an application in the AWS Cloud. The application's HTTP API contains critical information that is published in Amazon API Gateway. The critical information must be accessible from only a limited set of trusted IP addresses that belong to the company's internal network. Which solution will meet these requirements?",
    "options_en": {
      "A": "Set up an API Gateway private integration to restrict access to a predefined set of IP addresses.",
      "B": "Create a resource policy for the API that denies access to any IP address that is not specifically allowed.",
      "C": "Directly deploy the API in a private subnet. Create a network ACL. Set up rules to allow the trafic from specific IP addresses.",
      "D": "Modify the security group that is attached to API Gateway to allow inbound trafic from only the trusted IP addresses."
    },
    "correct_answer": "B",
    "vote_percentage": "",
    "question_cn": "一家公司正在 AWS 云中开发一个应用程序。该应用程序的 HTTP API 包含发布在 Amazon API Gateway 中的关键信息。关键信息只能从属于公司内部网络的有限的一组可信 IP 地址访问。哪种解决方案将满足这些要求？",
    "options_cn": {
      "A": "设置 API Gateway 私有集成以限制对预定义 IP 地址集的访问。",
      "B": "为 API 创建一个资源策略，该策略拒绝任何未明确允许的 IP 地址的访问。",
      "C": "直接在私有子网中部署 API。创建网络 ACL。设置规则以允许来自特定 IP 地址的流量。",
      "D": "修改附加到 API Gateway 的安全组，以仅允许来自可信 IP 地址的入站流量。"
    },
    "tags": [
      "APIGateway",
      "IAM",
      "NetworkACL",
      "SecurityGroup"
    ],
    "explanation": {
      "analysis": "【本题以社区投票为准，答案为 B（社区 —），解析仅供参考。】\n\n此题考察如何限制对 Amazon API Gateway 的访问。可以使用资源策略（Resource Policy）来控制对 API Gateway 资源的访问权限。通过在资源策略中明确拒绝或允许特定的 IP 地址，可以实现对访问来源的限制。\n\n本题社区投票率较低，存在分歧。以上解析仅供参考，请勿视为标准答案；建议结合 AWS 官方文档或上网搜索本题进行深入了解，避免被参考答案误导。",
      "why_correct": "当前采纳的参考答案为 B。理由简述：选项 B 正确，因为 API Gateway 的资源策略（Resource Policy）允许您控制对 API 的访问。通过在资源策略中指定允许的 IP 地址列表，可以限制对 API 的访问。 以上仅供参考；建议上网搜索本题或 AWS 文档进一步确认，避免被单一参考答案误导。",
      "why_wrong": "参考解析中对该题其它选项的常见说明如下（仅供参考，不代表标准答案）：\n\n选项 A 错误，因为 API Gateway 私有集成主要用于将 API 与 VPC 中的后端资源（如 EC2 实例、ALB 或 Lambda 函数）集成，而不是直接用于限制 IP 地址的访问。选项 C 错误，因为虽然可以在私有子网部署 API，但 API Gateway 本身不直接部署在 VPC 内。 网络 ACL（Network ACL）是用于控制子网级别的流量，而不是 API Gateway 的访问。选项 D 错误，因为 API Gateway 不支持直接附加 Security Group。Security Group 是用于保护 EC2 实例等资源的。\n\n建议上网搜索本题或相关知识点进一步确认，避免被参考答案误导。"
    },
    "related_terms": [
      "VPC",
      "EC2",
      "ALB",
      "Lambda",
      "APIGateway",
      "NetworkACL",
      "Security Group",
      "Resource Policy",
      "IP Address"
    ]
  }
]